[
  {
    "objectID": "paraview-menu.html",
    "href": "paraview-menu.html",
    "title": "Scientific visualization with ParaView",
    "section": "",
    "text": "Abstract: ParaView is an open source, multi-platform data analysis and visualization tool designed to run on a variety of hardware from an individual laptop to large supercomputers. With ParaView users can interactively visualize 2D and 3D datasets defined on structured, adaptive and unstructured meshes or particles, animate these datasets in time, and manipulate them with a variety of filters. ParaView supports both interactive (GUI) and scripted (including offscreen) visualization, and is an easy and fun tool to learn.\nThe first half of this workshop will cover some intermediate and advanced topics in 3D scientific visualization with ParaView including (1) scripting, (2) animation, and (3) remote and distributed visualization. In the second half participants will apply their newly gained knowledge of making animations on different datasets including one from a CFD simulation of a tidal turbine.\nInstructor: Alex Razoumov (SFU)"
  },
  {
    "objectID": "paraview-menu.html#workshop-materials",
    "href": "paraview-menu.html#workshop-materials",
    "title": "Scientific visualization with ParaView",
    "section": "Workshop materials",
    "text": "Workshop materials\nThe slides for Day 1 of this workshop (slides2.pdf) are included into the main ZIP file (~30 MB), along with sample datasets and various scripts."
  },
  {
    "objectID": "paraview-menu.html#prerequisites",
    "href": "paraview-menu.html#prerequisites",
    "title": "Scientific visualization with ParaView",
    "section": "Prerequisites",
    "text": "Prerequisites\nPlease install ParaView 6.0.x from https://www.paraview.org/download on your computer before the workshop.\nWe expect participants to be somewhat familiar with the basics of ParaView: loading local datasets, interacting with their visualizations via the GUI, applying filters, and constructing basic visualization pipelines. We ask participants with no prior experience in ParaView to watch the following videos before attending the workshop:\n\nRunning ParaView (11 min)\nFile formats and reading raw binary (7 min)\nVTK file formats: overview and legacy VTK (13 min)\nVTK file formats: XML VTK (4 min)\nScientific file formats (5 min)\nParaView filters (11 min)\nStore your visualization workflow with a state file (2 min)\nSide-by-side visualization (3 min)\nVisualizing vectors (6 min)\nCreating better streamlines (3 min)\nLine integral convolution (LIC) (3 min)\nReading CSV data (6 min)\nPutting your visualization online with ParaView Glance (5 min)\n\n\nQuick knowledge test – before studying the workshop materials\nStart ParaView on your computer, load the dataset data/disk_out_ref.ex2 and try to visualize temperature with a Clip and the velocity field with Stream Tracer With Custom Source and Glyph as shown in this image below:\n\n\n\n\nTest your ParaView skills"
  },
  {
    "objectID": "paraview-menu.html#remote-visualization",
    "href": "paraview-menu.html#remote-visualization",
    "title": "Scientific visualization with ParaView",
    "section": "Remote visualization",
    "text": "Remote visualization\nFor remote visualization, if you already have a CC account, you can use it today, and we have also prepared guest accounts on Cedar – please pick an account and add your name to the line in the Goodle Doc (shared in Zoom chat) so that no one else uses it. If you already have a CC account, please add your username to the same Google Doc so that we could add you to the reservation.\nWe have two reservations on Cedar until 2021-10-05T23:59:00 (Pacific):\n\n10-node CPU reservation --account=def-training-wa_cpu --reservation=paraview-wr_cpu\n2-node GPU reservation --account=def-training-wa_gpu --reservation=paraview-wr_gpu"
  },
  {
    "objectID": "paraview-menu.html#steps-for-partitioning-the-unstructured-dataset",
    "href": "paraview-menu.html#steps-for-partitioning-the-unstructured-dataset",
    "title": "Scientific visualization with ParaView",
    "section": "Steps for partitioning the unstructured dataset",
    "text": "Steps for partitioning the unstructured dataset\n\nLoad all m114f105_AL_2d_tsr_5_*.vtu files located in /project/6052247/fbaratchi/paraview_training/unstructured folder on Cedar\nApply Cell Data to Point Data.\nApply D3 filter.\nSave data as A_NAME.PVTU “decomposed”, write all timesteps as series, use fast compression.\n\nNOTE: Resulting partitioned data are located in /project/6052247/fbaratchi/paraview_training/partitioned on Cedar."
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "Intro to high-performance computing (HPC)",
    "section": "",
    "text": "This course is an introduction to High-Performance Computing on the Alliance clusters.\nAbstract: This course is an introduction to High-Performance Computing (HPC) on the Alliance clusters. We will start with the cluster hardware overview, then talk about some basic tools and the software environment on our clusters. Next we’ll give a quick tour of various parallel programming frameworks such as OpenMP, MPI, Python Dask, newer parallel languages such as Chapel and Julia, and we’ll try to compile some serial, shared-memory and distributed-memory codes using makefiles. We’ll then proceed to working with the Slurm scheduler, submitting and benchmarking our previously compiled codes. We will learn about batch and interactive cluster usage, best practices for submitting a large number of jobs, estimating your job’s resource requirements, and managing file permissions in shared cluster filesystems. There will be many demos and hands-on exercises on our training cluster.\nInstructor: Alex Razoumov (SFU)\nPrerequisites: Working knowledge of the Linux Bash shell. We will provide guest accounts to one of our Linux systems.\nSoftware: All attendees will need a remote secure shell (SSH) client installed on their computer in order to participate in the course exercises. On Mac and Linux computers SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there). Many versions of Windows also provide an OpenSSH client by default – try opening PowerShell and typing ssh to see if it is available. If not, then we recommend installing the free Home Edition of MobaXterm.\nMaterials: Please download a ZIP file with all slides (single PDF combining all chapters) and sample codes. A copy of this file is also available on the training cluster."
  },
  {
    "objectID": "hpc.html#videos-introduction",
    "href": "hpc.html#videos-introduction",
    "title": "Intro to high-performance computing (HPC)",
    "section": "Videos: introduction",
    "text": "Videos: introduction\nThese videos (recorded in 2020) cover the same materials we study in the course, but you can watch these at your own pace.\n\nIntroduction (3 min)\nCluster hardware overview (17 min)\nBasic tools on HPC clusters (18 min)\nFile transfer (10 min)\nProgramming languages and tools (16 min)\n\nUpdates:\n\nSince April 1st, 2022, your instructors in this course are based at Simon Fraser University.\nSome of the slides and links in the video have changed – please make sure to download the latest version of the slides (ZIP file).\nCompute Canada has been replaced by the Digital Research Alliance of Canada (the Alliance). All Compute Canada hardware and services are now provided to researchers by the Alliance and its regional partners. However, you will still see many references to Compute Canada in our documentation and support system.\nNew systems were added (e.g. Narval in Calcul Québec), and some older systems were replaced (Cedar → Fir, Béluga → Rorqual, Graham → Nibi, Niagara → Trillium)"
  },
  {
    "objectID": "hpc.html#videos-overview-of-parallel-programming-frameworks",
    "href": "hpc.html#videos-overview-of-parallel-programming-frameworks",
    "title": "Intro to high-performance computing (HPC)",
    "section": "Videos: overview of parallel programming frameworks",
    "text": "Videos: overview of parallel programming frameworks\nHere we give you a brief overview of various parallel programming tools. Our goal here is not to learn how to use these tools, but rather tell you at a high level what these tools do, so that you understand the difference between shared- and distributed-memory parallel programming models and know which tools you can use for each. Later, in the scheduler session, you will use this knowledge to submit parallel jobs to the queue.\nFeel free to skip some of these videos if you are not interested in parallel programming.\n\nOpenMP (3 min)\nMPI (message passing interface) (9 min)\nChapel parallel programming language (7 min)\nPython Dask (6 min)\nMake build automation tool (9 min)\nOther essential tools (5 min)\nPython and R on clusters (6 min)"
  },
  {
    "objectID": "hpc.html#videos-slurm-job-scheduler",
    "href": "hpc.html#videos-slurm-job-scheduler",
    "title": "Intro to high-performance computing (HPC)",
    "section": "Videos: Slurm job scheduler",
    "text": "Videos: Slurm job scheduler\n\nSlurm intro (8 min)\nJob billing with core equivalents (2 min)\nSubmitting serial jobs (12 min)\nSubmitting shared-memory jobs (9 min)\nSubmitting MPI jobs (8 min)\nSlurm jobs and memory (8 min)\nHybrid and GPU jobs (5 min)\nInteractive jobs (8 min)\nGetting information and other Slurm commands (6 min)\nBest computing / storage practices and summary (9 min)"
  },
  {
    "objectID": "bash1/bash-09-grep-find.html",
    "href": "bash1/bash-09-grep-find.html",
    "title": "Grep and find",
    "section": "",
    "text": "$ cd /path/to/data-shell/writing\n$ more haiku.txt\nFirst let’s search for text in files:\n$ grep not haiku.txt     # let's find all lines that contain the word 'not'\n$ grep day haiku.txt     # now search for word 'day'\n$ grep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\n$ grep -w today haiku.txt   # search for 'today'\n$ grep -w Today haiku.txt   # search for 'Today'\n$ grep -i -w today haiku.txt       # both upper and lower case 'today'\n$ grep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\n$ grep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\n$ man grep\nMore than two arguments to grep:\n$ grep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\n$ grep pattern *.txt   # the last argument will expand to the list of *.txt files\n\n\n\n\n\n\nCautionQuestion Dissecting a haiku\n\n\n\n\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Finding things with `grep` and `find`"
    ]
  },
  {
    "objectID": "bash1/bash-09-grep-find.html#searching-inside-files-with-grep",
    "href": "bash1/bash-09-grep-find.html#searching-inside-files-with-grep",
    "title": "Grep and find",
    "section": "",
    "text": "$ cd /path/to/data-shell/writing\n$ more haiku.txt\nFirst let’s search for text in files:\n$ grep not haiku.txt     # let's find all lines that contain the word 'not'\n$ grep day haiku.txt     # now search for word 'day'\n$ grep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\n$ grep -w today haiku.txt   # search for 'today'\n$ grep -w Today haiku.txt   # search for 'Today'\n$ grep -i -w today haiku.txt       # both upper and lower case 'today'\n$ grep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\n$ grep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\n$ man grep\nMore than two arguments to grep:\n$ grep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\n$ grep pattern *.txt   # the last argument will expand to the list of *.txt files\n\n\n\n\n\n\nCautionQuestion Dissecting a haiku\n\n\n\n\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Finding things with `grep` and `find`"
    ]
  },
  {
    "objectID": "bash1/bash-09-grep-find.html#finding-files-with-find",
    "href": "bash1/bash-09-grep-find.html#finding-files-with-find",
    "title": "Grep and find",
    "section": "Finding files with find",
    "text": "Finding files with find\nNow on to finding files:\ncd /path/to/data-shell/writing\n$ find . -type d     # search for directories inside current directory\n$ find . -type f     # search for files inside current directory\n$ find . -maxdepth 1 -type f     # depth 1 is the current directory\n$ find . -mindepth 2 -type f     # current directory and one level down\n$ find . -name haiku.txt      # finds specific file\n$ ls data       # shows one.txt two.txt\n$ find . -name *.txt      # still finds one file -- why? answer: expands *.txt to haiku.txt\n$ find . -name '*.txt'    # finds all three files -- good!\nLet’s wrap the last command into $() (called command substitution), as if it was a variable:\n$ echo $(find . -name '*.txt')   # will print ./data/one.txt ./data/two.txt ./haiku.txt\n$ ls -l $(find . -name '*.txt')   # will expand to ls -l ./data/one.txt ./data/two.txt ./haiku.txt\n$ wc -l $(find . -name '*.txt')   # will expand to wc -l ./data/one.txt ./data/two.txt ./haiku.txt\n$ grep elegant $(find . -name '*.txt')   # will look for 'elegant' inside all *.txt files\n\n\n\n\n\n\nCautionQuestion Somewhat tricky problem\n\n\n\n\n\nThe -v flag to grep inverts pattern matching, so that only lines that do not match the pattern are printed. Given that, which of the following commands will find all files in /data whose names end in ose.dat (e.g., sucrose.dat or maltose.dat), but whose names do not contain the word temp?\n1. find /data -name '*.dat' | grep ose | grep -v temp\n2. find /data -name ose.dat | grep -v temp\n3. grep -v temp $(find /data -name '*ose.dat')\n4. None of the above\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Finding things with `grep` and `find`"
    ]
  },
  {
    "objectID": "bash1/bash-09-grep-find.html#combining-find-and-grep",
    "href": "bash1/bash-09-grep-find.html#combining-find-and-grep",
    "title": "Grep and find",
    "section": "Combining find and grep",
    "text": "Combining find and grep\n\nLet’s say you want to run a command on each of the files in the output of find. You can always do something using command substitution like this:\n$ for f in $(find . -name \"*.txt\")\n&gt; do\n&gt;   command on $f\n&gt; done\nAlternatively, you can make it a one-liner:\nfind . -name \"*.txt\" -exec command {} \\;       # important to have spaces\nAnother – perhaps more elegant – one-line alternative is to use xargs. In its simplest usage, xargs command lets you construct a list of arguments:\n\nfind . -name \"*.txt\"                   # returns multiple lines\nfind . -name \"*.txt\" | xargs           # use those lines to construct a list\nfind . -name \"*.txt\" | xargs command   # pass this list as arguments to `command`\ncommand $(find . -name \"*.txt\")        # command substitution, achieving the same result (this is riskier!)\ncommand `(find . -name \"*.txt\")`       # alternative syntax for command substitution\nIn these examples, xargs achieves the same result as command substitution, but it is safer in terms of memory usage and the length of lists you can pass.\nWhere would you use this? Well, consider grep command that takes a search stream (and not a list of files) as its standard input:\ncat filename | grep pattern\nTo pass a list of files to grep, you can use xargs that takes that list from its standard input and converts it into a list of arguments that is then passed to grep:\nfind . -name \"*.txt\" | xargs grep pattern   # search for `pattern` inside all those files (`grep` does not take a list of files as standard input)\n\n\n\n\n\n\nCautionQuestion Recursive search\n\n\n\n\n\nWrite a one-line command that will search for a string in all files in the current directory and all its subdirectories, and will hide errors (e.g. due to permissions).\n\n\n\n\n\n\n\n\n\nCautionQuestion Command substitution\n\n\n\n\n\nPlay with command substitution using both $(...) and `...` syntax.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Finding things with `grep` and `find`"
    ]
  },
  {
    "objectID": "bash1/bash-07-loops.html",
    "href": "bash1/bash-07-loops.html",
    "title": "Loops",
    "section": "",
    "text": "Bash loops\n$ cd /path/to/data-shell/creatures\n$ ls   # shows basilisk.dat unicorn.dat -- let's pretend there are several hundred files here\nLet’s say we want to rename:\n\nbasilisk.dat  →  original-basilisk.dat\nunicorn.dat  →  original-unicorn.dat\n\nWe could try\n$ mv *.dat original-*.dat   # getting an error\nRemember if more than two arguments to mv, the last argument is the destination directory, but there is no directory matching original-*.dat, so we are getting an error. The proper solution is to use loops.\n$ for filename in basilisk.dat unicorn.dat     # filename is the loop variable here\n&gt; do\n&gt;   ls -l $filename                 # get the value of the variable by placing $ in front of it\n&gt; done\n\nLet’s simplify the previous loop:\n$ for f in *.dat\n&gt; do\n&gt;   ls -l $f\n&gt; done\nLet’s include two commands per each loop iteration:\n$ for f in *.dat\n&gt; do\n&gt;   echo $f\n&gt;   head -3 $f\n&gt; done\nNow to renaming basilisk.dat  ⮕  original-basilisk.dat, unicorn.dat  ⮕  original-unicorn.dat:\n$ for f in *.dat\n&gt; do\n&gt; cp $f original-$f\n&gt; done\nThe general syntax is\n$ for &lt;variable&gt; in &lt;collection&gt;\n&gt; do\n&gt;   commands with $variable\n&gt; done\nwhere a collection could be a explicit list of items, a list produced by a wildmask, or a collection of numbers/letters.\n$ echo {1..10}    # this is called brace expansion\n$ echo {1,2,5}    # very useful for loops or for including into large paths with multiple items, e.g.\n$ cd /path/to/data-shell/creatures\n$ ls -l ../molecules/{ethane,methane,pentane}.pdb\n$ echo {a..z}    # can also use letters\n$ echo {a..z}{1..10}   # this will produce 260 items\n$ echo {a..z}{a..z}    # this will produce 676 items\n$ seq 1 2 10      # step=2, so can use: for i in $(seq 1 2 10)\n$ for ((i=1; i&lt;=5; i++)) do echo $i; done   # can use C-style loops\n\n\n\n\n\n\nCautionQuestion 7.1\n\n\n\n\n\nIn a directory the command ls returns:\nfructose.dat  glucose.dat  sucrose.dat  maltose.txt\nWhat would be the output of the following loop?\nfor datafile in *.dat\ndo\n  cat $datafile &gt;&gt; sugar.dat\ndone\n\nAll of the text from fructose.dat, glucose.dat and sucrose.dat would be concatenated and saved to a file called sugar.dat\nThe text from sucrose.dat will be saved to a file called sugar.dat\nAll of the text from fructose.dat, glucose.dat, sucrose.dat, and maltose.txt would be concatenated and saved to a file called sugar.dat\nAll of the text from fructose.dat, glucose.dat and sucrose.dat will be printed to the screen and saved into a file called sugar.dat\n\n\n\n\n\n\n\n\n\n\nCautionTopic diff\n\n\n\n\n\nUsing diff to compare files and directories.\n\n\n\n\n\n\n\n\n\nCautionQuestion Nested braces\n\n\n\n\n\nDiscuss brace expansion. Try nested braces. Paste an example that works. What will this command do:\ntouch 2022-May-{0{1..9},{10..30}}.md\n\n\n\n\n\n\n\n\n\nCautionQuestion 7.4\n\n\n\n\n\nWrite a loop that concatenates all .pdb files in data-shell/molecules subdirectory into one file called allmolecules.txt, prepending each fragment with the name of the corresponding .pdb file, and separating different files with an empty line. Run the loop, make sure it works, bring it up with the  ↑  key and paste into the chat.\n\n\n\n\n\n\n\n\n\nCautionQuestion Infinite loop\n\n\n\n\n\nUse Ctrl-C to kill an infinite (or very long) loop or an unfinished command.\nwhile true\ndo\n    echo \"Press [ctrl+c] to stop\"\n    sleep 1\ndone\n\n\n\n\n\n\n\n\n\nCautionQuestion Looping through a collection\n\n\n\n\n\nWhat will the loop for i in hello 1 2 * bye; do echo $i; done print? Try answering without running the loop.\n\n\n\n\n\n\n\n\n\nCautionQuestion Writing to chapters\n\n\n\n\n\nCreate a loop that writes into 10 files chapter01.md, chapter02.md, …, chapter10.md. Each file should contain chapter-specific lines, e.g. chapter05.md will contain exactly these lines:\n## Chapter 05\nThis is the beginning of Chapter 05.\nContent will go here.\nThis is the end of Chapter 05.\n\n\n\n\n\n\n\n\n\nCautionQuestion Renaming with wildmask\n\n\n\n\n\nWhy mv *.txt *.bak does not work? Write a loop to rename all .txt files to .bak files. There are several solutions for changing a file extension inside a loop you know by now.\n\n\n\n\n\n\n\n\n\nCautionQuestion Spaces to underscores\n\n\n\n\n\nUsing knowledge from the previous question, write a loop to replace spaces to underscores in all file names in the current directory.\ntouch hello \"first phrase\" \"second phrase\" \"good morning, everyone\"\nls -l\nls *\\ *\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Loops"
    ]
  },
  {
    "objectID": "bash1/bash-06-wildcards-redirection-pipes.html",
    "href": "bash1/bash-06-wildcards-redirection-pipes.html",
    "title": "Tapping the power of Unix",
    "section": "",
    "text": "Covered topics: working with multiple files using wildmasks, standard output redirection to a file, constructing complex commands with Unix pipes.\n\nopen http://bit.ly/bashfile in your browser, it’ll download the file bfiles.zip\nunpack bfiles.zip to your home directory; you should see ~/data-shell\n\n$ cd &lt;parentDirectoryOf`data-shell`&gt;\n$ ls data-shell\n$ cd data-shell/molecules\n$ ls\n$ ls p*   # this is a Unix wildcard; bash will expand it to 'ls pentane.pdb propane.pdb'\n$ ls *.pdb      # another wildcard, will expand to \"ls cubane.pdb ethane.pdb ...'\n$ wc -l *.pdb   # list number of lines in each file\n$ wc -l *.pdb &gt; lengths.txt   # redirect the output of the last command into a file\n$ more lengths.txt\n$ sort -n lengths.txt   # sort numerically, i.e. 2 will go before 10, and 6 before 22\n$ sort -n lengths.txt &gt; sorted.txt\n$ head -1 sorted.txt    # show the length of the shortest (number of lines) file\n$ wc -l *.pdb | sort -n | head -1   # three commands can be shortened to one - this is called Unix pipe\nStandard input of a process. Standard output of a process. Pipes connect the two.\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.1\n\n\n\n\n\nRunning ls -F in ~/Desktop/Shell/Users/nelle/sugars results in:\nanalyzed/  glucose.dat  mannose.dat  sucrose.dat  fructose.dat  maltose.dat  raw/\nWhat code would you use to move all the .dat files into the analyzed subdirectory?\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.2\n\n\n\n\n\nIn a directory we want to find the 3 files that have the least number of lines. Which command would work for this?\n1. wc -l * &gt; sort -n &gt; head -3\n2. wc -l * | sort -n | head 1-3\n3. wc -l * | head -3 | sort -n\n4. wc -l * | sort -n | head -3\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.3\n\n\n\n\n\nUsing Unix pipes, write a one-line command to show the name of the longest .pdb file (by the number of lines). Paste your answer into the chat.\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.4\n\n\n\n\n\nCombine ls and head and/or tail into a one-line command to show three largest files (by the number of bytes) in a given directory. Paste your answer into the chat.\n\n\n\n\n\n\n\n\n\nCautionQuestion Echo with wildcards\n\n\n\n\n\nWhat will the command echo directoryName/* do? Try answering without running it. How is this output different from ls directoryName and ls directoryName/*?\n\n\n\n\n\n\n\n\n\nCautionQuestion redirection\n\n\n\n\n\nRedirection: 1&gt;, 2&gt;, &&gt;, /dev/null\n\n\n\n\n\n\n\n\n\nCautionQuestion Command separators\n\n\n\n\n\n; vs. && separators, e.g. mkdirr tmp; cd tmp\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Wildcards, redirection, pipes"
    ]
  },
  {
    "objectID": "bash1/bash-06-wildcards-redirection-pipes.html#wildcards-redirection-to-files-and-pipes",
    "href": "bash1/bash-06-wildcards-redirection-pipes.html#wildcards-redirection-to-files-and-pipes",
    "title": "Tapping the power of Unix",
    "section": "",
    "text": "Covered topics: working with multiple files using wildmasks, standard output redirection to a file, constructing complex commands with Unix pipes.\n\nopen http://bit.ly/bashfile in your browser, it’ll download the file bfiles.zip\nunpack bfiles.zip to your home directory; you should see ~/data-shell\n\n$ cd &lt;parentDirectoryOf`data-shell`&gt;\n$ ls data-shell\n$ cd data-shell/molecules\n$ ls\n$ ls p*   # this is a Unix wildcard; bash will expand it to 'ls pentane.pdb propane.pdb'\n$ ls *.pdb      # another wildcard, will expand to \"ls cubane.pdb ethane.pdb ...'\n$ wc -l *.pdb   # list number of lines in each file\n$ wc -l *.pdb &gt; lengths.txt   # redirect the output of the last command into a file\n$ more lengths.txt\n$ sort -n lengths.txt   # sort numerically, i.e. 2 will go before 10, and 6 before 22\n$ sort -n lengths.txt &gt; sorted.txt\n$ head -1 sorted.txt    # show the length of the shortest (number of lines) file\n$ wc -l *.pdb | sort -n | head -1   # three commands can be shortened to one - this is called Unix pipe\nStandard input of a process. Standard output of a process. Pipes connect the two.\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.1\n\n\n\n\n\nRunning ls -F in ~/Desktop/Shell/Users/nelle/sugars results in:\nanalyzed/  glucose.dat  mannose.dat  sucrose.dat  fructose.dat  maltose.dat  raw/\nWhat code would you use to move all the .dat files into the analyzed subdirectory?\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.2\n\n\n\n\n\nIn a directory we want to find the 3 files that have the least number of lines. Which command would work for this?\n1. wc -l * &gt; sort -n &gt; head -3\n2. wc -l * | sort -n | head 1-3\n3. wc -l * | head -3 | sort -n\n4. wc -l * | sort -n | head -3\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.3\n\n\n\n\n\nUsing Unix pipes, write a one-line command to show the name of the longest .pdb file (by the number of lines). Paste your answer into the chat.\n\n\n\n\n\n\n\n\n\nCautionQuestion 6.4\n\n\n\n\n\nCombine ls and head and/or tail into a one-line command to show three largest files (by the number of bytes) in a given directory. Paste your answer into the chat.\n\n\n\n\n\n\n\n\n\nCautionQuestion Echo with wildcards\n\n\n\n\n\nWhat will the command echo directoryName/* do? Try answering without running it. How is this output different from ls directoryName and ls directoryName/*?\n\n\n\n\n\n\n\n\n\nCautionQuestion redirection\n\n\n\n\n\nRedirection: 1&gt;, 2&gt;, &&gt;, /dev/null\n\n\n\n\n\n\n\n\n\nCautionQuestion Command separators\n\n\n\n\n\n; vs. && separators, e.g. mkdirr tmp; cd tmp\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Wildcards, redirection, pipes"
    ]
  },
  {
    "objectID": "bash1/bash-02-filesystem.html",
    "href": "bash1/bash-02-filesystem.html",
    "title": "Filesystem",
    "section": "",
    "text": "Covered topics: pwd, ls, absolute vs. relative paths, command flags, cd, path shortcuts.\n\npwd = Print Working Directory\nls = LiSt everything inside the current/given directory\ncd = Change Directory\n\nClick on a triangle to expand a question:\n\n\n\n\n\n\nCautionTopic 2.1\n\n\n\n\n\nRelative vs. absolute paths. Using ~ as part of a longer path.\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.2\n\n\n\n\n\nIn the figure above, if pwd displays /users/thing, what will ls ../backup display?\n1. ../backup: No such file or directory\n2. 2012-12-01 2013-01-08 2013-01-27\n3. 2012-12-01/ 2013-01-08/ 2013-01-27/\n4. original pnas_final pnas_sub\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.3\n\n\n\n\n\nGiven the same directory structure, if pwd displays /users/backup, and -r tells ls to display things in reverse order, what command will display:\npnas-sub/  pnas-final/  original/\n\nls pwd\nls -r -F\nls -r -F /users/backup\nEither #2 or #3 above, but not #1\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.4\n\n\n\n\n\nWhat does the command cd do if you do not pass it a directory name?\n1. It has no effect\n2. It changes the working directory to /\n3. It changes the working directory to the user’s home directory\n4. It produces an error message\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.5\n\n\n\n\n\nStarting from /Users/amanda/data/, which of the following commands could Amanda use to navigate to her home directory, which is /Users/amanda? Mark all correct answers.\n1. cd /\n2. cd /home/amanda\n3. cd ../..\n4. cd ~\n5. cd home\n6. cd ~/data/..\n7. cd\n8. cd ..\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Navigating the filesystem"
    ]
  },
  {
    "objectID": "bash1/bash-02-filesystem.html#navigating-directories",
    "href": "bash1/bash-02-filesystem.html#navigating-directories",
    "title": "Filesystem",
    "section": "",
    "text": "Covered topics: pwd, ls, absolute vs. relative paths, command flags, cd, path shortcuts.\n\npwd = Print Working Directory\nls = LiSt everything inside the current/given directory\ncd = Change Directory\n\nClick on a triangle to expand a question:\n\n\n\n\n\n\nCautionTopic 2.1\n\n\n\n\n\nRelative vs. absolute paths. Using ~ as part of a longer path.\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.2\n\n\n\n\n\nIn the figure above, if pwd displays /users/thing, what will ls ../backup display?\n1. ../backup: No such file or directory\n2. 2012-12-01 2013-01-08 2013-01-27\n3. 2012-12-01/ 2013-01-08/ 2013-01-27/\n4. original pnas_final pnas_sub\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.3\n\n\n\n\n\nGiven the same directory structure, if pwd displays /users/backup, and -r tells ls to display things in reverse order, what command will display:\npnas-sub/  pnas-final/  original/\n\nls pwd\nls -r -F\nls -r -F /users/backup\nEither #2 or #3 above, but not #1\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.4\n\n\n\n\n\nWhat does the command cd do if you do not pass it a directory name?\n1. It has no effect\n2. It changes the working directory to /\n3. It changes the working directory to the user’s home directory\n4. It produces an error message\n\n\n\n\n\n\n\n\n\nCautionQuestion 2.5\n\n\n\n\n\nStarting from /Users/amanda/data/, which of the following commands could Amanda use to navigate to her home directory, which is /Users/amanda? Mark all correct answers.\n1. cd /\n2. cd /home/amanda\n3. cd ../..\n4. cd ~\n5. cd home\n6. cd ~/data/..\n7. cd\n8. cd ..\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Navigating the filesystem"
    ]
  },
  {
    "objectID": "bash1/bash-02-filesystem.html#getting-help",
    "href": "bash1/bash-02-filesystem.html#getting-help",
    "title": "Filesystem",
    "section": "Getting help",
    "text": "Getting help\nCovered topics: man, navigating manual pages, --help flag.\n$ man ls\n$ ls --help\n\n\n\n\n\n\nCautionQuestion -h\n\n\n\n\n\nCheck the manual page for ls command: what does the -h (--human-readable) option do?\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Navigating the filesystem"
    ]
  },
  {
    "objectID": "bash1/bash-03-creating-moving-copying.html",
    "href": "bash1/bash-03-creating-moving-copying.html",
    "title": "Creating, moving, and copying",
    "section": "",
    "text": "Covered topics: creating directories with mkdir, using nano text editor, deleting with rm and rmdir.\n$ mkdir thesis\n$ ls -F\n$ ls -F thesis\n$ cd thesis\n$ nano draft.txt   # let's spend few minutes learning how to use nano; can also use other editors\n$ ls\n$ more draft.txt   # displays a file one page at a time\n$ cd ..\n$ rm thesis   # getting an error - why?\n$ rmdir thesis   # again getting an error - why?\n$ rm thesis/draft.txt\n$ rmdir thesis\nAlso could do ‘rm -r thesis’ in lieu of the last two commands.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Creating, moving and copying things, and aliases"
    ]
  },
  {
    "objectID": "bash1/bash-03-creating-moving-copying.html#creating-things",
    "href": "bash1/bash-03-creating-moving-copying.html#creating-things",
    "title": "Creating, moving, and copying",
    "section": "",
    "text": "Covered topics: creating directories with mkdir, using nano text editor, deleting with rm and rmdir.\n$ mkdir thesis\n$ ls -F\n$ ls -F thesis\n$ cd thesis\n$ nano draft.txt   # let's spend few minutes learning how to use nano; can also use other editors\n$ ls\n$ more draft.txt   # displays a file one page at a time\n$ cd ..\n$ rm thesis   # getting an error - why?\n$ rmdir thesis   # again getting an error - why?\n$ rm thesis/draft.txt\n$ rmdir thesis\nAlso could do ‘rm -r thesis’ in lieu of the last two commands.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Creating, moving and copying things, and aliases"
    ]
  },
  {
    "objectID": "bash1/bash-03-creating-moving-copying.html#moving-and-copying-things",
    "href": "bash1/bash-03-creating-moving-copying.html#moving-and-copying-things",
    "title": "Creating, moving, and copying",
    "section": "Moving and copying things",
    "text": "Moving and copying things\nCovered topics: mv and cp.\n$ mkdir thesis\n$ nano thesis/draft.txt\n$ ls thesis\n$ mv thesis/draft.txt thesis/quotes.txt\n$ ls thesis\n$ mv thesis/quotes.txt .   # . stands for current directory\n$ ls thesis\n$ ls\n$ ls quotes.txt\n$ cp quotes.txt thesis/quotations.txt\n$ ls quotes.txt thesis/quotations.txt\n$ rm quotes.txt\n$ ls quotes.txt thesis/quotations.txt\nMore than two arguments to mv and cp:\n$ touch  intro.txt  methods.txt  index.txt   # create three empty files\n$ ls\n$ mv  intro.txt  methods.txt  index.txt  thesis   # the last argument is the destination directory\n$ ls\n$ ls thesis\n\n\n\n\n\n\nCautionQuestion Misspelled file\n\n\n\n\n\nSuppose that you created a .txt file in your current directory to contain a list of the statistical tests you will need to do to analyze your data, and named it statstics.txt. After creating and saving this file, you realize you misspelled the filename! You want to correct the mistake, which of the following commands could you use to do so?\n1. cp statstics.txt statistics.txt\n2. mv statstics.txt statistics.txt\n3. mv statstics.txt .\n4. cp statstics.txt .\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Creating, moving and copying things, and aliases"
    ]
  },
  {
    "objectID": "bash1/bash-03-creating-moving-copying.html#aliases",
    "href": "bash1/bash-03-creating-moving-copying.html#aliases",
    "title": "Creating, moving, and copying",
    "section": "Aliases",
    "text": "Aliases\nAliases are one-line shortcuts/abbreviations to avoid typing a longer command, e.g.\n$ alias ls='ls -AFh'\n$ alias pwd='pwd -P'\n$ alias hi='history'\n$ alias top='top -o cpu -s 10 -stats \"pid,command,cpu,mem,threads,state,user\"' # MacOS only\n$ alias top='top -b -n 1 -o %CPU | head -n 20'                                 # Linux\n$ alias cedar='ssh -Y cedar.computecanada.ca'\n$ alias weather='curl wttr.in/vancouver'\n$ alias cal='cal -m'                  # starts on Monday\n$ alias cal='gcal --starting-day=1'   # if you use gcal instead; need gcal installed\nNow, instead of typing ssh -Y cedar.computecanada.ca, you can simply type cedar. To see all your defined aliases, type alias. To remove, e.g. the alias cedar, type unalias cedar.\nYou may want to put all your alias definitions into the file ~/.bashrc which is run every time you start a new local or remote shell.\n\n\n\n\n\n\nCautionQuestion Safer mv and cp\n\n\n\n\n\nWrite simple aliases for safer mv, cp so that these do not automatically overwrite the target. Hint: use their manual pages. Where would you store these aliases?\n\n\n\n\n\n\n\n\n\nCautionQuestion Safer rm\n\n\n\n\n\nWrite simple alias for safer rm.\n\n\n\n\n\n\n\n\n\nCautionQuestion 3.4\n\n\n\n\n\nWhat is the output of the last ls command in the sequence shown below?\n$ pwd\n/home/jamie/data\n$ ls\nproteins.dat\n$ mkdir recombine\n$ mv proteins.dat recombine\n$ cp recombine/proteins.dat ../proteins-saved.dat\n$ ls\n\nproteins-saved.dat recombine\nrecombine\nproteins.dat recombine\nproteins-saved.dat",
    "crumbs": [
      "PART 1",
      "Creating, moving and copying things, and aliases"
    ]
  },
  {
    "objectID": "bash1/bash-04-tar-gzip.html",
    "href": "bash1/bash-04-tar-gzip.html",
    "title": "Archiving and compressing",
    "section": "",
    "text": "In this section we will be working with a ZIP file that you can download and unpack with\n(alternative download link https://wgpages.netlify.app/files/bfiles.zip)\nUnlike an SSD or a hard drive on your laptop, the filesystem on HPC cluster was designed to store large files, ideally with parallel I/O. As a result, it handles any large number of small I/O requests (reads or writes) very poorly, sometimes bringing the I/O system to a halt. For this reason, we strongly recommend that users do not store many thousands of small files – instead you should pack them into a small number of large archives. This is where the archiving tool tar comes in handy.",
    "crumbs": [
      "PART 1",
      "Archives and compression"
    ]
  },
  {
    "objectID": "bash1/bash-04-tar-gzip.html#working-with-tar-and-gzipgunzip",
    "href": "bash1/bash-04-tar-gzip.html#working-with-tar-and-gzipgunzip",
    "title": "Archiving and compressing",
    "section": "Working with tar and gzip/gunzip",
    "text": "Working with tar and gzip/gunzip\nCovered topics: tar and g(un)zip.\nLet’s download some files in Windows’ ZIP format:\n$ wget http://bit.ly/bashfile -O bfiles.zip\n$ unzip bfiles.zip\n$ rm bfiles.zip\n$ ls\n$ ls data-shell\nZIP is a compression format from Windows, and it is not very popular in the Unix world. Let’s archive the directory data-shell using Unix’s native tar command:\n$ tar cvf bfiles.tar data-shell/\n$ gzip bfiles.tar\nYou can also create a gzipped TAR file in one step:\n$ rm bfiles.tar.gz\n$ tar cvfz bfiles.tar.gz data-shell/\nLet’s remove the directory and the original ZIP file (if still there), and extract directory from our new archive:\n$ /bin/rm -r data-shell/ bfiles.zip\n$ tar xvfz bfiles.tar.gz\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Archives and compression"
    ]
  },
  {
    "objectID": "bash1/bash-04-tar-gzip.html#managing-many-files-with-disk-archiver-dar",
    "href": "bash1/bash-04-tar-gzip.html#managing-many-files-with-disk-archiver-dar",
    "title": "Archiving and compressing",
    "section": "Managing many files with Disk ARchiver (DAR)",
    "text": "Managing many files with Disk ARchiver (DAR)\ntar is by far the most widely used archiving tool on UNIX-like systems. Since it was originally designed for sequential write/read on magnetic tapes, it does not index data for random access to its contents. A number of 3rd-party tools can add indexing to tar. However, there is a modern version of tar called DAR (stands for Disk ARchiver) that has some nice features:\n\neach DAR archive includes an index for fast file list/restore,\nDAR supports full / differential / incremental backup,\nDAR has build-in compression on a file-by-file basis to make it more resilient against data corruption and to avoid compressing already compressed files such as video,\nDAR supports strong encryption,\nDAR can detect corruption in both headers and saved data and recover with minimal data loss,\n\nand so on. Learning DAR is not part of this course. In the future, if you want to know more about working with DAR, please watch our DAR webinar (scroll down to see it), or check our DAR documentation page.",
    "crumbs": [
      "PART 1",
      "Archives and compression"
    ]
  },
  {
    "objectID": "chapel2/chapel-10-intro-parallel.html",
    "href": "chapel2/chapel-10-intro-parallel.html",
    "title": "Intro to parallel computing",
    "section": "",
    "text": "Chapel provides high-level abstractions for parallel programming no matter the grain size of your tasks, whether they run in a shared memory or a distributed memory environment, or whether they are executed “concurrently” (frequently switching between tasks) or truly in parallel. As a programmer you can focus on the algorithm: how to divide the problem into tasks that make sense in the context of the problem, and be sure that the high-level implementation will run on any hardware configuration. Then you could consider the specificities of the particular system you are going to use (whether is shared or distributed, the number of cores, etc.) and tune your code/algorithm to obtain a better performance.\nTo this effect, concurrency (the creation and execution of multiple tasks) and locality (on which set of resources these tasks are executed) are orthogonal (separate) concepts in Chapel. For example, we could have a set of several tasks that would be running as shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd again, Chapel could take care of all the stuff required to run our algorithm in most of the scenarios, but we can always add more specific detail to gain performance when targeting a particular scenario.\n\n\n\n\n\n\nNote\n\n\n\nTask parallelism is a style of parallel programming in which parallelism is driven by programmer-specified tasks. This is in contrast with data parallelism which is a style of parallel programming in which parallelism is driven by computations over collections of data elements or their indices.\n\n\nChapel provides functionality for both task- and data-parallel programming. Since task parallelism is lower level (you tell the computer how to divide your computation into tasks) than data parallelism, it is considerably more difficult. In this course we will focus mostly on data parallelism, but we will briefly cover task parallelism towards the end of the course.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Intro to parallel computing"
    ]
  },
  {
    "objectID": "chapel2/chapel-10-intro-parallel.html#parallel-programming-in-chapel",
    "href": "chapel2/chapel-10-intro-parallel.html#parallel-programming-in-chapel",
    "title": "Intro to parallel computing",
    "section": "",
    "text": "Chapel provides high-level abstractions for parallel programming no matter the grain size of your tasks, whether they run in a shared memory or a distributed memory environment, or whether they are executed “concurrently” (frequently switching between tasks) or truly in parallel. As a programmer you can focus on the algorithm: how to divide the problem into tasks that make sense in the context of the problem, and be sure that the high-level implementation will run on any hardware configuration. Then you could consider the specificities of the particular system you are going to use (whether is shared or distributed, the number of cores, etc.) and tune your code/algorithm to obtain a better performance.\nTo this effect, concurrency (the creation and execution of multiple tasks) and locality (on which set of resources these tasks are executed) are orthogonal (separate) concepts in Chapel. For example, we could have a set of several tasks that would be running as shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd again, Chapel could take care of all the stuff required to run our algorithm in most of the scenarios, but we can always add more specific detail to gain performance when targeting a particular scenario.\n\n\n\n\n\n\nNote\n\n\n\nTask parallelism is a style of parallel programming in which parallelism is driven by programmer-specified tasks. This is in contrast with data parallelism which is a style of parallel programming in which parallelism is driven by computations over collections of data elements or their indices.\n\n\nChapel provides functionality for both task- and data-parallel programming. Since task parallelism is lower level (you tell the computer how to divide your computation into tasks) than data parallelism, it is considerably more difficult. In this course we will focus mostly on data parallelism, but we will briefly cover task parallelism towards the end of the course.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Intro to parallel computing"
    ]
  },
  {
    "objectID": "chapel2/chapel-10-intro-parallel.html#running-single-local-parallel-chapel",
    "href": "chapel2/chapel-10-intro-parallel.html#running-single-local-parallel-chapel",
    "title": "Intro to parallel computing",
    "section": "Running single-local parallel Chapel",
    "text": "Running single-local parallel Chapel\nMake sure you have loaded the single-locale Chapel environment:\n$ module load chapel-multicore/2.4.0\n\n\n\nIn this lesson, we’ll be running on several cores on one node with a script shared.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./test",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Intro to parallel computing"
    ]
  },
  {
    "objectID": "chapel2/chapel-07-timing.html",
    "href": "chapel2/chapel-07-timing.html",
    "title": "Measuring code performance",
    "section": "",
    "text": "The code generated after Exercise “Basic.4” is the full implementation of our calculation. We will be using it as a benchmark, to see how much we can improve the performance with Chapel’s parallel programming features in the following lessons.\nBut first, we need a quantitative way to measure the performance of our code. Perhaps the easiest way to do this is to use the Unix command time:\n$ time ./juliaSetSerial --n=500\nreal ...\nuser ...\nsys  ...\nThe real time is what interest us. Our code is taking … seconds from the moment it is called at the command line until it returns. Sometimes, however, it could be useful to take the execution time of specific parts of the code. This can be achieved by modifying the code to output the information that we need. This process is called instrumentation of the code.\nAn easy way to instrument our code with Chapel is by using the module Time. Modules in Chapel are libraries of useful functions and methods that can be used in our code once the module is loaded. To load a module we use the keyword use followed by the name of the module. Once the Time module is loaded we can create a variable of the type stopwatch, and use the methods start, stopand elapsed to instrument our code.\nuse Time;\nvar watch: stopwatch;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n$ chpl --fast juliaSetSerial.chpl\n$ ./juliaSetSerial --n=500\n\n\n\n\n\n\nCautionQuestion Basic.5\n\n\n\n\n\nTry recompiling without --fast and see how it affects the execution time. If it becomes too slow, try reducing the problem size. What is the speedup factor with --fast?\n\n\n\nHere is our complete serial code juliaSetSerial.chpl:\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Measuring code performance"
    ]
  },
  {
    "objectID": "chapel2/chapel-07-timing.html#timing-the-execution-of-your-chapel-code",
    "href": "chapel2/chapel-07-timing.html#timing-the-execution-of-your-chapel-code",
    "title": "Measuring code performance",
    "section": "",
    "text": "The code generated after Exercise “Basic.4” is the full implementation of our calculation. We will be using it as a benchmark, to see how much we can improve the performance with Chapel’s parallel programming features in the following lessons.\nBut first, we need a quantitative way to measure the performance of our code. Perhaps the easiest way to do this is to use the Unix command time:\n$ time ./juliaSetSerial --n=500\nreal ...\nuser ...\nsys  ...\nThe real time is what interest us. Our code is taking … seconds from the moment it is called at the command line until it returns. Sometimes, however, it could be useful to take the execution time of specific parts of the code. This can be achieved by modifying the code to output the information that we need. This process is called instrumentation of the code.\nAn easy way to instrument our code with Chapel is by using the module Time. Modules in Chapel are libraries of useful functions and methods that can be used in our code once the module is loaded. To load a module we use the keyword use followed by the name of the module. Once the Time module is loaded we can create a variable of the type stopwatch, and use the methods start, stopand elapsed to instrument our code.\nuse Time;\nvar watch: stopwatch;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n$ chpl --fast juliaSetSerial.chpl\n$ ./juliaSetSerial --n=500\n\n\n\n\n\n\nCautionQuestion Basic.5\n\n\n\n\n\nTry recompiling without --fast and see how it affects the execution time. If it becomes too slow, try reducing the problem size. What is the speedup factor with --fast?\n\n\n\nHere is our complete serial code juliaSetSerial.chpl:\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Measuring code performance"
    ]
  },
  {
    "objectID": "chapel2/chapel-04-control-flow.html",
    "href": "chapel2/chapel-04-control-flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Chapel, as most high-level programming languages, has different staments to control the flow of the program or code. The conditional statements are the if statement and the while statement.\nThe general syntax of a while statement is one of the two:\nwhile condition do \n  instruction;\n  \nwhile condition {\n  instruction1;\n  ...\n  instructionN;\n}\nWith multiple instructions inside the curly brackets, all of them are executed one by one if the condition evaluates to True. This block will be repeated over and over again until the condition does not hold anymore.\nIn our Julia set code we don’t use the while construct, however, we need to check if our iteration goes beyond the \\(|z|=4\\) circle – this is the type of control that an if statement gives us. The general syntax is:\nif condition then\n  instruction1;\nelse\n  instruction2;\nand you can group multiple instructions into a block by using the curly brackets {}.\nLet’s package our calculation into a function: for each complex number we want to iterate until either we reach the maximum number of iterations, or we cross the \\(|z|=4\\) circle:\nproc pixel(z0) {\n  const c: complex = 0.355 + 0.355i;   // Julia set constant\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compile and run our code using the job script serial.sh:\n#!/bin/bash\n#SBATCH --time=00:05:00      # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\n./juliaSetSerial\n$ chpl juliaSetSerial.chpl\n$ sbatch serial.sh\n$ tail -f solution.out",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Control flow"
    ]
  },
  {
    "objectID": "chapel2/chapel-03-ranges-and-arrays.html",
    "href": "chapel2/chapel-03-ranges-and-arrays.html",
    "title": "Ranges, arrays, and loops",
    "section": "",
    "text": "A series of integers (1,2,3,4,5, for example), is called a range in Chapel. Ranges are generated with the .. operator, and are useful, among other things, to declare arrays of variables. For example, the following variable\nis a 2D array (matrix) with n rows and n columns of integer numbers, all initialized as 0. The two ranges 1..n not only define the size and shape of the array, they stand for the indices with which we could access particular elements of the array using the [X,X] notation. For example, stability[1,1] is the integer variable located at the first row and first column of the array stability, while stability[3,7] sits at the 3rd row and 7th column; stability[2,3..15] access columns 3 to 15 of the 2nd row, and stability[1..4,4] corresponds to the first 4 rows on the 4th column of stability.\nWe are now ready to start coding our computation … here is what we are going to do:",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Ranges and arrays"
    ]
  },
  {
    "objectID": "chapel2/chapel-03-ranges-and-arrays.html#structured-iterations-with-for-loops",
    "href": "chapel2/chapel-03-ranges-and-arrays.html#structured-iterations-with-for-loops",
    "title": "Ranges, arrays, and loops",
    "section": "Structured iterations with for-loops",
    "text": "Structured iterations with for-loops\nWe want to iterate over all points in our image. When it comes to iterating over a given number of elements, the for-loop is what we want to use. The for-loop has the following general syntax:\nfor index in iterand do\n  instruction;\n  \nfor index in iterand {\n  instruction1;\n  instruction2;\n  ...\n  }\nThe iterand is a statement that expresses an iteration, e.g. it could be a range 1..15. index is a variable that exists only in the context of the for-loop, and that will be taking the different values yielded by the iterand. The code flows as follows: index takes the first value yielded by the iterand, and keeps it until all the instructions inside the curly brackets are executed one by one; then, index takes the second value yielded by the iterand, and keeps it until all the instructions are executed again. This pattern is repeated until index takes all the different values exressed by the iterand.\nIn our case we iterate both over all rows and all columns in the image to compute every pixel. This can be done with nested for loops like this:\nfor i in 1..n { // process row i\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n { // process column j, row i\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nTo be able to compile the code, we also need a prototype pixel() function:\nproc pixel(z0) {\n  return z0; // to be replaced with an actual calculation\n}\nNow let’s compile and execute our code again:\n$ chpl juliaSetSerial.chpl\n$ sbatch serial.sh\n$ tail -f solution.out",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Ranges and arrays"
    ]
  },
  {
    "objectID": "chapel2/chapel-15-distributed-julia-set.html",
    "href": "chapel2/chapel-15-distributed-julia-set.html",
    "title": "Parallel Julia set",
    "section": "",
    "text": "Recall the serial code juliaSetSerial.chpl (without output):\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nNow let’s parallelize this code with forall in shared memory (single locale). Copy juliaSetSerial.chpl into juliaSetParallel.chpl and start modifying it:\n\nFor the outer loop, replace for with forall. This will produce an error about the scope of variables y and point:\n\nerror: cannot assign to const variable\nnote: The shadow variable 'y' is constant due to task intents in this loop\nerror: cannot assign to const variable\nnote: The shadow variable 'point' is constant due to task intents in this loop\n\n\nWhy do you think this message was produced? How do we solve this problem?\n\n\nWhat do we do next?\n\nCompile and run the code on several CPU cores on 1 node:\n#!/bin/bash\n# this is shared.sh\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./juliaSetParallel\nmodule load chapel-multicore/2.4.0\nchpl --fast juliaSetParallel.chpl\nsbatch shared.sh\nOnce you have the working shared-memory parallel code, study its performance.\nHere are my timings on the training cluster:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n1.181\n0.568\n0.307\n0.197\n\n\n\n\n\nWhy do you think the code’s speed does not scale linearly (~6X on 8 cores) with the number of cores?",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Parallel Julia set"
    ]
  },
  {
    "objectID": "chapel2/chapel-15-distributed-julia-set.html#shared-memory-julia-set",
    "href": "chapel2/chapel-15-distributed-julia-set.html#shared-memory-julia-set",
    "title": "Parallel Julia set",
    "section": "",
    "text": "Recall the serial code juliaSetSerial.chpl (without output):\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nNow let’s parallelize this code with forall in shared memory (single locale). Copy juliaSetSerial.chpl into juliaSetParallel.chpl and start modifying it:\n\nFor the outer loop, replace for with forall. This will produce an error about the scope of variables y and point:\n\nerror: cannot assign to const variable\nnote: The shadow variable 'y' is constant due to task intents in this loop\nerror: cannot assign to const variable\nnote: The shadow variable 'point' is constant due to task intents in this loop\n\n\nWhy do you think this message was produced? How do we solve this problem?\n\n\nWhat do we do next?\n\nCompile and run the code on several CPU cores on 1 node:\n#!/bin/bash\n# this is shared.sh\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./juliaSetParallel\nmodule load chapel-multicore/2.4.0\nchpl --fast juliaSetParallel.chpl\nsbatch shared.sh\nOnce you have the working shared-memory parallel code, study its performance.\nHere are my timings on the training cluster:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n1.181\n0.568\n0.307\n0.197\n\n\n\n\n\nWhy do you think the code’s speed does not scale linearly (~6X on 8 cores) with the number of cores?",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Parallel Julia set"
    ]
  },
  {
    "objectID": "chapel2/chapel-15-distributed-julia-set.html#julia-set-on-distributed-domains",
    "href": "chapel2/chapel-15-distributed-julia-set.html#julia-set-on-distributed-domains",
    "title": "Parallel Julia set",
    "section": "Julia set on distributed domains",
    "text": "Julia set on distributed domains\nCopy juliaSetParallel.chpl into juliaSetDistributed.chpl and start modifying it:\n\nLoad BlockDist\nReplace\n\nvar stability: [1..n,1..n] int;\nwith\nconst mesh: domain(2) = {1..n, 1..n};\nconst distributedMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = mesh;\nvar stability: [distributedMesh] int;\n\nLook into the loop variables: currently we have\n\nforall i in 1..n {\n  var y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    var point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\n– in the previous, shared-memory version of the code this fragment gave you a parallel loop running on multiple cores on the same node. If you run this loop now, it’ll run entirely on the first node!\nIn the distributed version of the code you want to loop in parallel over all elements of the distributed mesh distributedMesh (or, equivalently, over all elements of the distributed array stability) – this will send the computation to the locales holding these blocks:\nforall (i,j) in distributedMesh {\n  var y = 2*(i-0.5)/n - 1;\n  var point = 2*(j-0.5)/n - 1 + y*1i;\n  stability[i,j] = pixel(point);\n}\nor (equivalent):\nforall (i,j) in stability.domain {\n  var y = 2*(i-0.5)/n - 1;\n  var point = 2*(j-0.5)/n - 1 + y*1i;\n  stability[i,j] = pixel(point);\n}\nCompile and run a larger problem (\\(8000^2\\)) across several nodes:\n#!/bin/bash\n# this is distributed.sh\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --nodes=4\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\necho Running on $SLURM_NNODES nodes\n./juliaSetDistributed --n=8000 -nl $SLURM_NNODES\nsource /project/def-sponsor00/shared/syncHPC/startMultiLocale.sh\nchpl --fast juliaSetDistributed.chpl\nsbatch distributed.sh\nHere are my timings on the training cluster (even over a slow interconnect!):\n\n\n\n--nodes\n1\n2\n4\n4\n\n\n--cpus-per-task\n1\n1\n1\n8\n\n\nwallclock runtime (sec)\n36.56\n17.91\n9.51\n0.985\n\n\n\nThey don’t call it “embarrassing parallel” for nothing! There is some overhead at the start and at the end of computing each block, but this overhead is much smaller than the computing part itself, hence leading to almost perfect speedup.\n\n\n\n\n\n\nNote\n\n\n\nHere we have an example of a hybrid parallel code, utilizing multiples processes (one per locale) and multiple threads (on each locale) when available.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Parallel Julia set"
    ]
  },
  {
    "objectID": "chapel2/chapel-21-synchronising-tasks.html",
    "href": "chapel2/chapel-21-synchronising-tasks.html",
    "title": "Synchronization of tasks",
    "section": "",
    "text": "sync block\nThe keyword sync provides all sorts of mechanisms to synchronize tasks in Chapel. We can simply use sync to force the parent task to stop and wait until its spawned child-task ends. Consider this sync1.chpl:\nvar x = 0;\nwriteln('This is the main task starting a synchronous task');\nsync {\n  begin {\n    var count = 0;\n    while count &lt; 10 {\n      count += 1;\n      writeln('task 1: ', x + count);\n    }\n  }\n}\nwriteln('The first task is done ...');\nwriteln('This is the main task starting an asynchronous task');\nbegin {\n  var count = 0;\n  while count &lt; 10 {\n    count += 1;\n    writeln('task 2: ', x + count);\n  }\n}\nwriteln('This is the main task, I am done ...');\n$ chpl sync1.chpl -o sync1\n$ sed -i -e 's|gmax|sync1|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task starting a synchronous task\ntask 1: 1\ntask 1: 2\ntask 1: 3\ntask 1: 4\ntask 1: 5\ntask 1: 6\ntask 1: 7\ntask 1: 8\ntask 1: 9\ntask 1: 10\nThe first task is done ...\nThis is the main task starting an asynchronous task\nThis is the main task, I am done ...\ntask 2: 1\ntask 2: 2\ntask 2: 3\ntask 2: 4\ntask 2: 5\ntask 2: 6\ntask 2: 7\ntask 2: 8\ntask 2: 9\ntask 2: 10\n\n\n\n\n\n\nCautionDiscussion\n\n\n\n\n\nWhat would happen if we swap sync and begin in the first task:\nbegin {\n  sync {\n    var c = 0;\n    while c &lt; 10 {\n      c += 1;\n      writeln('task 1: ', x + c);\n    }\n  }\n}\nwriteln('The first task is done ...');\nDiscuss your observations.\n\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion Task.3\n\n\n\n\n\nUse begin and sync statements to reproduce the functionality of cobegin in cobegin.chpl, i.e., the main task should not continue until both tasks 1 and 2 are completed.\n\n\n\n\n\nsync variables\nA more elaborated and powerful use of sync is as a type qualifier for variables. When a variable is declared as sync, a state that can be full or empty is associated with it.\nTo assign a new value to a sync variable, its state must be empty (after the assignment operation is completed, the state will be set as full). On the contrary, to read a value from a sync variable, its state must be full (after the read operation is completed, the state will be set as empty again).\nvar x: sync int;\nwriteln('this is the main task launching a new task');\nbegin {\n  for i in 1..10 do\n    writeln('this is the new task working: ', i);\n  x.writeEF(2);   // write the value, state changes from Empty to Full\n  writeln('New task finished');\n}\nwriteln('this is the main task after launching new task ... I will wait until x is full');\nx.readFE();         // read the value, state changes from Full to Empty\nwriteln('and now it is done');\n$ chpl sync2.chpl -o sync2\n$ sed -i -e 's|sync1|sync2|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nthis is main task launching a new task\nthis is main task after launching new task ... I will wait until x is full\nthis is new task working: 1\nthis is new task working: 2\nthis is new task working: 3\nthis is new task working: 4\nthis is new task working: 5\nthis is new task working: 6\nthis is new task working: 7\nthis is new task working: 8\nthis is new task working: 9\nthis is new task working: 10\nNew task finished\nand now it is done\nHere the main task does not continue until the variable is full and can be read.\n\nLet’s add another line x.readFE(); – now it is stuck since we cannot read x while it’s empty!\nLet’s add x.writeEF(5); right before the last x.readFE(); – now we set is to full again (and assigned 5), and it can be read again.\n\nThere are a number of methods defined for sync variables. Suppose x is a sync variable of a given type:\n// general methods\nx.reset() - set the state as empty and the value as the default of x's type\nx.isfull() - return true is the state of x is full, false if it is empty\n\n// blocking read and write methods\nx.writeEF(value) - block until the state of x is empty, then assign the value and\n                   set the state to full\nx.writeFF(value) - block until the state of x is full, then assign the value and\n                   leave the state as full\nx.readFE() - block until the state of x is full, then return x's value and set\n             the state to empty\nx.readFF() - block until the state of x is full, then return x's value and\n             leave the state as full\n\n// non-blocking read and write methods\nx.writeXF(value) - assign the value no matter the state of x, then set the state as full\nx.readXX() - return the value of x regardless its state; the state will remain unchanged\n\n\nAtomic variables\nChapel also implements atomic operations with variables declared as atomic, and this provides another option to synchronize tasks. Atomic operations run completely independently of any other task or process. This means that when several tasks try to write an atomic variable, only one will succeed at a given moment, providing implicit synchronization between them. There is a number of methods defined for atomic variables, among them sub(), add(), write(), read(), and waitfor() are very useful to establish explicit synchronization between tasks, as shown in the next code atomic.chpl:\nvar lock: atomic int;\nconst numtasks = 5;\n\nlock.write(0);               // the main task set lock to zero\n\ncoforall id in 1..numtasks {\n  writeln('greetings form task ', id, '... I am waiting for all tasks to say hello');\n  lock.add(1);               // task id says hello and atomically adds 1 to lock\n  lock.waitFor(numtasks);  // then it waits for lock to be equal numtasks (which will happen when all tasks say hello)\n  writeln('task ', id, ' is done ...');\n}\n$ chpl atomic.chpl -o atomic\n$ sed -i -e 's|sync2|atomic|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\ngreetings form task 4... I am waiting for all tasks to say hello\ngreetings form task 5... I am waiting for all tasks to say hello\ngreetings form task 2... I am waiting for all tasks to say hello\ngreetings form task 3... I am waiting for all tasks to say hello\ngreetings form task 1... I am waiting for all tasks to say hello\ntask 1 is done...\ntask 5 is done...\ntask 2 is done...\ntask 3 is done...\ntask 4 is done...\n\n\n\n\n\n\nNote\n\n\n\nComment out the line lock.waitfor(numtasks) in the code above to clearly observe the effect of task synchronization.\n\n\n\n\n\n\n\n\nCautionQuestion Task.4\n\n\n\n\n\nSuppose we want to add another synchronization point right after the last writeln() command. What is wrong with adding the following at the end of the coforall loop?\n  lock.sub(1);      // task id says hello and atomically subtracts 1 from lock\n  lock.waitFor(0);  // then it waits for lock to be equal 0 (which will happen when all tasks say hello)\n  writeln('task ', id, ' is really done ...');\n\n\n\n\n\n\n\n\n\n\nCautionQuestion Task.5\n\n\n\n\n\nOk, then what is the solution if we want two synchronization points?\n\n\n\nFinally, with everything learned so far, we should be ready to parallelize our code for the simulation of the heat transfer equation.",
    "crumbs": [
      "PART 3: TASK PARALLELISM (BRIEFLY)",
      "Synchronization of tasks"
    ]
  },
  {
    "objectID": "chapel2/chapel-20-fire-and-forget-tasks.html",
    "href": "chapel2/chapel-20-fire-and-forget-tasks.html",
    "title": "Fire-and-forget tasks",
    "section": "",
    "text": "Let’s now talk about task parallelism. This is a lower-level style of programming, in which you explicitly tell Chapel how to subdivide your computation into tasks. As this is more human-labour intensive than data parallelism, here we will only outline the main concepts and pitfalls, without going into too much detail.\nA Chapel program always starts as a single main task (and here we use the term “task” loosely as it could be a thread). You can then start concurrent tasks with the begin statement. A task spawned by the begin statement will run in a different task while the main task continues its normal execution. Let’s start a new code begin.chpl with the following lines:\nvar x = 100;\nwriteln('This is the main task starting first task');\nbegin {\n  var count = 0;\n  while count &lt; 10 {\n    count += 1;\n    writeln('task 1: ', x + count);\n  }\n}\nwriteln('This is the main task starting second task');\nbegin {\n  var count = 0;\n  while count &lt; 10 {\n    count += 1;\n    writeln('task 2: ', x + count);\n  }\n}\nwriteln('This is the main task, I am done ...');\n$ chpl begin.chpl -o begin\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task starting first task\nThis is the main task starting second task\nThis is the main task, I am done ...\ntask 2: 101\ntask 1: 101\ntask 2: 102\ntask 1: 102\ntask 2: 103\ntask 1: 103\ntask 2: 104\n...\ntask 1: 109\ntask 2: 109\ntask 1: 110\ntask 2: 110\nAs you can see the order of the output is not what we would expected, and actually it is somewhat unpredictable. This is a well known effect of concurrent tasks accessing the same shared resource at the same time (in this case the screen); the system decides in which order the tasks could write to the screen.\n\n\n\n\n\n\nCautionDiscussion\n\n\n\n\n\n\nWhat would happen if in the last code we move the definition of count into the main task, but try to assign it from tasks 1 and 2?\n\nAnswer: we’ll get an error at compilation (“cannot assign to const variable”), since then count would belong to the main task (would be defined within the scope of the main task), and we could modify its value only in the main task.\n\nWhat would happen if we try to insert a second definition var x = 10; inside the first begin statement?\n\nAnswer: that will actually work, as we’ll simply create another, local instance of x with its own value.\n\n\n\n\n\n\n\n\n\nImportantKey idea\n\n\n\nAll variables have a scope in which they can be used. Variables declared inside a concurrent task are accessible only by that task. Variables declared in the main task can be read everywhere, but Chapel won’t allow other concurrent tasks to modify them.\n\n\n\n\n\n\n\n\nCautionDiscussion\n\n\n\n\n\nAre the concurrent tasks, spawned by the last code, running truly in parallel?\nAnswer: it depends on the number of cores available to your job. If you have a single core, they’ll run concurrently, with the CPU switching between the tasks. If you have two cores, task1 and task2 will likely run in parallel using the two cores.\n\n\n\n\n\n\n\n\n\nImportantKey idea\n\n\n\nTo maximize performance, start as many tasks (threads) as the number of available cores.\n\n\nA slightly more structured way to start concurrent tasks in Chapel is by using the cobegin statement. Here you can start a block of concurrent tasks, one for each statement inside the curly brackets. Another difference between the begin and cobegin statements is that with the cobegin, all the spawned tasks are synchronized at the end of the statement, i.e. the main task won’t continue its execution until all tasks are done. Let’s start cobegin.chpl:\nvar x = 0;\nwriteln('This is the main task, my value of x is ', x);\ncobegin {\n  {\n    var x = 5;\n    writeln('This is task 1, my value of x is ', x);\n  }\n  writeln('This is task 2, my value of x is ', x);\n}\nwriteln('This message will not appear until all tasks are done ...');\n$ chpl cobegin.chpl -o cobegin\n$ sed -i -e 's|begin|cobegin|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task, my value of x is 0\nThis is task 2, my value of x is 0\nThis is task 1, my value of x is 5\nThis message will not appear until all tasks are done...\nAs you may have concluded from the Discussion exercise above, the variables declared inside a task are accessible only by the task, while those variables declared in the main task are accessible to all tasks.\nAnother, and one of the most useful ways to start concurrent/parallel tasks in Chapel, is the coforall loop. This is a combination of the for-loop and the cobeginstatements. The general syntax is:\ncoforall index in iterand\n{instructions}\nThis will start a new task (thread) for each iteration. Each task will then perform all the instructions inside the curly brackets. Each task will have a copy of the loop variable index with the corresponding value yielded by the iterand. This index allows us to customize the set of instructions for each particular task. Let’s write coforall.chpl:\nvar x = 10;\nconfig var numtasks = 2;\nwriteln('This is the main task: x = ', x);\ncoforall taskid in 1..numtasks {\n  var count = taskid**2;\n  writeln('this is task ', taskid, ': my value of count is ', count, ' and x is ', x);\n}\nwriteln('This message will not appear until all tasks are done ...');\n$ chpl coforall.chpl -o coforall\n$ sed -i -e 's|cobegin|coforall --numtasks=5|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task: x = 10\nthis is task 1: my value of c is 1 and x is 10\nthis is task 2: my value of c is 4 and x is 10\nthis is task 4: my value of c is 16 and x is 10\nthis is task 3: my value of c is 9 and x is 10\nthis is task 5: my value of c is 25 and x is 10\nThis message will not appear until all tasks are done ...\nNotice the random order of the print statements. And notice how, once again, the variables declared outside the coforall can be read by all tasks, while the variables declared inside, are available only to the particular task.\n\n\n\n\n\n\nCautionQuestion Task.1\n\n\n\n\n\nWould it be possible to print all the messages in the right order? Modify the code in the last example as required and save it as consecutive.chpl. Hint: you can use an array of strings declared in the main task, into which all the concurrent tasks could write their messages in the right order. Then, at the end, have the main task print all elements of the array.\n\n\n\n\n\n\n\n\n\nCautionQuestion Task.2\n\n\n\n\n\nConsider the following code gmax.chpl to find the maximum array element. Complete this code, and also time the coforall loop.\nuse Random, Time;\nconfig const nelem = 1e8: int;\nvar x: [1..nelem] int;\nfillRandom(x);                     // fill array with random numbers\nvar gmax = 0;\n\nconfig const numtasks = 2;       // let's pretend we have 2 cores\nconst n = nelem / numtasks;      // number of elements per task\nconst r = nelem - n*numtasks;    // these elements did not fit into the last task\nvar lmax: [1..numtasks] int;    // local maxima for each task\ncoforall taskid in 1..numtasks {\n  var start, finish: int;\n  start = ...\n  finish = ...\n  ... compute lmax for this task ...\n}\n\n// put largest lmax into gmax\nfor taskid in 1..numtasks do                        // a serial loop\n  if lmax[taskid] &gt; gmax then gmax = lmax[taskid];\n\nwritef('The maximum value in x is %14.12dr\\n', gmax);   // formatted output\nwriteln('It took ', watch.elapsed(), ' seconds');\nWrite a parallel code to find the maximum value in the array x. Be careful: the number of tasks should not be excessive. Best to use numtasks to organize parallel loops. For each task compute the start and finish indices of its array elements and cycle through them to find the local maximum. Then in the main task cycle through all local maxima to find the global maximum.\n\n\n\n\n\n\n\n\n\nCautionDiscussion\n\n\n\n\n\nRun the code of last Exercise using different number of tasks, and different sizes of the array x to see how the execution time changes. For example:\n$ ./gmax --nelem=100_000_000 --numtasks=1\nDiscuss your observations. Is there a limit on how fast the code could run?\n\n\n\n\n\n\n\n\n\n\n\nCautionTry this…\n\n\n\n\n\nSubstitute your addition to the code to find gmax in the last exercise with:\ngmax = max reduce x;   // 'max' is one of the reduce operators (data parallelism example)\nTime the execution of the original code and this new one. How do they compare?\n\n\n\n\n\n\n\n\n\n\nImportantKey idea\n\n\n\nIt is always a good idea to check whether there is a built-in function or method that can do what we want as efficiently (or better) than our house-made code. In this case, the reduce statement reduces the given array to a single number using the operation max, and it is parallelized. Here is the full list of reduce operations: +   *   &&   ||   &   |   ^   min   max.",
    "crumbs": [
      "PART 3: TASK PARALLELISM (BRIEFLY)",
      "Fire-and-forget tasks"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html",
    "title": "Domains and data parallelism",
    "section": "",
    "text": "We start this section by recalling the definition of a range in Chapel. A range is a 1D set of integer indices that can be bounded or infinite:\nvar a: range = 1..10;    // 1, 2, 3, ..., 10\n\nvar x1 = 1234, x2 = 5678;\nvar b: range = x1..x2;   // using variables\n\nvar c: range(strides=strideKind.positive) = 2..10 by 2;\nwriteln(c);              // 2, 4, 6, 8, 10\n\nvar d = 2..10 by 2;      // the same but skipping type description\n\nvar e = 1.. ;            // unbounded range\nOn the other hand, domains are multi-dimensional (including 1D) sets of integer indices that are always bounded. To stress the difference between domain ranges and domains, domain definitions always enclose their indices in curly brackets. Ranges can be used to define a specific dimension of a domain:\nvar domain1to10: domain(1) = {1..10};                // 1D domain from 1 to 10 defined using the range 1..10\n\nvar twoDimensions: domain(2) = {-2..2, 0..2};        // 2D domain over a product of two ranges\n\nvar thirdDim: range = 1..16;                         // a range\nvar threeDims: domain(3) = {1..10, 5..10, thirdDim}; // 3D domain over a product of three ranges\n\nwrite('cycle through all points in 2D: ');\nfor idx in twoDimensions do\n  write(idx, ', ');\nwriteln();\n\nwrite('cycle using explicit tuples: ');\nfor (x,y) in twoDimensions {                         // can also cycle using explicit tuples (x,y)\n  write(x,\",\",y,\"  \");\n}\nwriteln();\nLet us define an \\(n^2\\) domain called mesh. It is defined by the single task in our code and is therefore defined in memory on the same node (locale 0) where this task is running. For each of \\(n^2\\) mesh points, let us print out:\n\nm.locale.id = the ID of the locale holding that mesh point (should be 0)\nhere.id = the ID of the locale on which the code is running (should be 0)\nhere.maxTaskPar = the number of available cores (max parallelism with 1 task/core) (should be 3)\n\n\n\n\n\n\n\nNote\n\n\n\nWe already saw some of these variables / functions: numLocales, Locales, here.id, here.name, here.numPUs(), here.physicalMemory(), here.maxTaskPar.\n\n\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n}; // a 2D domain defined in shared memory on a single locale\nforall m in mesh do                   // go in parallel through all n^2 mesh points\n  writeln(m, ' ', m.locale.id, ' ', here.id, ' ', here.maxTaskPar);\n((7, 1), 0, 0, 2)\n((1, 1), 0, 0, 2)\n((7, 2), 0, 0, 2)\n((1, 2), 0, 0, 2)\n...\n((6, 6), 0, 0, 2)\n((6, 7), 0, 0, 2)\n((6, 8), 0, 0, 2)\nNow we are going to learn two very important features of Chapel domains.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html#local-domains",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html#local-domains",
    "title": "Domains and data parallelism",
    "section": "",
    "text": "We start this section by recalling the definition of a range in Chapel. A range is a 1D set of integer indices that can be bounded or infinite:\nvar a: range = 1..10;    // 1, 2, 3, ..., 10\n\nvar x1 = 1234, x2 = 5678;\nvar b: range = x1..x2;   // using variables\n\nvar c: range(strides=strideKind.positive) = 2..10 by 2;\nwriteln(c);              // 2, 4, 6, 8, 10\n\nvar d = 2..10 by 2;      // the same but skipping type description\n\nvar e = 1.. ;            // unbounded range\nOn the other hand, domains are multi-dimensional (including 1D) sets of integer indices that are always bounded. To stress the difference between domain ranges and domains, domain definitions always enclose their indices in curly brackets. Ranges can be used to define a specific dimension of a domain:\nvar domain1to10: domain(1) = {1..10};                // 1D domain from 1 to 10 defined using the range 1..10\n\nvar twoDimensions: domain(2) = {-2..2, 0..2};        // 2D domain over a product of two ranges\n\nvar thirdDim: range = 1..16;                         // a range\nvar threeDims: domain(3) = {1..10, 5..10, thirdDim}; // 3D domain over a product of three ranges\n\nwrite('cycle through all points in 2D: ');\nfor idx in twoDimensions do\n  write(idx, ', ');\nwriteln();\n\nwrite('cycle using explicit tuples: ');\nfor (x,y) in twoDimensions {                         // can also cycle using explicit tuples (x,y)\n  write(x,\",\",y,\"  \");\n}\nwriteln();\nLet us define an \\(n^2\\) domain called mesh. It is defined by the single task in our code and is therefore defined in memory on the same node (locale 0) where this task is running. For each of \\(n^2\\) mesh points, let us print out:\n\nm.locale.id = the ID of the locale holding that mesh point (should be 0)\nhere.id = the ID of the locale on which the code is running (should be 0)\nhere.maxTaskPar = the number of available cores (max parallelism with 1 task/core) (should be 3)\n\n\n\n\n\n\n\nNote\n\n\n\nWe already saw some of these variables / functions: numLocales, Locales, here.id, here.name, here.numPUs(), here.physicalMemory(), here.maxTaskPar.\n\n\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n}; // a 2D domain defined in shared memory on a single locale\nforall m in mesh do                   // go in parallel through all n^2 mesh points\n  writeln(m, ' ', m.locale.id, ' ', here.id, ' ', here.maxTaskPar);\n((7, 1), 0, 0, 2)\n((1, 1), 0, 0, 2)\n((7, 2), 0, 0, 2)\n((1, 2), 0, 0, 2)\n...\n((6, 6), 0, 0, 2)\n((6, 7), 0, 0, 2)\n((6, 8), 0, 0, 2)\nNow we are going to learn two very important features of Chapel domains.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html#feature-1-arrays-on-top-of-domains",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html#feature-1-arrays-on-top-of-domains",
    "title": "Domains and data parallelism",
    "section": "Feature 1: arrays on top of domains",
    "text": "Feature 1: arrays on top of domains\nDomains can be used to define arrays of variables of any type on top of them. For example, let us define an \\(n^2\\) array of real numbers on top of mesh:\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n}; // a 2D domain defined in shared memory on a single locale\nvar T: [mesh] real;                   // a 2D array of reals defined in shared memory on a single locale (mapped onto this domain)\nforall t in T do                      // go in parallel through all n^2 elements of T\n  writeln(t, ' ', t.locale.id);\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\n0.0 0\n0.0 0\n0.0 0\n...\n0.0 0\n0.0 0\n0.0 0\nBy default, all \\(n^2\\) array elements are set to zero, and all of them are defined on the same locale as the underlying mesh. We can also cycle through all indices of T by accessing its domain:\nforall idx in T.domain do\n  writeln(idx, ' ', T[idx]);   // idx is a tuple (i,j); also print the corresponding array element\n(7, 1) 0.0\n(1, 1) 0.0\n(7, 2) 0.0\n(1, 2) 0.0\n...\n(6, 6) 0.0\n(6, 7) 0.0\n(6, 8) 0.0\nSince we use a paralell forall loop, the print statements appear in a random runtime order.\nWe can also define multiple arrays on the same domain:\nconst grid = {1..100};          // 1D domain\nconst alpha = 5;                // some number\nvar A, B, C: [grid] real;       // local real-type arrays on this 1D domain\nB = 2; C = 3;\nforall (a,b,c) in zip(A,B,C) do // parallel loop\n  a = b + alpha*c;              // simple example of data parallelism on a single locale\nwriteln(A);",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html#feature-2-distributed-domains",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html#feature-2-distributed-domains",
    "title": "Domains and data parallelism",
    "section": "Feature 2: distributed domains",
    "text": "Feature 2: distributed domains\nThe second important property of Chapel domains is that they can span multiple locales (nodes).\nDomains are fundamental Chapel concept for distributed-memory data parallelism.\nLet us now define an \\(n^2\\) distributed (over several locales) domain distributedMesh mapped to locales in blocks. On top of this domain we define a 2D block-distributed array A of strings mapped to locales in exactly the same pattern as the underlying domain. Let us print out:\n\na.locale.id = the ID of the locale holding the element a of A\nhere.name = the name of the locale on which the code is running\nhere.maxTaskPar = the number of cores on the locale on which the code is running\n\nInstead of printing these values to the screen, we will store this output inside each element of A as a string: a = int + string + int is a shortcut for a = int:string + string + int:string\nuse BlockDist; // use standard block distribution module to partition the domain into blocks\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n};\nconst distributedMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = mesh;\nvar A: [distributedMesh] string; // block-distributed array mapped to locales\nforall a in A { // go in parallel through all n^2 elements in A\n  // assign each array element on the locale that stores that index/element\n  a = a.locale.id:string + '-' + here.name[0..4] + '-' + here.maxTaskPar:string + '  ';\n}\nwriteln(A);\nThe syntax boundingBox=mesh tells the compiler that the outer edge of our decomposition coincides exactly with the outer edge of our domain. Alternatively, the outer decomposition layer could include an additional perimeter of ghost points if we specify the following:\nconst mesh: domain(2) = {1..n, 1..n};\nconst largerMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = {0..n+1,0..n+1};\n\nbut let us not worry about this for now.\nRunning our code on 3 locales, with 2 cores per locale, produces the following output:\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n\n\n\n\n\n\nNote\n\n\n\nIf we were to run it on 4 locales, with 2 cores per locale, we might see something like this:\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n\n\nAs we see, the domain distributedMesh (along with the string array A on top of it) was decomposed into 3x1 blocks stored on the three nodes, respectively. Equally important, for each element a of the array, the line of code filling in that element ran on the same locale where that element was stored. In other words, this code ran via the parallel forall loop on 3 nodes, using up to 2 cores on each node (or whatever you specified) to fill in the corresponding array elements. Once the parallel loop is finished, the last command writeln(A) runs on locale 0 gathering remote elements from the other locales and printing them to standard output.\nNow we can print the range of indices for each sub-domain by adding the following to our code:\nfor loc in Locales do\n  on loc do\n    writeln(A.localSubdomain());\nOn 3 locales we should get:\n{1..3, 1..8}\n{4..6, 1..8}\n{7..8, 1..8}\nLet us count the number of threads by adding the following to our code:\nvar count = 0;\nforall a in A with (+ reduce count) { // go in parallel through all n^2 elements\n  count = 1;\n}\nwriteln(\"actual number of threads = \", count);\nIf our n=8 is sufficiently large, there are probably enough array elements per node (\\(8*8/3\\approx 21\\) in our case) to fully utilize the two available cores on each node, so our output might be:\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\nactual number of threads = 6\n\n\n\n\n\n\nCautionQuestion Data.2\n\n\n\n\n\nTry reducing the array size n to see if that changes the output (fewer threads per locale), e.g., setting n=3. Also try increasing the array size to n=20 and study the output. Does the output make sense?\n\n\n\nSo far we looked at the block distribution BlockDist. It will distribute a 2D domain among nodes either using 1D or 2D decomposition (in our example it was 2D decomposition 2x2), depending on the domain size and the number of nodes.\nLet us take a look at another standard module for domain partitioning onto locales, called CyclicDist. For each element of the array we will print out again\n\na.locale.id = the ID of the locale holding the element a of A\nhere.name = the name of the locale on which the code is running\nhere.maxTaskPar = the number of cores on the locale on which the code is running\n\nuse CyclicDist; // elements are sent to locales in a round-robin pattern\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n};  // a 2D domain defined in shared memory on a single locale\nconst m2: domain(2) dmapped new cyclicDist(startIdx=mesh.low) = mesh; // mesh.low is the first index (1,1)\nvar A2: [m2] string;\nforall a in A2 {\n  a = a.locale.id:string + '-' + here.name[0..4]:string + '-' + here.maxTaskPar:string + '  ';\n}\nwriteln(A2);\n$ chpl -o test test.chpl\n$ sbatch distributed.sh\n$ cat solution.out\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n\n\n\n\n\n\nNote\n\n\n\nWith 4 locales, we might see something like this:\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n\n\nAs the name CyclicDist suggests, the domain was mapped to locales in a cyclic, round-robin pattern. We can also print the range of indices for each sub-domain by adding the following to our code:\nfor loc in Locales do\n  on loc do\n    writeln(A2.localSubdomain());\n{1..8 by 3, 1..8}\n{1..8 by 3 align 2, 1..8}\n{1..8 by 3 align 0, 1..8}\nIn addition to BlockDist and CyclicDist, Chapel has several other predefined distributions: BlockCycDist, ReplicatedDist, DimensionalDist2D, ReplicatedDim, BlockCycDim, and StencilDist – for details please see Chapel’s documentation on distributions. I would particularly advise to check out StencilDist which is an enhanced variant of the blockDist distribution: it reduces the amount of communication necessary for array accesses across locale boundaries on a grid.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "formats-20221104.html",
    "href": "formats-20221104.html",
    "title": "What format to choose to save your data",
    "section": "",
    "text": "Friday, November 4th, 2022, 1:00pm - 2:30pm\nYou can find this page at   https://folio.vastcloud.org/formats\nAbstract: Which file format should you use when saving your research dataset? Besides the obvious question of how to encode your data structures in a file, you might also want to consider portability (the ability to write/read data across different operating systems and different programming languages and libraries), the inclusion of metadata (data description), I/O bandwidth and file sizes, compression, and the ability to read/write data in parallel for large datasets. In this introductory in-person workshop, we will cover all these aspects starting from the very basic file formats and ending with scalable scientific dataset formats. We will cover CSV, JSON, YAML, XML, BSON, VTK, NetCDF, HDF5, using both structured and unstructured datasets and demoing all examples in Python."
  },
  {
    "objectID": "formats-20221104.html#links",
    "href": "formats-20221104.html#links",
    "title": "What format to choose to save your data",
    "section": "Links",
    "text": "Links\n\nFall 2022 training schedule: https://bit.ly/wg2022b\nResearch computing autumn in November: https://autumnschool2022.westdri.ca (free registration)\nTraining materials: https://training.westdri.ca (upcoming and archived events, ~100 recorded webinars)\nDocumentation: https://docs.alliancecan.ca\nEmail: training “at” westdri “dot” ca"
  },
  {
    "objectID": "formats-20221104.html#requirements",
    "href": "formats-20221104.html#requirements",
    "title": "What format to choose to save your data",
    "section": "Requirements",
    "text": "Requirements\nIn the demos we will use Python and quite a few Python libraries to write into all these file formats. If you are comfortable with running Python and installing Python libraries on your computer, we will mention the libraries as we go during the workshop. For everyone else, we will provide access to a remote system with all these libraries installed – all you’ll need is a browser."
  },
  {
    "objectID": "formats-20221104.html#themes",
    "href": "formats-20221104.html#themes",
    "title": "What format to choose to save your data",
    "section": "Themes",
    "text": "Themes\n\nMain theme:  tabular data  ⮕  Python data structures (hierarchical dictionaries)  ⮕  large arrays\nParallel theme:  ASCII  ⮕  binary  ⮕  scientific formats with metadata, compression, parallel I/O\nAlso to consider:  file sizes, bandwidth (I/O speed), portability"
  },
  {
    "objectID": "formats-20221104.html#remote-system",
    "href": "formats-20221104.html#remote-system",
    "title": "What format to choose to save your data",
    "section": "Remote system",
    "text": "Remote system\nLog in to the remote system and copy the workshop files into your home directory:\ncp /home/user19/shared/formats/{jeopardy.csv,mandelbulb300.nc,writeNodesEdges.py} .\nIn a Python notebook in our JupyterHub, use:\n!cp /home/user19/shared/formats/{jeopardy.csv,mandelbulb300.nc,writeNodesEdges.py} ."
  },
  {
    "objectID": "formats-20221104.html#python-packages",
    "href": "formats-20221104.html#python-packages",
    "title": "What format to choose to save your data",
    "section": "Python packages",
    "text": "Python packages\npyyaml bson dict2xml pickle netCDF4 networkx numpy pandas xarray vtk pyevtk"
  },
  {
    "objectID": "formats-20221104.html#csv",
    "href": "formats-20221104.html#csv",
    "title": "What format to choose to save your data",
    "section": "CSV",
    "text": "CSV\n\ntabular data, such as a spreadsheet or database\nASCII format  ⇨  good for small datasets\n\nimport csv\nfilename = \"jeopardy.csv\"  # from https://domohelp.domo.com/hc/en-us/articles/360043931814-Fun-Sample-DataSets\nrows = []\nwith open(filename, 'r') as csvfile:  # open the CSV file in READ mode\n    csvreader = csv.reader(csvfile)   # create a CSV reader iterator\n    fields = next(csvreader)          # extract first row; advance the iterator to the next row\n    for row in csvreader:             # extract each row one by one\n        rows.append(row)\n\nprint(\"total number of rows:\", len(rows))\nprint(\"field names are:\", fields)\n\nfor row in rows[:5]:   # print first 5 rows\n    print(row)\n\nfor col in rows[0]:    # cycle through the first-row columns\n    print(col)\n\nwith open(\"short.csv\", 'w') as csvfile:\n    csvwriter = csv.writer(csvfile)   # create a CSV writer object\n    csvwriter.writerow(fields)        # write the fields\n    csvwriter.writerows(rows[:10])    # write the data rows\nThere is alternative syntax:\ncsvfile = open(\"short.csv\", 'w')\ncsvwriter = csv.writer(csvfile)\ncsvwriter.writerow(fields)\ncsvwriter.writerows(rows[:10])\ncsvfile.close()\nLet’s now use pandas dataframes for processing our rows and columns:\nimport pandas as pd\ndata = pd.read_csv(\"jeopardy.csv\")\ndata.shape   # 216930 rows, 7 columns\ndata.head(10)   # first 10 rows\ndata.columns    # names of the columns\ndata.loc[data['Category']=='HISTORY'].shape   # 349 matches\ndata.loc[data['Category']=='HISTORY'].to_csv(\"history.csv\")   # write to a file\nSimple Python dictionaries can be stored as 2-column tables. Here is an example of writing a Python dictionary to CSV:\ncentres = {'Vancouver': 2463, 'Calgary': 1393, 'Toronto': 5928, 'Montreal': 4099, 'Halifax': 413} # Python dictionary\nwith open('cities.csv', 'w') as f:   # open a stream for writing\n    for key in centres.keys():\n        f.write(\"%s, %s\\n\" % (key, centres[key]))\n\n\n\n\n\n\nCautionExercise Header\n\n\n\n\n\nModify the last Python script to add a header “city, population” to this CSV file.\n\n\n\n\nHow about a more complex Python dictionary, with some hierarchical structure? For that a CSV file won’t work, but you can use JSON files."
  },
  {
    "objectID": "formats-20221104.html#json",
    "href": "formats-20221104.html#json",
    "title": "What format to choose to save your data",
    "section": "JSON",
    "text": "JSON\n\nJavaScript Object Notation, inspired by JavaScript object literal syntax, although not part of JavaScript\nhuman-readable text, although can contain non-ASCII characters\nvery popular for encoding all sorts of data among web developers\ninternal mapping is similar to that of a Python dictionary, with key-value pairs\n\nLet’s start again with a simple dictionary:\nimport json\ncentres = {'Vancouver': 2463, 'Calgary': 1393, 'Toronto': 5928, 'Montreal': 4099, 'Halifax': 413} # Python dictionary\ny = json.dumps(centres)      # convert it to a JSON string\ntype(y)                      # it is string, as far as Python is concerned\njson.loads(y)[\"Vancouver\"]   # back to a Python dictionary that we can subset\ntype(json.loads(y))          # it is really a dictionary\n\n# convert more complex Python object to a JSON string\na = ['something', {'more': ('complex', None, 1.0, 2)}]\njson.dumps(a)\n\n# can use custom separators\njson.dumps(a, separators=(\"; \", \" &gt; \"), sort_keys=True)\nYou can convert into JSON strings the following Python objects: int, float, string, logical, tuple, list, dict, None.\nimport json\nx = {\n  \"name\": \"John\",\n  \"age\": 30,\n  \"married\": True,\n  \"divorced\": False,\n  \"children\": (\"Ann\",\"Billy\"),\n  \"pets\": None,\n  \"cars\": [\n    {\"model\": \"BMW 230\", \"mpg\": 27.5},\n    {\"model\": \"Ford Edge\", \"mpg\": 24.1}\n  ]\n}\njstring = json.dumps(x)\nprint(jstring)\n\n# write the string to a file\nwith open('personal.json', 'w') as f:\n    f.write(jstring)\n\n# read the file\nwith open('personal.json', 'r') as f:\n    data = f.read()\n\ntype(data)             # it is a (JSON) string\ng = json.loads(data)   # convert to a dictionary\ng[\"name\"]\n\n\n\n\n\n\nCautionExercise Open-close\n\n\n\n\n\nConvert the last write operation from a stream to an “open-close” block.\n\n\n\n\n\n\n\n\nYou can also write the dictionary directly to a file, without creating a JSON string:\nwith open('personal.json', 'w') as outfile:\n    json.dump(x, outfile)\n\nwith open('personal.json') as file:   # read from a file into a dictionary\n    data = json.load(file)\n\nprint(data)\nTo summarize:\n\njson.dumps() converts a Python object to a JSON string\njson.loads() converts a JSON string to a Python object\njson.dump() writes a Python object to a stream as a JSON string\njson.load() loads a Python object from a JSON string stream\n\nFinally, let’s write a large multidimensional numpy array into JSON. The converter json.dumps() is not aware of the numpy data format. If you try to run f.write(json.dumps(x)), you will receive “TypeError: Object of type ndarray is not JSON serializable”. However, you can convert this array to a list:\nimport numpy as np\nx = np.zeros((100,100,100), dtype=np.float32)   # allocate single-precision 100^3 array\nx[:] = np.random.random(size=x.shape)   # fill it with random numbers drawn from a uniform [0,1) distribution\nx.dtype   # single precision\nx.shape   # (100, 100, 100)\n\nimport json\nf = open(\"array.json\", 'w')\nf.write(json.dumps(x.tolist()))\nf.close()   # the new file is 20,289,659 bytes - remember this number\nTo read it:\nimport json\nf = open(\"array.json\", 'r')\ndata = f.read()\nf.close()\nb = json.loads(data)   # convert to a nested list\ntype(b)        # it is a list\nlen(b)         # it has 100 elements (2D slices)\nlen(b[0])      # of which the first element also has 100 elements (1D columns)\nlen(b[0][0])   # of which the first element also has 100 elements (individual numbers)"
  },
  {
    "objectID": "formats-20221104.html#yaml",
    "href": "formats-20221104.html#yaml",
    "title": "What format to choose to save your data",
    "section": "YAML",
    "text": "YAML\n\nstands for YAML Ain’t Markup Language\nuses indentation to define structured data\noften used as a format for configuration files\nsimpler replacement for the JSON format\nhttps://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started\n\npip install pyyaml\nimport yaml\nx = {\n  \"name\": \"John\",\n  \"age\": 30,\n  \"married\": True,\n  \"divorced\": False,\n  \"children\": (\"Ann\",\"Billy\"),\n  \"pets\": None,\n  \"cars\": [\n    {\"model\": \"BMW 230\", \"mpg\": 27.5},\n    {\"model\": \"Ford Edge\", \"mpg\": 24.1}\n  ]\n}\nyaml.dump(x)                            # dump to the terminal\nwith open('personal.yaml', 'w') as f:   # write to a file\n    yaml.dump(x, f, sort_keys=False, default_flow_style=False)\nSetting default_flow_style=True would give you a more compact notation with separators, but it’ll be more difficult to read with a naked eye.\nLet’s now read it. There are different readers (“loaders”) ordered by their security level:\n\nBaseLoader loads all the basic YAML scalars as strings\nSafeLoader loads subset of the YAML safely, mainly used if the input is from an untrusted source; cannot deal with Python tuples\nFullLoader loads the full YAML but avoids arbitrary code execution; still poses a potential risk when used for the untrusted input\nUnsafeLoader is the original loader, generally used for backward compatibility\n\nimport yaml\nfrom yaml.loader import FullLoader\nwith open('personal.yaml') as f:\n    data = yaml.load(f, Loader=FullLoader)\n\nprint(data)\nLet’s try writing 2 documents into a YAML file:\nx2 = {'this': 77, 'there': 45, 'hi': 10, 'at': 23, 'Hello': 7}\nwith open('personal.yaml', 'w') as f:   # write to a file\n    data = yaml.dump_all([x,x2], f, sort_keys=False, default_flow_style=False)\nYou can read them with:\nimport yaml\nfrom yaml.loader import FullLoader\nwith open('personal.yaml', 'r') as f:\n    data = list(yaml.load_all(f, Loader=FullLoader))\n\nprint(len(data))\nprint(data)"
  },
  {
    "objectID": "formats-20221104.html#xml",
    "href": "formats-20221104.html#xml",
    "title": "What format to choose to save your data",
    "section": "XML",
    "text": "XML\n\neXtensible Markup Language\nuses a tag to define the structure, just like HTML\n3rd popular serialization format https://www.csestack.org/yaml-vs-json-vs-xml-difference\nwrite/read XML from Python\n\nhttps://stackabuse.com/reading-and-writing-xml-files-in-python\nhttps://www.geeksforgeeks.org/serialize-python-dictionary-to-xml\n\n\nHere let’s consider only a quick write example:\npip install xml-python\nimport xml.etree.ElementTree as ET\n\ndata = ET.Element('data')\nitems = ET.SubElement(data, 'items')\nitem1 = ET.SubElement(items, 'item')\nitem2 = ET.SubElement(items, 'item')\nitem1.set('name','item1')\nitem2.set('name','item2')\nitem1.text = 'item1-some-text'\nitem2.text = 'item2-some-text'\n\nmydata = ET.tostring(data)\ntype(mydata)                        # it is a `bytes` object\nmyfile = open(\"items.xml\", \"wb\")   # write it into a string\nmyfile.write(mydata)\nmyfile.close()\npip install dict2xml\nfrom dict2xml import dict2xml\nx = {\n  \"name\": \"John\",\n  \"age\": 30,\n  \"married\": True,\n  \"divorced\": False,\n  \"children\": (\"Ann\",\"Billy\"),\n  \"pets\": None,\n  \"cars\": [\n    {\"model\": \"BMW 230\", \"mpg\": 27.5},\n    {\"model\": \"Ford Edge\", \"mpg\": 24.1}\n  ]\n}\nxml = dict2xml(x)\nprint(xml)\nmyfile = open(\"personal.xml\", \"w\")\nmyfile.write(xml)\nmyfile.close()"
  },
  {
    "objectID": "formats-20221104.html#bson",
    "href": "formats-20221104.html#bson",
    "title": "What format to choose to save your data",
    "section": "BSON",
    "text": "BSON\n\nbinary Javascript Object Notation, i.e. binary-encoded serialization of JSON documents\ndata storage, main format for MongoDB\ncan even send/receive BSON objects over network sockets from Python\nused heavily for sending/receiving data in web development\na list of ordered elements containing a field name, type, and value\nmaximum structure size is 2GB (not very good …)\n\npip install bson\nIt is very easy to write into BSON files:\nimport bson\nx = {\"a\": [1, 2, 3, 4, 5, \"6\", {\"some\": \"text\"}]}\na = bson.dumps(x)   # convert dictionary -&gt; BSON object (string of binary codes)\nf = open(\"test.bson\", 'wb')\nf.write(a)\nf.close()\nReading is a reverse process:\nimport bson\nf = open(\"test.bson\", 'rb')\ndata = f.read()    # read a BSON string\nf.close()\nbson.loads(data)   # extract our dictionary\nLet’s consider a larger numpy array. If you try to pass it via a dictionary value to BSON, it’ll crash with an error, as BSON library is not smart enough to understand a numpy array wrapped into a dictionary. However, you can encode this numpy array into a byte stream with a numpy-aware pickle library (https://docs.python.org/3/library/pickle.html):\nimport numpy as np\nx = np.zeros((100,100,100), dtype=np.float32)   # allocate single-precision 100^3 array\nx[:] = np.random.random(size=x.shape)   # fill it with random numbers drawn from a uniform [0,1) distribution\nx.dtype   # single precision\n\nimport pickle   # binary protocols for serializing (converting into a byte stream) and de-serializing Python object structures\nimport bson\ntable = {\"random\": pickle.dumps(x)}\nf = open(\"array.bson\", 'wb')\nf.write(bson.dumps(table))\nf.close()   # the new file is 4,000,181 bytes - compare this to an earlier JSON example (20 MB)\n\n\n\n\n\n\nNote\n\n\n\nNote: For those wondering if we could convert a “pickled” numpy array to BSON, without using a dictionary, e.g.\nxstream = pickle.dumps(x)\nbson.dumps(xstream)\nthe answer is “no”, as the second command would produce “AttributeError: ‘bytes’ object has no attribute ‘keys’”, i.e. bson.dumps() expects a dictionary.\n\n\nNow, let’s read from BSON:\nimport pickle, bson\nf = open(\"array.bson\", 'rb')\ndata = f.read()\nf.close()\nnewTable = bson.loads(data)            # convert BSON -&gt; dictionary\nb = pickle.loads(newTable[\"random\"])   # de-serialize the value back into a numpy array\ntype(b)   # numpy array\nb.dtype   # single precision\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise Network\n\n\n\n\n\nHow would you store a network as a Python data structure?\n1. list of node coordinates\n2. list of connections, each with two node IDs\n3. optionally can wrap the previous two into a dictionary\n4. try to write it into a BSON file"
  },
  {
    "objectID": "formats-20221104.html#netcdf-and-hdf5",
    "href": "formats-20221104.html#netcdf-and-hdf5",
    "title": "What format to choose to save your data",
    "section": "NetCDF (and HDF5)",
    "text": "NetCDF (and HDF5)\nPreviously, we saw that storing a large data array was very efficient with BSON (as far as file sizes are concerned), but we had to jump through a number of hoops:\n\nput our array into a byte stream\nuse this byte stream as a value in a dictionary\nconvert this dictionary to BSON\nwrite this BSON to disk\n\nThere is a much better approach for large arrays: use a dedicated scientific array format library such as NetCDF or HDF5. These are available from (virtually) all programming languages. What they provide:\n\ndata description\ndata portability\ndata compression\nmultiple variables in a single file\nmultiple timesteps in a single file (if desired)\noptionally, parallel I/O\n\nNetCDF and HDF5 are related to each other and heavily borrow from each other’s source code. There are some differences too, e.g.\n\nNetCDF has an ecosystem of conventions, e.g. a CF (climate-forecast) convention\nNetCDF is understood natively by ParaView, HDF5 not so easily\nHDF5 was developed specifically for storing hierarchical/nested data\n\nHere we only show NetCDF.\npip install netCDF4\nimport numpy as np\nimport netCDF4 as nc\nf = nc.Dataset(\"array.nc\", 'w', format='NETCDF4')\nf.createDimension('x', 100)\nf.createDimension('y', 100)\nf.createDimension('z', 100)\nrho = f.createVariable('density', 'f4', ('x', 'y', 'z'))   # f4 stands for 4-byte float\nrho[:,:,:] = np.random.random(size=(100,100,100))\nf.close()   # 4,008,192 bytes\nNow let’s read it:\nimport netCDF4 as nc\nds = nc.Dataset(\"array.nc\")\nprint(ds)  # can see the dimensions, variables, precision\nprint(ds[\"density\"].shape)\nprint(ds[\"density\"][:])   # it is a numpy array\n\n\n\n\n\n\nCautionExercise Compression 1\n\n\n\n\n\nTurn on compression when saving the variable. Did it change the file size?\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise Compression 2\n\n\n\n\n\nRead a highly compressed dataset from file mandelbulb300.nc. Compare the file size to the size of embedded data – do they agree? Give a reason behind that.\n\n\n\nIf you want the functionality of pandas (easy 2D table manipulation) in a multidimensional array, then you can use xarray Python library. Xarray library is built on top of numpy and pandas. There are two main data structures in xarray:\n\nxarray.DataArray is a fancy, labelled version of numpy.ndarray\nxarray.Dataset is a collection of multiple xarray.DataArray’s that share dimensions\n\npip install xarray\nimport xarray as xa\nimport numpy as np\ndata = xa.DataArray(\n    np.random.random(size=(8,6)),\n    dims=(\"y\",\"x\"),  # dimension names (row,col); we want `y` to represent rows and `x` columns\n    coords={\"x\": [10,11,12,13,14,15], \"y\": [10,20,30,40,50,60,70,80]}  # coordinate labels/values\n)\ndata\nWe can access various attributes of this array:\ndata.values                 # the 2D numpy array\ndata.values[0,0] = 0.53     # can modify in-place\ndata.dims                   # ('y', 'x')\ndata.coords               # all coordinates, cannot modify!\ndata.coords['x'][1]       # a number\ndata.x[1]                 # the same\nArray subsetting:\n\nisel() selects by index, could be replaced by [index1] or [index1,…]\nsel() selects by value\ninterp() interpolates by value\n\ndata.isel()      # same as `data`\ndata.isel(y=1)   # second row\ndata.isel(y=0, x=[-2,-1])    # first row, last two columns\nVectorized operations:\ndata + 100      # element-wise like numpy arrays\n(data - data.mean()) / data.std()    # normalize the data\ndata - data[0,:]      # use numpy broadcasting =&gt; subtract first row from all rows\nLet’s initialize two 2D arrays with the identical dimensions:\ncoords = {\"x\": np.linspace(0,1,5), \"y\": np.linspace(0,1,5)}\ntemp = xa.DataArray(      # first 2D array\n    20 + np.random.randn(5,5),\n    dims=(\"y\",\"x\"),\n    coords=coords\n)\npres = xa.DataArray(       # second 2D array\n    100 + 10*np.random.randn(5,5),\n    dims=(\"y\",\"x\"),\n    coords=coords\n)\nFrom these we can form a dataset:\nds = xa.Dataset({\"temperature\": temp, \"pressure\": pres,\n                 \"bar\": (\"x\", 200+np.arange(5)), \"pi\": np.pi})\nds\nSubsetting works the usual way:\nds.sel(x=0)     # each 2D array becomes 1D array, the 1D array becomes a number, plus a number\nds.temperature.sel(x=0)     # 'temperature' is now a 1D array\nds.temperature.sel(x=0.25, y=0.5)     # one element of `temperature`\nWe can save this dataset to a file:\nds.to_netcdf(\"double.nc\")\nReading it is very easy:\nnew = xa.open_dataset(\"double.nc\")\nWe can even try opening this 2D dataset in ParaView - select (x,y) and deselect Spherical.\nSo far we’ve been working with datasets in Cartesian coordinates. How about spherical geometry – how do we initialize and store a dataset in spherical coordinates (longitude - latitude - elevation)? Very easy: define these coordinates and your data arrays on top, put everything into an xarray dataset, and then specify the following units:\nds.lat.attrs[\"units\"] = \"degrees_north\"   # this line is important to adhere to CF convention\nds.lon.attrs[\"units\"] = \"degrees_east\"    # this line is important to adhere to CF convention\n\n\n\n\n\n\nCautionExercise NetCDF CF convention\n\n\n\n\n\nCreate a \\(100\\times 300\\) (nlat\\(\\times\\)nlon) spherical dataset with two variables adhering to the NetCDF CF convention."
  },
  {
    "objectID": "formats-20221104.html#vtk-versatile-format-for-spatially-defined-data",
    "href": "formats-20221104.html#vtk-versatile-format-for-spatially-defined-data",
    "title": "What format to choose to save your data",
    "section": "VTK = versatile format for spatially defined data",
    "text": "VTK = versatile format for spatially defined data\nVTK is a computer graphics and visualization library that’s been around since early 1990s. Originally it was C++ only, but now there are interfaces from other languages as well, including Python.\n\n\nlegacy VTK: ASCII or binary data, ASCII metadata, could write manually or through APIs\nXML VTK: usually binary data, compression, parallel I/O, write via API\n\nIn simple cases using the original VTK library is probably an overkill, as doing even simple things could involve quite a few function calls. VTK is a very powerful library, but there is also a very steep learning curve. Consider the number of functions inside VTK’s Python interface:\npip install vtk pyevtk\nimport vtk\n[i for i in dir(vtk)]                    # returns 3593 functions\n[i for i in dir(vtk) if \"Writer\" in i]   # returns 104 functions\nWe will see an example of using the original VTK library below. But first let us consider a simple example of writing data on a Cartesian mesh (ImageData in VTK speak). For this we can use a more lightweight library called PyEVTK which is dedicated library for exporting data stored in numpy arrays to VTK files.\nimport numpy as np\nfrom pyevtk.hl import imageToVTK\nnx, ny, nz = 30, 30, 30\nncells = nx * ny * nz\nnpoints = (nx+1) * (ny+1) * (nz+1)\npressure = np.random.rand(ncells).reshape((nx, ny, nz))      # double precision\ntemp = np.random.rand(npoints).reshape((nx+1, ny+1, nz+1))   # double precision\nimageToVTK(\"image\", cellData = {\"pressure\": pressure}, pointData = {\"temp\": temp})\nTry loading this file into ParaView.\n\nNow let’s try writing a StructuredGrid dataset to a file:\nfrom pyevtk.hl import gridToVTK\nimport numpy as np\n\nnx, ny, nz = 30, 30, 30\nxside, yside, zside = 1.0, 1.0, 1.0\n\nncells = nx * ny * nz\nnpoints = (nx+1) * (ny+1) * (nz+1)\n\nX = np.linspace(0, xside, nx+1, dtype='float64')   # three coordinate axes\nY = np.linspace(0, yside, ny+1, dtype='float64')\nZ = np.linspace(0, zside, nz+1, dtype='float64')\n\nx, y, z = np.meshgrid(X,Y,Z)   # 3D coordinate arrays\n\n# perturb the grid to make it more interesting\nx += (0.5 - np.random.random(npoints).reshape(nx+1,ny+1,nz+1)) * 0.5 * xside/nx\ny += (0.5 - np.random.random(npoints).reshape(nx+1,ny+1,nz+1)) * 0.5 * yside/ny\nz += (0.5 - np.random.random(npoints).reshape(nx+1,ny+1,nz+1)) * 0.5 * zside/nz\n\npressure = np.random.rand(ncells).reshape((nx, ny, nz))\ntemp = np.random.rand(npoints).reshape((nx+1, ny+1, nz+1))\ngridToVTK(\"structured\", x, y, z, cellData = {\"pressure\" : pressure}, pointData = {\"temp\" : temp})\nFinally, here is the promised example with the original VTK library:\nimport networkx as nx\nfrom writeNodesEdges import writeObjects\n\nH = nx.complete_bipartite_graph(15,5) # two sets of nodes\nprint(nx.number_of_nodes(H), 'nodes and', nx.number_of_edges(H), 'edges')\n\npos = nx.spring_layout(H,dim=3,k=1)   # dictionary of positions keyed by node\nxyz = [list(pos[i]) for i in pos]     # convert to a nested list of positions\n\ndegree = [d for i,d in H.degree()]\nprint('degree =', degree)\nwriteObjects(xyz, edges=H.edges(), scalar=degree, name='degree', fileout='network')\nThis wrote to a PolygonalData file. Try loading this file into ParaView and displaying the network with nodes and edges.\nIf you want to write to an UnstructuredGrid file, replace the last line with:\nwriteObjects(xyz, edges=H.edges(), scalar=degree, name='degree', fileout='network', method = 'vtkUnstructuredGrid')"
  },
  {
    "objectID": "python2/python-18-other.html",
    "href": "python2/python-18-other.html",
    "title": "Other topics",
    "section": "",
    "text": "Caching function calls (“memoization”)\n\n\nSimple example\nConsider the following function:\nimport time\ndef slow(x):\n    time.sleep(1) # mimicking some heavy computation\n    return x\nCalling it 20 times will result in a 20-sec wait:\n%%time\nfor i in range(10):\n    slow(2)\n    slow(3)\nWall time: 20.1 s\nIn reality there are only 2 functions calls: slow(2) and slow(3), and everything else is just a repeat.\nNow let’s cache our function calls with a decorator:\nimport collections\nimport functools\n\nclass memoized(object):\n   '''Decorator. Caches a function's return value each time it is called.\n   If called later with the same arguments, the cached value is returned\n   (not reevaluated).\n   '''\n   def __init__(self, func):\n      self.func = func\n      self.cache = {}\n   def __call__(self, *args):\n      if not isinstance(args, collections.abc.Hashable):\n         # uncacheable. a list, for instance.\n         # better to not cache than blow up.\n         return self.func(*args)\n      if args in self.cache:\n         return self.cache[args]\n      else:\n         value = self.func(*args)\n         self.cache[args] = value\n         return value\n   def __repr__(self):\n      '''Return the function's docstring.'''\n      return self.func.__doc__\n   def __get__(self, obj, objtype):\n      '''Support instance methods.'''\n      return functools.partial(self.__call__, obj)\n\n@memoized\ndef fast(x):\n    time.sleep(1) # mimicking some heavy computation\n    return x\nThe function will be called twice, and the other 18 times it’ll reuse previously computed results:\n%%time\nfor i in range(10):\n    fast(2)\n    fast(3)\nWall time: 2.01 s\n\n\nBadly-coded Fibonacci problem\nConsider a top-down (and thus terribly inefficient) Fibonacci number calculation:\ndef fibonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    return fibonacci(n-1) + fibonacci(n-2)\n%%timeit\nfibonacci(30)\n181 ms ± 2.85 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nThe total number of fibonacci() calls in this example is 2,692,537, and it scales exponentially towards lower n, e.g. f(1) is evaluated 832,040 times. It turns out it is possible to reuse previous function calls and only compute new ones using the decorator defined earlier:\n@memoized\ndef fastFibonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    return fastFibonacci(n-1) + fastFibonacci(n-2)\n%%timeit\nfastFibonacci(30)\n268 ns ± 1.23 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\nWe have a speedup by a factor of 675,000! This is even better than the anticipated speedup of \\(2,692,537/30\n\\approx 89,751\\) (from the reduction in the number of function calls) which probably tells us something about the overhead of managing many thousands of simultaneous nested function calls in Python in the first place.\n\n\n\nProgramming Style and Wrap-Up\n\nComment your code as much as possible.\nUse meaningful variable names.\nVery good idea to break complex programs into blocks using functions.\nChange one thing at a time, then test.\nUse version control.\nUse docstrings to provide online help:\n\ndef average(values):\n    \"Return average of values, or None if no values are supplied.\"\n    if len(values) &gt; 0:\n        return(sum(values)/len(values))\n\nprint(average([1,2,3,4]))\nprint(average([]))\nhelp(average)\ndef moreComplexFunction(values):\n    \"\"\"This string spans\n       multiple lines.\n\n    Blank lines are allowed.\"\"\"\n\nhelp(moreComplexFunction)\n\nVery good idea to add assertions to your code to check input:\n\nassert n &gt; 0., 'Data should only contain positive values'\nis the same as\nimport sys\nif n &lt;= 0.:\n    print('Data should only contain positive values')\n    sys.exit(1)\n\n\nLinks\n\nPython 3 documentation\nMatplotlib gallery\nNumPy scientific computing package\nSciPy collection of scientific utilities\nPandas library\nXarray documentation",
    "crumbs": [
      "ADDITIONAL MATERIAL",
      "Other topics"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html",
    "href": "python2/python-14-xarray.html",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "",
    "text": "Xarray library is built on top of numpy and pandas, and it brings the power of pandas to multidimensional arrays. There are two main data structures in xarray:",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#data-array-simple-example-from-scratch",
    "href": "python2/python-14-xarray.html#data-array-simple-example-from-scratch",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Data array: simple example from scratch",
    "text": "Data array: simple example from scratch\nimport xarray as xr\nimport numpy as np\ndata = xr.DataArray(\n    np.random.random(size=(4,3)),\n    dims=(\"y\",\"x\"),  # dimension names (row,col); we want `y` to represent rows and `x` columns\n    coords={\"x\": [10,11,12], \"y\": [10,20,30,40]}  # coordinate labels/values\n)\ndata\ntype(data)   # &lt;class 'xarray.core.dataarray.DataArray'&gt;\nWe can access various attributes of this array:\ndata.values                 # the 2D numpy array\ndata.values[0,0] = 0.53     # can modify in-place\ndata.dims                   # ('y', 'x')\ndata.coords                 # all coordinates\ndata.coords['x']            # one coordinate\ndata.coords['x'][1]         # a number\ndata.x[1]                   # the same\nLet’s add some arbitrary metadata:\ndata.attrs = {\"author\": \"Alex\", \"date\": \"2020-08-26\"}\ndata.attrs[\"name\"] = \"density\"\ndata.attrs[\"units\"] = \"g/cm^3\"\ndata.x.attrs[\"units\"] = \"cm\"\ndata.y.attrs[\"units\"] = \"cm\"\ndata.attrs    # global attributes\ndata          # global attributes show here as well\ndata.x        # only `x` attributes",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#subsetting-arrays",
    "href": "python2/python-14-xarray.html#subsetting-arrays",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Subsetting arrays",
    "text": "Subsetting arrays\nWe can subset using the usual Python square brackets:\ndata[0,:]     # first row\ndata[:,-1]    # last column\nIn addition, xarray provides these functions:\n\nisel() selects by coordinate index, could be replaced by [index1] or [index1,…]\nsel() selects by coordinate value\ninterp() interpolates by coordinate value\n\ndata.isel()      # same as `data`\ndata.isel(y=1)   # second row\ndata.isel(x=2)   # third column\ndata.isel(y=0, x=[-2,-1])   # first row, last two columns\ndata.x.dtype     # it is integer\ndata.sel(x=10)   # certain value of `x`\ndata.y           # array([10, 20, 30, 40])\ndata.sel(y=slice(15,30))   # only values with 15&lt;=y&lt;=30 (two rows)\nThere are aggregate functions, e.g.\nmeanOfEachColumn = data.mean(dim='y')    # apply mean over y\nspatialMean = data.mean()\nspatialMean = data.mean(dim=['x','y'])   # same\nFinally, we can interpolate. However, this requires scipy library and might throw some warnings, so use at your own risk:\ndata.interp(x=10.5, y=10)    # first row, between 1st and 2nd columns\ndata.interp(x=10.5, y=15)    # between 1st and 2nd rows, between 1st and 2nd columns\n?data.interp                 # can use different interpolation methods",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#plotting",
    "href": "python2/python-14-xarray.html#plotting",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Plotting",
    "text": "Plotting\nMatplotlib is integrated directly into xarray:\ndata.plot(size=5)                         # 2D heatmap\ndata.isel(x=0).plot(marker=\"o\", size=5)   # 1D line plot",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#vectorized-operations",
    "href": "python2/python-14-xarray.html#vectorized-operations",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nYou can perform element-wise operations on xarray.DataArray like with numpy.ndarray:\ndata + 100                           # element-wise like numpy arrays\n(data - data.mean()) / data.std()    # normalize the data\ndata - data[0,:]      # use numpy broadcasting → subtract first row from all rows",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#split-your-data-into-multiple-independent-groups",
    "href": "python2/python-14-xarray.html#split-your-data-into-multiple-independent-groups",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Split your data into multiple independent groups",
    "text": "Split your data into multiple independent groups\ndata.groupby(\"x\")       # 3 groups with labels 10, 11, 12; each column becomes a group\ndata.groupby(\"x\")[10]   # specific group (first column)\ndata.groupby(\"x\").map(lambda v: v-v.min())   # apply separately to each group\n            # from each column (fixed x) subtract the smallest value in that column",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#dataset-simple-example-from-scratch",
    "href": "python2/python-14-xarray.html#dataset-simple-example-from-scratch",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Dataset: simple example from scratch",
    "text": "Dataset: simple example from scratch\nLet’s initialize two 2D arrays with the identical dimensions:\ncoords = {\"x\": np.linspace(0,1,5), \"y\": np.linspace(0,1,5)}\ntemp = xr.DataArray(      # first 2D array\n    20 + np.random.randn(5,5),\n    dims=(\"y\",\"x\"),\n    coords=coords\n)\npres = xr.DataArray(       # second 2D array\n    100 + 10*np.random.randn(5,5),\n    dims=(\"y\",\"x\"),\n    coords=coords\n)\nFrom these we can form a dataset:\nds = xr.Dataset({\"temperature\": temp, \"pressure\": pres,\n                 \"bar\": (\"x\", 200+np.arange(5)), \"pi\": np.pi})\nds\nAs you can see, ds includes two 2D arrays on the same grid, one 1D array on x, and one number:\nds.temperature   # 2D array\nds.bar           # 1D array\nds.pi            # one element\nSubsetting works the usual way:\nds.sel(x=0)     # each 2D array becomes 1D array, the 1D array becomes a number, plus a number\nds.temperature.sel(x=0)     # 'temperature' is now a 1D array\nds.temperature.sel(x=0.25, y=0.5)     # one element of `temperature`\nWe can save this dataset to a file:\n%pip install netcdf4\nds.to_netcdf(\"test.nc\")\nnew = xr.open_dataset(\"test.nc\")   # try reading it\nWe can even try opening this 2D dataset in ParaView - select (y,x) and deselect Spherical.\n\n\n\n\n\n\nCautionExercise 14.1\n\n\n\n\n\nRecall the 2D function we plotted when we were talking about numpy’s array broadcasting. Let’s scale it to a unit square x,y∈[0,1]:\nx = np.linspace(0, 1, 50)\ny = np.linspace(0, 1, 50).reshape(50,1)\nz = np.sin(5*x)**8 + np.cos(5+25*x*y)*np.cos(5*x)\nThis is will our image at z=0. Then rotate this image 90 degrees (e.g. flip x and y), and this will be our function at z=1. Now interpolate linearly between z=0 and z=1 to build a 3D function in the unit cube x,y,z∈[0,1]. Check what the function looks like at intermediate z. Write out a NetCDF file with the 3D function.",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#time-series-data",
    "href": "python2/python-14-xarray.html#time-series-data",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Time series data",
    "text": "Time series data\nIn xarray you can work with time-dependent data. Xarray accepts pandas time formatting, e.g. pd.to_datetime(\"2020-09-10\") would produce a timestamp. To produce a time range, we can use:\nimport pandas as pd\ntime = pd.date_range(\"2000-01-01\", freq=\"D\", periods=365*3+1) # 2000-Jan-01 to 2002-Dec-31 (3 full years)\ntime\ntime.shape    # 1096 days\ntime.month    # same length (1096), but each element is replaced by the month number\ntime.day      # same length (1096), but each element is replaced by the day-of-the-month\n?pd.date_range\nUsing this time construct, let’s initialize a time-dependent dataset that contains a scalar temperature variable (no space) mimicking seasonal change. We can do this directly without initializing an xarray.DataArray first – we just need to specify what this temperature variable depends on:\nimport xarray as xr\nimport numpy as np\nntime = len(time)\ntemp = 10 + 5*np.sin((250+np.arange(ntime))/365.25*2*np.pi) + 2*np.random.randn(ntime)\nds = xr.Dataset({ \"temperature\": (\"time\", temp),        # it's 1D function of time\n                  \"time\": time })\nds.temperature.plot(size=8)\nWe can do the usual subsetting:\nds.isel(time=100)   # 101st timestep\nds.sel(time=\"2002-12-22\")\nTime dependency in xarray allows resampling with a different timestep:\nds.resample(time='7D')                   # 1096 times -&gt; 157 time groups\nweekly = ds.resample(time='7D').mean()   # compute mean for each group\nweekly.dims\nweekly.temperature.plot(size=8)\nNow, let’s combine spatial and time dependency and construct a dataset containing two 2D variables (temperature and pressure) varying in time. The time dependency is baked into the coordinates of these xarray.DataArray’s and should come before the spatial coordinates:\ntime = pd.date_range(\"2020-01-01\", freq=\"D\", periods=91) # January - March 2020\nntime = len(time)\nn = 100      # spatial resolution in each dimension\naxis = np.linspace(0,1,n)\nX, Y = np.meshgrid(axis,axis)   # 2D Cartesian meshes of x,y coordinates\ninitialState = (1-Y)*np.sin(np.pi*X) + Y*(np.sin(2*np.pi*X))**2\nfinalState =   (1-X)*np.sin(np.pi*Y) + X*(np.sin(2*np.pi*Y))**2\nf = np.zeros((ntime,n,n))\nfor t in range(ntime):\n    z = (t+0.5) / ntime   # dimensionless time from 0 to 1\n    f[t,:,:] = (1-z)*initialState + z*finalState\n\ncoords = {\"time\": time, \"x\": axis, \"y\": axis}\ntemp = xr.DataArray(\n    20 + f,       # this 2D array varies in time from initialState to finalState\n    dims=(\"time\",\"y\",\"x\"),\n    coords=coords\n)\npres = xr.DataArray(   # random 2D array\n    100 + 10*np.random.randn(ntime,n,n),\n    dims=(\"time\",\"y\",\"x\"),\n    coords=coords\n)\nds = xr.Dataset({\"temperature\": temp, \"pressure\": pres})\nds.sel(time=\"2020-03-15\").temperature.plot(size=8)   # temperature distribution on a specific date\nds.to_netcdf(\"evolution.nc\")\nThe file evolution.nc should be 100^2 x 2 variables x 8 bytes x 91 steps = 14MB. We can load it into ParaView and play back the pressure and temperature!",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#climate-and-forecast-cf-netcdf-convention-in-spherical-geometry",
    "href": "python2/python-14-xarray.html#climate-and-forecast-cf-netcdf-convention-in-spherical-geometry",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Climate and forecast (CF) NetCDF convention in spherical geometry",
    "text": "Climate and forecast (CF) NetCDF convention in spherical geometry\nSo far we’ve been working with datasets in Cartesian coordinates. How about spherical geometry – how do we initialize and store a dataset in spherical coordinates (lon - lat - elevation)? It turns out this is very easy: 1. define these coordinates and your data arrays on top of these coordinates, 1. put everything into an xarray dataset, and 1. finally specify the following units:\nds.lat.attrs[\"units\"] = \"degrees_north\"   # this line is important to adhere to CF convention\nds.lon.attrs[\"units\"] = \"degrees_east\"    # this line is important to adhere to CF convention\n\n\n\n\n\n\nCautionExercise 14.2\n\n\n\n\n\nLet’s do it! Create a small (one-degree horizontal + some vertical resolution), stationary (no time dependency) dataset in spherical geometry with one 3D variable and write it to spherical.nc. Load it into ParaView to make sure the geometry is spherical.",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-14-xarray.html#working-with-atmospheric-data",
    "href": "python2/python-14-xarray.html#working-with-atmospheric-data",
    "title": "Multi-D arrays and datasets with xarray",
    "section": "Working with atmospheric data",
    "text": "Working with atmospheric data\nLet’s take a look at some real (but very low-resolution) data stored in the NetCDF-CF convention. Preparing for this workshop, I took one of the ECCC (Environment and Climate Change Canada) historical model datasets that contains only the near-surface air temperature and that was published on the CMIP6 Data-Archive. I reduced its size, picking only a subset of timesteps:\n# FYI - here is how I created a smaller dataset\nimport xarray as xr\ndata = xr.open_dataset('/Users/razoumov/tmp/xarray/atmosphere/tas_Amon_CanESM5_historical_r1i1p2f1_gn_185001-201412.nc')\ndata.sel(time=slice('2001', '2020')).to_netcdf(\"tasReduced.nc\")   # last 168 steps\nLet’s download the file tasReduced.nc in the terminal:\nwget http://bit.ly/atmosdata -O tasReduced.nc\nFirst, quickly check this dataset in ParaView (use Dimensions = (lat,lon)).\ndata = xr.open_dataset('tasReduced.nc')\ndata   # this is a time-dependent 2D dataset: print out the metadata, coordinates, data variables\ndata.time         # time goes monthly from 2001-01-16 to 2014-12-16\ndata.time.shape   # there are 168 timesteps\ndata.tas          # metadata for the data variable (time: 168, lat: 64, lon: 128)\ndata.tas.shape    # (168, 64, 128) = (time, lat, lon)\ndata.height       # at the fixed height=2m\nThese five lines all produce the same result:\ndata.tas[0] - 273.15   # take all values at data.time[0], convert to Celsius\ndata.tas[0,:] - 273.15\ndata.tas[0,:,:] - 273.15\ndata.tas.isel(time=0) - 273.15\nair = data.tas.sel(time='2001-01-16') - 273.15\nThese two lines produce the same result (1D vector of temperatures as a function of longitude):\ndata.tas[0,5]\ndata.tas.isel(time=0, lat=5)\nCheck temperature variation in the last timestep:\nair = data.tas.isel(time=-1) - 273.15   # last timestep, to celsius\nair.shape    # (64, 128)\nair.min(), air.max()   # -43.550903, 36.82956\nSelecting data is slightly more difficult with approximate floating coordinates:\ndata.tas.lat\ndata.tas.lat.dtype\ndata.tas.isel(lat=0)    # the first value lat=-87.86\ndata.lat[0]   # print the first latitude and try to use it below\ndata.tas.sel(lat=-87.86379884)    # does not work due to floating precision\ndata.tas.sel(lat=data.lat[0])     # this works\nlatSlice = data.tas.sel(lat=slice(-90,-80))    # only select data in a slice lat=[-90,-80]\nlatSlice.shape    # (168, 3, 128) - 3 latitudes in this slice\nMultiple ways to select time:\ndata.time[-10:]   # last ten times\nair = data.tas.sel(time='2014-12-16') - 273.15    # last date\nair = data.tas.sel(time='2014') - 273.15    # select everything in 2014\nair.shape     # 12 steps\nair.time\nair = data.tas.sel(time='2014-01') - 273.15    # select everything in January 2014\nAggregate functions:\nmeanOverTime = data.tas.mean(dim='time') - 273.15\nmeanOverSpace = data.tas.mean(dim=['lat','lon']) - 273.15     # mean over space for each timestep\nmeanOverSpace.shape     # time series (168,)\nmeanOverSpace.plot(marker=\"o\", size=8)     # calls matplotlib.pyplot.plot\nInterpolate to a specific location:\nvictoria = data.tas.interp(lat=48.43, lon=360-123.37) - 273.15\nvictoria.shape                      # (168,) only time\nvictoria.plot(marker=\"o\", size=8)   # simple 1D plot\nvictoria.sel(time=slice('2010','2020')).plot(marker=\"o\", size=8)   # zoom in on the 2010s points\nLet’s plot in 2D:\nair = data.tas.isel(time=-1) - 273.15   # last timestep\nair.time\nair.plot(size=8)     # 2D plot, very poor resolution (lat: 64, lon: 128)\nair.plot(size=8, y=\"lon\", x=\"lat\")     # can specify which axis is which\nWhat if we have time-dependency in the plot? We put each frame into a separate panel:\na = data.tas[-6:] - 273.15      # last 6 timesteps =&gt; 3D dataset =&gt; which coords to use for what?\na.plot(x=\"lon\", y=\"lat\", col=\"time\", col_wrap=3)\nBreaking into groups and applying a function to each group:\nlen(data.time)     # 168 steps\ndata.tas.groupby(\"time\")   # 168 groups\ndef standardize(x):\n    return (x - x.mean()) / x.std()\nstandard = data.tas.groupby(\"time\").map(standardize)   # apply this function to each group\nstandard.shape    # (1980, 64, 128) same shape as the original but now normalized over each group",
    "crumbs": [
      "MAIN PART",
      "Multidimensional labeled arrays and datasets with xarray"
    ]
  },
  {
    "objectID": "python2/python-11-numpy.html",
    "href": "python2/python-11-numpy.html",
    "title": "NumPy",
    "section": "",
    "text": "As you saw before, Python is not statically typed, i.e. variables can change their type on the fly:\nThis makes Python very flexible. Out of these variables you can form 1D lists, and these can be inhomogeneous and can change values and types on the fly:\nPython lists are very general and flexible, which is great for high-level programming, but it comes at a cost. The Python interpreter can’t make any assumptions about what will come next in a list, so it treats everything as a generic object with its own type and size. As lists get longer, eventually performance takes a hit.\nPython does not have any mechanism for a uniform/homogeneous list, where – to jump to element #1000 – you just take the memory address of the very first element and then increment it by (element size in bytes) x 999. NumPy library fills this gap by adding the concept of homogenous collections to python – numpy.ndarrays – which are multidimensional, homogeneous arrays of fixed-size items (most commonly numbers).\nLists and NumPy arrays behave very differently:",
    "crumbs": [
      "MAIN PART",
      "Numpy"
    ]
  },
  {
    "objectID": "python2/python-11-numpy.html#working-with-mathematical-arrays-in-numpy",
    "href": "python2/python-11-numpy.html#working-with-mathematical-arrays-in-numpy",
    "title": "NumPy",
    "section": "Working with mathematical arrays in NumPy",
    "text": "Working with mathematical arrays in NumPy\nNumPy arrays have the following attributes:\n\nndim = the number of dimensions\nshape = a tuple giving the sizes of the dimensions\nsize = the total number of elements\ndtype = the data type\nitemsize = the size (bytes) of individual elements\nnbytes = the total memory (bytes) occupied by the ndarray\nstrides = tuple of bytes to step in each dimension when traversing an array\ndata = memory address of the array\n\na = np.arange(10)      # 10 integer elements 0..9\na.ndim      # 1\na.shape     # (10,)\na.nbytes    # 80\na.dtype     # dtype('int64')\nb = np.arange(10, dtype=np.float)\nb.dtype     # dtype('float64')\nIn NumPy there are many ways to create arrays:\nnp.arange(11,20)               # 9 integer elements 11..19\nnp.linspace(0, 1, 100)         # 100 numbers uniformly spaced between 0 and 1 (inclusive)\nnp.linspace(0, 1, 100).shape\nnp.zeros(100, dtype=np.int)    # 1D array of 100 integer zeros\nnp.zeros((5, 5), dtype=np.float64)     # 2D 5x5 array of floating zeros\nnp.ones((3,3,4), dtype=np.float64)     # 3D 3x3x4 array of floating ones\nnp.eye(5)            # 2D 5x5 identity/unit matrix (with ones along the main diagonal)\nYou can create random arrays:\nnp.random.randint(0, 10, size=(4,5))    # 4x5 array of random integers in the half-open interval [0,10)\nnp.random.random(size=(4,3))            # 4x3 array of random floats in the half-open interval [0.,1.)\nnp.random.rand(3, 3)       # 3x3 array drawn from a uniform [0,1) distribution\nnp.random.randn(3, 3)      # 3x3 array drawn from a normal (Gaussian with x0=0, sigma=1) distribution",
    "crumbs": [
      "MAIN PART",
      "Numpy"
    ]
  },
  {
    "objectID": "python2/python-11-numpy.html#indexing-slicing-and-reshaping",
    "href": "python2/python-11-numpy.html#indexing-slicing-and-reshaping",
    "title": "NumPy",
    "section": "Indexing, slicing, and reshaping",
    "text": "Indexing, slicing, and reshaping\nFor 1D arrays:\na = np.linspace(0,1,100)\na[0]        # first element\na[-2]       # 2nd to last element\na[5:12]     # values [5..12), also a NumPy array\na[5:12:3]   # every 3rd element in [5..12), i.e. elements 5,8,11\na[::-1]     # array reversed\nSimilarly, for multi-dimensional arrays:\nb = np.reshape(np.arange(100),(10,10))      # form a 10x10 array from 1D array\nb[0:2,1]      # first two rows, second column\nb[:,-1]       # last column\nb[-1,:]       # last row\nb[5:7,5:7]    # 2x2 block\nConsider two rows:\na = np.array([1, 2, 3, 4])\nb = np.array([4, 3, 2, 1])\nnp.vstack((a,b))   # stack them vertically into a 2x4 array (use a,b as rows)\nnp.hstack((a,b))   # stack them horizontally into a 1x8 array\nnp.column_stack((a,b))         # use a,b as columns\nnp.vstack((a,b)).transpose()   # same result",
    "crumbs": [
      "MAIN PART",
      "Numpy"
    ]
  },
  {
    "objectID": "python2/python-11-numpy.html#vectorized-functions-on-array-elements-universal-functions",
    "href": "python2/python-11-numpy.html#vectorized-functions-on-array-elements-universal-functions",
    "title": "NumPy",
    "section": "Vectorized functions on array elements (universal functions)",
    "text": "Vectorized functions on array elements (universal functions)\nA big reason for using NumPy is that you can do fast numerical operations on a large number of elements of an array, without explicit for/while Python loops. The result is another ndarray.\na = np.arange(100)\na**2             # each element is a square of the corresponding element of a\nnp.log10(a+1)    # apply this operation to each element\n(a**2+a)/(a+1)   # the result should effectively be a floating-version copy of a\nnp.arange(10) / np.arange(1,11)  # this is np.array([ 0/1, 1/2, 2/3, 3/4, ..., 9/10 ])\n\n\n\n\n\n\nCautionQuestion 11.1\n\n\n\n\n\nLet’s verify the expression  using summation of elements of an ndarray.\nHint: Start with the first 10 terms k = np.arange(1,11). Then try the first 30 terms.\n\n\n\n\nArray broadcasting\nAn extremely useful feature of universal functions is the ability to operate between arrays of different sizes and shapes, a set of operations known as broadcasting.\na = np.array([0, 1, 2])    # 1D array\nb = np.ones((3,3))         # 2D array\na + b          # `a` is stretched/broadcast across the 2nd dimension before addition;\n               # effectively we add `a` to each row of `b`\nIn the following example both arrays are broadcast from 1D to 2D to match the shape of the other:\na = np.arange(3)                     # 1D row;                a.shape is (3,)\nb = np.arange(3).reshape((3,1))      # effectively 1D column; b.shape is (3, 1)\na + b                                # the result is a 2D array!\nNumPy’s broadcast rules are:\n\nthe shape of an array with fewer dimensions is padded with 1’s on the left\nany array with shape equal to 1 in that dimension is stretched to match the other array’s shape\nif in any dimension the sizes disagree and neither is equal to 1, an error is raised\n\nFirst example above:\n********************\na: (3,)   -&gt;  (1,3)  -&gt;  (3,3)\nb: (3,3)  -&gt;  (3,3)  -&gt;  (3,3)\n                                -&gt;  (3,3)\nSecond example above:\n*********************\na: (3,)  -&gt;  (1,3)  -&gt;  (3,3)\nb: (3,1) -&gt;  (3,1)  -&gt;  (3,3)\n                                -&gt;  (3,3)\nExample 3:\n**********\na: (2,3)  -&gt;  (2,3)  -&gt;  (2,3)\nb: (3,)   -&gt;  (1,3)  -&gt;  (2,3)\n                                -&gt;  (2,3)\nExample 4:\n**********\na: (3,2)  -&gt;  (3,2)  -&gt;  (3,2)\nb: (3,)   -&gt;  (1,3)  -&gt;  (3,3)\n                                -&gt;  error\n\"ValueError: operands could not be broadcast together with shapes (3,2) (3,)\"\nLet’s use broadcasting to plot a 2D function with matplotlib:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,12))\nx = np.linspace(0, 5, 50)\ny = np.linspace(0, 5, 50).reshape(50,1)\nz = np.sin(x)**8 + np.cos(5+x*y)*np.cos(x)    # broadcast in action!\nplt.imshow(z)\nplt.colorbar(shrink=0.8)\n\n\n\n\n\n\nCautionQuestion Building a 3D array\n\n\n\n\n\nUse NumPy broadcasting to build a 3D array from three 1D ones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion Converting velocity components\n\n\n\n\n\nThis is a take-home exercise. Consider the following (inefficient) Python code that converts the spherical velocity components to the Cartesian velocity components on a \\(500\\times 300\\times 800\\) spherical grid:\n#!/usr/bin/env python\nimport numpy as np\nfrom scipy.special import lpmv\nimport time\n\nnlat, nr, nlon = 500, 300, 800   # 120e6 grid points\n\nlatitude = np.linspace(-90, 90, nlat)\nradius = np.linspace(3485, 6371, nr)\nlongitude = np.linspace(0, 360, nlon)\n\n# spherical velocity components: use Legendre Polynomials to set values\nvlat = lpmv(0,3,latitude/90).reshape(nlat,1,1) + np.linspace(0,0,nr).reshape(nr,1) + np.linspace(0,0,nlon)\nvrad = np.linspace(0,0,nlat).reshape(nlat,1,1) + lpmv(0,3,(radius-4928)/1443).reshape(nr,1) + np.linspace(0,0,nlon)\nvlon = np.linspace(0,0,nlat).reshape(nlat,1,1) + np.linspace(0,0,nr).reshape(nr,1) + lpmv(0,2,longitude/180-1.)\n\n# Cartesian velocity components\nvx = np.zeros((nlat,nr,nlon))\nvy = np.zeros((nlat,nr,nlon))\nvz = np.zeros((nlat,nr,nlon))\n\nstart = time.time()\nfor i in range(nlat):\n    for j in range(nr):\n        for k in range(nlon):\n            vx[i,j,k] = - np.sin(np.radians(longitude[k]))*vlon[i,j,k] \\\n              - np.sin(np.radians(latitude[i]))*np.cos(np.radians(longitude[k]))*vlat[i,j,k] \\\n              + np.cos(np.radians(latitude[i]))*np.cos(np.radians(longitude[k]))*vrad[i,j,k]\n            vy[i,j,k] = np.cos(np.radians(longitude[k]))*vlon[i,j,k] \\\n              - np.sin(np.radians(latitude[i]))*np.sin(np.radians(longitude[k]))*vlat[i,j,k] \\\n              + np.cos(np.radians(latitude[i]))*np.sin(np.radians(longitude[k]))*vrad[i,j,k]\n            vz[i,j,k] = np.cos(np.radians(latitude[i]))*vlat[i,j,k] \\\n              + np.sin(np.radians(latitude[i]))*vrad[i,j,k]\nfinish = time.time()\nprint(\"It took\", finish - start, \"seconds\")\nUsing NumPy further, you can speed up the nested loop between the start = ... and finish = ... lines by at least a factor of 1,000X. If you achieve a significant speedup, please send us your solution at “training at westdri dot ca”.\n\n\n\n\n\nAggregate functions\nAggregate functions take an ndarray and reduce it along one (or more) axes. E.g., in 1D:\na = np.linspace(1, 2, 100)\na.mean()     # arithmetic mean\na.max()      # maximum value\na.argmax()   # index of the maximum value\na.sum()      # sum of all values\na.prod()     # product of all values\nOr in 2D:\nb = np.arange(25).reshape(5,5)\n&gt;&gt;&gt; b.sum()\n300\nb.sum(axis=0)   # add rows\nb.sum(axis=1)   # add columns\n\n\nBoolean indexing\na = np.linspace(1, 2, 100)\na &lt; 1.5       # array of True and/or False\na[a &lt; 1.5]    # will only return those elements that meet True condition\na[a &lt; 1.5].shape   # there are exactly 50 such elements\na.shape\nAn interesting question comes up: what will happen if we apply a mask to a multi-dimensional array? How will it show incomplete rows/columns that have both True and False masks?\nb = np.arange(25).reshape(5,5)   # 2D array\nb &gt; 22      # all rows are False, except for the last row [F,F,F,T,T]\nb[b &gt; 22]   # turns out we always get a 1D array with only True elements",
    "crumbs": [
      "MAIN PART",
      "Numpy"
    ]
  },
  {
    "objectID": "python2/python-11-numpy.html#more-numpy-functionality",
    "href": "python2/python-11-numpy.html#more-numpy-functionality",
    "title": "NumPy",
    "section": "More NumPy functionality",
    "text": "More NumPy functionality\nNumPy provides many standard linear algebra algorithms: matrix/vector products, decompositions, eigenvalues, solving linear equations, e.g.\na = np.random.randint(0, 10, size=(8,8))\nb = np.arange(1,9)\nx = np.linalg.solve(a, b)\nx\nnp.allclose(np.dot(a, x),b)    # check the solution",
    "crumbs": [
      "MAIN PART",
      "Numpy"
    ]
  },
  {
    "objectID": "python2/python-11-numpy.html#external-packages-built-on-top-of-numpy",
    "href": "python2/python-11-numpy.html#external-packages-built-on-top-of-numpy",
    "title": "NumPy",
    "section": "External packages built on top of NumPy",
    "text": "External packages built on top of NumPy\nA lot of other packages are built on top of NumPy. E.g., there is a Python package for analysis and visualization of 3D multi-resolution volumetric data called yt which is based on NumPy. Check out this visualization produced with yt.\nMany image-processing libraries use NumPy data structures underneath, e.g.\nimport skimage.io        # scikit-image is a collection of algorithms for image processing\nimage = skimage.io.imread(fname=\"https://raw.githubusercontent.com/razoumov/publish/master/grids.png\")\nimage.shape       # it's a 1024^2 image, with (R,G,B) channels\nLet’s plot this image using matplotlib:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.imshow(image[:,:,2], interpolation='nearest')\nplt.colorbar(orientation='vertical', shrink=0.75, aspect=50)\nUsing NumPy, you can easily manipulate pixels:\nimage[:,:,2] = 255 - image[:,:,2]\nand then rerun the previous (matplotlib) cell.\nAnother example of a package built on top of NumPy is pandas, for working with 2D tables. Going further, xarray was built on top of both NumPy and pandas. We will study pandas and xarray later in this workshop.",
    "crumbs": [
      "MAIN PART",
      "Numpy"
    ]
  },
  {
    "objectID": "python2/python-17-objects.html",
    "href": "python2/python-17-objects.html",
    "title": "Basics of object-oriented programming",
    "section": "",
    "text": "Object-oriented programming is a way of programming in which you bundle related variables and functions into objects, and then manipulate and use these objects as a whole. We already saw many examples of objects on Python – e.g. lists, dictionaries, numpy arrays, dataframes – that have both variables and methods inside them. In this section we will learn how to create our own objects.\nYou can think of:\nLet’s define out first class:\nLet’s define some instances of this class:\nInstances are guaranteed to have the attributes that we expect.\nLet’s add inside our class an instance method (with proper indentation):\nRedefine the class, and now:\nLet’s add another method (remember the indentation!):\nand now\nLet’s add another method (remember the indentation!):\nRedefine the class, and now:\nLet’s add our last method (remember the indentation!):\nRedefine the class, and now:\nImportant: As with any complex object in Python, assigning an instance to a new variable will simply create a pointer, i.e. if you modify one in place, you’ll see the change through the other one too:\nIf you want a separate copy:",
    "crumbs": [
      "ADDITIONAL MATERIAL",
      "Basics of object-oriented programming in Python"
    ]
  },
  {
    "objectID": "python2/python-17-objects.html#inherit-from-parent-classes",
    "href": "python2/python-17-objects.html#inherit-from-parent-classes",
    "title": "Basics of object-oriented programming",
    "section": "Inherit from parent classes",
    "text": "Inherit from parent classes\nLet’s create a child class Moon that would inherit the attributes and methods of Planet class:\nclass Moon(Planet):    # it inherits all the attributes and methods of the parent process\n    pass\n\nphobos = Moon(radius=22.2, mass=1.08e16)\ndeimos = Moon(radius=12.6, mass=2.0e15)\nphobos.g() / earth.g()        # 0.0001489\nisinstance(phobos, Moon)         # True\nisinstance(phobos, Planet)       # True - all objects of a child class are instances of the parent class\nisinstance(jupyter, Planet)      # True\nisinstance(jupyter, Moon)        # False\nissubclass(Moon,Planet)      # True\nChild classes can have their own attributes and methods that are distinct from (i.e. override) the parent class:\nclass Moon(Planet):\n    hostObject = 'Mars'\n    def g(self):\n        return 'too small to compute accurately'\n    \nphobos = Moon(radius=22.2, mass=1.08e16)\ndeimos = Moon(radius=12.6, mass=2.0e15)\nmars = Planet(3389.5,6.39e23)\nphobos.hostObject, mars.hostObject     # ('Mars', 'Sun')\nphobos.g(), mars.g()                   # ('too small to compute accurately', 371.1282569773226)\nOne thing to keep in mind about class inheritance is that changes to the parent class automatically propagate to child classes (when you follow the sequence of definitions), unless overridden in the child class:\nclass Parent:\n    ...\n    def __str__(self):\n        return \"Changed in the parent class\"\n    ...\n\nclass Moon(Planet):\n    hostObject = 'Mars'\n    def g(self):\n        return 'too small to compute accurately'\n\ndeimos = Moon(radius=12.6, mass=2.0e15)\nprint(deimos)            # prints \"Changed in the parent class\"\nYou can access the parent class namespace from inside a method of a child class by using super():\nclass Moon(Planet):\n    hostObject = 'Mars'\n    def parentHost(self):\n        return super().hostObject       # will return hostObject of the parent class\n\ndeimos = Moon(radius=12.6, mass=2.0e15)\ndeimos.hostObject, deimos.parentHost()     # ('Mars', 'Sun')",
    "crumbs": [
      "ADDITIONAL MATERIAL",
      "Basics of object-oriented programming in Python"
    ]
  },
  {
    "objectID": "python2/python-17-objects.html#generators",
    "href": "python2/python-17-objects.html#generators",
    "title": "Basics of object-oriented programming",
    "section": "Generators",
    "text": "Generators\nWe already saw that in Python you can loop over a collection using for:\nfor i in 'weather':\n    print(i)\nfor j in [5,6,7]:\n    print(j)\nBehind the scenes Python creates an iterator out of a collection. This iterator has a __next__() method, i.e. it does something like:\na = iter('weather')\na.__next__()    # 'w'\na.__next__()    # 'e'\na.__next__()    # 'a'\nYou can build your own iterator as if you were defining a function. Such function is called a generator in Python:\ndef cycle():\n    yield 1\n    yield 'hello'\n    yield [1,2,3]\n\n[i for i in cycle()]               # [1, 'hello', [1, 2, 3]]\n\ndef square(x):   # `x` is an input string in this generator\n    for letter in x:\n        yield int(letter)**2       # yields a sequence of numbers that you can cycle through\n\n[i for i in square('12345')]       # [1, 4, 9, 16, 25]\n\na = square('12345')\n[a.__next__() for i in range(3)]   # [1, 4, 9]",
    "crumbs": [
      "ADDITIONAL MATERIAL",
      "Basics of object-oriented programming in Python"
    ]
  },
  {
    "objectID": "wrf-container.html",
    "href": "wrf-container.html",
    "title": "Building instructions for a WRF container",
    "section": "",
    "text": "These instructions describe building an Apptainer image for WRF following the build script from https://github.com/Hos128/WRF-CMAQ-Installation.\nFirst, log in to the cluster, create a new temporary directory and from inside it start an interactive job:\nmkdir -p tmp && cd tmp\nmodule load apptainer/1.2.4\nsalloc --time=3:0:0 --mem-per-cpu=3600 --cpus-per-task=4 --account=def-...\nDownload the latest Ubuntu image and build an Apptainer sandbox from it:\napptainer build --sandbox wrf.dir docker://ubuntu\nNext, enter the sandbox with --fakeroot:\nunset APPTAINER_BIND\napptainer shell -e --fakeroot --writable wrf.dir\nInside, create the top-level installation directory and install some packages:\nmkdir -p /data && cd /data\napt-get update\napt-get -y install git make wget unzip emacs apt-utils   # answer few questions along the way\nDownload and fix the installation script:\ngit clone https://github.com/Hos128/WRF-CMAQ-Installation.git\ncd WRF-CMAQ-Installation\nsed -i.bak -e '200,214d' OfflineWRFCMAQ.sh\nsed -i.bak -e 's|export HOME=`cd;pwd`|export HOME=`cd /data;pwd`|' OfflineWRFCMAQ.sh\nsed -i.bak -e 's|echo $PASSWD \\| sudo -S ||' OfflineWRFCMAQ.sh\nsed -i.bak -e 's| python2 python2-dev||' OfflineWRFCMAQ.sh\nsed -i.bak -e 's| libncurses5||' OfflineWRFCMAQ.sh\nsed -i.bak -e 's| mlocate||' OfflineWRFCMAQ.sh\nsed -i.bak -e 's|apt -y install python3 python3-dev|apt -y install python3 python3-dev python3-pip|' OfflineWRFCMAQ.sh\nmkdir -p /root/.config/pip\ncat &lt;&lt; EOF &gt; /root/.config/pip/pip.conf\n[global]\ntrusted-host = pypi.org\n               pypi.python.org\n               pypi.org\n               pypi.co\n               files.pythonhosted.org\n               pip.pypa.io\nEOF\nsed -i.bak -e 's|pip3 install python-dateutil|pip3 install --break-system-packages python-dateutil|' OfflineWRFCMAQ.sh\nwget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: en-US,en;q=0.9,fa;q=0.8\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1NkmRAeG7w_LLhh_HDW_X39jZwGYw8mml&export=download&authuser=2&confirm=t&uuid=a4f92a10-cbd5-4064-bb83-d79567b60537&at=APZUnTWngzWjzHFwWurVdIQsFaAL:1711680004113\" -c -O 'Downloads.zip'\nunzip Downloads.zip\n/bin/rm -rf Downloads.zip /data/WRF\nsed -i.bak '/Downloads.zip/d' OfflineWRFCMAQ.sh\nsed -i.bak -e 's|mkdir $HOME/WRF|mkdir -p $HOME/WRF|' OfflineWRFCMAQ.sh\nsed -i.bak -e 's|mkdir $WRF_FOLDER/MET|mkdir -p $WRF_FOLDER/MET|' OfflineWRFCMAQ.sh\nawk '/Missing one or more/{for(x=NR-1;x&lt;=NR+2;x++)d[x];}{a[NR]=$0}END{for(i=1;i&lt;=NR;i++)if(!(i in d))print a[i]}' OfflineWRFCMAQ.sh &gt; new.sh && mv new.sh OfflineWRFCMAQ.sh\nchmod u+x OfflineWRFCMAQ.sh\nFinally, run it (this will take a very, very long time, as it downloads and builds 47GB of software - everything but the kitchen sink):\n./OfflineWRFCMAQ.sh 2&gt;&1 | tee OfflineWRFCMAQ.log\n   Do you want to continue? - yes\n   Would you like to download the WPS Geographical Input Data for Specific Applications? - no\n   Would you like to download the GEOG Optional WPS Geographical Input Data? - no\n/bin/rm -rf Downloads\nOne of the downloads is MPICH. Inside the script, we are installing it entirely inside the container, not relying on the host’s OpenMPI. This means that we’ll be limited to MPI runs on one node.\nNow exit the container and the job.\nNext, start a batch job to convert the sandbox into a SIF image:\ncd scratch\ntar xvf wrf.tar\ncat &lt;&lt; EOF &gt; submit.sh\n#!/bin/bash\n#SBATCH --time=10:0:0\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=3600\n#SBATCH --account=def-...\nmkdir -p \\$SLURM_TMPDIR/{tmp,cache}\nmodule load apptainer/1.2.4\nexport APPTAINER_TMPDIR=\\$SLURM_TMPDIR/tmp\nexport APPTAINER_CACHEDIR=\\$SLURM_TMPDIR/cache\napptainer build wrf.sif wrf.dir\nEOF\nsbatch submit.sh\nThe resulting image wrf.sif should be 8.1GB. To use it interactively from a job, do this:\ncd ~/scratch\nmodule load apptainer/1.2.4\nsalloc --time=1:0:0 --mem-per-cpu=3600 --account=def-...\napptainer shell wrf.sif"
  },
  {
    "objectID": "bash-menu.html",
    "href": "bash-menu.html",
    "title": "Introduction to Bash command line",
    "section": "",
    "text": "June 9th, 9:30am–12:30pm and 1:30pm-4:30pm Pacific Time\n\nThis is a hands-on introduction to the Linux command line and the interaction with a remote server. This course mixes Software Carpentry materials with our own custom training.\nInstructor: Alex Razoumov (SFU)\nPrerequisites: This is an introductory course, no previous experience is required. We will provide guest accounts to one of our Linux systems.\nSoftware: All attendees will need a remote secure shell (SSH) client installed on their computer in order to participate in the course exercises. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there).\n\nSolutions\nYou can find the solutions here.",
    "crumbs": [
      "Front page"
    ]
  },
  {
    "objectID": "pythonhpc.html",
    "href": "pythonhpc.html",
    "title": "Part 1: towards high-performance Python",
    "section": "",
    "text": "June 13th, 9:30am–12:30pm and 1:30pm-4:30pm Pacific Time\nAbstract: Python has become the dominant language in scientific computing thanks to its high-level syntax, extensive ecosystem, and ease of use. However, its performance often lags behind traditional compiled languages like C, C++, and Fortran, as well as newer contenders like Julia and Chapel. This course is designed to help you speed up your Python workflows using a variety of tools and techniques.\nWe’ll begin with classic optimization methods such as NumPy-based vectorization, and explore just-in-time compilation using Numba, along with performance profiling techniques. From there, we’ll delve into parallelization – starting with multithreading using external libraries like NumExpr and Python 3.13’s new free-threading capabilities – but placing greater emphasis on multiprocessing.\nNext, we’ll dive into Ray, a powerful and flexible framework for scaling Python applications. While Ray is widely used in AI, our focus will be on its core capabilities for distributed computing and data processing. You’ll learn how to parallelize CPU-bound numerical workflows – with and without reduction – as well as optimize I/O-bound tasks. We’ll also explore combining Ray with Numba and will discuss coding tightly coupled parallel problems.\nPlease note: this course does not cover GPU computing (which merits its own course), nor will we dive into mpi4py, the standard MPI library for Python."
  },
  {
    "objectID": "pythonhpc.html#installation",
    "href": "pythonhpc.html#installation",
    "title": "Part 1: towards high-performance Python",
    "section": "Installation",
    "text": "Installation\nToday we’ll be running Python on the training cluster. If instead you want to run everything locally on your computer, you can install Python and the libraries, but the installation instructions will vary depending on how you typically install Python, and whether/how you use Python virtual environments. Here is what I did on my computer (with pyenv installed earlier):\npyenv install 3.12.9\npyenv virtualenv 3.12.9 hpc-env   # goes into ~/.pyenv/versions/&lt;version&gt;/envs/hpc-env\npyenv activate hpc-env\npip install --upgrade pip\npip install numpy\npip install --upgrade \"ray[default]\"\npip install --upgrade \"ray[data]\"\npip install tqdm netcdf4 scipy numexpr psutil multiprocess numba scalene Pillow\n...\npyenv deactivate\n\n\nOn a production HPC cluster:\nmodule load python/3.12.4 arrow/19.0.1 scipy-stack/2025a netcdf/4.9.2\nvirtualenv --no-download hpc-env\nsource hpc-env/bin/activate\npip install --no-index --upgrade pip\npip install --no-index numba multiprocess numexpr\navail_wheels \"ray\"\npip install --no-index ray tqdm scalene grpcio\npip install modin\n...\ndeactivate\nIn this course we will be doing all work on our training cluster on which we have already installed Python and all the necessary libraries."
  },
  {
    "objectID": "pythonhpc.html#introduction",
    "href": "pythonhpc.html#introduction",
    "title": "Part 1: towards high-performance Python",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\nPython pros\nPython cons\n\n\n\n\nelegant scripting language\nslow (interpreted, dynamically typed)\n\n\neasy (fast) to develop and read code\nuses indentation for code blocks\n\n\npowerful, compact constructs for many tasks\n\n\n\nhuge number of external libraries\n\n\n\nvery popular across all fields\n\n\n\n\n\n\n\nitem1\n\n\nitem2\n\n\n\nPython is interpreted: - translation to machine code happens line-by-line - no cross-line optimization\nPython is dynamically typed: - types are part of the data (since data collections can be very diverse)  ⇒  significant overhead - automatic memory management: on-the-fly reference counting, garbage collection, range checking  ⇒  slow\nAll these high-level features make Python slow. In this course we will concentrate on getting the most performance out of it."
  },
  {
    "objectID": "pythonhpc.html#python-setup-in-our-course",
    "href": "pythonhpc.html#python-setup-in-our-course",
    "title": "Part 1: towards high-performance Python",
    "section": "Python setup in our course",
    "text": "Python setup in our course\nToday we’ll be running Python inside a shell on our training cluster thu.vastcloud.org. Let’s log in now!\nWe have pre-installed all the required libraries for you in a virtual Python environment in /project/def-sponsor00/shared/hpc-env that everyone on the system can read.\nOnce on the system, our workflow is going to be:\nmkdir -p ~/tmp && cd ~/tmp\nmodule load python/3.12.4 arrow/19.0.1 scipy-stack/2025a netcdf/4.9.2\nsource /project/def-sponsor00/shared/hpc-env/bin/activate\n\nsalloc --time=2:00:0 --mem-per-cpu=12000   # memory needed for several serial examples\n...\nexit\n\nsalloc --cpus-per-task=4 --time=2:00:0 --mem-per-cpu=3600   # our default mode\n# sometimes we'll use 4 cores and few GBs memory per core\n# sometimes we'll use one core and almost all available memory\n...\nexit\n\nsalloc --ntasks=4 --nodes=1 --time=2:00:0 --mem-per-cpu=3600   # very similar\n...\nexit\n\nsalloc --ntasks=4 --time=2:00:0 --mem-per-cpu=3600   # might run on multiple nodes\n...\nexit\n\ndeactivate\nPlease do not run Python on the login node, as it’ll make the login node slow for everyone.\nFinally, to monitor CPU usage inside a Slurm job, from the login node connect to the running job and launch htop or a similar utility:\nsrun --jobid=&lt;jobID&gt; --pty bash\nalias htop='htop -u $USER -s PERCENT_CPU'\nhtop                     # monitor all your processes\nhtop --filter \"python\"   # filter processes by name\nor you can use a one-liner from the login node:\nsrun --jobid=&lt;jobID&gt; --pty htop -u $USER -s PERCENT_CPU\nInside htop you can repeatedly press “Shift+H” to show individual threads or group them inside their parent processes."
  },
  {
    "objectID": "pythonhpc.html#slow-series",
    "href": "pythonhpc.html#slow-series",
    "title": "Part 1: towards high-performance Python",
    "section": "Slow series",
    "text": "Slow series\nWhen I teach parallel computing in other languages (Julia, Chapel), the approach is to take a numerical problem and parallelize it using multiple processors, and concentrate on various issues and bottlenecks (variable locks, load balancing, false sharing, messaging overheads, etc.) that lead to less than 100% parallel efficiency. For the numerical problem I usually select something that is very simple to code, yet forces the computer to do brute-force calculation that cannot be easily optimized.\nOne such problem is a slow series. It is a well-known fact that the harmonic series \\(\\sum\\limits_{k=1}^\\infty{1\\over k}\\) diverges. It turns out that if we omit the terms whose denominators in decimal notation contain any digit or string of digits, it converges, albeit very slowly (Schmelzer & Baillie 2008), e.g.\n\nBut this slow convergence is actually good for us: our answer will be bounded by the exact result (22.9206766192…) on the upper side, and we will force the computer to do CPU-intensive work. We will sum all the terms whose denominators do not contain the digit “9”, and evaluate the first \\(10^8\\) terms.\nI implemented and timed this problem running in serial in Julia (356ms) and Chapel (300ms) – both are compiled languages. Here is one possible Python implementation:\nfrom time import time\ndef slow(n: int):\n    total = 0\n    for i in range(1,n+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    return total\n\nstart = time()\ntotal = slow(100_000_000)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nLet’s save this code inside the file slowSeries.py and run it. Depending on the power supplied to my laptop’s CPU (which I find varies quite a bit depending on the environment), I get the average runtime of 6.625 seconds. That’s ~20X slower than in Julia and Chapel!\nNote that for other problems you will likely see an even bigger (100-200X) gap between Python and the compiled languages. In other languages looking for a substring in a decimal representation of a number takes a while, and there you will want to code this calculation using integer numbers. If we also do this via integer operations in Python:\ndef digitsin(num: int):\n    base = 10\n    while 9//base &gt; 0: base *= 10\n    while num &gt; 0:\n        if num%base == 9: return True\n        num = num//10\n    return False\n\ndef slow(n: int):\n    total = 0\n    for i in range(1,n+1):\n        if not digitsin(i):\n            total += 1.0 / i\n    return total\nour code will be ~3-4X slower, so we will use the first version of the code with if not \"9\" in str(i) – it turns out that in this particular case Python’s high-level substring search is actually quite well optimized!\n\n\n\n\nLooking at other problems, you will see that Python’s performance is worse on “tightly coupled” calculations on fundamental data types (integers, doubles), e.g. when you try to run the same arithmetic calculation on many elements of an array which is often the case in numerical simulation.\nOn the other hand, Python performs much better (or “less worse” compared to other languages) when doing file I/O and text processing. In addition, if your Python code spends most of its time in a compiled numerical library (often written in C or C++, and more recently in languages like Rust), your overall performance might be not that bad.\nWe will come back to the slow-series problem, but let’s first take a look at speeding up Python with NumPy."
  },
  {
    "objectID": "pythonhpc.html#numpy-vectorization",
    "href": "pythonhpc.html#numpy-vectorization",
    "title": "Part 1: towards high-performance Python",
    "section": "NumPy vectorization",
    "text": "NumPy vectorization\nPython is dynamically typed, i.e. variables can change their type on the fly:\na = 5\na = 'apple'\nprint(a)\nThis makes Python very flexible. Out of these variables you can form 1D lists, and these can be inhomogeneous and can change values and types on the fly:\na = [1, 2, 'Vancouver', ['Earth', 'Moon'], {'list': 'an ordered collection of values'}]\na[1] = 'Sun'\na\nPython lists are very general and flexible, which is great for high-level programming, but it comes at a cost. The Python interpreter can’t make any assumptions about what will come next in a list, so it treats everything as a generic object with its own type and size. As lists get longer, eventually performance takes a hit.\nPython does not have any mechanism for a uniform/homogeneous list, where – to jump to element #1000 – you just take the memory address of the very first element and then increment it by (element size in bytes) x 999. NumPy library fills this gap by adding the concept of homogenous collections to python – numpy.ndarrays – which are multidimensional, homogeneous arrays of fixed-size items (most commonly numbers).\n\nThis brings large performance benefits!\n\n\nno reading of extra bits (type, size, reference count)\nno type checking\ncontiguous allocation in memory\nNumPy was written in C  ⇒  pre-compiled\n\n\nNumPy lets you work with mathematical arrays.\n\nLists and NumPy arrays behave very differently:\na = [1, 2, 3, 4]\nb = [5, 6, 7, 8]\na + b              # this will concatenate two lists: [1,2,3,4,5,6,7,8]\nimport numpy as np\nna = np.array([1, 2, 3, 4])\nnb = np.array([5, 6, 7, 8])\nna + nb            # this will sum two vectors element-wise: array([6,8,10,12])\nna * nb            # element-wise product\n\nVectorized functions on array elements\nOne of the big reasons for using NumPy is so you can do fast numerical operations on a large number of elements. The result is another ndarray. In many calculations you can use replace the usual for/while loops with functions on NumPy elements.\na = np.arange(100)\na**2          # each element is a square of the corresponding element of a\nnp.log10(a+1)     # apply this operation to each element\n(a**2+a)/(a+1)    # the result should effectively be a floating-version copy of a\nnp.arange(10) / np.arange(1,11)  # this is np.array([ 0/1, 1/2, 2/3, 3/4, ..., 9/10 ])\n\n\nArray broadcasting\nAn extremely useful feature of vectorized functions is their ability to operate between arrays of different sizes and shapes, a set of operations known as broadcasting.\na = np.array([0, 1, 2])    # 1D array\nb = np.ones((3,3))         # 2D array\na + b          # `a` is stretched/broadcast across the 2nd dimension before addition;\n               # effectively we add `a` to each row of `b`\nIn the following example both arrays are broadcast from 1D to 2D to match the shape of the other:\na = np.arange(3)                     # 1D row;                a.shape is (3,)\nb = np.arange(3).reshape((3,1))      # effectively 1D column; b.shape is (3, 1)\na + b                                # the result is a 2D array!\nNumPy’s broadcast rules are:\n\nthe shape of an array with fewer dimensions is padded with 1’s on the left\nany array with shape equal to 1 in that dimension is stretched to match the other array’s shape\nif in any dimension the sizes disagree and neither is equal to 1, an error is raised\n\nFirst example above:\n********************\na: (3,)   -&gt;  (1,3)  -&gt;  (3,3)\nb: (3,3)  -&gt;  (3,3)  -&gt;  (3,3)\n                                -&gt;  (3,3)\nSecond example above:\n*********************\na: (3,)  -&gt;  (1,3)  -&gt;  (3,3)\nb: (3,1) -&gt;  (3,1)  -&gt;  (3,3)\n                                -&gt;  (3,3)\nExample 3:\n**********\na: (2,3)  -&gt;  (2,3)  -&gt;  (2,3)\nb: (3,)   -&gt;  (1,3)  -&gt;  (2,3)\n                                -&gt;  (2,3)\nExample 4:\n**********\na: (3,2)  -&gt;  (3,2)  -&gt;  (3,2)\nb: (3,)   -&gt;  (1,3)  -&gt;  (3,3)\n                                -&gt;  error\n\"ValueError: operands could not be broadcast together with shapes (3,2) (3,)\"\nLet’s see how these rules work on a real-life problem!\n\n\nConverting velocity components\nConsider a spherical dataset describing Earth’s mantle convection, defined on a spherical grid \\(n_{lat}\\times\nn_{lon}\\times n_r = 500\\times 800\\times 300\\), with the total of \\(120\\times10^6\\) grid points. For each grid point, we need to convert from the spherical (lateral - longitudinal - radial) velocity components to their Cartesian equivalents.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo run all code fragments in this example at full \\(500\\times 800\\times 300\\) resolution, you will need to increase your memory ask to 7200M, or otherwise your Python session will be killed mid-way. Alternatively, you can reduce the problem size by 8X (yielding much less impressive runtime difference) and continue running with 3600M memory:\nnlat, nlon, nr = nlat//2, nlon//2, nr//2   # if short on memory\n\n\nLet’s initialize our problem:\nimport numpy as np\nfrom scipy.special import lpmv\nimport time\n\nnlat, nlon, nr = 500, 800, 300   # 120e6 grid points\n\nlatitude = np.linspace(-90, 90, nlat)\nlongitude = np.linspace(0, 360, nlon)\nradius = np.linspace(3485, 6371, nr)\n\n# spherical velocity components: use Legendre Polynomials to set values\nvlat = lpmv(0,3,latitude/90).reshape(nlat,1,1) + np.linspace(0,0,nr).reshape(nr,1) + np.linspace(0,0,nlon)\nvlon = np.linspace(0,0,nlat).reshape(nlat,1,1) + np.linspace(0,0,nr).reshape(nr,1) + lpmv(0,2,longitude/180-1.)\nvrad = np.linspace(0,0,nlat).reshape(nlat,1,1) + lpmv(0,3,(radius-4928)/1443).reshape(nr,1) + np.linspace(0,0,nlon)\n\n# Cartesian velocity components\nvx = np.zeros((nlat,nr,nlon))\nvy = np.zeros((nlat,nr,nlon))\nvz = np.zeros((nlat,nr,nlon))\nWe need to go through all grid points, and at each point perform a matrix-vector multiplication. Here is our first attempt:\n\n\n\n\n\n\nNote\n\n\n\nYou might want to use Python’s tqdm library to provide a progress bar for real-time runtime estimate.\n\n\nfrom tqdm import tqdm\nstart = time.time()\nfor i in tqdm(range(nlat)):\n    for k in range(nlon):\n        for j in range(nr):\n            vx[i,j,k] = - np.sin(np.radians(longitude[k]))*vlon[i,j,k] - \\\n                np.sin(np.radians(latitude[i]))*np.cos(np.radians(longitude[k]))*vlat[i,j,k] + \\\n                np.cos(np.radians(latitude[i]))*np.cos(np.radians(longitude[k]))*vrad[i,j,k]\n            vy[i,j,k] = np.cos(np.radians(longitude[k]))*vlon[i,j,k] - \\\n                np.sin(np.radians(latitude[i]))*np.sin(np.radians(longitude[k]))*vlat[i,j,k] + \\\n                np.cos(np.radians(latitude[i]))*np.sin(np.radians(longitude[k]))*vrad[i,j,k]\n            vz[i,j,k] = np.cos(np.radians(latitude[i]))*vlat[i,j,k] + \\\n                np.sin(np.radians(latitude[i]))*vrad[i,j,k]\n\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\nOn my laptop this calculation took 1280s. There are quite a lot of redundancies (repeated calculations), e.g. we compute all angles to radians multiple times, compute \\(\\sin\\) and \\(\\cos\\) of the same latitude and longitude multiple times, and so on. Getting rid of these redundancies:\nfrom tqdm import tqdm\nstart = time.time()\nfor i in tqdm(range(nlat)):\n    lat = np.radians(latitude[i])\n    sinlat = np.sin(lat)\n    coslat = np.cos(lat)\n    for k in range(nlon):\n        lon = np.radians(longitude[k])\n        sinlon = np.sin(lon)\n        coslon = np.cos(lon)\n        for j in range(nr):\n            vx[i,j,k] = - sinlon*vlon[i,j,k] - sinlat*coslon*vlat[i,j,k] + coslat*coslon*vrad[i,j,k]\n            vy[i,j,k] = coslon*vlon[i,j,k] - sinlat*sinlon*vlat[i,j,k] + coslat*sinlon*vrad[i,j,k]\n            vz[i,j,k] = coslat*vlat[i,j,k] + sinlat*vrad[i,j,k]\n\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\nbrings the runtime down to 192s.\n\n\n\n\n\n\nCautionQuestion 1\n\n\n\n\n\nDoes using NumPy’s matrix-vector multiplication function np.dot speed this calculation? In this workflow, at each latitude-longitude you would define a rotation matrix and at each point a spherical velocity vector to compute their dot product, i.e. you would replace this fragment:\n        for j in range(nr):\n            vx[i,j,k] = - sinlon*vlon[i,j,k] - sinlat*coslon*vlat[i,j,k] + coslat*coslon*vrad[i,j,k]\n            vy[i,j,k] = coslon*vlon[i,j,k] - sinlat*sinlon*vlat[i,j,k] + coslat*sinlon*vrad[i,j,k]\n            vz[i,j,k] = coslat*vlat[i,j,k] + sinlat*vrad[i,j,k]\nwith this one:\n        rot = np.array([[-sinlon, -sinlat*coslon, coslat*coslon],\n                        [coslon, -sinlat*sinlon, coslat*sinlon],\n                        [0, coslat, sinlat]])\n        for j in range(nr):\n            vspherical = np.array([vlon[i,j,k], vlat[i,j,k], vrad[i,j,k]])\n            vx[i,j,k], vy[i,j,k], vz[i,j,k] = np.dot(rot, vspherical)\n\n\n\n\nTo speed up our computation, we should vectorize over one of the dimensions, e.g. longitudes:\nfrom tqdm import tqdm\nstart = time.time()\nlon = np.radians(longitude[0:nlon])\nsinlon = np.sin(lon)\ncoslon = np.cos(lon)\nfor i in tqdm(range(nlat)):\n    lat = np.radians(latitude[i])\n    sinlat = np.sin(lat)\n    coslat = np.cos(lat)\n    for j in range(nr):\n        vx[i,j,0:nlon] = - sinlon*vlon[i,j,0:nlon] - sinlat*coslon*vlat[i,j,0:nlon] + coslat*coslon*vrad[i,j,0:nlon]\n        vy[i,j,0:nlon] = coslon*vlon[i,j,0:nlon] - sinlat*sinlon*vlat[i,j,0:nlon] + coslat*sinlon*vrad[i,j,0:nlon]\n        vz[i,j,0:nlon] = coslat*vlat[i,j,0:nlon] + sinlat*vrad[i,j,0:nlon]\n\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\nLet’s see how broadcasting works in here. Consider the dimensions of all variables in the expression for computing vx[i,j,0:nlon]:\n[1,1,nlon] = - [nlon]*[1,1,nlon] - [1]*[nlon]*[1,1,nlon] + [1]*[nlon]*[1,1,nlon]\nOmitting scalars ([1]), padding with 1’s on the left will produce:\n[1,1,nlon] = - [1,1,nlon]*[1,1,nlon] - [1,1,nlon]*[1,1,nlon] + [1,1,nlon]*[1,1,nlon]\nNow all variables have the same dimensions, and all operations will be element-wise. The calculation time is now 3.503s, which is a huge improvement!\nVectorizing over two dimensions, e.g. over radii and longitudes leaves us with a single loop:\nfrom tqdm import tqdm\nstart = time.time()\nlon = np.radians(longitude[0:nlon])\nsinlon = np.sin(lon)\ncoslon = np.cos(lon)\nfor i in tqdm(range(nlat)):\n    lat = np.radians(latitude[i])\n    sinlat = np.sin(lat)\n    coslat = np.cos(lat)\n    vx[i,0:nr,0:nlon] = - sinlon*vlon[i,0:nr,0:nlon] - sinlat*coslon*vlat[i,0:nr,0:nlon] + coslat*coslon*vrad[i,0:nr,0:nlon]\n    vy[i,0:nr,0:nlon] = coslon*vlon[i,0:nr,0:nlon] - sinlat*sinlon*vlat[i,0:nr,0:nlon] + coslat*sinlon*vrad[i,0:nr,0:nlon]\n    vz[i,0:nr,0:nlon] = coslat*vlat[i,0:nr,0:nlon] + sinlat*vrad[i,0:nr,0:nlon]\n\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\nLet’s check broadcasting, taking the expression for computing vx[i,0:nr,0:nlon] and omitting scalars ([1]):\n[1,nr,nlon] = - [nlon]*[1,nr,nlon] - [1]*[nlon]*[1,nr,nlon] + [1]*[nlon]*[1,nr,nlon]          # original dimensions\n[1,nr,nlon] = - [1,1,nlon]*[1,nr,nlon] - [1,1,nlon]*[1,nr,nlon] + [1,1,nlon]*[1,nr,nlon]      # after padding with 1's on the left\n[1,nr,nlon] = - [1,nr,nlon]*[1,nr,nlon] - [1,nr,nlon]*[1,nr,nlon] + [1,nr,nlon]*[1,nr,nlon]   # after stretching 1's\nNow all variables have the same dimensions, and all operations will be element-wise. The calculation time goes down to 1.487s.\nYou can also vectorize in all three dimensions (latitudes, radii and longitudes), resulting in no explicit Python loops in your calculation at all, but this requires a little bit of extra work, and the computation time will actually slightly go up – any idea why?\nIn the end, with NumPy’s element-wise vectorized operations, we improved our time from 1280s to 1.5s! The entire calculation was done as a batch in a compiled C code, instead of cycling through individual elements and then calling a compiled C code on each. There are a lot fewer Python code lines to interpret and run.\n\n\nBack to the slow series\nLet’s use NumPy for our slow series calculation. We will:\n\nwrite a function that acts on each integer \\(k\\) in the series,\nconvert this function to a vectorized function that takes in an array of integer numbers and returns an array of terms,\nsum these terms to get the result.\n\nLet’s save the following code as slow2.py:\nfrom time import time\nimport numpy as np\nn = 100_000_000\ndef combined(k):\n    if \"9\" not in str(k):\n        return 1.0/k\n    else:\n        return 0.0\n\nv3 = np.vectorize(combined)\nstart = time()\ni = np.arange(1,n+1)\ntotal = np.sum(v3(i))\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\n\n\n\n\n\n\nNote\n\n\n\nThis particular code uses a lot of memory, so you might want to request --mem-per-cpu=14400 (and single core).\n\n\nThe vectorized function is supposed to speed up calculating the terms, but our time becomes significantly worse (14.72s) than the original calculation (6.625s). The reason: we are no longer replacing multiple Python lines with a single line. The code inside combined() is still native Python code that is being interpreted on the fly, and we applying all its lines to each element of array i.\nThe function np.vectorize does not compile combined() – it simply adapts it to work with arrays, but underneath you are still running Python loops. If, instead, our vectorization could produce a compiled function that takes an array of integers and computes the slow series sum all inside the same C/C++/Fortran/Rust function, it would run ~20X faster than the original calculation. This is what a JIT compiler can do – we will study it later.\nAs it turns out, to speed up this problem with NumPy, you really need to perform the check for digit “9” via low-level custom NumPy code with a combination of vectorizable integer and floating operations, and this is very difficult – but not impossible – to do."
  },
  {
    "objectID": "pythonhpc.html#parallelization",
    "href": "pythonhpc.html#parallelization",
    "title": "Part 1: towards high-performance Python",
    "section": "Parallelization",
    "text": "Parallelization\nWe can only start parallelizing the code when we are certain that our serial performance is not too bad, or at the very least we have optimized our serial Python code as much as possible. At this point, we don’t know if we have the best optimized Python code, as we are only starting to look into various tools that (hopefully) could speed up our code. We know that our code is ~20X slower than a compiled code in Julia/Chapel/C/Fortran, but do we have the best-performing Python code?\nNext we’ll try to speed up our code with NumExpr expression evaluator, in which simple mathematical / NumPy expressions can be parsed and then evaluated using compiled C code. NumExpr has an added benefit in that you can do this evaluation with multiple threads in parallel. But first we should talk about threads and processes.\n\n\n\n\n\nThreads vs processes\nIn Unix a process is the smallest independent unit of processing, with its own memory space – think of an instance of a running application. The operating system tries its best to isolate processes so that a problem with one process doesn’t corrupt or cause havoc with another process. Context switching between processes is relatively expensive.\nA process can contain multiple threads, each running on its own CPU core (parallel execution), or sharing CPU cores if there are too few CPU cores relative to the number of threads (parallel + concurrent execution). All threads in a Unix process share the virtual memory address space of that process, e.g. several threads can update the same variable, whether it is safe to do so or not (we’ll talk about thread-safe programming in this course). Context switching between threads of the same process is less expensive.\n \n\nThreads within a process communicate via shared memory, so multi-threading is always limited to shared memory within one node.\nProcesses communicate via messages (over the cluster interconnect or via shared memory). Multi-processing can be in shared memory (one node, multiple CPU cores) or distributed memory (multiple cluster nodes). With multi-processing there is no scaling limitation, but traditionally it has been more difficult to write code for distributed-memory systems. Various libraries tries to simplify it with high-level abstractions.\n\n\nDiscussion\nWhat are the benefits of each type of parallelism: multi-threading vs. multi-processing? Consider: 1. context switching, e.g. starting and terminating or concurrent execution on the same CPU core, 1. communication, 1. scaling up."
  },
  {
    "objectID": "pythonhpc.html#multithreading",
    "href": "pythonhpc.html#multithreading",
    "title": "Part 1: towards high-performance Python",
    "section": "Multithreading",
    "text": "Multithreading\nPython uses reference counting for memory management. Each object in memory that you create in Python has a counter storing the number of references to that object. Once this counter reaches zero (when you delete objects), Python releases memory occupied by the object. If you have multiple threads, letting them modify the same counter at the same time can lead to a race condition where you can write an incorrect value to the counter, leading to either memory leaks (too much memory allocated) or to releasing memory incorrectly when a there is still a reference to that object on another thread.\nOne way to solve this problem is to have locks on all shared data structures, where only one thread at a time can modify data. This can also lead to problems, and Python’s solution prior to version 3.13 is to use a lock on the interpreter itself (Global Interpreter Lock = GIL), so that only one thread can run at a time. Recall that in Python each line of code is being interpreted on the fly, so placing a lock on the interpreter means pausing code execution while some other thread is running.\nStarting with v3.13, you can install Python with the GIL removed. This mode called free threading is still considered experimental and is usually not enabled by default. There is a good Real Python article (might require subscription) describing various build options to enable free threading; here is what worked on my Macbook:\nbrew install pyenv\ngit clone https://github.com/pyenv/pyenv-update.git $(pyenv root)/plugins/pyenv-update\n\npyenv update\npyenv install --list | grep 3.13   # list all available versions, with \"t\" standing for free-threaded variant\nPYTHON_CONFIGURE_OPTS='--enable-experimental-jit' pyenv install 3.13.0t # build free-threaded Python with JIT\n                                                                        # support in ~/.pyenv/versions/3.13.0t\npyenv versions        # show installed versions\npyenv shell 3.13.2t   # switch to the new build in this shell only\npython\n\n\nPython has several threading libraries. For example, threading library provides a function Thread() to launch new threads. Here is a full example of the slow series calculation that instead uses ThreadPoolExecutor() function from concurrent library. If you run it with v3.12 or earlier, you will get about the same runtime independently of the number of threads, as – due to the GIL – these threads will be taking turns running. With a free-threaded Python 3.13 build, you will see better runtimes with additional threads, provided you have the CPU cores to run all threads in parallel:\npython sum.py\nrunning with 1 threads over [(1, 100000000)]\nTime in seconds: 13.747\nsum = 13.277605949858103\npython sum.py --ntasks=4\nrunning with 4 threads over [(1, 25000000), (25000001, 50000000), (50000001, 75000000), (75000001, 100000000)]\nTime in seconds: 5.831\nsum = 13.277605949854326\npython sum.py --ntasks=8\nrunning with 8 threads over [(1, 12500000), (12500001, 25000000), (25000001, 37500000), (37500001, 50000000), (50000001, 62500000), (62500001, 75000000), (75000001, 87500000), (87500001, 100000000)]\nTime in seconds: 4.324\nsum = 13.277605949855293\nNote that the free-threaded build has some additional overhead when executing Python code compared to the default GIL-enabled build. In 3.13, this overhead is ~40%.\n\nNumExpr\nAlternatively, you can do multithreading in Python via 3rd-party libraries that were written in other languages in which there is no GIL. One such famous library is NumExpr which is essentially a compiler for NumPy operations. It takes its input NumPy expression as a string and can run it with multiple threads.\n\nsupported operators: - + - * / % &lt;&lt; &gt;&gt; &lt; &lt;= == != &gt;= &gt; & | ~ **\nsupported functions: where, sin, cos, tan, arcsin, arccos arctan, arctan2, sinh, cosh, tanh, arcsinh, arccosh arctanh, log, log10, log1p, exp, expm1, sqrt, abs, conj, real, imag, complex, contains\nsupported reductions: sum, product\nsupported data types: 8-bit boolean, 32-bit signed integer (int or int32), 64-bit signed integer (long or int64), 32-bit single-precision floating point number (float or float32), 64-bit double-precision floating point number (double or float64), 2x64-bit double-precision complex number (complex or complex128), raw string of bytes\n\nHere is a very simple NumPy example, timing only the calculation itself:\nfrom time import time\nimport numpy as np\n\nn = 100_000_000\na = np.random.rand(n)\n\nstart = time()\nb = np.sin(2*np.pi*a)**2 + np.cos(2*np.pi*a)**2\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(a,b)\nI get the average runtime of 2.04s. Here is how you would implement this with NumExpr:\nfrom time import time\nimport numpy as np, numexpr as ne\n\nn = 100_000_000\na = np.random.rand(n)\n\nprev = ne.set_num_threads(1)   # specify the number of threads, returns the previous setting\nprint(prev)                    # on first use tries to grab all cores\nstart = time()\ntwoPi = 2*np.pi\nb = ne.evaluate('sin(twoPi*a)**2 + cos(twoPi*a)**2')\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(a,b)\nAverage time:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n1.738\n0.898\n0.478\n0.305\n\n\n\n\n\n\nHow would we implement the slow series with NumExpr? We need a mechanism to check if a substring is present in a string:\nimport numpy as np, numexpr as ne\nx = np.array([b'hi', b'there'])   # an array of byte strings (stored as an array of ASCII codes)\nx.dtype     # each element is a 5-element string\nx.nbytes    # 10 bytes in total, i.e. 1 byte per ASCII character\nne.evaluate(\"contains(x, b'hi')\")   # returns array([True, False])\nne.evaluate(\"contains(x, b'h')\")    # returns array([True, True])\nne.evaluate(\"~contains(x, b'h')\")   # not contains =&gt; returns array([False, False])\nWe can use NumExpr to parallelize checking for substrings (store this as slow3.py):\nfrom time import time\nimport numpy as np, numexpr as ne\nn = 100                        # later set to 100_000_000\nprev = ne.set_num_threads(1)   # specify the number of threads\nstart = time()\ni = np.arange(1,n+1)           # array of integers\nj = i.astype(np.bytes_)        # convert to byte strings\nmask = ne.evaluate(\"~contains(j, '9')\")   # parallel part; returns an array of True/False\nnonZeroTerms = i[mask]\ninverse = np.vectorize(lambda x: 1.0/x)\ntotal = sum(inverse(nonZeroTerms))\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nHere are the average (over three runs) times:\n\n\n\nncores\n1\n2\n4\n\n\nwallclock runtime (sec)\n18.427\n17.375\n17.029\n\n\n\nClearly, we are bottlenecked by the serial part of the code. Let’s use another NumExpr call to evaluate 1.0/x – store this as updated slow3.py:\n&lt; inverse = np.vectorize(lambda x: 1.0/x)\n&lt; total = sum(inverse(nonZeroTerms))\n---\n&gt; total = np.sum(ne.evaluate(\"1.0/nonZeroTerms\"))\nHere are the improved times:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n12.094\n11.304\n10.965\n10.83\n\n\n\nObviously, we still have some bottleneck(s): look at the scaling, and recall the original serial runtime 6.625s on which we are trying to improve. How do we find these? Let’s use a profiler!"
  },
  {
    "objectID": "pythonhpc.html#profiling-and-performance-tuning",
    "href": "pythonhpc.html#profiling-and-performance-tuning",
    "title": "Part 1: towards high-performance Python",
    "section": "Profiling and performance tuning",
    "text": "Profiling and performance tuning\n\nSince we are talking about bottlenecks, now is a good time to look into profilers. There are several good open-source profilers for Python codes, e.g. cProfile, line_profiler, Scalene, Pyinstrument, Linux’s perf profiler, and few others.\n\n\n\nIn our last code we had a sequential workflow with multiple lines, so let’s measure its performance line by line using Scalene profiler. It tracks both CPU and memory usage (and more recently GPU), it is fast and very easy to use:\n\nscalene slow3.py         # on your own computer, will open result in a browser\nscalene --cli slow3.py   # on a remote system\n\nNote: to run scalene --cli slow3.py on the training cluster, you might need more memory, so I found it useful to request --mem-per-cpu=7200.\n\nIf running on your computer, this will open the result in a browser:\n\nAs you can see, our actual computing is quite fast! ~65% of the time is taken by the line initializing an array of byte strings j = i.astype(np.bytes_), and the second biggest offender is initializing an array of integers.\nIf we could find a way to implement this initialization in NumExpr, we could vastly improve our code’s performance. I have not found a way to do this quickly and elegantly in NumExpr (very marginal improvement with j = i.astype(dtype='S9')), but perhaps there is a solution?\nI will leave it as a take-home exercise, and please let me know if you find a solution!"
  },
  {
    "objectID": "pythonhpc.html#multiprocessing",
    "href": "pythonhpc.html#multiprocessing",
    "title": "Part 1: towards high-performance Python",
    "section": "Multiprocessing",
    "text": "Multiprocessing\n\nWith multiprocessing you can launch multiple processes and allocate each process to a separate CPU core to run in parallel. Each process will have its own interpreter with its own GIL and memory space, and – unlike with multithreading – they can all run at the same time in parallel.\nPython has the multiprocessing module as part of its Standard Library. Unfortunately, it has some limitations: (1) it only accepts certain Python functions, and (2) it cannot handle a lot of different types of Python objects. This is due to the way Python serializes (packs) data and sends it to the other processes.\nFortunately, there is a fork of the multiprocessing module called multiprocess (https://pypi.org/project/multiprocess) that solves most of these problems by using a different serialization mechanism. The way you use it is identical to multiprocessing, so it should be straightforward to convert your code between the two.\nConsider Python’s serial map() function:\nhelp(map)\nr = map(lambda x: x**2, [1,2,3])\nlist(r)\nWe can write a Python function to sleep for 1 second, and run it 10 times with serial map():\nimport time\nstart = time.time()\nr = list(map(lambda x: time.sleep(1), range(10)))   # use 0, 1, ..., 9 as arguments to the lambda function\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\nThis takes 10.045s as expected.\nAt this point we need to restart our job on 4 cores:\nsalloc --time=2:00:0 --ntasks=4 --mem-per-cpu=3600\nLet’s parallelize it with multiprocess, creating a pool of workers and replacing serial map() with parallel pool.map():\n\nNote: serial map() returns an iterable, whereas parallel pool.map() already returns a list  ⇒  no need to convert it to a list.\n\nimport time\nfrom multiprocess import Pool\nncores = 4            # set it to the actual number of cores\nprint(\"Running on\", ncores, \"cores\")\npool = Pool(ncores)   # create a pool of workers\nstart = time.time()\nr = pool.map(lambda x: time.sleep(1), range(10))   # use 0, 1, ..., 9 as arguments to the lambda function\npool.close()          # turn off your parallel workers\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\n\n\n\n\nOn 4 cores this takes 3.005 seconds: running batches of 4 + 4 + 2 calls, i.e. in the 1st second we run four sleep(1) calls in parallel, in the 2nd second we run another set of four sleep(1) calls, and the remaining two calls run during the 3rd second.\nOn 8 cores this takes 2.007 seconds: running batches of 8+2 calls.\nOn 1 core it takes 10.013 seconds.\n\n\n\n\n\n\n\nNote\n\n\n\nWe are not doing real calculations here, just waiting for 1 wallclock time second in each call. If you attempt to run multiple processes (e.g. ncores = 4 in the code above) on 1 physical core, they will take turns running at the same time, but they will compare their time against the wallclock time (not CPU time), and all of them will finish running as if you had 4 physical cores.\n\n\nHow do we parallelize the slow series with parallel map()? One idea is to create a function to process each of the \\(10^8\\) terms and use map() to apply it to each term. Here is the serial version of this code:\nfrom time import time\nn = 100_000_000\ndef combined(k):\n    if \"9\" in str(k):\n        return 0.0\n    else:\n        return 1.0/k\n\nstart = time()\ntotal = sum(map(combined, range(1,n+1)))   # use 1,...,n as arguments to `combined` function\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nThis takes 9.403 seconds.\nWith multiprocess we would be inclined to use sum(pool.map(combined, range(1,n+1)))). Unfortunately, pool.map() returns a list, and with \\(10^8\\) input integers it will return a \\(10^8\\)-element list, processing and summing which will take a long time … (85 seconds in my tests). Simply put, lists perform poorly in Python.\nHow do we speed it up?\nIdea: Break down the problem into ncores partial sums, and do it each of them on a separate core.\nHint: Here is how this approach would work on two cores (not the full code):\ndef partial(interval):\n    return sum(map(combined, range(interval[0],interval[1]+1)))\n\npool = Pool(2)\ntotal = pool.map(partial, [(1, n//2-1), (n//2,n)])\nprint(sum(total))\n\n\n\n\n\n\nCautionQuestion 2\n\n\n\n\n\nComplete and run this code on two cores. Add timing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionQuestion 3\n\n\n\n\n\nWrite a scalable version of this code and run it on an arbitrary number of cores (up to the maximum number of cores you can use). Use the following strategy to divide the workload:\nncores = ...\npool = Pool(ncores)\nsize = n // ncores\nintervals = [(i*size+1,(i+1)*size) for i in range(ncores)]\nif n &gt; intervals[-1][1]:\n    intervals[-1] = (intervals[-1][0], n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is what I get for timing on my laptop and on the training cluster:\n\n\n\nncores\n1\n2\n4\n8\n\n\nlaptop wallclock runtime (sec)\n8.294\n4.324\n2.200\n1.408\n\n\n–ntasks=4 cluster runtime (sec)\n17.557\n8.780\n4.499\n4.508\n\n\n\nOur parallel scaling looks great, but in terms of absolute numbers we are still lagging behind the same problem implemented with a serial code in compiled languages …"
  },
  {
    "objectID": "pythonhpc.html#numba-jit-compiler",
    "href": "pythonhpc.html#numba-jit-compiler",
    "title": "Part 1: towards high-performance Python",
    "section": "Numba JIT compiler",
    "text": "Numba JIT compiler\n\n\nHopefully, I have convinced you that – in order to get decent performance out of your Python code – you need to compile it with a proper compiler (and not just NumExpr). There are several Python compilers worth mentioning here:\n\nWe have already looked at NumExpr (only takes simple expressions).\nNumba open-source just-in-time compiler that uses LLVM underneath, can also parallelize your code for multi-core CPUs and GPUs; often requires only minor code changes.\nCython open-source compiled language is a superset of Python: Python-like code with Cython-specific extensions for C types.\nCodon is research project from MIT: not endorsing it, but it consistently comes up high in my search results, source code https://github.com/exaloop/codon and the related article https://bit.ly/3uUvTmd.\nNew proprietary programming language https://www.modular.com/max/mojo is a superset of Python, with somewhat misleading (68,000X) speedup claims on their front page, documentation/examples/workshops at https://github.com/modularml/mojo.\n\n\n\nHere we’ll take a look at Numba: - compiles selected blocks of Python code to machine code, - can process multi-dimensional arrays, - can do multithreading on multiple CPU cores (and we’ll later learn how to use it with multiprocessing), - can use GPUs (not covered here): you’d be writing CUDA kernels.\nLet’s compute the following sum:\n\n\\[\\sum_{i=1}^\\infty\\frac{\\cos(i)}{i} = -\\ln\\left(2\\sin0.5\\right) \\]\nHere is the familiar serial implementation (let’s save it as trig.py):\nfrom time import time\nfrom math import cos, sin, log\n\ndef trig(n):\n    total = 0\n    for i in range(1,n+1):\n        total += cos(i)/i\n    return total\n\nstart = time()\ntotal = trig(100_000_000)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(\"approximate / exact =\", -total/log(2*sin(0.5)))\nOur runtime is 7.493 seconds.\nLet’s add the following Python decorator just before our function definition:\nfrom numba import jit\n@jit(nopython=True)\n...\nThe first time you use Numba in a code, it might be slow, but all subsequent uses will be fast: needs time to compile the code. Our runtime went down to 0.693 seconds – that’s more than a 10X speedup!\n\n\n\n\n\n\nNote\n\n\n\nOn the training cluster I see ~5X speedup.\n\n\nThere are two compilation modes in Numba: 1. object mode: generates a more stable, slower code 1. nopython mode: generates much faster code that requires knowledge of all types, has limitations that can force Numba to fall back to the object mode; the flag nopython=True enforces faster mode and raises an error in case of problems\n\nParallelizing\nLet’s add parallel=True to our decorator and change range(1,n+1) to prange(1,n+1) - it’ll be subdividing loops into pieces to pass them to all available CPU cores via multithreading. On my 8-core laptop the runtime goes down to 0.341 seconds – a further ~2X improvement … This is not so impressive …\nIt turns out there is quite a bit of overhead with subdividing loops, starting/stopping threads and orchestrating everything. If instead of \\(10^8\\) we consider \\(10^{10}\\) terms, a single thread processes this in 57.890 seconds, whereas 8 threads take 10.125 seconds – factor of 5.7X speedup!\n\n\n\nBack to the slow series\nLet’s apply Numba to our slow series problem. Take the very first version of the code (6.625 seconds):\nfrom time import time\ndef slow(n: int):\n    total = 0\n    for i in range(1,n+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    return total\n\nstart = time()\ntotal = slow(100_000_000)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nand add from numba import jit and @jit(nopython=True) to it. Turns out, when run in serial, our time improves only by ~10-20%. There is some compilation overhead, so for bigger problems you could gain few additional %.\nAs you can see, Numba is not a silver bullet when it comes to speeding up Python. It works great for many numerical problems including the trigonometric series above, but for problems with high-level Python abstractions – in our case the specific line if not \"9\" in str(i) – Numba does not perform very well.\n\n\nFast implementation\nIt turns out with Numba there is a way to make the slow series code almost as fast as with the compiled languages. Check out this implementation:\nfrom time import time\nfrom numba import jit\n\n@jit(nopython=True)\ndef combined(k):\n    base, k0 = 10, k\n    while 9//base &gt; 0: base *= 10\n    while k &gt; 0:\n        if k%base == 9: return 0.0\n        k = k//10\n    return 1.0/k0\n\n@jit(nopython=True)\ndef slow(n):\n    total = 0\n    for i in range(1,n+1):\n        total += combined(i)\n    return total\n\nstart = time()\ntotal = slow(100_000_000)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nIt finishes in 0.601 seconds, i.e. ~10X faster than the first Numba implementation – any ideas why?\n\n\n\n\n\n\nCautionQuestion 4\n\n\n\n\n\nParallelize this code with multiple threads and time it on several cores – does your time improve? (it should somewhat) Let’s fill this table together:\nncores           1      2      4\nwallclock runtime (sec)\n\n\n\nIn Part 2 we will combine Numba with multiprocessing via Ray tasks – this approach can scale beyond a single node.\n\n\n\n\n\n\nCautionQuestion 5\n\n\n\n\n\nWe covered a lot of material in this section! Let’s summarize the most important points. Which tool would you use, in what situation, and why?\n- which of today’s tools let you do multithreading?\n- which of today’s tools let you do multiprocessing?\n- which Python compilers did we try?\n\n\n\n\n\n\n\n\n\nCautionQuestion 6\n\n\n\n\n\nWhich tool would you use for your research problem and why? Are you ready to parallelize your workflow?"
  },
  {
    "objectID": "python3/python-13-scraping.html",
    "href": "python3/python-13-scraping.html",
    "title": "Web scraping",
    "section": "",
    "text": "Web scraping refers to extracting data from the web in a semi-automatic fashion. There is some programming involved, but Python web-scraping tools attempt to make this as painless as possible.\nimport requests                 # to download the html data from a site\nfrom bs4 import BeautifulSoup   # to parse these html data\nimport pandas as pd             # to store our data in a dataframe\n\nurl = \"https://arxiv.org/list/econ/new\"\nr = requests.get(url)\nr   # &lt;Response [200]&gt; means our request was successful\n\nprint(r.text[:200])   # the first 200 characters in the raw data\n\nmainpage = BeautifulSoup(r.text, \"html.parser\")\nmainpage.prettify()   # still very messy ...\nThere is a lot of text there, and it’s not particularly readable even after .prettify()! At this point we need to identify relevant markers in the HTML from which we could extract interesting data. There are several ways of doing this, e.g. you can use SelectorGadget bookmarklet on your site and mouse over various elements on the page, but here I will just look at the HTML source.\nIn Firefox I load https://arxiv.org/list/econ/new, select Tools | Browser Tools | Page Source and then try to identify relevant tags. For example, I might see some useful text inside the &lt;div&gt; container tag:\n&lt;div class=\"list-title mathjax\"&gt;\n&lt;span class=\"descriptor\"&gt;Title:&lt;/span&gt; This is the first article's title\n&lt;/div&gt;\nLet’s search for all &lt;div&gt; tags with an attribute class starting with “list-title”:\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-title'})\nlen(divs)   # number of article titles on this page\nLet’s inspect the first title:\ndiv[0]\ndiv[0].text           # get the actual text inside this container\ndiv[0].text.strip()   # remove leading and trailing whitespaces and end-of-line characters\ndiv[0].text.strip().replace('Title: ', '')\nWe can wrap this in a loop through all titles:\nfor div in divs:\n    print(div.text.strip().replace('Title: ', ''))\nLet’s store our data in a dataframe with three columns:\ntitles = []\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-title'})\nfor div in divs:\n    titles.append(div.text.strip().replace('Title: ', ''))\n\nauthors = []\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-authors'})\nfor div in divs:\n    authors.append(div.text.strip().replace('Authors:', '').replace('\\n', ''))\n\nsubjects = []\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-subjects'})\nfor div in divs:\n    subjects.append(div.text.strip().replace('Subjects: ', ''))\n\nd = {'titles': titles, 'authors': authors, 'subjects': subjects}\npapers = pd.DataFrame(d)\npapers\nFinally, let’s filter articles based on a topic:\nmask = [\"Machine Learning\" in subject for subject in papers.subjects]\npapers[mask]\n\n\n\n\n\n\nCautionExercise 13.1\n\n\n\n\n\nCreate a list of abstracts from today’s articles on https://arxiv.org/list/econ/new – no titles, authors, keywords, just abstracts. Print the first 5 elements of this list. Paste the entire Python code to do this from start to finish.",
    "crumbs": [
      "DAY 3",
      "Web scraping"
    ]
  },
  {
    "objectID": "python3/python-02-variables.html",
    "href": "python3/python-02-variables.html",
    "title": "Variables and data types",
    "section": "",
    "text": "Python is a dynamically typed language: all variables have types, but types can change on the fly\npossible names for variables\n\ndon’t use built-in function names for variables, e.g. setting a sum variable will prevent you from using the built-in sum() function\n\nPython is case-sensitive\n\nage = 100\nfirstName = 'Jason'\nprint(firstName, 'is', age, 'years old')\na = 1; b = 2    # can use ; to separate multiple commands in one line\na, b = 1, 2   # assign variables in a tuple notation; same as last line\na = b = 10    #  assign a value to multiple variables at the same time\nb = \"now I am a string\"    # variables can change their type on the fly\n\nvariables persist between cells\nvariables must be defined before use\nvariables can be used in calculations\n\nage = age + 3   # another syntax: age += 3\nprint('age in three years:', age)\n\n\n\n\n\n\nCautionExercise 2.1\n\n\n\n\n\nWhat is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.)\ninitial = \"left\"\nposition = initial\ninitial = \"right\"\n\n\n\nWith simple variables in Python, assigning var2 = var1 will create a new object in memory var2. Here we have two distinct objects in memory: initial and position.\n\n\n\n\n\n\nNote\n\n\n\nWith more complex (mutable) objects, its name could be a pointer. E.g. when we study lists, we’ll see that initial and new below really point to the same list in memory:\ninitial = [1,2,3]\nnew = initial        # create a pointer to the same object\ninitial.append(4)    # change the original list to [1, 2, 3, 4]\nprint(new)           # [1, 2, 3, 4]\nnew = initial[:]     # one way to create a new object in memory\nimport copy\nnew = copy.deepcopy(initial)   # another way to create a new object in memory\n\n\nUse square brackets to get a substring:\nelement = 'helium'\nprint(element[0])     # single character\nprint(element[0:3])   # a substring\n\nPython is case-sensitive\nuse meaningful variable names",
    "crumbs": [
      "DAY 1",
      "Variables and data types"
    ]
  },
  {
    "objectID": "python3/python-02-variables.html#variables-and-assignment",
    "href": "python3/python-02-variables.html#variables-and-assignment",
    "title": "Variables and data types",
    "section": "",
    "text": "Python is a dynamically typed language: all variables have types, but types can change on the fly\npossible names for variables\n\ndon’t use built-in function names for variables, e.g. setting a sum variable will prevent you from using the built-in sum() function\n\nPython is case-sensitive\n\nage = 100\nfirstName = 'Jason'\nprint(firstName, 'is', age, 'years old')\na = 1; b = 2    # can use ; to separate multiple commands in one line\na, b = 1, 2   # assign variables in a tuple notation; same as last line\na = b = 10    #  assign a value to multiple variables at the same time\nb = \"now I am a string\"    # variables can change their type on the fly\n\nvariables persist between cells\nvariables must be defined before use\nvariables can be used in calculations\n\nage = age + 3   # another syntax: age += 3\nprint('age in three years:', age)\n\n\n\n\n\n\nCautionExercise 2.1\n\n\n\n\n\nWhat is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.)\ninitial = \"left\"\nposition = initial\ninitial = \"right\"\n\n\n\nWith simple variables in Python, assigning var2 = var1 will create a new object in memory var2. Here we have two distinct objects in memory: initial and position.\n\n\n\n\n\n\nNote\n\n\n\nWith more complex (mutable) objects, its name could be a pointer. E.g. when we study lists, we’ll see that initial and new below really point to the same list in memory:\ninitial = [1,2,3]\nnew = initial        # create a pointer to the same object\ninitial.append(4)    # change the original list to [1, 2, 3, 4]\nprint(new)           # [1, 2, 3, 4]\nnew = initial[:]     # one way to create a new object in memory\nimport copy\nnew = copy.deepcopy(initial)   # another way to create a new object in memory\n\n\nUse square brackets to get a substring:\nelement = 'helium'\nprint(element[0])     # single character\nprint(element[0:3])   # a substring\n\nPython is case-sensitive\nuse meaningful variable names",
    "crumbs": [
      "DAY 1",
      "Variables and data types"
    ]
  },
  {
    "objectID": "python3/python-02-variables.html#data-types-and-type-conversion",
    "href": "python3/python-02-variables.html#data-types-and-type-conversion",
    "title": "Variables and data types",
    "section": "Data Types and Type Conversion",
    "text": "Data Types and Type Conversion\nprint(type(52))\nprint(type(52.))\nprint(type('52'))\nprint(name+' Smith')   # can add strings\nprint(name*10)         # can replicate strings by mutliplying by a number\nprint(len(name))       # strings have lengths\nprint(1+'a')        # cannot add strings and numbers\nprint(str(1)+'a')   # this works\nprint(1+int('2'))   # this works\n\n\n\n\n\n\nCautionExercise 2.2\n\n\n\n\n\nIf you assign some arbitrary integer value to a, e.g. a=123 or a=87236, write a code to get the second digit of a.",
    "crumbs": [
      "DAY 1",
      "Variables and data types"
    ]
  },
  {
    "objectID": "python3/python-12-images.html",
    "href": "python3/python-12-images.html",
    "title": "Images, hierarchical data, time",
    "section": "",
    "text": "Several image-processing libraries use numpy data structures underneath, e.g. Pillow and skimage.io. Let’s take a look at the latter.\nfrom skimage import io   # scikit-image is a collection of algorithms for image processing\nimage = io.imread(fname=\"https://raw.githubusercontent.com/razoumov/publish/master/grids.png\")\ntype(image)   # numpy array\nimage.shape   # 1024^2 image, with three colour (RGB) channels\nLet’s plot this image using matplotlib:\nio.imshow(image)\n# io.show()   # only if working in a terminal\n# io.imsave(\"tmp.png\", image)\nUsing numpy, you can easily manipulate pixels, e.g.\nimage[:,:,2] = 255 - image[:,:,2]\nand then plot it again.",
    "crumbs": [
      "DAY 3",
      "Image manipulation, hierarchical data, time"
    ]
  },
  {
    "objectID": "python3/python-12-images.html#image-manipulation-with-scikit-image",
    "href": "python3/python-12-images.html#image-manipulation-with-scikit-image",
    "title": "Images, hierarchical data, time",
    "section": "",
    "text": "Several image-processing libraries use numpy data structures underneath, e.g. Pillow and skimage.io. Let’s take a look at the latter.\nfrom skimage import io   # scikit-image is a collection of algorithms for image processing\nimage = io.imread(fname=\"https://raw.githubusercontent.com/razoumov/publish/master/grids.png\")\ntype(image)   # numpy array\nimage.shape   # 1024^2 image, with three colour (RGB) channels\nLet’s plot this image using matplotlib:\nio.imshow(image)\n# io.show()   # only if working in a terminal\n# io.imsave(\"tmp.png\", image)\nUsing numpy, you can easily manipulate pixels, e.g.\nimage[:,:,2] = 255 - image[:,:,2]\nand then plot it again.",
    "crumbs": [
      "DAY 3",
      "Image manipulation, hierarchical data, time"
    ]
  },
  {
    "objectID": "python3/python-12-images.html#hierarchical-data-formats",
    "href": "python3/python-12-images.html#hierarchical-data-formats",
    "title": "Images, hierarchical data, time",
    "section": "Hierarchical data formats",
    "text": "Hierarchical data formats\n\nWe already saw Python dictionaries. You can save them in a file using a variety of techniques. One of the most popular techniques, especially among web developers, is JSON (JavaScript Object Notation), as its internal mapping is similar to that of a Python dictionary, with key-value pairs. In the file all data are stored as human-readable text, including any non-ASCII (Unicode) characters.\nimport json\nx = {\n  \"name\": \"John\",\n  \"age\": 30,\n  \"married\": True,\n  \"children\": (\"Ann\",\"Billy\"),\n  \"pets\": None,\n  \"cars\": [\n    {\"model\": \"BMW 230\", \"mpg\": 27.5},\n    {\"model\": \"Ford Edge\", \"mpg\": 24.1}\n  ]\n}\nlen(x)     # 6 key-value pairs\nx.keys()   # here are the keys\n\nfilename = open(\"personal.json\", \"w\")\njson.dump(x, filename, indent = 2)   # serialize `x` as a JSON-formatted stream to `filename`\n                  # `indent` sets field offsets in the file (for human readability)\nfilename.close()\n\n...\n\nimport json\nfilename = open(\"personal.json\", \"r\")\ndata = json.load(filename)   # read into a new dictionary\nfilename.close()\nfor k in data:\n    print(k, data[k])\nIf you want to read larger and/or binary data, there is BSON format. Going step further, there are popular scientific data formats such as NetCDF and HDF5 for storing large multi-dimensional arrays and/or large hierarchical datasets, but we won’t study them here.",
    "crumbs": [
      "DAY 3",
      "Image manipulation, hierarchical data, time"
    ]
  },
  {
    "objectID": "python3/python-12-images.html#working-with-time",
    "href": "python3/python-12-images.html#working-with-time",
    "title": "Images, hierarchical data, time",
    "section": "Working with time",
    "text": "Working with time\nIn its standard library Python has high-level functions to work with time and dates:\nfrom time import *\ngmtime(0)   # show the starting epoch on my system (typically 1970-Jan-01 on Unix-like systems)\ntime()      # number of seconds since then = current time\nctime(time())   # convert that to human-readable time\nctime()         # same = current time\n\nlocal = localtime()   # convert current date/time to a structure\nlocal.tm_year, local.tm_mon, local.tm_mday\nlocal.tm_hour, local.tm_min, local.tm_sec\nlocal.tm_zone     # my time zone\nlocal.tm_isdst    # Daylight Saving Time 1=on or 0=off\nYou can find many more examples here.",
    "crumbs": [
      "DAY 3",
      "Image manipulation, hierarchical data, time"
    ]
  },
  {
    "objectID": "python3/python-04-conditionals.html",
    "href": "python3/python-04-conditionals.html",
    "title": "Conditionals",
    "section": "",
    "text": "Python implements conditionals via if, elif (short for “else if”) and else. Use an if statement to control whether some block of code is executed or not. Let’s consider the boundary between the Antiquity and the Middle Ages:\nyear = 830\nif year &gt; 476:\n    print('year', year, 'falls into the medieval era')\nLet’s modify the year:\nyear = 205\nif year &gt; 476:\n    print('year', year, 'falls into the medieval era')\nAdd an else statement:\nyear = 205\nif year &gt; 476:\n    print('year', year, 'falls into the medieval era')\nelse:\n    print('year', year, 'falls into the classical antiquity period')\nAdd an elif statement:\nyear = 1500\nif year &gt; 1450:\n    print('year', year, 'falls into the modern era')\nelif year &gt; 476:\n    print('year', year, 'falls into the medieval era')\nelse:\n    print('year', year, 'falls into the classical antiquity period')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise 4.1\n\n\n\n\n\nComplete this code:\nname = input(\"Please enter your name\")\n...\nto print “Welcome on board 007” if you enter any combination of upper/lower case “Bond”, otherwise do not print anything.\n\n\n\n\n\n\n\n\n\n\nCautionExercise 4.2\n\n\n\n\n\nWhat is the problem with the following code?\ngrade = 85\nif grade &gt;= 70:\n    print('grade is C')\nelif grade &gt;= 80:\n    print('grade is B')\nelif grade &gt;= 90:\n    print('grade is A')",
    "crumbs": [
      "DAY 1",
      "Conditionals"
    ]
  },
  {
    "objectID": "python3/python-03-builtin.html",
    "href": "python3/python-03-builtin.html",
    "title": "Built-in functions and help",
    "section": "",
    "text": "Python comes with a number of built-in functions.\nA function may take zero or more arguments.\n\nprint('hello')\nprint()\nprint(max(1,2,3,10))\nprint(min(5,2,10))\nprint(min('a', 'A', '0'))   # works with characters, the order is (0-9, A-Z, a-z)\nprint(max(1, 'a'))    # can't compare these\nround(3.712)      # to the nearest integer\nround(3.712, 1)   # can specify the number of decimal places\nhelp(round)       # works everywhere, including the command line\nround?            # Jupyter Notebook's additional syntax\n?round            # same\n\nEvery function returns something, whether it is a variable or None:\n\nresult = print('example')\nprint('result of print is', result)   # what happened here? Answer: print returns None",
    "crumbs": [
      "DAY 1",
      "Built-in functions and help"
    ]
  },
  {
    "objectID": "python3/python-14-matplotlib.html",
    "href": "python3/python-14-matplotlib.html",
    "title": "Plotting with matplotlib",
    "section": "",
    "text": "There are hundreds of visualization libraries in Python. Check out this diagram of the Python Visualization Landscape (circa 2017, by Nicolas Rougier) which focuses on 1D+2D packages at the time (and only barely mentions 3D sci-vis packages).\nIf you want more advanced 3D rendering, in Python there are libraries to create 3D scientific visualizations, and in the Alliance federation we provide free-of-charge support should you require help with these tools.\nOne of the most widely used Python plotting libraries is matplotlib. Matplotlib is open source and produces static images (and non-interactive animations).",
    "crumbs": [
      "DAY 3",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python3/python-14-matplotlib.html#simple-linescatter-plots",
    "href": "python3/python-14-matplotlib.html#simple-linescatter-plots",
    "title": "Plotting with matplotlib",
    "section": "Simple line/scatter plots",
    "text": "Simple line/scatter plots\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\nplt.plot(x, y, 'bo-')\nplt.xlabel('x', fontsize=18)\nplt.ylabel('f(x)', fontsize=18)\n# plt.show()       # not needed inside the Jupyter notebook\n# plt.savefig('tmp.png')\n\n\nOffscreen plotting - You can create the same plot with offscreen rendering directly to a file:\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.use('Agg')   # enable PNG backend\nplt.figure(figsize=(10,8))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\nplt.plot(x, y, 'bo-')\nplt.xlabel('x', fontsize=18)\nplt.ylabel('f(x)', fontsize=18)\nplt.savefig('tmp.png')\n\nLet’s add the second line, the labels, and the legend. Note that matplotlib automatically adjusts the axis ranges to fit both plots:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\nplt.plot(x, y, 'bo-', label='one')\nplt.plot(x+0.3, 2*sin(10*x), 'r-', label='two')\nplt.legend(loc='lower right')\nplt.xlabel('x', fontsize=18)\nplt.ylabel('f(x)', fontsize=18)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s plot these two functions side-by-side:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(12,4))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\n\nax = fig.add_subplot(121)   # on 1x2 layout create plot #1 (`axes` object with some data space)\nplt.plot(x, y, 'bo-', label='one')\nax.set_ylim(-1.5, 1.5)\nplt.xlabel('x')\nplt.ylabel('f1')\nplt.legend(loc='lower right')\n\nfig.add_subplot(122)   # on 1x2 layout create plot #2\nplt.plot(x+0.2, 2*sin(10*x), 'r-', label='two')\nplt.xlabel('x')\nplt.ylabel('f2')\nplt.legend(loc='lower right')\nThere is also an option to specify absolute coordinates of each plot with fig.add_axes():\n\nreplace the first ax = fig.add_subplot(121) with ax = fig.add_axes([0.1, 0.7, 0.8, 0.3])   # left, bottom, width, height\nreplace the second fig.add_subplot(122) with fig.add_axes([0.1, 0.2, 0.8, 0.4])   # left, bottom, width, height\n\nThe 3rd option for more fine-grained control is plt.axes() – it creates an axes object (a region of the figure with some data space). These two lines are equivalent - both create a new figure with one subplot:\nfig = plt.figure(figsize=(8,8)); ax = fig.add_subplot(111)\nfig = plt.figure(figsize=(8,8)); ax = plt.axes()\nShortly we will see that we can pass additional flags to fig.add_subplot() and plt.axes() for more coordinate system control.\n\n\n\n\n\n\nCautionExercise 14.1\n\n\n\n\n\nBreak the plot into two subplots, the fist taking 1/3 of the space on the left, the second one 2/3 of the space on the right.\n\n\n\nLet’s plot a simple line in the x-y plane:\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111)\nx = np.linspace(0,1,100)\nplt.plot(2*np.pi*x, x, 'b-')\nplt.xlabel('x')\nplt.ylabel('f1')\nReplace ax = fig.add_subplot(111) with ax = fig.add_subplot(111, projection='polar'). Now we have a plot in the phi-r plane, i.e. in polar coordinates. Phi goes [0,2\\(\\pi\\)], whereas r goes [0,1].\n?fig.add_subplot    # look into `projection` parameter\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111, projection='mollweide')\nlon = np.radians(np.linspace(30,90,10))\nlat = np.radians(np.linspace(15,18,10))\nplt.plot(lon, lat, 'go-')\nYou can use this projection parameter together with cartopy package to process 2D geospatial data to produce maps, while all plotting is still being done by Matplotlib. We teach cartopy in a separate workshop.\nLet’s try a scatter plot:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.figure(figsize=(10,8))\nx = np.random.random(size=1000)   # 1D array of 1000 random numbers in [0,1]\ny = np.random.random(size=1000)\nsize = 1 + 50*np.random.random(size=1000)\nplt.scatter(x, y, s=size, color='lightblue')\nFor other plot types, click on any example in the Matplotlib gallery.\nFor colours, see Choosing Colormaps in Matplotlib.",
    "crumbs": [
      "DAY 3",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python3/python-14-matplotlib.html#heatmaps",
    "href": "python3/python-14-matplotlib.html#heatmaps",
    "title": "Plotting with matplotlib",
    "section": "Heatmaps",
    "text": "Heatmaps\nLet’s plot a heatmap of monthly temperatures at the South Pole:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nplt.figure(figsize=(15,10))\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Year']\nrecordHigh = [-14.4,-20.6,-26.7,-27.8,-25.1,-28.8,-33.9,-32.8,-29.3,-25.1,-18.9,-12.3,-12.3]\naverageHigh = [-26.0,-37.9,-49.6,-53.0,-53.6,-54.5,-55.2,-54.9,-54.4,-48.4,-36.2,-26.3,-45.8]\ndailyMean = [-28.4,-40.9,-53.7,-57.8,-58.0,-58.9,-59.8,-59.7,-59.1,-51.6,-38.2,-28.0,-49.5]\naverageLow = [-29.6,-43.1,-56.8,-60.9,-61.5,-62.8,-63.4,-63.2,-61.7,-54.3,-40.1,-29.1,-52.2]\nrecordLow = [-41.1,-58.9,-71.1,-75.0,-78.3,-82.8,-80.6,-79.3,-79.4,-72.0,-55.0,-41.1,-82.8]\n\nvlabels = ['record high', 'average high', 'daily mean', 'average low', 'record low']\n\nZ = np.stack((recordHigh,averageHigh,dailyMean,averageLow,recordLow))\nplt.imshow(Z, cmap=cm.winter)\nplt.colorbar(orientation='vertical', shrink=0.45, aspect=20)\nplt.xticks(range(13), months, fontsize=15)\nplt.yticks(range(5), vlabels, fontsize=12)\nplt.ylim(-0.5, 4.5)\n\nfor i in range(len(months)):\n    for j in range(len(vlabels)):\n        text = plt.text(i, j, Z[j,i],\n                       ha=\"center\", va=\"center\", color=\"w\", fontsize=14, weight='bold')\n\n\n\n\n\n\nCautionExercise 14.2\n\n\n\n\n\nChange the text colour to black in the brightest (green) rows and columns. You can do this either by specifying rows/columns explicitly, or (better) by setting a threshold background colour.\n\n\n\n\n\n\n\n\n\nCautionExercise 14.3\n\n\n\n\n\nThis is a take-home exercise. Modify the code to display only 4 seasons instead of the individual months.",
    "crumbs": [
      "DAY 3",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python3/python-14-matplotlib.html#d-topographic-elevation",
    "href": "python3/python-14-matplotlib.html#d-topographic-elevation",
    "title": "Plotting with matplotlib",
    "section": "3D topographic elevation",
    "text": "3D topographic elevation\nFor this we need a data file – let’s download it. Open a terminal inside your Jupyter dashboard. Inside the terminal, type:\nwget http://bit.ly/pythfiles -O pfiles.zip\nunzip pfiles.zip && rm pfiles.zip        # this should unpack into the directory data-python/\nThis will download and unpack the ZIP file into your home directory. You can now close the terminal panel. Let’s switch back to our Python notebook and check our location:\n%pwd       # run `pwd` bash command\n%ls        # make sure you see data-python/\nLet’s plot tabulated topographic elevation data:\nfrom matplotlib import cm\nfrom matplotlib.colors import LightSource\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ntable = pd.read_csv('data-python/mt_bruno_elevation.csv')\nz = np.array(table)\nnrows, ncols = z.shape\nx = np.linspace(0,1,ncols)\ny = np.linspace(0,1,nrows)\nx, y = np.meshgrid(x, y)\nrgb = LightSource(270, 45).shade(z, cmap=cm.gist_earth, vert_exag=0.1, blend_mode='soft')\n\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(10,10))    # figure with one subplot\nax.view_init(20, 30)      # (theta, phi) viewpoint\nsurf = ax.plot_surface(x, y, z, facecolors=rgb, linewidth=0, antialiased=False, shade=False)\n\n\n\n\n\n\n\n\nCautionExercise 14.4\n\n\n\n\n\nReplace fig, ax = plt.subplots() with fig = plt.figure() followed by ax = fig.add_subplot(). Don’t forget about the 3d projection. This one is a little tricky – feel free to google the problem, or even better use our earlier examples.\n\n\n\n\n\n\n\nLet’s add the following to the previous code (running this takes ~10s on my laptop):\nfor angle in range(90):\n    print(angle)\n    ax.view_init(20, 30+angle)\n    plt.savefig('frame%04d'%(angle)+'.png')\nAnd then we can create a movie in bash:\nffmpeg -r 30 -i frame%04d.png -c:v libx264 -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" spin.mp4",
    "crumbs": [
      "DAY 3",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python3/python-14-matplotlib.html#matplotlibs-built-in-animation",
    "href": "python3/python-14-matplotlib.html#matplotlibs-built-in-animation",
    "title": "Plotting with matplotlib",
    "section": "Matplotlib’s built-in animation",
    "text": "Matplotlib’s built-in animation\nMatplotlib can do live animation with one of its Animation classes:\n\nFuncAnimation class\nFuncAnimation creates an animation by repeatedly calling a function.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\nfig = plt.figure(figsize=(8,5))\nax = plt.subplot(111)\n\nax.set_xlim(( 0, 2))            \nax.set_ylim((-2, 2))\nax.set_xlabel('time')\nax.set_ylabel('magnitude')\n\n# create an empty title and 2 empty plots\ntitle = ax.set_title('')\nline1 = ax.plot([], [], 'b', lw=1)[0]   # `ax.plot` returns a list of 2D line objects\nline2 = ax.plot([], [], 'r', lw=2)[0]\n\nax.legend(['sin','cos'])\n\ndef drawframe(j):\n    x = np.linspace(0, 2, 100)\n    y1 = np.sin(2 * np.pi * (x-0.01*j))\n    y2 = np.cos(2 * np.pi * (x-0.01*j))\n    line1.set_data(x, y1)\n    line2.set_data(x, y2)\n    title.set_text('frame = {0:4d}'.format(j))\n    return (line1,line2,title)   # the animation function must return a sequence of Artist objects\n\n# blit=True re-draws only the parts that have changed, update every 20ms, calls drawframe() with j=0..99\nanim = animation.FuncAnimation(fig, drawframe, frames=100, interval=20, blit=True)\n\n# ---\n\n# Output option 1: Python shell, open a new window\nplt.show()\n\n# Output option 2: Jupyter notebook\nfrom IPython.display import HTML\nHTML(anim.to_html5_video())\n\n# Output option 3: save to a file\nanim.save(\"twoLines.mp4\")\n\n# Output option 4: save to a file, more granular control\nwriter = animation.FFMpegWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nanim.save(\"twoLines.mp4\", writer=writer)\n\n\nArtistAnimation class\nArtistAnimation creates an animation by using a fixed set of Artist objects.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import animation\n\nfig, ax = plt.subplots()\n\ndef f(x, y):\n    return np.sin(x) + np.cos(y)\n\nx = np.linspace(0, 2 * np.pi, 120)\ny = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n\nims = []   # list of rows, each row is a list of artists (images) to draw in the current frame\nfor i in range(60):\n    x += np.pi / 15\n    y += np.pi / 30\n    im = ax.imshow(f(x, y), animated=True)\n    if i == 0:\n        ax.imshow(f(x, y))  # show an initial one first\n    ims.append([im])\n\n# blit=True re-draws only the parts that have changed, update every 50ms\nanim = animation.ArtistAnimation(fig, ims, interval=50, blit=True)\n\n# ---\n\n# Output option 1: Python shell, open a new window\nplt.show()\n\n# Output option 2: Jupyter notebook\nfrom IPython.display import HTML\nHTML(anim.to_html5_video())\n\n# Output option 3: save to a file\nanim.save(\"movingPlane.mp4\")\n\n# Output option 4: save to a file, more granular control\nwriter = animation.FFMpegWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nanim.save(\"movingPlane.mp4\", writer=writer)",
    "crumbs": [
      "DAY 3",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python3/python-14-matplotlib.html#d-parametric-plot",
    "href": "python3/python-14-matplotlib.html#d-parametric-plot",
    "title": "Plotting with matplotlib",
    "section": "3D parametric plot",
    "text": "3D parametric plot\nHere is something visually very different, still using ax.plot_surface():\nfrom matplotlib import cm\nfrom matplotlib.colors import LightSource\nimport matplotlib.pyplot as plt\nfrom numpy import pi, sin, cos, mgrid\n\ndphi, dtheta = pi/250, pi/250    # 0.72 degrees\n[phi, theta] = mgrid[0:pi+dphi*1.5:dphi, 0:2*pi+dtheta*1.5:dtheta]\n        # define two 2D grids: both phi and theta are (252,502) numpy arrays\nr = sin(4*phi)**3 + cos(2*phi)**3 + sin(6*theta)**2 + cos(6*theta)**4\nx = r*sin(phi)*cos(theta)   # x is also (252,502)\ny = r*cos(phi)              # y is also (252,502)\nz = r*sin(phi)*sin(theta)   # z is also (252,502)\n\nrgb = LightSource(270, 45).shade(z, cmap=cm.gist_earth, vert_exag=0.1, blend_mode='soft')\n\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(10,10))\nax.view_init(20, 30)\nsurf = ax.plot_surface(x, y, z, facecolors=rgb, linewidth=0, antialiased=False, shade=False)\n\n\n\n\n\n\nCautionExercise 14.5\n\n\n\n\n\nCreate an animation in which you change the light source position.",
    "crumbs": [
      "DAY 3",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python3/python-14-matplotlib.html#more-examples",
    "href": "python3/python-14-matplotlib.html#more-examples",
    "title": "Plotting with matplotlib",
    "section": "More examples",
    "text": "More examples\nFor more 3D examples in matplotlib, click on any example in the 3D gallery to see the code behind that plot. Try pasting it into your Jupyter notebook and running it, and try to modify the code.\n\nMatplotlib cheatsheets and handouts",
    "crumbs": [
      "DAY 3",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python3/python-09-scope.html",
    "href": "python3/python-09-scope.html",
    "title": "Strings, variable scope, exceptions",
    "section": "",
    "text": "A string object in Python has a number of built-in methods:\nevent = \"HSS Winter Series\"\nevent.&lt;hit TAB once or twice&gt;   # strings can access a number of methods = functions\nevent.capitalize()\nevent.count(\"S\")\nevent.find(\"S\")       # the first occurrence\nevent.find(\"hello\")   # -1 = not found\nevent = event.replace(\"HSS\", \"Humanities and Social Sciences\")\nevent += \" 2024\"\nevent.lower()\nevent.upper()\nWe can do all of this manipulation in one line:\nevent = \"HSS Winter Series\"\n(event.replace(\"HSS\", \"Humanities and Social Sciences\")+\" 2024\").upper()\nAt this point you might ask about finding all occurrences of “S” inside events. One possible solution:\nimport re   # regular expressions library\n[m.start() for m in re.finditer(\"S\", event)]   # [1, 2, 11]\nLet’s now split our string:\nwords = event.split()   # split into words\n'_'.join(words)         # join these words into a string with the `-` separator\nThis .join() syntax is useful for many purposes, e.g. you can use it to convert a list to a string:\nsentence = [\"Good\", \"morning\"]\nstr(sentence)        # this is not what we want ...\n\" \".join(sentence)   # this works!",
    "crumbs": [
      "DAY 2",
      "Working with strings, variable scope, exceptions"
    ]
  },
  {
    "objectID": "python3/python-09-scope.html#working-with-strings",
    "href": "python3/python-09-scope.html#working-with-strings",
    "title": "Strings, variable scope, exceptions",
    "section": "",
    "text": "A string object in Python has a number of built-in methods:\nevent = \"HSS Winter Series\"\nevent.&lt;hit TAB once or twice&gt;   # strings can access a number of methods = functions\nevent.capitalize()\nevent.count(\"S\")\nevent.find(\"S\")       # the first occurrence\nevent.find(\"hello\")   # -1 = not found\nevent = event.replace(\"HSS\", \"Humanities and Social Sciences\")\nevent += \" 2024\"\nevent.lower()\nevent.upper()\nWe can do all of this manipulation in one line:\nevent = \"HSS Winter Series\"\n(event.replace(\"HSS\", \"Humanities and Social Sciences\")+\" 2024\").upper()\nAt this point you might ask about finding all occurrences of “S” inside events. One possible solution:\nimport re   # regular expressions library\n[m.start() for m in re.finditer(\"S\", event)]   # [1, 2, 11]\nLet’s now split our string:\nwords = event.split()   # split into words\n'_'.join(words)         # join these words into a string with the `-` separator\nThis .join() syntax is useful for many purposes, e.g. you can use it to convert a list to a string:\nsentence = [\"Good\", \"morning\"]\nstr(sentence)        # this is not what we want ...\n\" \".join(sentence)   # this works!",
    "crumbs": [
      "DAY 2",
      "Working with strings, variable scope, exceptions"
    ]
  },
  {
    "objectID": "python3/python-09-scope.html#variable-scope",
    "href": "python3/python-09-scope.html#variable-scope",
    "title": "Strings, variable scope, exceptions",
    "section": "Variable scope",
    "text": "Variable scope\nThe scope of a variable is the part of a program that can see that variable.\na = 5\ndef adjust(b):\n    sum = a + b\n    return sum\nadjust(10)   # what will the outcome be?\n\na is the global variable  ⇨  visible everywhere\nb and sum are local variables  ⇨  visible only inside the function\n\nInside a function we can access methods of global variables:\na = []\ndef add():\n    a.append(5)   # modify global `a`\nadd()\nprint(a)          # [5]\nHowever, from a local scope we cannot assign to a global variable directly:\na = []\ndef add():\n    a = [1,2,3]   # this will create a local copy of `a` inside the function\n    print(a)      # [1,2,3]\nadd()\nprint(a)          # []",
    "crumbs": [
      "DAY 2",
      "Working with strings, variable scope, exceptions"
    ]
  },
  {
    "objectID": "python3/python-09-scope.html#handling-exceptions",
    "href": "python3/python-09-scope.html#handling-exceptions",
    "title": "Strings, variable scope, exceptions",
    "section": "Handling exceptions",
    "text": "Handling exceptions\nPutting some explicit logic into your code to deal with errors will make your code more robust. For example, if the input data are missing, you can have some printout to show the instructions to get the data at runtime.\nThis error handling can be applied to calculations too. Let’s say we want to write a code to calculate mortgage payments. For a fixed interest rate in percent, we have two of the following three variables: (1) the principal amount, (2) the term in years, or (3) a monthly payment. Our code will calculate the third variable, based on the other two. There are three scenarios:\n\nIf we have the principal amount and the term in years, here is how you would calculate the monthly payment:\n\nr = rate / 1200\nq = (1 + r)**(12*term)\npayment = principal * r * q / (q - 1)\n\nIf we have the monthly payment and the principal amount, here is how you would calculate the term in years:\n\nr = rate / 1200\nq =  payment / (payment - principal * r)\nif q &lt; 0.:\n    print(\"you will never pay it off ...\")\n    exit(1)\nterm = log(q) / (log(1+r)*12)\n\nIf we have the term in years and the monthly payment, here is how you would calculate the principal amount:\n\nr = rate / 1200\nq = (1 + r)**(12*term)\nprincipal = payment * (q - 1) / (r * q)\nHow can we tell the code to decide on the fly which variable it needs to compute, based on the two existing ones?\nWe would like to do something like this (not actual Python code):\nif payment is not defined:\n    use formula 1\nif term is not defined:\n    use formula 2\nif principal is not defined:\n    use formula 3\nConsider this syntax:\ntry:\n    someUndefinedVariable       # here is what we try to do\nexcept NameError:\n    someUndefinedVariable = 1   # if that produces NameError, don't show it, but do this instead\n    print(someUndefinedVariable)\nAnd you can combine multiple error codes, e.g.\n...\nexcept (RuntimeError, TypeError, NameError):\n...\n\n\n\n\n\n\nCautionExercise 9.1\n\n\n\n\n\nWrite the rest of the mortgage calculation code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also implement this code setting payment = None for a missing variable, and then placing the code under if payment == None condition. Alternatively, you can put everything into a function with optional arguments that default to None unless assigned values. I personally like the implementation with the exception handling, as this way you don’t have to assign the missing variable at all.",
    "crumbs": [
      "DAY 2",
      "Working with strings, variable scope, exceptions"
    ]
  },
  {
    "objectID": "python3/python-09-scope.html#number-crunching-in-python-if-we-have-time",
    "href": "python3/python-09-scope.html#number-crunching-in-python-if-we-have-time",
    "title": "Strings, variable scope, exceptions",
    "section": "Number crunching in Python: if we have time",
    "text": "Number crunching in Python: if we have time\n\nHow would you explain the following:\n\n1 + 2 == 3         # returns True (makes sense!)\n0.1 + 0.2 == 0.3   # returns False -- be aware of this when you use conditionals\nabs(0.1+0.2 - 0.3) &lt; 1.e-8   # compare floats for almost equality\nimport numpy as np\nnp.isclose(0.1+0.2, 0.3, atol=1e-8)\n\n\n\n\n\n\nCautionCheck your solution\n\n\n\n\n\n0.1 in binary will be 0.0001(1001) which we then truncate with round-off, then perform arithmetic, then convert back to decimal with round-off.\n\n\n\n\n\n\n\n\n\nCautionMore challenging floating-point exercise\n\n\n\n\n\nWrite a code to solve \\(x^3+4x^2-10=0\\) with a bisection method in the interval \\([1.3, 1.4]\\) with tolerance \\(10^{-8}\\).",
    "crumbs": [
      "DAY 2",
      "Working with strings, variable scope, exceptions"
    ]
  },
  {
    "objectID": "solutions-bash.html",
    "href": "solutions-bash.html",
    "title": "Bash solutions",
    "section": "",
    "text": "function swap() {\n    if [ -e $1 ] && [ -e $2 ] ; then\n        /bin/mv $2 $2.bak\n        /bin/mv $1 $2\n        /bin/mv $2.bak $1\n    else\n        echo at least one of these files does not exist ...\n    fi\n}\n\n\n\nfunction countfiles() {\n    if [ $# -eq 0 ]; then\n        echo \"No arguments given. Usage: countfiles dir1 dir2 ...\"\n        return 1\n    fi\n    for dir in $@; do\n        echo in $dir we found $(find $dir -type f | wc -l) files\n    done\n}"
  },
  {
    "objectID": "solutions-bash.html#scripts-functions-and-variables",
    "href": "solutions-bash.html#scripts-functions-and-variables",
    "title": "Bash solutions",
    "section": "",
    "text": "function swap() {\n    if [ -e $1 ] && [ -e $2 ] ; then\n        /bin/mv $2 $2.bak\n        /bin/mv $1 $2\n        /bin/mv $2.bak $1\n    else\n        echo at least one of these files does not exist ...\n    fi\n}\n\n\n\nfunction countfiles() {\n    if [ $# -eq 0 ]; then\n        echo \"No arguments given. Usage: countfiles dir1 dir2 ...\"\n        return 1\n    fi\n    for dir in $@; do\n        echo in $dir we found $(find $dir -type f | wc -l) files\n    done\n}"
  },
  {
    "objectID": "solutions-bash.html#text-manipulation",
    "href": "solutions-bash.html#text-manipulation",
    "title": "Bash solutions",
    "section": "10. Text manipulation",
    "text": "10. Text manipulation\n\n10.1\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" | tr '[:upper:]' '[:lower:]' | \\\n  sed 's/ /\\'$'\\n/g' | sed '/^$/d' | sort | uniq -c | sort -gr &gt; frequency.txt\n\n\n10.3\nawk -F, '{print $1 \",\" $2}' cities.csv &gt; populations.csv\n\n\n10.4\nawk -F, 'NR%10==2' cities.csv\n\n\ncopy every 10th file\nfind /project/def-sponsor00/shared/toyModel -type f | sort | awk 'NR%10==0'\n\n\narchive every 20th file\nThere are many solutions:\n# the downside of this solution is that it'll include paths (without the leading /) into the arhive\ntar cvf toy.tar $(find /project/def-sponsor00/shared/toyModel -type f | sort | awk 'NR%20==0')\ncd /project/def-sponsor00/shared/toyModel   # if you are allowed to cd into that directory\ntar cvf ~/tmp/toy.tar $(find . -type f | sort | awk 'NR%20==0')\ncd -\nfind /project/def-sponsor00/shared/toyModel -type f | sort | awk 'NR%20==0' &gt; list.txt\ntar cfz toy.tar --files-from=list.txt\n/bin/rm list.txt\n\n\n10.7\nfind . -type f | xargs /bin/ls -lSh | awk '{print $5 \"  \" $9}' | head -5"
  },
  {
    "objectID": "humanitiesPython.html",
    "href": "humanitiesPython.html",
    "title": "Python Basics for Humanists",
    "section": "",
    "text": "Thursday, February 15, 2024, 2:00-3:00pm EST\nYou can find this page at   https://folio.vastcloud.org/humanitiespython\nPresented by: Alex Razoumov (SFU)\nDescription: In this short session, we will demo some of Python’s capabilities to researchers new to the language, starting with multiple ways to run Python, high-level data collections such as lists and dictionaries, using Python for data processing and manipulation, and data visualization. This short lecture-style course will be followed by a separate, full-day interactive Python workshop in which we will thoroughly study all these topics through hands-on exercises. This follow-up workshop will occur in the weeks after the 2024 HSS Winter Series.\nThe goal of today’s workshop is to demo some Python functionality, to give you a taste of the language’s capabilities. We are not learning Python today, and we will not be doing any hands-on exercises, so please relax and watch the presentation. The three followup sessions on February 26-28 will teach you Python with many hands-on exercises (6 hours in total)."
  },
  {
    "objectID": "humanitiesPython.html#why-python",
    "href": "humanitiesPython.html#why-python",
    "title": "Python Basics for Humanists",
    "section": "Why Python?",
    "text": "Why Python?\nPython is a free, open-source programming language first developed in the late 1980s and 90s that became really popular for scientific computing in the past 15 years. With Python in a few minutes you can: - analyze thousands of texts, - process tables with billions of records, - manipulate thousands of images, - restructure and process data any way you want.\n\nPython vs. Excel\n\nUnlike Excel, Python can read any type of data, both structured and unstructured.\nPython is free and open-source, so no artificial limitations on where/how you run it.\nPython works on all platforms: Windows, Mac, Linux, Android, etc.\nData manipulation is much easier in Python. There are hundreds of data processing, machine learning, and visualization libraries.\nPython can handle much larger amounts of data: limited not by Python, but by your available computing resources. In addition, Python can run at scale (in parallel) on larger systems.\nPython is more reproducible (rerun / modify the script).\n\n\n\n\n\n\n\n\n\n\n\nPython vs. other programming languages\n\n\n\n\n\n\n\nPython pros\nPython cons\n\n\n\n\nelegant scripting language\nslow (interpreted, dynamically typed)\n\n\neasy to write and read code\nuses indentation for code blocks\n\n\npowerful, compact constructs for many tasks\n\n\n\nvery popular across all fields\n\n\n\nhuge number of external libraries"
  },
  {
    "objectID": "humanitiesPython.html#installing-python",
    "href": "humanitiesPython.html#installing-python",
    "title": "Python Basics for Humanists",
    "section": "Installing Python",
    "text": "Installing Python\nOption 1: Install Python from https://www.python.org/downloads making sure to check the option “Add Python to PATH” during the installation.\nOption 2: Install Python and the packages via Anaconda from https://www.anaconda.com/download.\nOption 3: Install Python via your favourite package manager, e.g. in MacOS – assuming you have Homebrew installed – run the command brew install python.\nPost-installation: Install 3rd-party Python packages in the Command Prompt / terminal via pip install &lt;packageName&gt;, e.g. to be able to run Python inside a Jupyter Notebook run pip install jupyter."
  },
  {
    "objectID": "humanitiesPython.html#starting-python",
    "href": "humanitiesPython.html#starting-python",
    "title": "Python Basics for Humanists",
    "section": "Starting Python",
    "text": "Starting Python\nThere are many ways to run Python commands:\n\nfrom a Unix shell you can start a Python shell and type commands there,\nyou can launch Python scripts saved in plain text *.py files,\nyou can execute Python cells inside a Jupyter notebook; the code is stored inside JSON files, displayed as HTML"
  },
  {
    "objectID": "humanitiesPython.html#navigating-jupyter-interface",
    "href": "humanitiesPython.html#navigating-jupyter-interface",
    "title": "Python Basics for Humanists",
    "section": "Navigating Jupyter interface",
    "text": "Navigating Jupyter interface\nTypically you can launch a new Jupyter Notebook via jupyter notebook. In recent MacOS + Homebrew there is a known issue that you can solve by running this command instead:\njupyter lab --app-dir /opt/homebrew/share/jupyter/lab\nHowever you start your Jupyter session, the interface should be quite intuitive:\n\nFile | Save As - to rename your notebook\nFile | Download - download the notebook to your computer\nFile | New Launcher - to open a new launcher dashboard, e.g. to start a terminal\nFile | Logout - to terminate your job (everything is running inside a Slurm job!)\n\nExplain: tab completion, annotating code, displaying figures inside the notebook.\n\nEsc - leave the cell (border changes colour) to the control mode\nA - insert a cell above the current cell\nB - insert a cell below the current cell\nX - delete the current cell\nM - turn the current cell into the markdown cell\nH - to display help\nEnter - re-enter the cell (border becomes green) from the control mode\nyou can enter Latex equations in a markdown cell, e.g. \\(int_0^\\infty f(x)dx\\)\n\nprint(1/2)   # to run all commands in the cell, either use the Run button, or press shift+return"
  },
  {
    "objectID": "humanitiesPython.html#getting-help",
    "href": "humanitiesPython.html#getting-help",
    "title": "Python Basics for Humanists",
    "section": "Getting help",
    "text": "Getting help\nhelp(print)\n?print    # only inside Jupyter"
  },
  {
    "objectID": "humanitiesPython.html#high-level-data-collections",
    "href": "humanitiesPython.html#high-level-data-collections",
    "title": "Python Basics for Humanists",
    "section": "High-level data collections",
    "text": "High-level data collections\nPython has a number of built-in composite data structures: tuples, lists, sets, dictionaries. Here we take a look at lists and dictionaries.\n\nLists\nA list stores many values in a single structure:\nevents = [267, 1332, 1772, 1994, 493, 1373, 1044, 156, 1515, 1788]  # array of years\nprint('events:', events)\nprint('length:', len(events))\nprint('first item of events is', events[0])   # indexing starts witgh 0\nevents[2] = 1773   # individual elements are mutable\nprint('events is now:', events)\nevents.append(1239)\nevents.append(606)\nprint('events is now:', events)\nevents.pop(4)      # remove element #4\nprint('events is now:', events)\nevents.remove(...)   # remove by value (first occurrence)\nLists can be inhomogeneous:\na = []\na.append(1)\na.append('Vancouver')\na.append(3.5)\nand can even be nested (contain other lists):\na.append(['Mercury', 'Venus', 'Earth', 'Mars'])\na\na[-1]\nYou can search inside a list:\n'Venus' in a[-1]       # returns True\n'Pluto' in a[-1]       # returns False\n\na[-1].index('Venus')   # returns 1 (position index)\nplanets = a[-1]\nplanets.sort()         #  you sort lists alphabetically\nThe initial list I showed you was generated via a list comprehension:\n[random.randint(0,2024) for i in range(10)]\n[x**2 for x in range(1,31)]\n[x**2 for x in range(1,11) if x%2==1]   # list only odd number squares\ncolours = ['red', 'green', 'white', 'black', 'pink', 'yellow']\n[c for c in colours if len(c) &lt;= 4]\nLet’s compute a difference between two lists, i.e. find unique elements in each:\na = [1, 2, 3, 4, 6, 10]\nb = [1, 2, 5, 10]\nfor i in a:\n    if i not in b:\n        print(i, \"is not in b\")\n\nfor i in b:\n    if i not in a:\n        print(i, \"is not in a\")\nIn Python very often you can have multiple solutions to the same problem. E.g., let’s write a script to get the frequency of all elements in a given list:\nSolution 1:\na = [77, 9, 23, 67, 73, 21, 23, 9]\na.count(77)        # prints 1\na.count(9)         # prints 2\nfor i in a:\n    a.count(i)    # counts the frequency of 'i' in list 'a' ... redundant output\nSolution 2:\na = [77, 9, 23, 67, 73, 21, 23, 9]\nfor i in set(a):\n    print(i, \"is seen\", a.count(i))   # no redundant output\nSolution 3:\na = [77, 9, 23, 67, 73, 21, 23, 9]\nimport collections\nprint(collections.Counter(a))\n\n\nDictionaries\nAs you just saw, Python’s lists are ordered sets of objects that you access via their position/index. Dictionaries are unordered sets in which the objects are accessed via their keys. In other words, dictionaries are unordered key-value pairs.\nConsider two lists:\nnames = ['Mary', 'John', 'Eric', 'Jeff', 'Anne']               # people\ncolours = ['orange', 'green', 'blue', 'burgundy', 'turquoise'] # and their respective favourite colours\nThere is nothing connecting these two lists, as far as figuring a person’s favourite colour goes. You could do something like this using indices:\ncolours[names.index('Eric')]\nbut this is a little too convoluted … A dictionary can help you connect the two datasets directly:\nfav = {}  # start with an empty dictionary\nfor name, colour in zip(names, colours):   # go through both lists simultaneously\n    fav[name] = colour\n\nfav   # {'Mary': 'orange', 'John': 'green', 'Eric': 'blue', 'Jeff': 'burgundy', 'Anne': 'turquoise'}\n\nfav['John']      # returns 'green'\nfav['Mary']      # returns 'orange'\nfor key in fav:\n    print(key, fav[key])   # will print the names (keys) and the colours (values)\nThere are other ways to organize the same information using dictionaries. For example, you can create a list of dictionaries, one dictionary per person:\nnames = ['Mary', 'John', 'Eric', 'Jeff', 'Anne']                 # people names\ncolours = ['orange', 'green', 'blue', 'burgundy', 'turquoise']   # and their respective favourite colours\nages = [25, 23, 27, 32, 26]                                       # let's include a third attribute\n\ndata = []\nfor name, colour, age in zip(names, colours, ages):   # go through both lists simultaneously\n    data.append({'name': name, 'colour': colour, 'age': age})\n\nperson = data[0]\nprint(person)\nprint(person[\"name\"], person[\"colour\"])\nThe benefit of this approach is that you can have many more attributes per person, that just name and colour, and this is very common way to organize structured and/or hierarchical data in Python. The downside is that – to search for by name – you have to do it explicitly:\nfor person in data:\n    if person[\"name\"]==\"Jeff\": print(person[\"colour\"], person[\"age\"])\nor in a single line:\n[(person[\"colour\"], person[\"age\"]) for person in data if person[\"name\"]==\"Jeff\"]\nFinally, if you want performance, this will be the fastest way:\nlist(filter(lambda person: person[\"name\"] == \"Jeff\", data))\n\n\n\nHere we: 1. apply the anonymous “lambda” function lambda person: person[\"name\"] == \"Jeff\" to each item in the collection data; it returns True or False, 2. create an iterator yielding only those items in data that produced True, and 3. create a list from this iterator, in this case containing only one element.\nYou can see where dictionary got its name:\nconcepts = {}\nconcepts['list'] = 'an ordered collection of values'\nconcepts['dictionary'] = 'a collection of key-value pairs'\nconcepts\nLet’s modify values:\nconcepts['list'] = concepts['list'] + ' - very simple'\nconcepts['dictionary'] = concepts['dictionary'] + ' - used widely in Python'\nconcepts\nValues can also be numerical:\ngrades = {}\ngrades['Mary'] = 5\ngrades['John'] = 4.5\ngrades\nAnd so can be the keys:\ngrades[1] = 2\ngrades"
  },
  {
    "objectID": "humanitiesPython.html#working-with-strings",
    "href": "humanitiesPython.html#working-with-strings",
    "title": "Python Basics for Humanists",
    "section": "Working with strings",
    "text": "Working with strings\nevent = \"HSS Winter Series\"\nevent.&lt;hit TAB once or twice&gt;   # strings can access a number of methods = functions\nevent.capitalize()\nevent.count(\"S\")\nevent.find(\"S\")       # the first occurrence\nevent.find(\"hello\")   # -1 = not found\nevent = event.replace(\"HSS\", \"Humanities and Social Sciences\")\nevent += \" 2024\"\nevent.lower()\nevent.upper()\nWe can do all of this manipulation in one line:\nevent = \"HSS Winter Series\"\n(event.replace(\"HSS\", \"Humanities and Social Sciences\")+\" 2024\").upper()\nAt this point you might ask about finding all occurrences of “S” inside events. One possible solution:\nimport re   # regular expressions library\n[m.start() for m in re.finditer(\"S\", event)]   # [1, 2, 11]\nLet’s now split our string:\nwords = event.split()   # split into words\n'_'.join(words)         # join these words into a string with the `-` separator\nThis .join() syntax is useful for many purposes, e.g. you can use it to convert a list to a string:\nsentence = [\"Good\", \"morning\"]\nstr(sentence)        # this is not what we want ...\n\" \".join(sentence)   # this works!\nIn tomorrow’s 3D Visualization workshop you will see some more complex text manipulation."
  },
  {
    "objectID": "humanitiesPython.html#libraries",
    "href": "humanitiesPython.html#libraries",
    "title": "Python Basics for Humanists",
    "section": "Libraries",
    "text": "Libraries\nA library is a collection of functions that can be used by other programs. Python’s standard library includes many functions we worked with before (print, list, …) and is included with Python. There are many other additional modules in the standard library such as math:\nprint('pi is', pi)\nimport math\nprint('pi is', math.pi)\nYou can also import math’s items directly:\nfrom math import pi, sin\nprint('pi is', pi)\nsin(pi/6)\ncos(pi)\nhelp(math)   # help for libraries works just like help for functions\nfrom math import *\nYou can also create an alias from the library:\nimport math as m\nprint(m.pi)"
  },
  {
    "objectID": "humanitiesPython.html#installing-libraries-and-creating-virtual-python-environments",
    "href": "humanitiesPython.html#installing-libraries-and-creating-virtual-python-environments",
    "title": "Python Basics for Humanists",
    "section": "Installing libraries and creating virtual Python environments",
    "text": "Installing libraries and creating virtual Python environments\nTo install a 3rd-party library into the current Python environment, in the OS shell run the command:\npip install &lt;packageName&gt;   # e.g. try bson\nIf you are inside a Jupyter notebook, you can try:\n%pip install &lt;packageName&gt;   # e.g. try bson\nand you will probably need to restart the kernel before you can use the package.\nIn Python you can create an isolated environment for each project, into which all of its dependencies will be installed. This could be useful if your several projects have very different sets of dependencies. On the computer running your Jupyter notebooks, open a terminal and type:\npip install virtualenv\nvirtualenv hss   # create a new virtual environment in your current directory\nsource hss/bin/activate\nwhich python && which pip\npip install numpy ...\nOptionally, you can add your environment to Jupyter:\npip install ipykernel    # install ipykernel (IPython kernel for Jupyter) into this environment\npython -m ipykernel install --user --name=hss --display-name \"My HSS project\"   # add your environment to Jupyter\nTo use this environment in the terminal, you would do:\nsource hss/bin/activate\n...\ndeactivate\nTo use this environment via Jupyter, you would open the notebook dashboard, and one of the options in New below Python 3 should be My HSS project.\nTo delete the environment, in the terminal type:\njupyter kernelspec list                  # `hss` should be one of them\njupyter kernelspec uninstall hss     # remove your environment from Jupyter\n/bin/rm -rf hss"
  },
  {
    "objectID": "humanitiesPython.html#quick-overview-of-some-external-libraries",
    "href": "humanitiesPython.html#quick-overview-of-some-external-libraries",
    "title": "Python Basics for Humanists",
    "section": "Quick overview of some external libraries",
    "text": "Quick overview of some external libraries\n\nnumpy is a library for working with large, multi-dimensional arrays, along with a large collection of linear algebra functions\n\nprovides missing uniform collections (arrays) in Python, along with a large number of ways to quickly process these collections ⮕ great for speeding up calculations in Python\n\npandas is a library for working with 2D tables / spreadsheets, built on top of numpy\nscikit-image is a collection of algorithms for image processing, built on top of numpy\nmatplotlib and plotly are two plotting packages for Python\nscikit-image is a collection of algorithms for image processing\nxarray is a library for working with labelled multi-dimensional arrays and datasets in Python\n\n“pandas for multi-dimensional arrays”\ngreat for large scientific datasets; writes into NetCDF files"
  },
  {
    "objectID": "humanitiesPython.html#numpy",
    "href": "humanitiesPython.html#numpy",
    "title": "Python Basics for Humanists",
    "section": "Numpy",
    "text": "Numpy\nPython lists are very general and flexible, which is great for high-level programming, but it comes at a cost. The Python interpreter can’t make any assumptions about what will come next in a list, so it treats everything as a generic object with its own type and size. As lists get longer, eventually performance takes a hit.\nPython does not have any mechanism for a uniform/homogeneous list, where – to jump to element #1000 – you just take the memory address of the very first element and then increment it by (element size in bytes) x 999. Numpy library fills this gap by adding the concept of homogenous collections to python – numpy.ndarrays – which are multidimensional, homogeneous arrays of fixed-size items (most commonly numbers, but could be strings too). This brings huge performance benefits!\nTo speed up calculations with numpy, typically you perform operations on entire arrays, and this by extension applies the same operation to each array element. Since numpy was written in C, it is much faster for processing multiple data elements than manually looping over these elements in Python.\nLearning numpy is outside the scope of this introductory workshop, but there are many packages built on top of numpy that could be used in HSS."
  },
  {
    "objectID": "humanitiesPython.html#pandas",
    "href": "humanitiesPython.html#pandas",
    "title": "Python Basics for Humanists",
    "section": "Pandas",
    "text": "Pandas\nLet’s try reading some public-domain data about Jeopardy questions with pandas (31MB file, so it might take a while):\nimport pandas as pd\ndata = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndata.shape      # 216930 rows, 7 columns\ndata.head(10)   # first 10 rows\ndata.tail()     # last 5 rows\ndata.iloc[2:5]  # rows 2-4\ndata.columns    # names of the columns\n\ndata['Category']\ndata['Category']=='HISTORY'\ndata.loc[data['Category']=='HISTORY'].shape   # 349 matches\ndata.loc[data['Category']=='HISTORY'].to_csv(\"history.csv\")   # write to a file\nLet’s check what time period is covered by these data:\ndata[\"Air Date\"]\ndata[\"Air Date\"][0][-2:]   # first row, last two digits is the year\nyear = data[\"Air Date\"].apply(lambda x: x[-2:])   # last two digits of the year from all rows\nyear.min(); year.max()     # '00' and '99' - not very informative, wraps at the turn of the century\n\nfor y in range(100):\n    twoDigits = str(y).zfill(2)\n    print(twoDigits, sum(year==twoDigits))\nThis shows that this table covers years from 1984 to 2012."
  },
  {
    "objectID": "humanitiesPython.html#image-manipulation-with-scikit-image",
    "href": "humanitiesPython.html#image-manipulation-with-scikit-image",
    "title": "Python Basics for Humanists",
    "section": "Image manipulation with scikit-image",
    "text": "Image manipulation with scikit-image\n\nSeveral image-processing libraries use numpy data structures underneath, e.g. Pillow and skimage.io. Let’s take a look at the latter.\nfrom skimage import io   # scikit-image is a collection of algorithms for image processing\nimage = io.imread(fname=\"https://raw.githubusercontent.com/razoumov/publish/master/grids.png\")\ntype(image)   # numpy array\nimage.shape   # 1024^2 image, with three colour (RGB) channels\nLet’s plot this image using matplotlib:\nio.imshow(image)\n# io.show()   # only if working in a terminal\n# io.imsave(\"tmp.png\", image)\nUsing numpy, you can easily manipulate pixels, e.g.\nimage[:,:,2] = 255 - image[:,:,2]\nand then plot it again."
  },
  {
    "objectID": "humanitiesPython.html#working-with-more-complex-data-formats",
    "href": "humanitiesPython.html#working-with-more-complex-data-formats",
    "title": "Python Basics for Humanists",
    "section": "Working with more complex data formats",
    "text": "Working with more complex data formats\n\nWe already saw Python dictionaries. You can save them in a file using a variety of techniques. One of the most popular techniques, especially among web developers, is JSON (JavaScript Object Notation), as its internal mapping is similar to that of a Python dictionary, with key-value pairs. In the file all data are stored as human-readable text, including any non-ASCII (Unicode) characters.\nimport json\nx = {\n  \"name\": \"John\",\n  \"age\": 30,\n  \"married\": True,\n  \"children\": (\"Ann\",\"Billy\"),\n  \"pets\": None,\n  \"cars\": [\n    {\"model\": \"BMW 230\", \"mpg\": 27.5},\n    {\"model\": \"Ford Edge\", \"mpg\": 24.1}\n  ]\n}\nlen(x)     # 6 key-value pairs\nx.keys()   # here are the keys\n\nfilename = open(\"personal.json\", \"w\")\njson.dump(x, filename, indent = 2)   # serialize `x` as a JSON-formatted stream to `filename`\n                  # `indent` sets field offsets in the file (for human readability)\nfilename.close()\n\n...\n\nimport json\nfilename = open(\"personal.json\", \"r\")\ndata = json.load(filename)   # read into a new dictionary\nfilename.close()\nfor k in data:\n    print(k, data[k])\nIf you want to read larger and/or binary data, there is BSON format. Going step further, there are popular scientific data formats such as NetCDF and HDF5 for storing large multi-dimensional arrays and/or large hierarchical datasets, but we won’t study them here."
  },
  {
    "objectID": "humanitiesPython.html#working-with-time",
    "href": "humanitiesPython.html#working-with-time",
    "title": "Python Basics for Humanists",
    "section": "Working with time",
    "text": "Working with time\nIn its standard library Python has high-level functions to work with time and dates:\nfrom time import *\ngmtime(0)   # show the starting epoch on my system (typically 1970-Jan-01 on Unix-like systems)\ntime()      # number of seconds since then = current time\nctime(time())   # convert that to human-readable time\nctime()         # same = current time\n\nlocal = localtime()   # convert current date/time to a structure\nlocal.tm_year, local.tm_mon, local.tm_mday\nlocal.tm_hour, local.tm_min, local.tm_sec\nlocal.tm_zone     # my time zone\nlocal.tm_isdst    # Daylight Saving Time 1=on or 0=off\nYou can find many more examples here."
  },
  {
    "objectID": "humanitiesPython.html#web-scraping",
    "href": "humanitiesPython.html#web-scraping",
    "title": "Python Basics for Humanists",
    "section": "Web scraping",
    "text": "Web scraping\n\nWeb scraping refers to extracting data from the web in a semi-automatic fashion. There is some programming involved, but Python web-scraping tools attempt to make this as painless as possible.\nimport requests                 # to download the html data from a site\nfrom bs4 import BeautifulSoup   # to parse these html data\nimport pandas as pd             # to store our data in a dataframe\n\nurl = \"https://arxiv.org/list/econ/new\"\nr = requests.get(url)\nr   # &lt;Response [200]&gt; means our request was successful\n\nprint(r.text[:200])   # the first 200 characters in the raw data\n\nmainpage = BeautifulSoup(r.text, \"html.parser\")\nmainpage.prettify()   # still very messy ...\nThere is a lot of text there, and it’s not particularly readable even after .prettify()! At this point we need to identify relevant markers in the HTML from which we could extract interesting data. There are several ways of doing this, e.g. you can use SelectorGadget bookmarklet on your site and mouse over various elements on the page, but here I will just look at the HTML source.\nIn Firefox I load https://arxiv.org/list/econ/new, select Tools | Browser Tools | Page Source and then try to identify relevant tags. For example, I might see some useful text inside the &lt;div&gt; container tag:\n&lt;div class=\"list-title mathjax\"&gt;\n&lt;span class=\"descriptor\"&gt;Title:&lt;/span&gt; This is the first article's title\n&lt;/div&gt;\nLet’s search for all &lt;div&gt; tags with an attribute class starting with “list-title”:\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-title'})\nlen(divs)   # number of article titles on this page\nLet’s inspect the first title:\ndiv[0]\ndiv[0].text           # get the actual text inside this container\ndiv[0].text.strip()   # remove leading and trailing whitespaces and end-of-line characters\ndiv[0].text.strip().replace('Title: ', '')\nWe can wrap this in a loop through all titles:\nfor div in divs:\n    print(div.text.strip().replace('Title: ', ''))\nLet’s store our data in a dataframe with three columns:\ntitles = []\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-title'})\nfor div in divs:\n    titles.append(div.text.strip().replace('Title: ', ''))\n\nauthors = []\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-authors'})\nfor div in divs:\n    authors.append(div.text.strip().replace('Authors:', '').replace('\\n', ''))\n\nsubjects = []\ndivs = mainpage.findAll(\"div\", attrs={'class':'list-subjects'})\nfor div in divs:\n    subjects.append(div.text.strip().replace('Subjects: ', ''))\n\nd = {'titles': titles, 'authors': authors, 'subjects': subjects}\npapers = pd.DataFrame(d)\npapers\nFinally, let’s filter articles based on a topic:\nmask = [\"Machine Learning\" in subject for subject in papers.subjects]\npapers[mask]\n\nMore advanced: extracting all links from a standalone HTML file\n\nimport requests                 # to download the html data from a site\nfrom bs4 import BeautifulSoup   # to parse these html data\nimport pandas as pd             # to store our data in a dataframe\n\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.response import HTTPResponse\nclass FileAdapter(HTTPAdapter):\n    def send(self, request, *args, **kwargs):\n        resp = HTTPResponse(body=open(request.url[7:], 'rb'), status=200, preload_content=False)\n        return self.build_response(request, resp)\n\nsession = requests.Session()\nsession.mount('file://', FileAdapter())\nr = session.get('file:///path/to/file.html')\nmainpage = BeautifulSoup(r.text, \"html.parser\")\nlinks = mainpage.findAll(\"a\", href=True)\nfor link in links:\n    print(link['href'])"
  },
  {
    "objectID": "humanitiesPython.html#visualization",
    "href": "humanitiesPython.html#visualization",
    "title": "Python Basics for Humanists",
    "section": "Visualization",
    "text": "Visualization\nThere are hundreds of visualization libraries in Python. Rather than demo one library here, let me walk you through several popular choices:\n\nMatplotlib is a good starting point, creates non-interactive images and movies\nPlotly if you want HTML interactivity\nSeaborn is based on matplotlib with a high-level interface for statistical graphics\nPlotnine is an implementation of a grammar of graphics in Python based on R’s ggplot2\n\nIf you want more advanced 3D rendering, in Python there are libraries to create 3D scientific visualizations, and in the Alliance federation we provide free-of-charge support should you require help with these tools.\nIn tomorrow’s 3D Visualization workshop I will demo using Python for analyzing a set of texts, building a global dictionary for all these texts, positioning each paragraph in this dictionary, and then visualizing differences between these paragraphs’ vocabularies as a 3D scatter plot:\n\n\n\n\nFour English and four Greek texts, each node representing a paragraph\n\n\n\nand as a 3D graph:\n\n\n\n\nLines connect paragraphs with at least 15 words in common"
  },
  {
    "objectID": "humanitiesPython.html#links",
    "href": "humanitiesPython.html#links",
    "title": "Python Basics for Humanists",
    "section": "Links",
    "text": "Links\n\nWhat format to choose to save your data workshop from November 2022\nPython Tutorials for Digital Humanities by William Mattingly\n2024 HSS Winter Series playlist"
  },
  {
    "objectID": "3dayPython-menu.html",
    "href": "3dayPython-menu.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "February 26th-28th, 9:00am - 11:00am Pacific Time each day\nDescription: Python can be used in many humanities and social sciences workflows, and it is an easy and fun language to learn. This introductory 3-day, 6-hour course will walk you through the basics of programming in Python starting at the beginner’s level. We will cover the main language features – variables and data types, conditionals, lists, for/while loops, list comprehensions, dictionaries, writing functions, and working with external libraries, doing many exercises along the way. In the second part we will take a look at some of the libraries in more details, including pandas for working with large tables, simple plotting with matplotlib, and few others.\nInstructor: Alex Razoumov (SFU)\nPrerequisites: None\nSoftware: We will be using Python on our training cluster, so no need to install it on your computer. However, in the long run you would probably benefit from Python on your computer, so you might want to look into this. To access the training cluster, you will need a remote secure shell (SSH) client installed on your computer in order to participate in the course exercises. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there)."
  },
  {
    "objectID": "3dayPython-menu.html#links",
    "href": "3dayPython-menu.html#links",
    "title": "Introduction to Python",
    "section": "Links",
    "text": "Links\n\nWhat format to choose to save your data workshop from November 2022\nYouTube channel Python Tutorials for Digital Humanities by William Mattingly"
  },
  {
    "objectID": "bashScripting.html",
    "href": "bashScripting.html",
    "title": "Bash scripting for beginners",
    "section": "",
    "text": "Thursday, Mar-24, 2022, 1:00pm - 2:30pm Pacific Time\nYou can find this page at   https://folio.vastcloud.org/bashscripting\nAbstract: The command line is a quick and powerful text-based interface to Linux computers that allows us to accomplish a wide set of tasks very efficiently. In this workshop we focus on writing and running scripts and functions in Bash. Scripts are great for simplifying and automating your workflows on any computer with Bash installed, whether it is your own laptop or a remote HPC cluster (where the command line is often the only available interface). You don’t need to know Bash to attend this workshop, as we will start from the basics and will slowly build up our skills using many hands-on examples.\nSoftware: If you want to follow along the hands-on part of this workshop, you will need a terminal emulator. Linux and MacOS probably already have one, and Windows users can install the free version of MobaXterm (see https://mobaxterm.mobatek.net/download.html for installation) to connect to our remote server. With MobaXterm you can also run a local terminal emulator, but we are not sure about the set of bash commands available there, so with Windows we highly recommend connecting to our system (we will provide guest accounts)."
  },
  {
    "objectID": "bashScripting.html#access-to-the-remote-system",
    "href": "bashScripting.html#access-to-the-remote-system",
    "title": "Bash scripting for beginners",
    "section": "Access to the remote system",
    "text": "Access to the remote system\nWe will be connecting to a remote training cluster at 206.12.96.223. We will paste the link to the Google Doc with the usernames into the Zoom chat."
  },
  {
    "objectID": "bashScripting.html#simple-script-examples",
    "href": "bashScripting.html#simple-script-examples",
    "title": "Bash scripting for beginners",
    "section": "Simple script examples",
    "text": "Simple script examples\n\n#!/bin/bash she-bang\nwrite a script countFiles.sh to count files in the current directory\nsetting internal variables\nresult of a bash command inside the script \\(~\\Rightarrow~\\) echo \"we found $num files\"\n\n#!/bin/bash\nnum=$(find . -type f | wc -l)\necho \"we found $num files\""
  },
  {
    "objectID": "bashScripting.html#scripts-vs.-functions",
    "href": "bashScripting.html#scripts-vs.-functions",
    "title": "Bash scripting for beginners",
    "section": "Scripts vs. functions",
    "text": "Scripts vs. functions\nA bash script is an executable file sitting at a given path. A bash function is defined in your environment, usually from your ~/.bashrc file.\nLet’s convert countFiles.sh into a function countFiles() that we define inside our ~/.bashrc. To do that, we’ll have to pass a directory name to the script."
  },
  {
    "objectID": "bashScripting.html#processing-command-line-arguments",
    "href": "bashScripting.html#processing-command-line-arguments",
    "title": "Bash scripting for beginners",
    "section": "Processing command-line arguments",
    "text": "Processing command-line arguments\n\nprocessing individual arguments\nprocessing all arguments\nreturn a usage page if there are no arguments: if [ $# -eq 0 ]; then ... fi\nask countFiles() to count files in all directories passed as arguments: need to loop through all arguments\n\n\n\n\n\n\n\nCautionCheck your solution\n\n\n\n\n\nfunction countfiles() {\n    if [ $# -eq 0 ]; then\n        echo \"No arguments given. Usage: countfiles dir1 dir2 ...\"\n        return 1\n    fi\n    for dir in $@; do\n        echo in $dir we found $(find $dir -type f | wc -l) files\n    done\n}"
  },
  {
    "objectID": "bashScripting.html#archiveunarchive-scripts",
    "href": "bashScripting.html#archiveunarchive-scripts",
    "title": "Bash scripting for beginners",
    "section": "Archive/unarchive scripts",
    "text": "Archive/unarchive scripts\nTask: write function archive() that would archive all directories passed to it. For example:\nmkdir intro chapter{1..3} conclusion\nfor i in {1..3}; do\n    echo \"## Chapter $i\" &gt; chapter${i}/main.md\ndone\narchive chapter* intro conclusion\nMostly likely, in this exercise we will need to process strings – this is how you can do this in bash:\n$ word=\"hello\"\n$ echo $word\nhello\n$ echo ${word/l/L}\nheLlo\n$ echo ${word//l/L}\nheLLo\n\n\n\n\n\n\nCautionCheck your solution\n\n\n\n\n\nfunction archive() {\n    if [ $# -eq 0 ]; then\n        echo \"No arguments given. Usage: archive dir1 dir2 ...\"\n        return 1\n    fi\n    for dir in $@; do\n        tar cvfz ${dir/\\//}.tar.gz $dir && /bin/rm -r $dir\n    done\n}\n\n\n\nTake-home exercise: write unarchive() that would do the opposite, i.e. take a set of .tar.gz files via arguments and expand each of them."
  },
  {
    "objectID": "bashScripting.html#rename-all-files-with-a-pattern",
    "href": "bashScripting.html#rename-all-files-with-a-pattern",
    "title": "Bash scripting for beginners",
    "section": "Rename all files with a pattern",
    "text": "Rename all files with a pattern\nStart with a simple example with 83 files:\ntouch 2022-Jan-{0{1..9},{10..31}}.md\ntouch 2022-Feb-{0{1..9},{10..28}}.md\ntouch 2022-Mar-{0{1..9},{10..24}}.md\nTask: convert all months in the filenames to digital months, e.g. 2022-Jan-01.md should become 20220101.md.\n\n\n\n\n\n\nCautionCheck your solution\n\n\n\n\n\nfor f in *Jan*md; do\n  mv $f ${f/-Jan-/01}\ndone"
  },
  {
    "objectID": "bashScripting.html#convert-spaces-to-underscores",
    "href": "bashScripting.html#convert-spaces-to-underscores",
    "title": "Bash scripting for beginners",
    "section": "Convert spaces to underscores",
    "text": "Convert spaces to underscores\ntouch hello \"first phrase\" \"second phrase\" \"good morning, everyone\"\nls -l\nls *\\ *\nLet’s write takeOutSpaces() that will scan the current directory for all files with spaces in their file names and convert these spaces to underscores.\n\n\n\n\n\n\nCautionCheck your solution\n\n\n\n\n\nfunction takeOutSpaces() {\n    for file in *\\ *; do\n        mv \"$file\" \"${file// /_}\"\n    done\n}\n\n\n\nAfter we are done: how about a recursive scan? This is trickier! E.g., count the number of --- in the output of this script:\nfor f in \"$(find . -type f -name '* *')\"; do\n    echo \"$f ---\"\ndone"
  },
  {
    "objectID": "bashScripting.html#incorporating-python-into-your-bash-script",
    "href": "bashScripting.html#incorporating-python-into-your-bash-script",
    "title": "Bash scripting for beginners",
    "section": "Incorporating Python into your bash script",
    "text": "Incorporating Python into your bash script\nYou might want to write a bash function that processes text, e.g.\n$ echo \"email us at training@westgrid.ca more text some.name.here@sfu.com more text \\\"bob@ubc.com\\\"\" &gt; message.txt\n$ echo \"another.name@ubc.ca some text here\" &gt;&gt; message.txt\n$ extractEmails message.txt\ntraining@westgrid.ca, some.name.here@sfu.com, bob@ubc.com, another.name@ubc.ca\nWhile you can do this in bash, it is much easier to achieve this in Python which is fantastic at text processing. It turns out that you can easily call a Python script from a bash function! Consider this:\nfunction test() {\n    cat &lt;&lt; EOF &gt; e9nsp0lsb1.py   # random fixed string\n#!/usr/bin/python3\nprint(\"do something in Python\")\nEOF\n    chmod u+x e9nsp0lsb1.py\n    ./e9nsp0lsb1.py\n    /bin/rm e9nsp0lsb1.py\n}\nHere is an example of a more complex function extracting all emails from a text file:\nfunction extractEmails() {\n    mv $1 nidhefsxzd.txt\n    cat &lt;&lt; EOF &gt; e9nsp0lsb1.py\n#!/usr/bin/python3\nimport re\nfilename = open(\"nidhefsxzd.txt\", \"r\")\ncontent = filename.readlines()\nemails = []\nfor i, line in enumerate(content):\n    email = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', line)\n    if len(email) &gt; 0:\n        for e in email:\n            emails.append(e)\nprint(', '.join(map(str, emails)))   # print all emails in a single line without quotes\nEOF\n    chmod u+x e9nsp0lsb1.py\n    ./e9nsp0lsb1.py\n    /bin/rm e9nsp0lsb1.py\n    mv nidhefsxzd.txt $1\n}"
  },
  {
    "objectID": "programmableFilter.html",
    "href": "programmableFilter.html",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "",
    "text": "November 14th, 10am-noon Pacific Time\nInstructor: Alex Razoumov (SFU)\nSoftware: To participate in the course exercises, you will need to install ParaView on your computer and download this ZIP file (27MB). Unpack it to find the following data files: cube.txt, tabulatedGrid.csv, compact150.nc, and sineEnvelope.nc."
  },
  {
    "objectID": "programmableFilter.html#paraview",
    "href": "programmableFilter.html#paraview",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "ParaView",
    "text": "ParaView\n\n\nhttp://www.paraview.org\nStarted in 2000 as a collaboration between Los Alamos National Lab and Kitware Inc., later joined by Sandia National Labs and other partners; first public release in 2002\nSource code available on GitHub, or can be downloaded as a pre-compiled binary for Linux/Mac/Windows\nTo visualize extremely large datasets on distributed memory machines\nBoth interactive and Python scripting\nClient-server for remote interactive visualization\nUses MPI for distributed-memory parallelism on HPC clusters\nParaView is based on VTK (Visualization Toolkit)\n\nnot the only VTK-based open-source scientific renderer, e.g. VisIt, MayaVi (Python + numpy + scipy + VTK), an of course a number of Kitware’s own tools besides ParaView are based on VTK\nVTK can be used as a standalone renderer from C++, Python, and now JavaScript\n\n\nWe teach both beginner’s (typically full-day) ParaView workshops and additional, more advanced topics:\n🔵 VTK programming (overlap with today’s topic)\n✅ Web-based 3D scientific visualization: ParaViewWeb, VTK.js, ParaView Glance, ParaView Lite, Visualizer\n✅ ParaView Cinema\n✅ Catalyst in-situ visualization library\n🔵 AMR (multi-resolution) & multi-block datasets\n✅ CPU-based ray tracing and photorealistic rendering with OSPRay\n✏️ Ray tracing on CUDA-supported cards with Nvidia OptiX ✏️ Scalable 3D volumetric visualization on GPU clusters with NVIDIA IndeX\n✅ Remote and parallel interactive visualization\n✅ Batch visualization\n✅ Advanced scripting\n✅ Programmable Filter & Source in ParaView\n🔵 Writing ParaView plugins\n✅ Advanced animation\n✅ Topological data analysis\n✏️ Using ParaView with special hardware: HMDs, stereo projectors, Looking Glass\nYou can watch some of these webinar recordings at https://bit.ly/vispages.\n\nPython scripting\nInside ParaView you can use Python scripting in several places:\n\nAny ParaView workflow can be saved as a Python script (Tools | Start/Stop Trace)\n\nrun via pvpython, or pvbatch (especially useful for batch visualization)\nrun via View | Python Shell\n\nAny ParaView state can be saved as a Python script (File | Save State)\n\nrun via File | Load State\n\nFilters | Python Calculator\nFilters | Programmable Filter / Source\n\nWhy would you want to use the Programmable Filter / Source? Fundamentally, these tools let you create custom VTK objects directly from ParaView. Let’s say we want to plot a projection of a cubic dataset along one of its principal axes, or do some other transformation for which there is no filter.\n\nCalculator / Python Calculator filter cannot modify the geometry …\n\n\nVTK data types\n\nIn today’s examples we will create datasets in the following formats:\n\nvtkImageData,\nvtkPolyData, and\nvtkStructuredGrid.\n\nFor simplicity, we will skip the examples with Output Data Set Type = vtkUnstructuredGrid – that would produce the most versatile object, but with any unstructured cells you will have to describe the connections between their points needed to form these cells. You can find a couple of examples of creating vtkUnstructuredGrid in a 2021 webinar.\n\n\nProgrammable Filter workflow\n\nApply Programmable Filter to a ParaView pipeline object\nSelect Output Data Set Type: either Same as Input, or one of the VTK data types from above\nIn the Script box, write Python code: input from a pipeline object → output\n\n\nuse inputs[0].Points[:,0:3] and/or inputs[0].PointData['array name'] and/or inputs[0].CellData['array name'] to compute your output: points, cells, and data arrays\nsome objects take/pass multiple inputs[:]\n\n\n\n\nDepending on output’s type, might need to describe it in the RequestInformation Script box\nHit Apply\n\n\n\nProgrammable Source workflow\nSame as Programmable Filter but without an input.\n\nbuild an object programmatically\nread and process data from a file"
  },
  {
    "objectID": "programmableFilter.html#simple-programmable-filter-example",
    "href": "programmableFilter.html#simple-programmable-filter-example",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Simple Programmable Filter example",
    "text": "Simple Programmable Filter example\nStart with a very simple example. Let’s create a 2D Gaussian\n\\[f(\\vec r)=e^{-\\frac{|\\vec r-\\vec r_0|^2}{2\\sigma^2}},\\quad\\vec r\\in\\mathbb{R}^2\\]\ncentered at (0,0).\n\nApply Sources | Plane at \\(100^2\\) resolution\nApply Programmable Filter\n\n\nset Output Data Set Type = Same as Input\nthis will create the same discretization (vtkPolyData) as its input, without the fields (PointData)\n\n\nLet’s print some info\n\nprint(type(output))   # paraview.vtk.numpy_interface.dataset_adapter.PolyData\nprint(dir(output))\nprint(output.Points.shape)   # actually a 3D dataset\nprint(output.Points)         # but all z-coordinates are 0s\n\nPaste into Script\n\nx = inputs[0].Points[:,0]\ny = inputs[0].Points[:,1]\noutput.Points[:,2] = 0.5*exp(-(x**2+y**2)/(2*0.02))\ndataArray = 0.3 + 2*output.Points[:,2]\noutput.PointData.append(dataArray, \"pnew\")   # add the new data array to our output\n\nDisplay in 3D, colour with pnew\n\n\n\n\n\n\n\n\nA scalar field is stored as a flat, 1D array over 3D points\n\nprint(output.PointData[\"pnew\"].shape)\n\nMultiple ways to access data, e.g. these two lines point to the same array\n\nprint(output.PointData[\"pnew\"])\nprint(output.GetPointData().GetArray(\"pnew\"))\n\nThe same for points\n\nprint(output.GetNumberOfPoints())\nprint(output.GetPoint(1005))\nprint(output.Points[1005,:])"
  },
  {
    "objectID": "programmableFilter.html#programmable-source",
    "href": "programmableFilter.html#programmable-source",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Programmable Source",
    "text": "Programmable Source\nLet’s switch to the Programmable Source. It has no input, so we will need to set its Output Data Set Type to one of the VTK data types.\n\nBuild a custom reader\nA common use of Programmable Source is to prototype readers. Let’s quickly create our own reader for a CSV file with 3D data on a grid.\nWe will start with a conventional approach to reading CSV files:\n\nFile | Open, navigate to tabulatedGrid.csv, select CSV Reader\nPass tabular data through Table To Structured Grid\n\n\nset data extent in each dimension\nspecify x/y/z columns\ncolour with scalar\n\nNow let’s replace this with reading the CSV file directly to the Cartesian mesh, without using the File | Open dialogue. Reset your ParaView and then:\n\nApply Programmable Source, set Output Data Set Type = vtkImageData\nInto Script paste this code: \n\nimport numpy as np\ndata = np.genfromtxt(\"/Users/razoumov/programmableFilter/tabulatedGrid.csv\",\n                     dtype=None, names=True, delimiter=',', autostrip=True)\n\nnx = len(set(data['x']))\nny = len(set(data['y']))\nnz = len(set(data['z']))\n\noutput.SetDimensions(nx,ny,nz)\noutput.SetExtent(0,nx-1,0,ny-1,0,nz-1)\noutput.SetOrigin(0,0,0)\noutput.SetSpacing(.1,.1,.1)\noutput.AllocateScalars(vtk.VTK_FLOAT,1)\n\nvtk_data_array = vtk.util.numpy_support.numpy_to_vtk(num_array=data['scalar'], array_type=vtk.VTK_FLOAT)\nvtk_data_array.SetNumberOfComponents(1)\nvtk_data_array.SetName(\"density\")\noutput.GetPointData().SetScalars(vtk_data_array)\n\nThe Image (Uniform Rectilinear Grid) array is properly created (check the Information tab!), but nothing shows up …\n\nWe need to tell the ParaView pipeline about the dimensionality of our vtkImageData!\n\nInto Request Information paste this code:\n\nfrom paraview import util\nnx, ny, nz = 10, 10, 10\nutil.SetOutputWholeExtent(self, [0,nx-1,0,ny-1,0,nz-1])\n\n\nExercise: reveal the data in the cube\nThe file cube.txt has 125,000 integers that were computed and printed with this Julia code: - 50 lines - each line contains 2500 integers\nfor i in 1:50\n    for j in 1:50\n        for k in 1:50\n            print(data[i,j,k], \" \")\n        end\n    end\n    println()\nend\nYou need to modify the previous Programmable Source to read this data.\n\n\n\nWrite a 3D function directly into Cartesian mesh: numpy → VTK\nSimilar to using a Source to read data from a file, you can use a Source to create a discretized version of a 3D function.\n\nApply Programmable Source, set Output Data Set Type = vtkImageData\nInto Script paste this code:\n\nfrom numpy import linspace, sin, sqrt\n\nn = 100   # the size of our grid\noutput.SetDimensions(n,n,n)\noutput.SetOrigin(0,0,0)\noutput.SetSpacing(.1,.1,.1)\noutput.SetExtent(0,n-1,0,n-1,0,n-1)\noutput.AllocateScalars(vtk.VTK_FLOAT,1)\n\nx = linspace(-7.5,7.5,n)   # three orthogonal discretization vectors\ny = x.reshape(n,1)\nz = x.reshape(n,1,1)\ndata = ((sin(sqrt(y*y+x*x)))**2-0.5)/(0.001*(y*y+x*x)+1.)**2 + \\\n    ((sin(sqrt(z*z+y*y)))**2-0.5)/(0.001*(z*z+y*y)+1.)**2 + 1.\n\nvtk_data_array = vtk.util.numpy_support.numpy_to_vtk(\n    num_array=data.ravel(), deep=False, array_type=vtk.VTK_FLOAT)\nvtk_data_array.SetNumberOfComponents(1)\nvtk_data_array.SetName(\"density\")\noutput.GetPointData().SetScalars(vtk_data_array)   # use it as a scalar field in the output\n\nInto Request Information paste this code:\n\nfrom paraview import util\nn = 100\nutil.SetOutputWholeExtent(self, [0,n-1,0,n-1,0,n-1])\n\n\n\nSingle helix source example from ParaView docs\nLet’s create from scratch a Polygonal Data object.\n\nApply Programmable Source, set Output Data Set Type = vtkPolyData\nPaste this code:   \n\nimport numpy as np\nimport vtk.numpy_interface.algorithms as alg\n\nnumPoints, length, rounds = 300, 8.0, 5\n\nindex = np.arange(0, numPoints, dtype=np.int32)   # 0, ..., numPoints-1\nphi = rounds * 2 * np.pi * index / numPoints\nx, y, z = index * length / numPoints, np.sin(phi), np.cos(phi)\ncoordinates = alg.make_vector(x, y, z)   # numpy array (numPoints,3)\n\noutput.Points = coordinates             # set point coordinates\noutput.PointData.append(phi, 'angle')   # append a scalar field on points\n\npointIds = vtk.vtkIdList()\npointIds.SetNumberOfIds(numPoints)\nfor i in range(numPoints):   # define a single polyline connecting all the points in order\n   pointIds.SetId(i, i)      # point i in the line is formed from point i in vtkPoints\n\noutput.Allocate(1)     # allocate space for one vtkPolyLine 'cell' to the vtkPolyData object\noutput.InsertNextCell(vtk.VTK_POLY_LINE, pointIds)   # add this 'cell' to the vtkPolyData object\n\n\n\n\nProgrammatically generated point cloud\nHere we use exactly the same technique (use Source to create a vtkPolyData object, add a bunch of points, create a single cell) with a very different looking result.\n\nApply Programmable Source, set Output Data Set Type = vtkPolyData\nPaste this code:    \n\nfrom numpy import abs, random, sin, cos, pi, arcsin\n\nnumPoints = 1_000_000\nr = abs(random.randn(numPoints))               # 1D array drawn from a normal (Gaussian) distribution\ntheta = arcsin(2.*random.rand(numPoints)-1.)   # use 1D array drawn from a uniform [0,1) distribution\nphi = 2.*pi*random.rand(numPoints)             # use 1D array drawn from a uniform [0,1) distribution\n\nx = r*cos(theta)*sin(phi)\ny = r*cos(theta)*cos(phi)\nz = r*sin(theta)\ncoordinates = vtk.numpy_interface.algorithms.make_vector(x, y, z)   # numpy array (numPoints,3)\n\noutput.Points = coordinates              # set point coordinates\noutput.PointData.append(r, \"r\")          # append a scalar field on points\noutput.PointData.append(phi, \"phi\")      # append another scalar field on points\n\nPoints are not visible in ParaView ⇨ either (a) switch to Point Gaussian representation, or (b) in Script’s output create a single cell without connections\n\npointIds = vtk.vtkIdList()\npointIds.SetNumberOfIds(numPoints)\nfor p in range(numPoints):\n    pointIds.SetId(p, p)\noutput.Allocate(1)     # allocate space for a single cell\noutput.InsertNextCell(vtk.VTK_POLY_VERTEX, pointIds)\n\n\nColour by radius\nApply Clip (need the cell for that)\nOptionally switch back to Point Gaussian representation"
  },
  {
    "objectID": "programmableFilter.html#projection-to-a-plane-numpy-vtk",
    "href": "programmableFilter.html#projection-to-a-plane-numpy-vtk",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Projection to a plane: numpy → VTK",
    "text": "Projection to a plane: numpy → VTK\n\nLoad sineEnvelope.nc\nApply Programmable Filter, set Output Data Set Type = vtkImageData\nInto Script paste this code: \n\nnumPoints = inputs[0].GetNumberOfPoints()\nside = round(numPoints**(1./3.))\nlayer = side*side\nrho = inputs[0].PointData['density']\n\noutput.SetOrigin(inputs[0].GetPoint(0)[0], inputs[0].GetPoint(0)[1], -20.)\noutput.SetSpacing(1.0, 1.0, 1.0)\noutput.SetDimensions(side, side, 1)\noutput.SetExtent(0,99,0,99,0,1)\noutput.AllocateScalars(vtk.VTK_FLOAT, 1)\n\nrho3D = rho.reshape(side, side, side)\nvtk_data_array = vtk.util.numpy_support.numpy_to_vtk(rho3D.sum(axis=2).ravel(),\n                       deep=False, array_type=vtk.VTK_FLOAT)\nvtk_data_array.SetNumberOfComponents(1)\nvtk_data_array.SetName(\"projection\")\noutput.GetPointData().SetScalars(vtk_data_array)\n\nInto Request Information paste this code:\n\nfrom paraview import util\nn = 100\nutil.SetOutputWholeExtent(self, [0,n-1,0,n-1,0,0])"
  },
  {
    "objectID": "programmableFilter.html#exercise-plotting-spherical-dataset-as-3d-mollweide-map",
    "href": "programmableFilter.html#exercise-plotting-spherical-dataset-as-3d-mollweide-map",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Exercise: plotting spherical dataset as 3D Mollweide map",
    "text": "Exercise: plotting spherical dataset as 3D Mollweide map\n\n\n\nSpherical dataset animation: the traditional view\nBest Cover Visualization submission with the 3D Mollweide projection\nOn presenter’s laptop watch the full video open $(fd vis21g-sub1007-i5.mp4 ~/Documents)\n\n\nPlaying with the dataset in standalone Python\nimport xarray as xr\ndata = xr.open_dataset('/Users/razoumov/programmableFilter/compact150.nc')\nprint(data)                      # show all variables inside this dataset\n\nprint(data.r)                    # radial discretization\nprint(data.temperature.values)   # this is a 180x201x360 numpy array\ndata[\"temperature anomaly\"].shape\n\nprint(data.lon.values)\nprint(data.lat.values)\nAlthough the Mollweide projection is available in packages like matplotlib, we need a standalone implementation that takes the geographical coordinates and returns the Cartesian coordinates in the projection. I used the formulae from the wiki page to write my own function:\nfrom math import sin, cos, pi, sqrt, radians\ndef mollweide(lon, lat):\n    \"\"\"\n    lon: input longitude in degrees\n    lat: input latitude in degrees\n    \"\"\"\n    lam, phi = radians(lon), radians(lat)\n    lam0 = radians(180)   # central meridian\n    tolerance = 1.0e-5\n    if abs(abs(phi)-pi/2.) &lt; tolerance:\n        theta = phi\n    else:\n        theta, dtheta = phi, 1e3\n        while abs(dtheta) &gt; tolerance:\n            twotheta = 2 * theta\n            dtheta = theta\n            theta = theta - (twotheta + sin(twotheta) - pi*sin(phi))/(2 + 2*cos(twotheta))\n            dtheta -= theta\n    s2 = sqrt(2)\n    x = 2 * s2/pi * (lam-lam0) * cos(theta)\n    y = s2 * sin(theta)\n    return (x,y)\n\nlam = data.lon.values[100]\nphi = data.lat.values[50]\nmollweide(lam,phi)\n\n\nBack to ParaView\nOur goal is to create something like this: \n\nload compact150.nc, uncheck Spherical Coordinates\napply Programmable Filter, set Output Data Set Type = vtkStructuredGrid\nlet’s start playing with the data in the Script dialogue:\n\nprint(inputs[0].GetNumberOfPoints())         # 13024800 = 360*201*180\npointIndex = 0   # any inteteger between 0 and 13024799\nprint(inputs[0].GetPoint(pointIndex)[0:3])   # (0.0, 3485.0, 90.0)\nprint(inputs[0].PointData['temperature'])\nprint(inputs[0].PointData[\"temperature\"].GetValue(pointIndex))\nWe can paste the entire mollweide(lon, lat) function definition into the Programmable Filter, or – to shorten the code inside the filter – we can load it from a file. Let’s save it in mollweide.py and then test it from a standalone Python shell:\nimport sys\nsys.path.insert(0, \"/Users/razoumov/programmableFilter\")\nfrom mollweide import mollweide\nmollweide(100,3)\nHere is what our filter will look like:\nfrom math import sin, cos, pi, sqrt, radians\nimport numpy as np\nimport sys\nsys.path.insert(0, \"/Users/razoumov/programmableFilter\")\nfrom mollweide import mollweide\n\ntemp_in = inputs[0].PointData[\"temperature\"]\n\nnlon, nr, nlat = 360, 201, 180\npoints = vtk.vtkPoints()\npoints.Allocate(nlon*nr*nlat)\ntemp = vtk.vtkDoubleArray() # create vtkPoints instance, to contain 100^2 points in the projection\ntemp.SetName(\"temperature\")\n\noutput.SetExtent([0,nlon-1,0,nr-1,0,nlat-1])   # should match SetOutputWholeExtent below\nx, y = np.zeros((nlon,nlat)), np.zeros((nlon,nlat))\nfor i in range(nlat):   # the order of loops should reverse-match SetExtent\n    for j in range(nr):\n        for k in range(nlon):\n            inx = k + i*nlon*nr + j*nlon\n            if j == 0:   # inner radius = base layer\n                lon, r, lat = inputs[0].GetPoint(inx)[0:3]\n                x[k,i], y[k,i] = mollweide(lon,lat)\n            points.InsertNextPoint(x[k,i],y[k,i],0.01*j)\n            temp.InsertNextValue(temp_in.GetValue(inx))\n\noutput.SetPoints(points)\noutput.GetPointData().SetScalars(temp)\nOptionally, you can paste the following into RequestInformation Script:\nfrom paraview import util\nnlon, nr, nlat = 360, 201, 180\nutil.SetOutputWholeExtent(self,[0,nlon-1,0,nr-1,0,nlat-1])   # the order is fixed and copied from input"
  },
  {
    "objectID": "programmableFilter.html#saving-workflow",
    "href": "programmableFilter.html#saving-workflow",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Saving workflow",
    "text": "Saving workflow\n\nSave as a ParaView state file ⇨ filter’s Python code with appear inside an XML element\nSave as a Python state file ⇨ filter’s Python code with appear inside programmableFilter1.Script variable"
  },
  {
    "objectID": "programmableFilter.html#summary",
    "href": "programmableFilter.html#summary",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Summary",
    "text": "Summary\n\nWorkflow suggestions\n\nuse print from inside the Filter/Source\ngo slowly and save frequently, as ParaView will crash when you don’t allocate objects properly inside Programmable Filter/Source\n\nLinks\n\nProgrammable Filter in the offical ParaView documentation\nan article NumPy to VTK: Converting your NumPy arrays to VTK arrays and files\nfirst half of the talk by Jean M. Favre (Swiss National Supercomputing Centre)\n\nFuture topic: converting Programmable Filter’s code to a plugin\n\ncode its own custom GUI Properties using Python decorators\nafter loading your plugin, it should be available as a normal filter or source in the menu"
  },
  {
    "objectID": "julia1/julia-07-distributed1.html",
    "href": "julia1/julia-07-distributed1.html",
    "title": "Distributed.jl - basics",
    "section": "",
    "text": "Parallelizing with multiple Unix processes (MPI tasks)\nJulia’s Distributed package provides multiprocessing environment to allow programs to run on multiple processors in shared or distributed memory. On each CPU core you start a separate Unix / MPI process, and these processes communicate via messages. Unlike in MPI, Julia’s implementation of message passing is one-sided, typically with higher-level operations like calls to user functions on a remote process.\n\na remote call is a request by one processor to call a function on another processor; returns a remote/future reference\nthe processor that made the call proceeds to its next operation while the remote call is computing, i.e. the call is non-blocking\nyou can obtain the remote result with fetch() or make the calling processor block with wait()\n\nIn this workflow you have a single control process + multiple worker processes. Processes pass information via messages underneath, not via variables in shared memory.\n\n\nLaunching worker processes\nThere are three different ways you can launch worker processes:\n\nwith a flag from bash:\n\njulia -p 8             # open REPL, start Julia control process + 8 worker processes\njulia -p 8 code.jl     # run the code with Julia control process + 8 worker processes\n\nfrom a job submission script:\n\n#!/bin/bash\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\n#SBATCH --account=def-someuser\nsrun hostname -s &gt; hostfile   # parallel I/O\nsleep 5\nmodule load julia/1.7.0\njulia --machine-file ./hostfile ./code.jl\n\nfrom the control process, after starting Julia as usual with julia:\n\nusing Distributed\naddprocs(8)\n\n\n\n\n\n\nNote\n\n\n\nAll three methods launch workers, so combining them will result in 16 (or 24!) workers – probably not the best idea. Select one method and use it.\n\n\nWith Slurm, methods (1) and (3) work very well, so—when working on a CC cluster—usually there is no need to construct a machine file.\n\n\nProcess control\nLet’s start an interactive MPI job:\nsource /project/def-sponsor00/shared/julia/config/loadJulia.sh\nsalloc --mem-per-cpu=3600M --time=2:0:0 --ntasks=4\nInside this job, start Julia with julia (single control process).\nusing Distributed\naddprocs(4)   # add 4 worker processes; this might take a while on the training cluster\nprintln(\"number of processes = \", nprocs())   # control process + 4 workers\nprintln(\"number of workers = \", nworkers())   # 4 workers\nworkers()                                     # list worker IDs\nYou can easily remove selected workers from the pool:\nrmprocs(2, 3, waitfor=0)   # remove processes 2 and 3 immediately\nworkers()\nor you can remove all of them:\nfor i in workers()     # cycle through all workers\n    t = rmprocs(i, waitfor=0)\n    wait(t)            # wait for this operation to finish\nend\nworkers()\ninterrupt()   # will do the same (remove all workers)\naddprocs(4)   # add 4 new worker processes (notice the new IDs!)\nworkers()\n\n\n\n\n\n\nNoteDiscussion\n\n\n\nIf from the control process we start \\(N=8\\) workers, where will these processes run? Consider the following cases:\n1. a laptop with 2 CPU cores,\n2. a cluster login node with 16 CPU cores,\n3. a cluster Slurm job with 4 CPU cores.\n\n\n\n\nRemote calls\nLet’s restart Julia with julia (single control process).\nusing Distributed\naddprocs(4)       # add 4 worker processes\nLet’s define a function on the control process and all workers and run it:\n@everywhere function showid()   # define the function everywhere\n    println(\"my id = \", myid())\nend\nshowid()                        # run the function on the control process\n@everywhere showid()            # run the function on the control process + all workers\n@everywhere does not capture any local variables (unlike @spawnat that we’ll study below), so on workers we don’t see any variables from the control process:\nx = 5     # local (control process only)\n@everywhere println(x)    # get errors: x is not defined elsewhere\nHowever, you can still obtain the value of x from the control process by using this syntax:\n@everywhere println($x)   # use the value of `x` from the control process\nThe macro that we’ll use a lot today is @spawnat. If we type:\na = 12\n@spawnat 2 println(a)     # will print 12 from worker 2\nit will do the following:\n\npass the namespace of local variables to worker 2\nspawn function execution on worker 2\nreturn a Future handle (referencing this running instance) to the control process\nreturn the REPL to the control process (while the function is running on worker 2), so we can continue running commands\n\nNow let’s modify our code slightly:\na = 12\n@spawnat 2 a+10          # Future returned but no visible calculation\nThere is no visible calculation happening; we need to fetch the result from the remote function before we can print it:\nr = @spawnat 2 a+10\ntypeof(r)\nfetch(r)                 # get the result from the remote function; this will pause\n                         #         the control process until the result is returned\nYou can combine both @spawnat and fetch() in one line:\nfetch(@spawnat 2 a+10)   # combine both in one line; the control process will pause\n@fetchfrom 2 a+10        # shorter notation; exactly the same as the previous command\n\n\n\n\n\n\nCautionExercise Distributed.1\n\n\n\n\n\nTry to define and run a function on one of the workers, e.g.\nfunction cube(x)\n    return x*x*x\nend\nHint: Use @everywhere to define the function on all workers. Julia may not have a high-level mechanism to define a function on a specific worker, short of loading that function as a module from a file. Something like this\n@fetchfrom 2 function cube(x)\n    return x*x*x\nend\ndoes not seem to have any effect.\n\n\n\n\n\n\n\n\n\nCautionExercise Distributed.2\n\n\n\n\n\nNow run the same function on all workers, but not on the control process. Hint: use workers() to cycle through all worker processes and println() to print from each worker.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can also spawn computation on any available worker:\nr = @spawnat :any log10(a)   # start running on one of the workers\nfetch(r)\n\n[@spawnat :any showid() for i in 1:10]   # using array comprehension\n\n\nBack to the slow series: serial code\nLet’s restart Julia with julia -p 2 (control process + 2 workers). We’ll start with our serial code (below), save it as serialDistributed.jl, and run it.\nusing Distributed\nusing BenchmarkTools\n\n@everywhere function digitsin(digitSequence::Int, num)\n    base = 10\n    while (digitSequence ÷ base &gt; 0); base *= 10; end\n    while num &gt; 0; (num % base) == digitSequence && return true; num ÷= 10; end\n    return false\nend\n\n@everywhere function slow(n::Int64, digitSequence::Int)\n    total = Int64(0)\n    for i in 1:n\n        if !digitsin(digitSequence, i)\n            total += 1.0 / i\n        end\n    end\n    return total\nend\n\n@btime slow(Int64(1e8), 9)     # serial run: total = 13.277605949858103\nFor me this serial run takes 2.225 on the training cluster. Next, let’s run it on 3 (control + 2 workers) cores simultaneously:\n@everywhere using BenchmarkTools\n@everywhere @btime slow(Int64(1e8), 9)   # runs on 3 (control + 2 workers) cores simultaneously\nHere we are being silly: this code is serial, so each core performs the same calculation … I see the following times printed on my screen: 3.220s, 2.927s, 3.211s – each is from a separate process running the code in a serial fashion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we make this code parallel and faster?\n\n\nParallelizing our slow series: non-scalable version\nLet’s restart Julia with julia (single control process) and add 2 worker processes:\nusing Distributed\naddprocs(2)\nworkers()\nWe need to redefine digitsin() everywhere, and then let’s modify slow() to compute a partial sum:\n@everywhere function slow(n::Int, digitSequence::Int, taskid, ntasks)   # two additional arguments\n    println(\"running on worker \", myid())\n    total = 0.0\n    @time for i in taskid:ntasks:n   # partial sum with a stride `ntasks`\n        if !digitsin(digitSequence, i)\n            total += 1.0 / i\n        end\n    end\n    return(total)\nend\nNow we can actually use it:\n# slow(Int64(10), 9, 1, 2)   # precompile the code\nprecompile(slow, (Int, Int, Int, Int))\na = @spawnat :any slow(Int64(1e8), 9, 1, 2)\nb = @spawnat :any slow(Int64(1e8), 9, 2, 2)\nprint(\"total = \", fetch(a) + fetch(b))   # 13.277605949852546\nFor timing I got 1.30 s and 1.66 s, running concurrently, which is a 2X speedup compared to the serial run—this is great! Notice that we received a slightly different numerical result, due to a different order of summation.\nHowever, our code is not scalable: it is limited to a small number of sums each spawned with its own Future reference. If we want to scale it to 100 workers, we’ll have a problem.\nHow do we solve this problem—any idea before I show the solution in the next section?",
    "crumbs": [
      "PART 2",
      "Distributed.jl: basics"
    ]
  },
  {
    "objectID": "julia1/julia-01-intro-language.html",
    "href": "julia1/julia-01-intro-language.html",
    "title": "Introduction to Julia language",
    "section": "",
    "text": "High-performance, dynamically typed programming language for scientific computing\nUses just-in-time (JIT) compiler to compile all code, includes an interactive command line (REPL = read–eval–print loop, and can also be run in Jupyter), i.e. tries to combine the advantages of both compiled and interpreted languages\nBuilt-in package manager\nLots of interesting design decisions, e.g. macros, support for Unicode, etc\nSupport for parallel and distributed computing via its Standard Library and many 3rd party packages\n\nbeing added along the way, e.g. @threads were first introduced in v0.5\ncurrently under active development, both in features and performance",
    "crumbs": [
      "PART 1",
      "Introduction to Julia language"
    ]
  },
  {
    "objectID": "julia1/julia-01-intro-language.html#the-julia-programming-language",
    "href": "julia1/julia-01-intro-language.html#the-julia-programming-language",
    "title": "Introduction to Julia language",
    "section": "",
    "text": "High-performance, dynamically typed programming language for scientific computing\nUses just-in-time (JIT) compiler to compile all code, includes an interactive command line (REPL = read–eval–print loop, and can also be run in Jupyter), i.e. tries to combine the advantages of both compiled and interpreted languages\nBuilt-in package manager\nLots of interesting design decisions, e.g. macros, support for Unicode, etc\nSupport for parallel and distributed computing via its Standard Library and many 3rd party packages\n\nbeing added along the way, e.g. @threads were first introduced in v0.5\ncurrently under active development, both in features and performance",
    "crumbs": [
      "PART 1",
      "Introduction to Julia language"
    ]
  },
  {
    "objectID": "julia1/julia-01-intro-language.html#running-julia-locally",
    "href": "julia1/julia-01-intro-language.html#running-julia-locally",
    "title": "Introduction to Julia language",
    "section": "Running Julia locally",
    "text": "Running Julia locally\nIf you have Julia installed on your own computer, you can run it there: on a multi-core laptop/desktop you can launch multiple threads and processes and run them in parallel.\nIf you want to install Julia after this workshop, you can find it here.",
    "crumbs": [
      "PART 1",
      "Introduction to Julia language"
    ]
  },
  {
    "objectID": "julia1/julia-01-intro-language.html#using-julia-on-supercomputers",
    "href": "julia1/julia-01-intro-language.html#using-julia-on-supercomputers",
    "title": "Introduction to Julia language",
    "section": "Using Julia on supercomputers",
    "text": "Using Julia on supercomputers\n\nJulia on the Alliance production clusters\nJulia is among hundreds of software packages installed on the Alliance clusters. To use Julia on one of them, you would load the following module:\n$ module spider julia   # show the available versions\n$ module load julia     # load the default version\n\nInstalling Julia packages on a production cluster\n\n\n\n\n\n\nNote\n\n\n\nDon’t do this on the training cluster! We already have everything installed in a central location for all guest accounts.\n\n\nBy default, all Julia packages you install from REPL will go into $HOME/.julia, and that we will count against your /home quota. If you want to put packages into another location, you will need to (1) install inside your Julia session with (from within Julia):\nempty!(DEPOT_PATH)\npush!(DEPOT_PATH,\"/scratch/path/to/julia/packages\") \n] add BenchmarkTools\nand (2) before running Julia modify two variables (from the command line):\n$ module load julia\n$ export JULIA_DEPOT_PATH=/home/\\$USER/.julia:/scratch/path/to/julia/packages\n$ export JULIA_LOAD_PATH=@:@v#.#:@stdlib:/scratch/path/to/julia/packages\n\n\n\n\n\n\nNote\n\n\n\nSome Julia packages rely on precompiled bits that developers think would work on all architectures, but they don’t. For example, Plots package comes with several precompiled libraries, it installs without problem on the Alliance clusters, but then at runtime you might see an error about “GLIBC_2.18 not found”. The proper solution would be to recompile the package on the cluster, but it is not done correctly in Julia packaging, and the error persists even after “recompilation”. There is a solution for this, and you can always contact us at support@tech.alliancecan.ca and ask for help. Another example is Julia’s NetCDF package: as of February 2022, it was installing fine on Apple Silicon Macs, but it came with a precompiled C package that had been compiled only for Intel Macs and did not work on M1. This issue has been resolved since then, but the point remains: successful installation of a Julia package does not mean it’ll work on your architecture.\n\n\n\n\n\nJulia on the training cluster for this workshop\nWe have Julia on our training cluster julia.vastcloud.org.\n\n\n\n\n\n\nNote\n\n\n\nOur training cluster has:\n\n\none fairly small login node,\n\n\n6 compute nodes with 8 cores and 30GB of memory on each  ⟹  3.75GB/core, 48 cores in total\n\n\n\n\nNormally in our introductory Julia course we would use Julia inside a Jupyter notebook. Today we will be starting multiple threads and processes, with the eventual goal of running our workflows as batch jobs on an HPC cluster, so we’ll be using Julia from the command line.\n\n\n\n\n\n\nCautionTraining cluster\n\n\n\nWe will now distribute accounts and passwords to connect to the cluster.\n\n\nNormally, you would install Julia packages yourself. A typical package installation however takes several hundred MBs of RAM, a fairly long time, and creates many small files. Our training cluster runs on top of virtualized hardware with a shared filesystem. If several dozen workshop participants start installing packages at the same time, this will hammer the filesystem and will make it slow for all participants for quite a while.\nInstead, for this workshop, you will run:\n$ source /project/def-sponsor00/shared/julia/config/loadJulia.sh\nThis script loads the Julia module and sets environment variables to point to a central environment in which we have pre-installed all the packages for this workshop.\n\n\n\n\n\n\nNote\n\n\n\nNote that you can still install additional packages if you want. These will install in your own environment at ~/.julia.",
    "crumbs": [
      "PART 1",
      "Introduction to Julia language"
    ]
  },
  {
    "objectID": "julia1/julia-01-intro-language.html#running-julia-in-the-repl",
    "href": "julia1/julia-01-intro-language.html#running-julia-in-the-repl",
    "title": "Introduction to Julia language",
    "section": "Running Julia in the REPL",
    "text": "Running Julia in the REPL\n\nWhere to run the REPL\nYou could now technically launch a Julia REPL (Read-Eval-Print-Loop). However, this would launch it on the login node and if everybody does this at the same time, we would probably crash our training cluster.\nInstead, you will first launch an interactive job by running the Slurm command salloc:\n$ salloc --mem-per-cpu=3600M --cpus-per-task=2 --time=2:0:0\nThis puts you on a compute node and gives you 2 CPU cores for up to 2 hours.\nNow you can launch the Julia REPL and try to run a couple of commands:\n$ julia\n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.8.5 (2023-01-08)\n _/ |\\__'_|_|_|\\__'_|  |\n|__/                   |\n\njulia&gt; using BenchmarkTools\n\n julia&gt; @btime sqrt(2)\n  1.310 ns (0 allocations: 0 bytes)\n1.4142135623730951\n\n\nREPL modes\nThe Julia REPL has 4 modes:\njulia&gt;       Julian mode to run code. Default mode. Go back to it from other modes with Backspace\nhelp?&gt;       Help mode to access documentation. Enter it with ?\nshell&gt;       Shell mode to run Bash commands. Enter it with ;\n(env) pkg&gt;   Package manager mode to manage external packages. Enter it with ]\n(env is the name of your current project environment.)\n\n\nREPL keybindings\nIn the REPL, you can use standard command-line (Emacs-like) keybindings:\nC-c     cancel\nC-d     quit\nC-l     clear console\n\nC-u     kill from the start of line\nC-k     kill until the end of line\n\nC-a     go to start of line\nC-e     go to end of line\n\nC-f     move forward one character\nC-b     move backward one character\n\nM-f     move forward one word\nM-b     move backward one word\n\nC-d     delete forward one character\nC-h     delete backward one character\n\nM-d     delete forward one word\nM-Backspace delete backward one word\n\nC-p     previous command\nC-n     next command\n\nC-r     backward search\nC-s     forward search\n\n\nREPL for parallel work\nRemember our workflow to launch a Julia REPL:\n# Step 0: start tmux (optional), gives you left-right panes, persistent terminal session\n$ tmux\n\n# Step 1: run the script to load our Julia environment with pre-installed packages\n$ source /project/def-sponsor00/shared/julia/config/loadJulia.sh\n\n# Step 2: launch an interactive job on a compute node\n$ salloc --mem-per-cpu=3600M --cpus-per-task=2 --time=2:0:0\n\n# Step 3: launch the Julia REPL\n$ julia\nThis will launch Julia in serial. To use multiple threads, you want to pass the -t flag when starting Julia:\n# Launch Julia on 2 threads\n$ julia -t 2",
    "crumbs": [
      "PART 1",
      "Introduction to Julia language"
    ]
  },
  {
    "objectID": "julia1/julia-01-intro-language.html#running-scripts-as-batch-jobs",
    "href": "julia1/julia-01-intro-language.html#running-scripts-as-batch-jobs",
    "title": "Introduction to Julia language",
    "section": "Running scripts as batch jobs",
    "text": "Running scripts as batch jobs\nNow, if we want to get an even bigger speedup, we could use even more CPUs per task. The problem is that our training cluster only has ~60 compute cores, so if we use --cpus-per-task=4 some of us could be left waiting for Slurm while the others play with several CPUs for two hours. This is not an efficient approach. This is equally true on production clusters: if you want to run an interactive job using a lot of resources, you might have to wait for a long time.\nOn production clusters, a much better approach is to put our Julia code in a Julia script and run it through a batch job by using the Slurm command sbatch.\n\n\nLet’s save the following into a file job_script.sh:\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\njulia -t 4 julia_script.jl\nThen we submit our job script:\n$ sbatch job_script.sh\nIn this session, we continue using salloc --cpus-per-task=2 ...",
    "crumbs": [
      "PART 1",
      "Introduction to Julia language"
    ]
  },
  {
    "objectID": "julia1/julia-01-intro-language.html#serial-julia-features-worth-noting-in-10-mins",
    "href": "julia1/julia-01-intro-language.html#serial-julia-features-worth-noting-in-10-mins",
    "title": "Introduction to Julia language",
    "section": "Serial Julia features worth noting in 10 mins",
    "text": "Serial Julia features worth noting in 10 mins\n\nJIT compilation\nProgramming languages are either interpreted or compiled.\nInterpreted languages use interpreters: programs that execute your code directly (Python, for instance, uses the interpreter CPython, written in C). Interpreted languages are very convenient since you can run sections of your code as soon as you write them. However, they are slow.\nCompiled languages use compilers: programs that translate your code into machine code. Machine code is extremely efficient, but of course, having to compile your code before being able to run it makes for less convenient workflows when it comes to writing or debugging code.\nJulia uses just-in-time compilation or JIT based on LLVM: the source code is compiled at run time. This combines the flexibility of interpretation with the speed of compilation, bringing speed to an interactive language. It also allows for dynamic recompilation, continuous weighing of gains and costs of the compilation of parts of the code, and other on the fly optimizations.\nHere is a great blog post covering this topic if you want to dive deeper into the functioning of JIT compilers.\n\n\nMacros\nIn the tradition of Lisp, Julia has strong metaprogramming capabilities, in particular in the form of macros.\nMacros are parsed and evaluated first, then their output gets evaluated like a classic expression. This allows the language to modify itself in a reflective manner.\nMacros have a @ prefix and are defined with a syntax similar to that of functions.\n@time for instance is a macro that executes an expression and prints the execution time and other information.\n\n\nFun fact\nJulia supports unicode. In a Julia REPL, type \\:snail: followed by the TAB key, and you get 🐌.\nWhile assigning values to a “snail variable” might not be all that useful, a wide access to – for instance – Greek letters, would make Julia’s code look nicely similar to the equations it represents. For instance, if you type TAB after each variable name, the following:\n\\pgamma = ((\\alpha \\pi + \\beta) / \\delta) + \\upepsilon\nlooks like:\nɣ = ((α π + β) / δ) + ε\nIn Julia you can omit the multiplication sign in some cases, e.g.\njulia&gt; 2π\n6.283185307179586\n\n\nAdditional basic information\nOur beginner’s introduction to Julia course has, amongst others, sections on:\n\nworking with packages\nworking with functions\ncontrol flow\narrays",
    "crumbs": [
      "PART 1",
      "Introduction to Julia language"
    ]
  },
  {
    "objectID": "julia1/julia-15-linear-algebra.html",
    "href": "julia1/julia-15-linear-algebra.html",
    "title": "Distributed linear algebra in Julia",
    "section": "",
    "text": "In the previous session there was a question about parallel linear algebra with DistributedArrays.jl, i.e. whether it was possible to solve linear systems defined with distributed arrays, without having to code your own solver. Let’s start with serial solvers which can scale well to surprisingly large linear systems.\n\nSerial solvers\n\nLinearSolve.jl provides serial dense matrix solvers with a focus on performance.\nusing LinearSolve\nn = 100;\nA = rand(n,n);                     # uniform numbers from [0, 1)\nb = rand(n);\n@time prob = LinearProblem(A, b)   # define a linear system problem\n@time sol = solve(prob)            # solve this problem\n*(A,sol) - b                       # check the result\nLet’s check performance on progressively larger systems:\nk = 1000\nfor n in (10, 100, 1k, 10k, 20k, 30k)\n    A = rand(n,n);\n    b = rand(n);\n    prob = LinearProblem(A, b);\n    @time sol = solve(prob);\nend\nFor these cases I receive:\n  0.000026 seconds (14 allocations: 2.375 KiB)\n  0.000115 seconds (17 allocations: 81.984 KiB)\n  0.010350 seconds (15 allocations: 7.654 MiB)\n  2.819498 seconds (18 allocations: 763.170 MiB, 0.07% gc time)\n 23.127729 seconds (18 allocations: 2.981 GiB, 0.55% gc time)\n123.673966 seconds (18 allocations: 6.706 GiB, 0.28% gc time)\n\n\nDistributed solvers\nThere are few projects that use Distributed.jl to implement parallel dense and/or sparse linear solvers:\n\nParallelLinalg.jl\nParallelLinalg.jl implements distributed dense linear algebra but was last updated 7 years ago …\n\n\nPardiso.jl\nPardiso.jl provides an interface to the Intel’s MKL PARDISO library, which is a highly optimized direct solver for sparse linear systems. It supports distributed computation through MPI.jl.\n\n\n\n\nPartitionedArrays.jl\nPartitionedArrays.jl provides HPC sparse linear algebra solvers and relies on MPI.jl. I played with it, but launching it was kind of tricky …\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIterativeSolvers.jl\nIterativeSolvers.jl provides iterative solvers for linear systems and eigenvalue problems. It supports distributed computation through DistributedArrays.jl and ParallelStencil.jl. I played with it, but ran into errors while following their examples …\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPETSc.jl\nPETSc.jl provides a low-level interface to PETSc (Portable, Extensible Toolkit for Scientific Computation), a parallel library for linear algebra, PDEs, and optimization. It supports distributed computation through MPI.jl. I have not tried it in Julia.\n\n\nElemental.jl\nElemental.jl provides a distributed-memory library of linear algebra routines, including solvers for linear systems, eigenvalue problems, and singular value problems. It supports a wide range of parallel architectures, including multicore CPUs, GPUs, and clusters. This one seems to work quite nicely, although you cannot use DistributedArrays out of the box.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have to use Elemental.DistMatrix type which gets distributed only when you run it on top of Julia’s MPI:\nusing MPI, MPIClusterManagers, Distributed\nman = MPIWorkerManager(4)\naddprocs(man);\n@everywhere using DistributedArrays, Elemental\n@everywhere n = 100\n@mpi_do man A = Elemental.DistMatrix(Float64);\n@mpi_do man Elemental.gaussian!(A,n,n);\n@mpi_do man b = Elemental.DistMatrix(Float64);\n@mpi_do man Elemental.gaussian!(b,n);\n@time @mpi_do man Elemental.solve!(A,b)\n@mpi_do man println(size(b))   # each worker prints global size\n@mpi_do man println(b)         # each worker prints the (same) solution\nIn this example storage and computation happen on all worker processes. You can monitor all Julia processes with the following command on the compute node that is running them:\nhtop -p $(echo $(pgrep julia) | sed 's/ /,/g')\nI had trouble printing the array b from a single process or printing its subsections from all processes, but I probably just used wrong syntax. I have not tried scaling to bigger problems and to more CPU cores.",
    "crumbs": [
      "SUPPLEMENTAL MATERIALS",
      "Distributed linear algebra in Julia"
    ]
  },
  {
    "objectID": "julia1/julia-10-distributed-arrays.html",
    "href": "julia1/julia-10-distributed-arrays.html",
    "title": "DistributedArrays.jl",
    "section": "",
    "text": "DistributedArrays package provides DArray object that can be split across several processes (set of workers), either on the same or multiple nodes. This allows use of arrays that are too large to fit in memory on one node. Each process operates on the part of the array that it owns – this provides a very natural way to achieve parallelism for large problems.\nDistributedArrays is not part of the standard library, so – if running on your own computer – you would usually need to install it yourself:\nWe have DistributedArrays already installed on our training cluster. To use it, we need to load this package on every worker:\nLet’s check data distribution across workers:\nYou can assign localindices() output to variables:\nIf we try to write into our distributed array on the control process, e.g. data[6,1] = 1, we’ll get an error! If we try @fetchfrom 3 data[6,1] = 1, most likely we get an error as well! The reason is that we can only write into:\nRunning on one worker:\nRunning on all workers:\nOne-liners to generate distributed arrays:\nYou can find more information about the arguments by typing ?DArray. For example, you have a lot of control over the DArray’s distribution across workers. Before I show the examples, let’s define a convenient function to show the array’s distribution:\nYou can take a local array and distribute it across workers:",
    "crumbs": [
      "PART 2",
      "DistributedArrays.jl"
    ]
  },
  {
    "objectID": "julia1/julia-10-distributed-arrays.html#footnotes",
    "href": "julia1/julia-10-distributed-arrays.html#footnotes",
    "title": "DistributedArrays.jl",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis example was adapted from Baolai Ge’s (SHARCNET) presentation.↩︎",
    "crumbs": [
      "PART 2",
      "DistributedArrays.jl"
    ]
  },
  {
    "objectID": "julia1/julia-05-threads-julia-set.html",
    "href": "julia1/julia-05-threads-julia-set.html",
    "title": "Parallelizing the Julia set with Base.Threads",
    "section": "",
    "text": "The project is the mathematical problem to compute a Julia set – no relation to Julia language! A Julia set is defined as a set of points on the complex plane that remain bound under infinite recursive transformation \\(f(z)\\). We will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\nBelow is the serial code juliaSetSerial.jl. If you are running Julia on your own computer, make sure you have the required packages:\nLet’s study the code:\nLet’s run this code with julia juliaSetSerial.jl. On my laptop it reports 931.024 ms.\nThis code will produce the file test.nc that you can download to your computer and render with ParaView or other visualization tool.",
    "crumbs": [
      "PART 1",
      "Parallelizing the Julia set with Base.Threads"
    ]
  },
  {
    "objectID": "julia1/julia-05-threads-julia-set.html#parallelizing",
    "href": "julia1/julia-05-threads-julia-set.html#parallelizing",
    "title": "Parallelizing the Julia set with Base.Threads",
    "section": "Parallelizing",
    "text": "Parallelizing\n\nLoad Base.Threads.\nAdd @threads before the outer loop, and time this parallel loop.\n\nOn my laptop with 8 threads the timing is 193.942 ms (4.8X speedup) which is good but not great – certainly worse than linear speedup … The speedup on the training cluster is not great either. There could be several potential problems:\n\nFalse sharing effect (cache issues with multiple threads writing into adjacent array elements).\nLess than perfect load balancing between different threads.\nRow-major vs. column-major loop order for filling in the stability array.\nSome CPU cores are lower efficiency, and they are slowing down the whole calculation.\n\n\n\n\n\n\n\nCautionExercise Fractal.2\n\n\n\n\n\nHow would you fix this issue? If you manage to get close to 100% parallel efficiency with Base.Threads on multiple cores, we would love to see your solution! Please only check the solution once you have worked on the problem yourself.\n\n\n\n\n\n\n\n\n\nCautionExercise Fractal.3\n\n\n\n\n\nBuild a 3D cube based on the Julia set where the 3rd axis would be a slowly varying c constant. For example, try to interpolate linearly between \\(c = 0.355 + 0.355i\\) and \\(c = 1.34-0.45i\\), or between any other two complex values. Send us an animation traversing your volume once you are done. What highest cube resolution could you compute?\n\n\n\nYou can also generate a truly 3D fractal in Julia. Check out this dataset which was computed in Julia and visualized in ParaView (both on a laptop).",
    "crumbs": [
      "PART 1",
      "Parallelizing the Julia set with Base.Threads"
    ]
  },
  {
    "objectID": "julia1/julia-11-distributed-julia-set.html",
    "href": "julia1/julia-11-distributed-julia-set.html",
    "title": "Parallelizing Julia set",
    "section": "",
    "text": "The Julia set problem was described in one of the earlier sections.\n\nParallelizing\nHow would we parallelize this problem with multi-processing? We have a large array, so we can use DistributedArrays and compute it in parallel. Copy juliaSetSerial.jl to juliaSetDistributedArrays.jl and start editing the latter:\n\nLoad Distributed on the control process.\nLoad DistributedArrays on all processes.\nstability array should be distributed:\n\nstability = dzeros(Int32, height, width);   # distributed 2D array of 0's\n\nDefine function pixel() on all processes.\nCreate fillLocalBlock(stability) to compute local pieces stability.localpart on each worker in parallel. If you don’t know where to start, begin with checking the complete example with fillLocalBlock() from the previous section. This function will cycle through all local indices localindices(stability). This function needs to be defined on all processes.\nReplace the loop\n\n@btime for i in 1:height, j in 1:width\n    point = (2*(j-0.5)/width-1) + (2*(i-0.5)/height-1)im\n    stability[i,j] = pixel(point)\nend\nwith\n@btime @sync for w in workers()\n    @spawnat w fillLocalBlock(stability)\nend\n\nWhy do we need @sync in the previous for block?\nTo the best of my knowledge, both Plots’ heatmap() and NetCDF’s ncwrite() are serial in Julia, and they cannot take distributed arrays. How do we convert a distributed array to a local array to pass to one of these functions?\nIs your parallel code faster?\n\nTo get the full script, click on “Solution” below.\n\n\n\n\n\n\nCautionSolution\n\n\n\n\n\nusing Distributed, BenchmarkTools\n@everywhere using DistributedArrays\n\n@everywhere function pixel(z)\n    c = 0.355 + 0.355im\n    z *= 1.2   # zoom out\n    for i = 1:255\n        z = z^2 + c\n        if abs(z) &gt;= 4\n            return i\n        end\n    end\n    return 255\nend\n\n@everywhere function fillLocalBlock(stability)\n    height, width = size(stability)\n    h, w = localindices(stability)\n    for iGlobal in collect(h)\n        iLocal = iGlobal - h.start + 1\n        y = 2*(iGlobal-0.5)/height - 1\n        for jGlobal in collect(w)\n            jLocal = jGlobal - w.start + 1\n            point = (2*(jGlobal-0.5)/width-1) + (y)im # rescale to -1:1 in the complex plane\n            @inbounds stability.localpart[iLocal,jLocal] = pixel(point)\n        end\n    end\nend\n\nheight, width = repeat([2_000],2)   # 2000^2 image\n\nprintln(\"Computing Julia set ...\")\nstability = dzeros(Int32, height, width);   # distributed 2D array of 0's\n@btime @sync for w in workers()\n    @spawnat w fillLocalBlock(stability)\nend\n\nprintln(\"Plotting to PNG ...\")\nusing Plots\ngr()                       # initialize the gr backend\nENV[\"GKSwstype\"] = \"100\"   # operate in headless mode\nfname = \"$(height)x$(width)\"\nnonDistributed = zeros(Int32, height, width);\nnonDistributed[:,:] = stability[:,:];   # ncwrite does not accept DArray type\npng(heatmap(nonDistributed, size=(width,height), color=:gist_ncar), fname)\n\nprintln(\"Writing NetCDF ...\")\nusing NetCDF\nfilename = \"test.nc\"\nisfile(filename) && rm(filename)\nnccreate(filename, \"stability\", \"x\", collect(1:height), \"y\", collect(1:width), t=NC_INT, mode=NC_NETCDF4, compress=9);\nnonDistributed = zeros(Int32, height, width);\nnonDistributed[:,:] = stability[:,:];   # ncwrite does not accept DArray type\nncwrite(nonDistributed, filename, \"stability\");\n\n\n\n\n\nResults for 1000^2\nFinally, here are my timings on (some old iteration of) the training cluster:\n\n\n\n\n\n\n\n\nCode\nTime on login node (p-flavour vCPUs)\nTime on compute node (c-flavour vCPUs)\n\n\n\n\njulia juliaSetSerial.jl (serial runtime)\n147.214 ms\n123.195 ms\n\n\njulia -p 1 juliaSetDistributedArrays.jl (on 1 worker)\n157.043 ms\n128.601 ms\n\n\njulia -p 2 juliaSetDistributedArrays.jl (on 2 workers)\n80.198 ms\n66.449 ms\n\n\njulia -p 4 juliaSetDistributedArrays.jl (on 4 workers)\n42.965 ms\n66.849 ms\n\n\njulia -p 8 juliaSetDistributedArrays.jl (on 8 workers)\n36.067 ms\n67.644 ms\n\n\n\n\nLots of things here to discuss!\nOne could modify our parallel code to offload some computation to the control process (not just compute on workers as we do now), so that you would see speedup when running on 2 CPUs (control process + 1 worker).",
    "crumbs": [
      "PART 2",
      "Parallelizing the Julia set with DistributedArrays"
    ]
  },
  {
    "objectID": "julia1/julia-08-distributed2.html",
    "href": "julia1/julia-08-distributed2.html",
    "title": "Distributed.jl - three scalable slow-series codes",
    "section": "",
    "text": "Solution 1: an array of Future references\nWe could create an array (using array comprehension) of Future references and then add up their respective results. An array comprehension is similar to Python’s list comprehension:\na = [i for i in 1:5];   # array comprehension in Julia\ntypeof(a)               # 1D array of Int64\nWe can cycle through all available workers:\n[w for w in workers()]                      # array of worker IDs\n[(i,w) for (i,w) in enumerate(workers())]   # array of tuples (counter, worker ID)\n\n\n\n\n\n\nCautionExercise Distributed.3\n\n\n\n\n\nUsing this syntax, construct an array r of Futures, and then get their results and sum them up with\nprint(\"total = \", sum([fetch(r[i]) for i in 1:nworkers()]))\nYou can do this exercise using either the array comprehension from above, or the good old for loops.\n\n\n\n\n\n\n\n\nWith two workers and two CPU cores, we should get times very similar to the last run. However, now our code can scale to much larger numbers of cores!\n\n\n\n\n\n\nCautionExercise Distributed.4\n\n\n\n\n\nIf you did the previous exercise with an interactive job, now submit a Slurm batch job running the same code on 4 CPU cores. Next, try 8 cores. Did your timing change?\n\n\n\n\n\nSolution 2: parallel for loop with summation reduction\nUnlike the Base.Threads module, Distributed provides a parallel loop with reduction. This means that we can implement a parallel loop for computing the sum. Let’s write parallelFor.jl with this version of the function:\nfunction slow(n::Int64, digitSequence::Int)\n    @time total = @distributed (+) for i in 1:n\n        !digitsin(digitSequence, i) ? 1.0 / i : 0\n    end\n    println(\"total = \", total);\nend\nA couple of important points:\n\nWe don’t need @everywhere to define this function. It is a parallel function defined on the control process, and running on the control process.\nThe only expression inside the loop is the compact if/else statement. Consider this:\n\n1==2 ? println(\"Yes\") : println(\"No\")\nThe outcome of the if/else statement is added to the partial sums at each loop iteration, and all these partial sums are added together.\nNow let’s measure the times:\n# slow(10, 9)\nprecompile(slow, (Int, Int))\nslow(Int64(1e8), 9)   # total = 13.277605949855722\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe two macros @btime and @distributed do not work nicely with each other. If you try to replace @time with @btime inside slow(), you will run into weird errors, so best to combine @distributed with @time when doing this in the same line. However, you can use @btime when calling the function:\nfunction slow(n::Int64, digitSequence::Int)\n    total = @distributed (+) for i in 1:n\n        !digitsin(digitSequence, i) ? 1.0 / i : 0\n    end\n    return(total)\nend\n@btime slow(Int64(1e8), 9)\nIn either case – whether using @time or @btime – we’ll get the single time for the entire parallel loop (1.498s in my case).\n\n\n\n\n\n\n\n\nCautionExercise Distributed.6\n\n\n\n\n\nRepeat on 8 CPU cores. Did your timing improve?\n\n\n\nI tested this code (parallelFor.jl) on Cedar with v1.5.2 and n=Int64(1e9):\n#!/bin/bash\n#SBATCH --ntasks=...   # number of MPI tasks\n#SBATCH --cpus-per-task=1\n#SBATCH --nodes=1-1   # change process distribution across nodes\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=0:5:0\n#SBATCH --account=...\nmodule load julia\necho $SLURM_NODELIST\n# comment out addprocs() in the code\njulia -p $SLURM_NTASKS parallelFor.jl\n\n\n\nCode\nTime\n\n\n\n\nserial\n48.2s\n\n\n4 cores, same node\n12.2s\n\n\n8 cores, same node\n7.6s\n\n\n16 cores, same node\n6.8s\n\n\n32 cores, same node\n2.0s\n\n\n32 cores across 6 nodes\n11.3s\n\n\n\n\n\nSolution 3: use pmap to map arguments to processes\nThe pmap() function provides another mechanism to launch a function on all available workers:\n@everywhere function showid(message)   # define the function everywhere\n    println(message, myid())           # this function returns nothing\nend\nshowid(\"I am \")   # on control process\npmap(showid, [\"my id = \", \"and mine = \", \"reporting\", \"here we go: \"]);\nThere are two pmap() syntaxes:\npmap(slow, args)\npmap(x-&gt;x^2, [1,2,3,4])   # anonymous/lambda function\npmap(x-&gt;println(\"I am worker \", myid()), workers())\nLet’s apply this mapping tool to our slow series. We’ll start mappingArguments.jl with a new version of slow() that will compute partial sum on each worker:\n@everywhere function slow((n, digitSequence, taskid, ntasks))   # the argument is now a tuple\n    println(\"running on worker \", myid())\n    total = 0.0\n    @time for i in taskid:ntasks:n   # partial sum with a stride `ntasks`\n        !digitsin(digitSequence, i) && (total += 1.0 / i)   # compact if statement (similar to bash)\n    end\n    return(total)\nend\nand launch the function on each worker:\nslow((10, 9, 1, 1))   # package arguments in a tuple; serial run\nnw = nworkers()\nargs = [(Int64(1e8), 9, j, nw) for j in 1:nw]   # array of tuples to be mapped to workers\nprintln(\"total = \", sum(pmap(slow, args)))      # launch the function on each worker and sum the results\nWe see the following times from individual processes:\nFrom worker 2:  running on worker 2\nFrom worker 3:  running on worker 3\nFrom worker 4:  running on worker 4\nFrom worker 5:  running on worker 5\nFrom worker 2:    0.617099 seconds\nFrom worker 3:    0.619604 seconds\nFrom worker 4:    0.656923 seconds\nFrom worker 5:    0.675806 seconds\ntotal = 13.277605949854518\n\n\nHybrid parallelism\nHere is a simple example of a hybrid multi-threaded / multi-processing code contributed by Xavier Vasseur following the October 2021 workshop:\nusing Distributed\n@everywhere using Base.Threads\n\n@everywhere function greetings_from_task(())\n    @threads for i=1:nthreads()\n    println(\"Hello from thread $(threadid()) on proc $(myid())\")\n    end\nend\n\nargs_pmap  = [() for j in workers()];\npmap(x-&gt;greetings_from_task(x), args_pmap)\nSave this code as hybrid.jl and then run it specifying the number of workers with -p and the number of threads per worker with -t flags:\n$ julia -p 4 -t 2 hybrid.jl\n    From worker 5:  Hello, I am thread 2 on proc 5\n    From worker 5:  Hello, I am thread 1 on proc 5\n    From worker 2:  Hello, I am thread 1 on proc 2\n    From worker 2:  Hello, I am thread 2 on proc 2\n    From worker 3:  Hello, I am thread 1 on proc 3\n    From worker 3:  Hello, I am thread 2 on proc 3\n    From worker 4:  Hello, I am thread 1 on proc 4\n    From worker 4:  Hello, I am thread 2 on proc 4\n\n\nOptional integration with Slurm\nClusterManagers.jl package lets you submit Slurm jobs from your Julia code. This way you can avoid writing a separate Slurm script in bash, and put everything (Slurm submission + parallel launcher + computations) into a single code. Moreover, you can use Julia as a language for writing complex HPC workflows, i.e. write your own distributed worflow manager for the cluster.\nHowever, for the types of workflows we consider in this workshop ClusterManagers.jl is an overkill, and we don’t recommend it for beginner Julia users.",
    "crumbs": [
      "PART 2",
      "Distributed.jl: three scalable versions of the slow series"
    ]
  },
  {
    "objectID": "bashFeatures-20221207.html",
    "href": "bashFeatures-20221207.html",
    "title": "Lesser known but very useful Bash features",
    "section": "",
    "text": "You can find this page at   https://folio.vastcloud.org/bashfeatures"
  },
  {
    "objectID": "bashFeatures-20221207.html#links",
    "href": "bashFeatures-20221207.html#links",
    "title": "Lesser known but very useful Bash features",
    "section": "Links",
    "text": "Links\n\nFall 2022 training schedule\nWinter/spring 2023 training schedule\nTraining materials: upcoming and archived events, ~100 recorded webinars, subscribe to our training events mailing list\nAlliance’s wiki documentation\nEmail: training “at” westdri “dot” ca\n\n\nAbstract: Knowing basic Linux commands is essential for researchers using remote systems such as HPC clusters. Bash is the most commonly used Linux shell, which you will use by default on most Alliance hardware. Although we teach Bash basics in various online schools and in-person workshops many times a year, there are some useful Bash features and tricks that we never get to teach, due to our usual time constraints. Finally we can share some of them with you in this webinar!\nIn this presentation, we talk about running commands in a subshell, subsetting string variables, Bash arrays, modifying separators with IFS, running Python code from inside self-contained Bash functions, editing your command history, running unaliased versions of commands, handy use of brace expansion, and a few other topics."
  },
  {
    "objectID": "bashFeatures-20221207.html#intro",
    "href": "bashFeatures-20221207.html#intro",
    "title": "Lesser known but very useful Bash features",
    "section": "Intro",
    "text": "Intro\nWe regularly teach bash in our summer/etc schools:\n\nnavigating files and directories; creating, moving and copying things\narchives and compression\ntransferring files and directories to/from remote computers\nwildcards, redirection, pipes, and aliases; brace expansion\nloops and variables; command substitution\nscripts and functions, briefly on conditionals\nfinding things with grep and find; tying things with xargs\ntext manipulation with sed and awk\n\nIn previous webinars we’ve also taught 3rd-party command-line tools such as fuzzy finder fzf, Git terminal UI lazygit, syntax highlighter bat, a fast alternative to grep ripgrep, a really fast find alternative fd, autojump replacement for cd that learns and adapts to your use, and so on.\nToday we would like to focus on some useful built-in bash features that we rarely get to demo."
  },
  {
    "objectID": "bashFeatures-20221207.html#first-part-alex",
    "href": "bashFeatures-20221207.html#first-part-alex",
    "title": "Lesser known but very useful Bash features",
    "section": "First part (Alex)",
    "text": "First part (Alex)\n\nSubshells with ()\n\n\ncan be used to avoid any side-effects in the current shell\ncommands inside () execute in a subshell\nuse directly in the shell or when defining a function\n\nAll operations inside () will be local to the subshell:\ncd ~/Documents\n(cd ~/Desktop; pwd)\npwd\n(export greeting=\"hello\" && echo $greeting)\necho $greeting\n(alias ll=\"ls -A\" && alias ll)\nalias ll\nOne common use: when testing, cd temporarily into another directory and run something there, Ctrl-C will break and take you back to the original directory. Consider a code with separate src and run subdirectories:\ncd src\nfunction run() {\n  make\n  /bin/cp pi ../run\n  cd ../run\n  ./pi\n}\nrun\nBreaking execution with Ctrl-C will leave you in run every time. You can modify your function to change directory and run the code in a subshell so that Ctrl-C will always take you to src:\ncd ../src\nfunction run() {\n  make\n  /bin/cp pi ../run\n  (cd ../run ; ./pi)\n}\nrun\nAnother solution is to define run(){...} as run()(...) – then the entire function will run in a subshell:\nfunction run() (\n  make\n  /bin/cp pi ../run\n  cd ../run ; ./pi\n)\nrun\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother use case: utilize subshells for testing things, so you don’t pollute the current shell with temporary definitions.\n\npro: very easy to use\ncon: takes slightly longer to execute (opens a subshell) but in most use cases this is probably not an issue\n\n\n\nSubsetting string variables\n\n\nauthor=\"Charles Dickens\"\necho $author\necho \"$author was an English writer\"\necho $author\\'s novels     # works in this case\necho \"$author\"\\'s novels   # safer approach\necho ${author}\\'s novels   # another safer approach\necho \"${author}'s novels\"  # another safer approach\n\necho \"string's length is ${#author} characters\"\necho ${author/Charles/Ch.}   # replace the first match of a substring\necho ${author//s/S}          # replace all first matches of a substring\n\necho ${author/Charles }      # if no replacement string supplied, the substring will be deleted\necho ${author/Charles /}     # the same\n\noriginal=\"Charles\"           # can use a variable for the substring\nshort=\"C.\"                   # can use a variable for a replacement string\necho ${author/$original/$short}\n\necho ${author/#Ch/ch}        # replace the match only at the start (if found)\necho ${author/%ns/ns ---}    # replace the match only at the end (if found)\n\necho ${author/#Charles }ian  # form an adjective (Dickénsian)\nE.g. you can use this to change file extensions:\ntouch {a..z}{0..9}.txt   # create 260 files\nfor file in *.txt; do\n    mv $file ${file/%txt/md}\ndone\nAnother solution of course:\n/bin/rm ??.md\ntouch {a..z}{0..9}.txt   # create 260 files\nfor file in *.txt; do\n    mv $file ${file/.txt/.md}\ndone\nQuestion: what will this do echo ${author/#/---}?\necho ${author:5:2}     # display 2 characters starting from number 5 (indexing from 0)\necho ${author::2}      # display the first 2 characters\necho ${author:5:${#author}}     # display all characters starting from number 5 to the end\necho ${author:5:999}            # simpler\necho ${author:5}                # even simpler\necho ${author: -2}     # last two characters; important to have space there!\necho ${author: -5:3}   # display 3 characters starting from number -5; important to have space there!\nNote: If you want to perform more granular operations with bash strings, e.g. work with patterns, you can look into regular expressions (not covered in this webinar).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBash arrays\na=(10 20 30 hello)\necho $a              # only the first element\necho ${a[@]}         # all elements, i.e. @=all\necho ${a[0]}         # specific element\necho $a[0]           # concatenate $a (the first element) and \"[0]\"\nfor x in ${a[@]}; do\n  echo $x\ndone\n\na=(10 20 30 hello \"hi there\")\nfor x in ${a[@]}; do     # puts hi,there in separate lines (6 loop iterations)\n  echo $x\ndone\nfor x in \"${a[@]}\"; do   # better way to iterate over bash array elements; 5 loop iterations\n  echo $x\ndone\n\necho ${!a[@]}   # list of all array indices (0 1 2 3 4)\necho ${#a[@]}   # number of elements (5)\n\na+=(100 200)    # append 2 elements to the array\nLet’s do some timing of a Julia code:\n$ julia slowSeries.jl    # will report shortest time in seconds\n  0.518063834\n$ julia -t 2 slowSeries.jl\n  0.270241666\n\nthreads=(1 2 4 8 16)\nruntime=()\nfor n in ${threads[@]}; do\n    time=$(julia -t $n slowSeries.jl)\n    runtime+=($time)    # adding one element per cycle\ndone\necho ${runtime[@]}\nfor i in ${!threads[@]}; do   # cycling through array indices\n    echo ${threads[i]} threads completed in ${runtime[i]} seconds\ndone\n\nruntime=()\nfor n in ${threads[@]}; do\n    time=$(julia -t $n slowSeries.jl)\n    runtime+=(\"$n threads: $time\")   # also adding one element per cycle with the quotes\ndone\necho ${#runtime[@]}\nfor x in \"${runtime[@]}\"; do\n  echo $x\ndone\nUsing arrays in a backup script:\nif [ -e /Volumes/gdrive ]; then\n    BSRC=(~/Documents ~/Desktop\n          ~/Downloads/{books,images})\n    BDEST='/Volumes/gdrive/backups'\nelif [ -e /Volumes/t7 ]; then\n    BSRC=(~/Pictures ~/Music)\n    BDEST='/Volumes/t7/backups'\nfi\necho ${BSRC[@]}\necho backing up `echo \"${BSRC[@]}\" | sed -e 's|/Users/razoumov/||g'` to $BDEST\nborg create --stats --list --filter='AM' --compression=lz4 --noflags $BDEST::$(date \"+%Y%b%d%H%M\") \"${BSRC[@]}\"\nUsing arrays for compilation flags:\nFLAGS=(\n -DCMAKE_INSTALL_PREFIX=$HOME/paraviewcpu591\n -DVTK_OPENGL_HAS_OSMESA=ON\n -DPARAVIEW_USE_MPI=ON -DBUILD_TESTING=OFF\n -DVTK_USE_X=OFF -DPARAVIEW_USE_QT=OFF\n -DPARAVIEW_USE_PYTHON=ON\n # -DPARAVIEW_BUILD_SHARED_LIBS=ON    this is a commented flag; won't show up\n -DPARAVIEW_ENABLE_RAYTRACING=ON\n)\necho \"${FLAGS[@]}\"\nfor x in \"${FLAGS[@]}\"; do\n  echo $x\ndone\ncmake .. \"${FLAGS[@]}\"\nCommand substitution to an array:\nstr=$(ls)              # command substitution to save `ls` output as a string\narr=($(ls))            # save `ls` output as an array of file/directory names\necho ${arr[@]:2:3}     # retrieve 3 elements starting at index 2\n                       # ${a[@]:3:1} is the same as ${a[3]}\nArray cheatsheet:\narr=()          # create an empty array\narr=(1 2 3)     # initialize array\n${arr[2]}       # retrieve third element\n${arr[@]}       # retrieve all elements\n${!arr[@]}      # retrieve array indices\n${#arr[@]}      # calculate array size\narr[0]=3        # overwrite 1st element\narr+=(40 50)    # append two elements\n${arr[@]:i:j}   # retrieve j elements starting at index i\n\n\nLittle practical example\nNow let’s apply this knowledge!\nHere is the standard bash syntax for arguments passed to a function:\n$1    # first argument\n$2    # second argument\n$#    # number of arguments\n$@    # all arguments\nAlternatively, we can treat all arguments as an array:\narr=($@)                  # store all arguments inside an array\nnum=${#arr[@]}            # the length of this array\nnum=$#                    # same\necho ${arr[@]:0:$num-1}   # all arguments but the last\necho ${arr[$num-1]}       # last argument\nfunction move() {\n    arr=($@)\n    num=${#arr[@]}\n    objects=${arr[@]:0:$num-1}\n    last=${arr[$num-1]}\n    echo MOVING $objects TO $last\n    /bin/cp $objects $last && /bin/rm $objects\n}\nWhy do we want to use it? - on our HPC clusters in /project the 1TB (or higher) quota is applied to all files with the group ID def-&lt;PI&gt; - the /project quota is applied to the entire research group - the quota for group ID $USER is almost zero - by default, all files in /home, /scratch have group ID $USER - problem: the usual mv command preserves group ID  ⮕  moving files with mv from /home,/scratch to /project will almost certainly exceed your quota for group ID $USER  ⮕  trouble writing files, running jobs, etc. - solution: use cp (modifies quota accordingly) followed by rm, i.e. replace mv with our new function move\n\n\nIFS to edit separators\n\nThe IFS variable – which stands for Internal Field Separator – controls how Bash does word splitting.\nphrase=\"one,two three four\"\nfor word in $phrase; do\n    echo $word\ndone\nDefault IFS is any of space/newline/tab, i.e. IFS=$’_:\nexport IFS     # shows an empty line ... as if it was not set\necho ${#IFS}   # there are actually three characters there: $' \\n\\t'\nIFS=,\nfor word in $phrase; do\n    echo $word\ndone\nIFS=\", \"       # both characters will be used as separators\nfor word in $phrase; do\n    echo $word\ndone\nunset IFS      # back to default behaviour\nfor word in $phrase; do\n    echo $word\ndone\nWhy is this useful? One use: IFS can help you deal with files with spaces in their names. Imagine you want to process some files in a loop:\nunset IFS\ntouch \"my thesis.md\" \"first results.md\"   # really bad idea, but 99% of people do it anyway\nfor i in *.md; do     # the wildcard gets expanded here into a string with 2 items =&gt; 2 loop iterations\n    ls -l $i          # $i is a string with space; this gives an error, as `ls` sees this string as 2 names\n    mv $i ${i/.md/.tex}   # this gives an error too, as `mv` sees each string as 2 names\ndone\nThis would be a bad way to fix this:\nfor i in \"*.md\"; do   # loop over one element (the string with *.md inside) =&gt; 1 loop iteration\n    ls -l $i          # $i contains a wildcard that gets expanded here; `ls -l` over 2 items =&gt; works\n    mv $i ${i/.md/.tex}   # 1st wildcat gets expanded into 2 items, 2nd wildcard does not get expanded =&gt; error\ndone\nA good way to fix this:\nfor i in *.md; do   # the wildcard gets expanded here into a string with 2 items =&gt; 2 loop iterations\n    ls -l \"$i\"      # `ls` acts on a string inside the quotes =&gt; works\n    mv \"$i\" \"${i/.md/.tex}\"   # `mv` acts on 2 strings inside the quotes =&gt; works\ndone\nOr you can do this with IFS, without having to use quotes:\n/bin/rm *.tex\ntouch \"my thesis.md\" \"first results.md\"   # really bad idea, but 99% of people do it anyway\nIFS=$'\\n\\t'   # more restrictive IFS\nfor i in *.md; do\n    ls -l $i\n    mv $i ${i/.md/.tex}\ndone\nYou can specifically use a newline character as a separator. Let’s create a file and prepend each line with the character count in that line:\necho first line &gt; a.txt\necho second line &gt;&gt; a.txt\ncat a.txt\n\nunset IFS\nfor w in $(cat a.txt); do    # counts characters in individual words\n    echo ${#w} $w\ndone\nIFS=$'\\n'\nfor w in $(cat a.txt); do    # counts characters in individual lines\n    echo ${#w} $w\ndone\nOf course, there are always alternative solutions without IFS, e.g.\ncat a.txt | while read line\n    do\n    echo ${#line} $line\ndone\nIFS can work with arrays too, but you have to be careful, as an array will always break between elements, no matter the value of IFS. With IFS set, it will break at the IFS characters and between elements.\na=(102030 hello there \"hi there\")\nunset IFS\nfor x in ${a[@]}; do     # breaks at spaces and between elements\n  echo $x\ndone\nfor x in \"${a[@]}\"; do   # breaks between elements\n  echo $x\ndone\nIFS=$'\\n\\t'\nfor x in ${a[@]}; do     # breaks between elements\n  echo $x\ndone\nIFS=\"0\"\nfor x in ${a[@]}; do     # breaks at 0 and between elements\n  echo $x\ndone\n\n\nPython inside self-contained bash functions\nThe operator &lt;&lt; – called here-document structure in bash – is used to pass some text input along with its ending pattern to a program, e.g.\nwc -l &lt;&lt; EOF\nline 1\nline 2\nEOF\nYou can save this input to a file:\ncat &lt;&lt; EOF &gt; b.txt\nline 1\nline 2\nEOF\nYou can use this mechanism to define some Python code inside a bash function:\nfunction pi() {\n    cat &lt;&lt; EOF &gt; uniqueCode.py\n#!/usr/bin/env python\nimport math as m\nprint(m.pi)\nEOF\n    chmod 700 uniqueCode.py\n    ./uniqueCode.py\n    /bin/rm uniqueCode.py\n}\nHere is a useful example:\nfunction extractEmails() {\n    cat &lt;&lt; EOF &gt; uniqueCode.py\n#!/usr/bin/env python\nimport sys, re\nfilename = open(sys.argv[1], \"r\")\ncontent = filename.readlines()\nemails = []\nfor i, line in enumerate(content):\n    email = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', line)\n    if len(email) &gt; 0 and \"...\" not in email[0]:\n        for e in email:\n            emails.append(e)\nprint(', '.join(map(str, emails)))   # print all emails in a single line without quotes\nEOF\n    chmod 700 uniqueCode.py\n    ./uniqueCode.py $1\n    /bin/rm uniqueCode.py\n}\ncat contact.txt\nextractEmails contact.txt"
  },
  {
    "objectID": "bashFeatures-20221207.html#second-part-marie",
    "href": "bashFeatures-20221207.html#second-part-marie",
    "title": "Lesser known but very useful Bash features",
    "section": "Second part (Marie)",
    "text": "Second part (Marie)\n\nFix commands\nThe builtin utility fc (“fix command”) allows to rerun or edit and rerun previous commands. This is particularly useful if you made a typo in a long command or a series of commands. It is also convenient if you want to rerun a series of commands.\nWithout any flag, fc will open your default text editor with the last command for you to edit. After saving and exiting your editor, the edited command will run.\nWith flags, you can:\n\nlist previous commands (they will be numbered): fc -l,\nopen a particular command from that list in your editor: fc &lt;number&gt;,\nopen all commands between &lt;number1&gt; and &lt;number2&gt; in your editor (they will all rerun once you save and close): fc &lt;number1&gt; &lt;number2&gt;,\nre-execute the last command with fc -s,\nre-execute a particular command without edit: fc -s &lt;number&gt;,\nre-execute all commands between &lt;number1&gt; and &lt;number2&gt; without edit: fc -s &lt;number1&gt; &lt;number2&gt;.\n\nAdditionally, you can specify the editor with the -e flag.\n\n\n\n\n\n\nNoteExamples:\n\n\n\nfc           # open last command with default editor to edit, then rerun\nfc -e emacs  # open last command with Emacs to edit, then rerun\n\nfc -l        # list past commands (they will be numbered)\nfc 34 38     # open default editor with commands number 34 to 38 to edit, then rerun\n\nfc -s 54     # rerun command number 54 without edit\n\n\n\n\nQuick substitution\nStill on the subject of fixing commands, if you want to rerun your last command with a substitution (e.g. you made a typo in the last command and you want to re-run it without the typo, or you are running a second similar command), you could recall the last command with C-p and navigate to the part that needs changing or you could run fc and do the same in your editor.\nBut there is a much faster method: the quick substitution of old by new simply by typing: ^old^new.\n\n\n\n\n\n\nNoteExample\n\n\n\nI already ran echo This is a test. Now, if I run ^test^cool test, it will actually run the command:\necho This is a cool test\n\n\n\n\nEasy access to unaliased versions of commands\nIf you have created aliases which use the names of Bash commands, calling those commands will call the aliases. You may however occasionally need to use the non-aliased commands.\nOne way to do this is to unalias your alias with unalias &lt;command&gt;. But then, you have lost your alias for the rest of your session or until you resource your .bashrc file.\nAnother option is to use the full path of the command (e.g. /usr/bin/&lt;command&gt;). If you don’t know the path of the command, you can find it with which &lt;command&gt;.\nStill, there is an even easier method: simply prepend your alias with \\.\n\n\n\n\n\n\nNoteExample\n\n\n\nI have an alias called ls for ls --color. I can check this by typing any of:\nalias ls\ntype ls\nI can run the original ls command without loosing my alias and without bothering with the full path of ls with:\n\\ls\n\n\n\n\nDetermine file types\nThe command file runs tests to determine the types of files based on their content (thus independently of any extension(s)).\n\n\n\n\n\n\nNoteOutput examples\n\n\n\ndirectory                                                            # directory\n\nsymbolic link to &lt;/some/path&gt;                                        # symlink\n\nPOSIX shell script, ASCII text executable                            # executable shell script\nPython script, ASCII text executable                                 # executable Python script\nPerl script text executable                                          # executable Perl script\n\nASCII text                                                           # text file\n\nempty                                                                # empty file\n\nPDF document, version 1.4                                            # .pdf\nPDF document, version 1.7 (zip deflate encoded)                      # .pdf\n\nGit index, version 2, 208 entries                                    # index in .Git repository\n\nGNU dbm 1.x or ndbm database, little endian, 64-bit                  # .db database\n\nZstandard compressed data (v0.8+)                                    # .zst compressed file\ngzip compressed data, was \"&lt;file&gt;.tar\", last modified: \\\n     Wed Feb 28 09:25:16 2007, \\\n     from FAT filesystem (MS-DOS, OS/2, NT), \\\n     original size modulo 2^32 24064                                 # .tar.gz compressed archive\n\nMPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, JntStereo              # .mp3 sound\nMPEG ADTS, layer III, v1, 128 kbps, 44.1 kHz, Stereo                 # .mp3 sound\nMPEG ADTS, layer III v1, 96 kbps, 44.1 kHz, Monaural                 # .mp3 sound\nFLAC audio bitstream data, 16 bit, stereo, 44.1 kHz, 7670460 samples # .flack sound\nMicrosoft ASF                                                        # .wma sound\n\nPNG image data, 665 x 742, 8-bit/color RGBA, non-interlaced          # .png image\nGIMP XCF image data, version 011, 161 x 157, RGB Color               # .xcf GIMP file\nSVG Scalable Vector Graphics image                                   # .svg image\nGIF image data, version 89a, 160 x 40                                # .gif image\n\nEPUB document                                                        # .epub book\nDjVu multiple page document                                          # .djvu book\n\n\nFor some file types, you get a lot more information. Here are two examples for .jpg:\nJPEG image data, JFIF standard 1.01, resolution (DPI), density 72x72, segment length 16, \\\n     Exif Standard: [\\012- TIFF image data, little-endian, direntries=6, xresolution=86, \\\n                           yresolution=94, resolutionunit=2, software=GIMP 2.10.14, \\\n                           datetime=2019:12:04 23:53:09], progressive, precision 8, 161x157, \\\n     components 3\n\nJPEG image data, JFIF standard 1.01, resolution (DPI), density 72x72, segment length 16, \\\n     progressive, precision 8, 395x533, components 3\nThis is most useful for binaries from which it is harder to gather information.\n\n\n\n\n\n\nNoteExample of Executable and Linkable Format on Linux\n\n\n\nELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, \\\n    interpreter /lib64/ld-linux-x86-64.so.2, \\\n    BuildID[sha1]=0e291ede656cae727e7a1d056c54392452b0fc59, for GNU/Linux 4.4.0, stripped\n\n\n\n\n\n\n\n\nNoteExample of Windows .exe file\n\n\n\nPE32+ executable (console) x86-64, for MS Windows\n\n\n\n\n\n\n\n\nNoteExample of object file (.o)\n\n\n\nELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped\n\n\n\n\nGet information on a program\nTo get information on a program, you can run command -V &lt;program&gt;.\n\n\n\n\n\n\nNoteExamples (outputs in comments)\n\n\n\ncommand -V python  # python is /usr/bin/python\ncommand -V pwd     # pwd is a shell builtin\ncommand -V ls      # ls is aliased to `ls --color'\n# The last one because I have this alias on my system\n\n\n\n\nBrace expansion to create backup files or change file extensions\nBrace expansion is useful in countless contexts (e.g. touch file{1..5}.txt will create the files file1.txt to file5.txt).\nHere are two situations where brace expansion is convenient: creating backup files and changing the extensions of files.\n\n\n\n\n\n\nNoteExample 1, creating backup files\n\n\n\ncp &lt;file&gt;{,.bak}    # Creates a copy of &lt;file&gt; called &lt;file&gt;.bak\n\n\n\n\n\n\n\n\nNoteExample 2, changing the extension of files\n\n\n\nmv &lt;file&gt;.{txt,md}  # Changes &lt;file&gt;.txt to &lt;file&gt;.md\n\n\n\n\nExpansion to the last argument of previous command\n$_ will expand to the last argument of the previous command.\n\n\n\n\n\n\nNoteExample\n\n\n\nmkdir test\ncd $_\n\n\nWhen using commands with long arguments (e.g. long file paths), this can be really convenient."
  },
  {
    "objectID": "chapel-compact.html",
    "href": "chapel-compact.html",
    "title": "Parallel programming in Chapel",
    "section": "",
    "text": "June 12th, 1:30pm-4:30pm Pacific Time\nChapel is a modern programming language designed for both shared and distributed memory systems, offering high-level, easy-to-use abstractions for task and data parallelism. Its intuitive syntax makes it an excellent choice for novice HPC users learning parallel programming. Chapel supports a wide range of parallel hardware – from multicore processors and multi-node clusters to GPUs – using consistent syntax and concepts across all levels of hardware parallelism.\nChapel dramatically reduces the complexity of parallel coding by combining the simplicity of Python-style programming with the performance of compiled languages like C and Fortran. Parallel operations that might require dozens of lines in MPI can often be written in just a few lines of Chapel. As an open-source language, it runs on most Unix-like operating systems and scales from laptops to large HPC systems.\nThis course begins with Chapel fundamentals, then focuses on data parallelism through two numerical examples: one embarrassingly parallel and one tightly coupled. We’ll also briefly explore task parallelism (a more complex topic, and not the primary focus in this course). Finally, we’ll introduce GPU programming with Chapel.\nInstructor: Alex Razoumov (SFU)\nPrerequisites: basic understanding of HPC at the introductory level (how to submit jobs with Slurm scheduler) and basic knowledge of the Linux command line.\nSoftware: For the hands-on, we will use Chapel on our training cluster. To access the training cluster, you will need a remote secure shell (SSH) client installed on your computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there). We will provide guest accounts to all participants. No need to install Chapel on your computer.\n\nSolutions\nYou can find the solutions here.\n\n\nLinks\n\nChapel homepage\nWhat is Chapel? (HPE Developer Portal)\nLinuxCon 2023 Introducing Chapel slides (PDF)\nGetting started guide for Python programmers\nLearn X=Chapel in Y minutes\nChapel on StackOverflow\nWatch Chapel: Productive, Multiresolution Parallel Programming talk by Brad Chamberlain\nWestGrid’s April 2019 webinar Working with distributed unstructured data in Chapel\nWestGrid’s March 2020 webinar Working with data files and external C libraries in Chapel discusses writing arrays to NetCDF and HDF5 files from Chapel",
    "crumbs": [
      "Front page"
    ]
  },
  {
    "objectID": "chapel-gpu.html",
    "href": "chapel-gpu.html",
    "title": "GPU computing with Chapel",
    "section": "",
    "text": "June 12th, 1:30pm-4:30pm Pacific Time\nChapel was designed as a parallel-first programming language targeting any hardware that supports parallel execution: multicore processors, multiple nodes in an HPC cluster, and now also GPUs on HPC systems. This is very different from the traditional approach to parallel programming in which you would use a dedicated tool for each level of hardware parallelism, e.g. MPI, or OpenMP, or CUDA, etc.\nLater in this workshop I will show a compact Chapel code that uses multiple nodes on a cluster, multiple GPUs on each node, and automatically utilizes all available (in our Slum job) CPU cores on each node, while copying the data as needed between all these hardware layers. But let’s start with the basics!"
  },
  {
    "objectID": "chapel-gpu.html#running-gpu-chapel-on-the-alliance-systems",
    "href": "chapel-gpu.html#running-gpu-chapel-on-the-alliance-systems",
    "title": "GPU computing with Chapel",
    "section": "Running GPU Chapel on the Alliance systems",
    "text": "Running GPU Chapel on the Alliance systems\nIn general, you need to configure and compile Chapel for your specific GPU type and your specific cluster interconnect. For NVIDIA and AMD GPUs (the only ones currently supported), you must use LLVM as the backend compiler. With NVIDIA GPUs, you must build LLVM to use LLVM’s NVPTX backend to support GPU programming, so usually you cannot use the system-provided LLVM module – instead you should set CHPL_LLVM=bundled during Chapel compilation.\nAs of this writing, on the Alliance systems, you can use GPUs from Chapel on the following systems:\n\nnative multi-locale on Fir, Rorqual, Narval; the binaries might not be in place everywhere, so please get in touch if you want to run it on a specific system\nnative single-locale on Nibi\non an Arbutus VM in a project with access to vGPUs (our plan for today)\nvia a single-locale GPU Chapel container on any Alliance system (clusters, cloud) with NVIDIA GPUs; let me know if you would like to access this container\n\nEfforts are underway to compile native Chapel 2.2 as a series of modules on all Alliance systems, but that might take a while."
  },
  {
    "objectID": "chapel-gpu.html#running-gpu-chapel-on-your-computer",
    "href": "chapel-gpu.html#running-gpu-chapel-on-your-computer",
    "title": "GPU computing with Chapel",
    "section": "Running GPU Chapel on your computer",
    "text": "Running GPU Chapel on your computer\nIf you have an NVIDIA GPU on your computer and run Linux, and have all the right GPU drivers and CUDA installed, it should be fairly straightforward to compile Chapel with GPU support. Here is what worked for me in AlmaLinux 9.4. Please let me know if these steps do not work for you.\nIn Windows, Chapel with GPU support works under the Windows Subsystem for Linux (WSL) as explained in this post. You could also run Chapel inside a Docker container, although you need to find a GPU-enabled Docker image."
  },
  {
    "objectID": "chapel-gpu.html#todays-setup",
    "href": "chapel-gpu.html#todays-setup",
    "title": "GPU computing with Chapel",
    "section": "Today’s setup",
    "text": "Today’s setup\nToday’s we’ll run Chapel on the virtual training cluster. We’ll now distribute the usernames and passwords – let’s try to log in.\nThere are two Chapel configurations we will use today.\n\nChapel with GPU support compiled for NVIDIA cards. We have 1 virtual GPU on the training cluster to share among all participants and the instructor, so we won’t be able to use it all at the same time. The idea is to try your final production code on this GPU allocating it only for a couple of minutes at a time per user:\n\n#!/bin/bash\n#SBATCH --time=00:02:00\n#SBATCH --mem=3600\n#SBATCH --gpus-per-node=1\nnvidia-smi\nchpl --fast test.chpl\n./test\nsource /project/def-sponsor00/shared/syncHPC/startSingleLocaleGPU.sh\nsbatch submit.sh\n\nWe will be doing most debugging on CPUs using the so-called ‘CPU-as-device’ mode to run a GPU code on a CPU. This is very handy for debugging a Chapel GPU code on a computer without a dedicated GPU and/or vendor SDK installed. You can find more details on this mode here. To enable this mode, I recompiled Chapel with export CHPL_GPU=cpu, but you need to load this version of Chapel separately, and you can use it via an interactive job:\n\nsource /project/def-sponsor00/shared/syncHPC/startSingleLocaleCPUasDevice.sh\nsalloc --time=2:0:0 --mem=3600\nchpl --fast test.chpl\n./test\nIn this mode there are some restrictions, e.g. data movement between the device and the host will not be captured (as there are no data moved!), some parallel reductions might not be available in this mode (to be confirmed), and the GPU kernel breakdown might be different. Still, all GPU kernels will be launched on the CPU, and you can even use some of the Chapel’s diagnostic features in this mode, e.g. @assertOnGpu and @gpu.assertEligible attributes will fail at compile time for ineligible loops.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease do not start nested Slurm jobs. When switching from one Chapel configuration to the other, please do this on the login node."
  },
  {
    "objectID": "chapel-gpu.html#useful-built-in-variables",
    "href": "chapel-gpu.html#useful-built-in-variables",
    "title": "GPU computing with Chapel",
    "section": "Useful built-in variables",
    "text": "Useful built-in variables\nFrom inside your Chapel code, you can access the following predefined variables:\n\nLocales is the list of locales (nodes) that your code can run on (invoked in code execution)\nnumLocales is the number of these locales\nhere is the current locale (node), and by extension current CPU\nhere.name is its name\nhere.maxTaskPar is the number of CPU cores on this locale\nhere.gpus is the list of available GPUs on this locale (sometimes called “sublocales”)\nhere.gpus.size is the number of available GPUs on this locale\nhere.gpus[0] is the first GPU on this locale\n\nLet’s try some of these out; store this code as test.chpl:\nwriteln(\"Locales: \", Locales);\nwriteln(\"on \", here.name, \" I see \", here.gpus.size, \" GPUs\");\nwriteln(\"and their names are: \", here.gpus);\nchpl --fast test.chpl\n./test\n\n\n\n\nLocales: LOCALE0\non cdr2514.int.cedar.computecanada.ca I see 1 GPUs\nand their names are: LOCALE0-GPU0\nThere is one GPU (even in ‘CPU-as-device’ mode), and it is available to us as the first (and only) element of the array here.gpus."
  },
  {
    "objectID": "chapel-gpu.html#our-first-gpu-code",
    "href": "chapel-gpu.html#our-first-gpu-code",
    "title": "GPU computing with Chapel",
    "section": "Our first GPU code",
    "text": "Our first GPU code\nTo benefit from GPU acceleration, you want to run a computation that can be broken into many independent identical pieces. An obvious example is a for loop in which each loop iteration does not depend on other iterations. Let’s run such an example on the GPU:\nconfig const n = 10;\non here.gpus[0] {\n  var A: [1..n] int;     // kernel launch to initialize an array\n  foreach i in 1..n do   // thread parallelism on a CPU or a GPU =&gt; kernel launch\n    A[i] = i**2;\n  writeln(\"A = \", A);    // copy A to host\n}\nA = 1 4 9 16 25 36 49 64 81 100\n\nthe array A is stored on the GPU\norder-independent loops will be executed in parallel on the GPU\nif instead of parallel foreach we use serial for, the loop will run on the CPU\nin our case the array A is both stored and computed on the GPU in parallel\ncurrently, to be computed on a GPU, an array must be stored on that GPU\nin general, when you run a code block on the device, parallel lines inside will launch kernels"
  },
  {
    "objectID": "chapel-gpu.html#alternative-syntax",
    "href": "chapel-gpu.html#alternative-syntax",
    "title": "GPU computing with Chapel",
    "section": "Alternative syntax",
    "text": "Alternative syntax\nWe can modify this code so that it runs on a GPU if present; otherwise, it will run on the CPU:\nvar operateOn =\n  if here.gpus.size &gt; 0 then here.gpus[0]   // use the first GPU\n  else here;                                // use the CPU\nwriteln(\"operateOn: \", operateOn);\nconfig const n = 10;\non operateOn {\n  var A: [1..n] int;\n  foreach i in 1..n do\n    A[i] = i**2;\n  writeln(\"A = \", A);\n}\nOf course, in ‘CPU-as-device’ mode this code will always run on the CPU. \n\n\nYou can also force a GPU check by hand:\nif here.gpus.size == 0 {\n  writeln(\"need a GPU ...\");\n  exit(1);\n}\noperateOn = here.gpus[0];"
  },
  {
    "objectID": "chapel-gpu.html#gpu-diagnostics",
    "href": "chapel-gpu.html#gpu-diagnostics",
    "title": "GPU computing with Chapel",
    "section": "GPU diagnostics",
    "text": "GPU diagnostics\nWrap our code into the following lines:\nuse GpuDiagnostics;\nstartGpuDiagnostics();\n...\nstopGpuDiagnostics();\nwriteln(getGpuDiagnostics());\noperateOn: LOCALE0-GPU0\nA = 1 4 9 16 25 36 49 64 81 100\n(kernel_launch = 2, host_to_device = 0, device_to_host = 10, device_to_device = 0)\nin 'CPU-as-device' mode: (kernel_launch = 2, host_to_device = 0, device_to_host = 0, device_to_device = 0)\nLet’s break down the events:\n\nwe have two kernel launches\n\n  var A: [1..n] int;     // kernel launch to initialize an array\n  foreach i in 1..n do   // kernel launch to run a loop in parallel\n    A[i] = i**2;\n\nwe copy 10 array elements device-to-host to print them (not shown in ‘CPU-as-device’ mode)\n\n  writeln(\"A = \", A);\n\nno other data transfer\n\nLet’s take a look at the example from https://chapel-lang.org/blog/posts/intro-to-gpus. They define a function:\nuse GpuDiagnostics;\nproc numKernelLaunches() {\n  stopGpuDiagnostics();   // assuming you were running one before\n  var result = getGpuDiagnostics().kernel_launch;\n  resetGpuDiagnostics();\n  startGpuDiagnostics();  // restart diagnostics\n  return result;\n}\nwhich can then be applied to these 3 examples (all in one on here.gpus[0] block):\nstartGpuDiagnostics();\non here.gpus[0] {\n  var E = 2 * [1,2,3,4,5]; // one kernel launch to initialize the array\n  writeln(E);\n  assert(numKernelLaunches() == 1);\n\n  use Math;\n  const n = 10;\n  var A = [i in 0..#n] sin(2 * pi * i / n); // one kernel launch\n  writeln(A);\n  assert(numKernelLaunches() == 1);\n\n  var rows, cols = 1..5;\n  var Square: [rows, cols] int;         // one kernel launch\n  foreach (r, c) in Square.indices do   // one kernel launch\n    Square[r, c] = r * 10 + c;\n  writeln(Square);\n  assert(numKernelLaunches() == 2); // 2 on GPU and 7 on CPU-as-device\n}\n\n\n\n\n\n\nNote\n\n\n\nOn CPU-as-device we have 7 kernel launches in the last block. My initial interpretation: one launch to initialize Square + one launch to access Square.indices + one launch per loop iteration, but it’s actually not entire correct …\n\n\n\n\n\n\n\n\nCautionQuestion 1\n\n\n\n\n\nLet’s play with this in the CPU-as-device mode! Make the upper limit of rows, cols a config variable, recompile, and play with the array size."
  },
  {
    "objectID": "chapel-gpu.html#verifying-if-a-loop-can-run-on-a-gpu",
    "href": "chapel-gpu.html#verifying-if-a-loop-can-run-on-a-gpu",
    "title": "GPU computing with Chapel",
    "section": "Verifying if a loop can run on a GPU",
    "text": "Verifying if a loop can run on a GPU\nThe loop attribute @assertOnGpu (applied to a loop) does two things:\n\nat compilation, will fail to compile a code that cannot run on a GPU and will tell you why\nat runtime, will halt execution if called from outside a GPU\n\nConsider the following serial code:\nconfig const n = 10;\non here.gpus[0] {\n  var A: [1..n] int;\n  for i in 1..n do\n    A[i] = i**2;\n  writeln(\"A = \", A);\n}\nA = 1 4 9 16 25 36 49 64 81 100\nThis code compiles fine (chpl --fast test.chpl), and it appears to run fine, printing the array. But it does not run on the GPU! Let’s mark the for loop with @assertOnGpu and try to compile it again. Now we get:\nerror: loop marked with @assertOnGpu, but 'for' loops don't support GPU execution\nSerial for loops cannot run on a GPU! Without @assertOnGpu the code compiled for and ran on the CPU. To port this code to the GPU, replace for with either foreach or forall (both are parallel loops), and it should compile with @assertOnGpu.\n\n\n\n\n\n\nNote\n\n\n\nWhen running in ‘CPU-as-device’ mode, @assertOnGpu attribute will produce a warning “ignored with CHPL_GPU=cpu”.\n\n\nAlternatively, you can count kernel launches – it’ll be zero for the for loop.\nMore on @assertOnGpu and other attributes at https://chapel-lang.org/docs/main/modules/standard/GPU.html.\n\n\n\n\n\n\n\nNote\n\n\n\nStarting with Chapel 2.2, there is an additional attribute @gpu.assertEligible that asserts that a statement is suitable for GPU execution (same as @assertOnGpu), without requiring it to be executed on a GPU. This is perfect in the ‘CPU-as-device’ mode: fails to compile a for loop, but no warnings at runtime."
  },
  {
    "objectID": "chapel-gpu.html#timing-on-the-cpu",
    "href": "chapel-gpu.html#timing-on-the-cpu",
    "title": "GPU computing with Chapel",
    "section": "Timing on the CPU",
    "text": "Timing on the CPU\nLet’s pack our computation into a function, so that we can call it from both a CPU and a GPU. For timing, we can use a stopwatch from the Time module:\nuse Time;\n\nconfig const n = 10;\nvar watch: stopwatch;\n\nproc squares(device) {\n  on device {\n    var A: [1..n] int;\n    foreach i in 1..n do\n      A[i] = i**2;\n    writeln(\"A = \", A[n-2..n]); // last 3 elements\n  }\n}\n\nwriteln(\"--- on CPU:\"); watch.start();\nsquares(here);\nwatch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\n\nwatch.clear();\n\nwriteln(\"--- on GPU:\"); watch.start();\nsquares(here.gpus[0]);\nwatch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\n$ chpl --fast test.chpl\n$ ./test --n=100_000_000\n--- on CPU:\nA = 9999999600000004 9999999800000001 10000000000000000\nIt took 7.94598 seconds\n--- on GPU:\nA = 9999999600000004 9999999800000001 10000000000000000\nIt took 0.003673 seconds\nYou can also call start and stop functions from inside the on device block – they will still run on the CPU. We will see an example of this later in this workshop."
  },
  {
    "objectID": "chapel-gpu.html#timing-on-the-gpu",
    "href": "chapel-gpu.html#timing-on-the-gpu",
    "title": "GPU computing with Chapel",
    "section": "Timing on the GPU",
    "text": "Timing on the GPU\nObtaining timing from within a running CUDA kernel is tricky as you are running potentially thousands of simultaneous threads, so you definitely cannot measure the wallclock time. However, you can measure GPU clock cycles spent on a partucular part of the kernel function. The GPU module provides a function gpuClock() that returns the clock cycle counter (per multiprocessor), and it needs to be called to time code blocks within a GPU-enabled loop.\n\n\n\nHere is an example (modelled after measureGpuCycles.chpl) to demonstrate its use. This is not the most efficient code, as on the GPU we are parallelizing the loop with n=10 iterations, and then inside each iteration we run a serial loop to keep the (few non-idle) GPU cores busy, but it gives you an idea.\n\n\n\n\n\n\nNote\n\n\n\nDo not run this code in the ‘CPU-as-device’ mode, as its output will not be particularly meaningful: you need a physical GPU to see actual counts.\n\n\nuse GPU;\n\nconfig const n = 10;\n\non here.gpus[0] {\n  var A: [1..n] int;\n  var clockDiff: [1..n] uint;\n  @assertOnGpu foreach i in 1..n {\n    var start, stop: uint;\n    A[i] = i**2;\n    start = gpuClock();\n    for j in 0..&lt;1000 do\n      A[i] += i*j;\n    stop = gpuClock();\n    clockDiff[i] = stop - start;\n  }\n  writeln(\"Cycle count = \", clockDiff);\n  writeln(\"Time = \", (clockDiff[1]: real) / (gpuClocksPerSec(0): real), \" seconds\");\n  writeln(\"A = \", A);\n}\nCycle count = 227132 227132 227132 227132 227132 227132 227132 227132 227132 227132\nTime = 0.148452 seconds\nA = 49501 49604 49709 49816 49925 50036 50149 50264 50381 50500"
  },
  {
    "objectID": "chapel-gpu.html#prime-factorization-of-each-element-of-a-large-array",
    "href": "chapel-gpu.html#prime-factorization-of-each-element-of-a-large-array",
    "title": "GPU computing with Chapel",
    "section": "Prime factorization of each element of a large array",
    "text": "Prime factorization of each element of a large array\nNow let’s compute a more interesting problem where we do some significant processing of each element, but independently of other elements – this will port nicely to a GPU.\nPrime factorization of an integer number is finding all its prime factors. For example, the prime factors of 60 are 2, 2, 3, and 5. Let’s write a function that takes an integer number and returns the sum of all its prime factors. For example, for 60 it will return 2+2+3+5 = 12, for 91 it will return 7+13 = 20, for 101 it will return 101, and so on.\nproc primeFactorizationSum(n: int) {\n  var num = n, total = 0, count = 0;\n  while num % 2 == 0 {\n    num /= 2;\n    count += 1;\n  }\n  for j in 1..count do total += 2;\n  for i in 3..(sqrt(n:real)):int by 2 {\n    count = 0;\n    while num % i == 0 {\n      num /= i;\n      count += 1;\n    }\n    for j in 1..count do total += i;\n  }\n  if num &gt; 2 then total += num;\n  return total;\n}\nWe can test it quickly:\nwriteln(primeFactorizationSum(60));\nwriteln(primeFactorizationSum(91));\nwriteln(primeFactorizationSum(101));\nwriteln(primeFactorizationSum(100_000_000));\nSince 1 has no prime factors, we will start computing from 2, and then will apply this function to all integers in the range 2..n, where n is a larger number. We will do all computations separately from scratch for each number, i.e. we will not cache our results (caching can significantly speed up our calculations but the point here is to focus on brute-force computing).\nWith the procedure primeFactorizationSum defined, here is the CPU version primesSerial.chpl:\nconfig const n = 10;\nvar A: [2..n] int;\nfor i in 2..n do\n  A[i] = primeFactorizationSum(i);\n\nvar lastFewDigits =\n  if n &gt; 5 then n-4..n  // last 5 digits\n  else 2..n;            // or fewer\n\nwriteln(\"A = \", A[lastFewDigits]);\nHere is the GPU version primesGPU.chpl:\nconfig const n = 10;\non here.gpus[0] {\n  var A: [2..n] int;\n  @gpu.assertEligible foreach i in 2..n do\n    A[i] = primeFactorizationSum(i);\n  var lastFewDigits =\n    if n &gt; 5 then n-4..n  // last 5 digits\n    else 2..n;            // or fewer\n  writeln(\"A = \", A[lastFewDigits]);\n}\nchpl --fast primesSerial.chpl\n./primesSerial --n=10_000_000\nchpl --fast primesGPU.chpl\n./primesGPU --n=10_000_000\nIn both cases we should see the same output:\nA = 4561 1428578 5000001 4894 49\n\nLet’s add timing to both codes:\nuse Time;\nvar watch: stopwatch;\n...\nwatch.start();\n...\nwatch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\nNote that this problem does not scale linearly with n, as with larger numbers you will get more primes. Here are my timings on Cedar’s V100 GPU:\n\n\n\nn\nCPU time in sec\nGPU time in sec\nspeedup factor\n\n\n\n\n1_000_000\n3.04051\n0.001649\n1844\n\n\n10_000_000\n92.8213\n0.042215\n2199\n\n\n100_000_000\n2857.04\n1.13168\n2525\n\n\n\n\nand on Fir’s full H100-80GB-HBM3 GPUs:\n\n\n\nn\nCPU time in sec\nGPU time in sec\nspeedup factor\n\n\n\n\n1_000_000\n0.5330\n0.000717\n743\n\n\n10_000_000\n16.49\n0.017904\n921\n\n\n100_000_000\n516.8\n0.545646\n947"
  },
  {
    "objectID": "chapel-gpu.html#finer-control",
    "href": "chapel-gpu.html#finer-control",
    "title": "GPU computing with Chapel",
    "section": "Finer control",
    "text": "Finer control\n\nThere are various settings that you can fine-tune via attributes for maximum performance, e.g. you can change the number of threads per block (default 512, should be a multiple of 32) when launching kernels:\n @gpu.blockSize(64) foreach i in 1..128 { ...}\nYou can also change the default when compiling Chapel via CHPL_GPU_BLOCK_SIZE variable, or when compiling Chapel codes by passing the flag --gpu-block-size=&lt;block_size&gt; to Chapel compiler\nAnother setting to play with is the number of iterations per thread:\n@gpu.itersPerThread(4) foreach i in 1..128 { ... }\nThis setting is probably specific to your computational problem. For these and other per-kernel attributes, please see this page."
  },
  {
    "objectID": "chapel-gpu.html#multiple-locales-and-multiple-gpus",
    "href": "chapel-gpu.html#multiple-locales-and-multiple-gpus",
    "title": "GPU computing with Chapel",
    "section": "Multiple locales and multiple GPUs",
    "text": "Multiple locales and multiple GPUs\nIf we have access to multiple locales and then multiple GPUs on each of those locales, we would utilize all this processing power through two nested loops, first cycling through all locales and then through all available GPUs on each locale:\ncoforall loc in Locales do\n  on loc {\n    writeln(\"on \", loc.name, \" I see \", loc.gpus.size, \" GPUs\");\n    coforall gpu in loc.gpus {\n      on gpu {\n        ... do some work in parallel ...\n      }\n    }\n  }\nHere we assume that we are running inside a multi-node job on the cluster, e.g.\nsalloc --time=1:0:0 --nodes=3 --mem-per-cpu=3600 --gpus-per-node=2 --account=...\nchpl --fast test.chpl\n./test -nl 3\nHow would we use this approach in practice? Let’s consider our primes factorization problem. Suppose we want to collect the results on one node (LOCALE0), maybe for printing or for some additional processing. We need to break our array A into pieces, each computed on a separate GPU from the total pool of 6 GPUs available to us inside this job. Here is our approach, following the ideas outlined in https://chapel-lang.org/blog/posts/gpu-data-movement – let’s store this file as primesGPU-distributed.chpl:\nimport RangeChunk.chunks;\n\nproc primeFactorizationSum(n: int) {\n  ...\n}\n\nconfig const n = 1000;\nvar A_on_host: [2..n] int;   // suppose we want to collect the results on one node (LOCALE0)\n\n// let's assume that numLocales = 3\n\ncoforall (loc, locChunk) in zip(Locales, chunks(2..n, numLocales)) {\n  /* loc=LOCALE0, locChunk=2..334 */\n  /* loc=LOCALE1, locChunk=335..667 */\n  /* loc=LOCALE2, locChunk=668..1000 */\n  on loc {\n    writeln(\"loc = \", loc, \"   chunk = \", locChunk);\n    const numGpus = here.gpus.size;\n    coforall (gpu, gpuChunk) in zip(here.gpus, chunks(locChunk, numGpus)) {\n      /* on LOCALE0 will see gpu=LOCALE0-GPU0, gpuChunk=2..168 */\n      /*                     gpu=LOCALE0-GPU1, gpuChunk=169..334 */\n      on gpu {\n        writeln(\"loc = \", loc, \"   gpu = \", gpu, \"   chunk = \", gpuChunk);\n        var A_on_device: [gpuChunk] int;\n        foreach i in gpuChunk do\n          A_on_device[i] = primeFactorizationSum(i);\n        A_on_host[gpuChunk] = A_on_device; // copy the chunk from the GPU via the host to LOCALE0\n      }\n    }\n  }\n}\n\nvar lastFewDigits = if n &gt; 5 then n-4..n else 2..n;   // last 5 or fewer digits\nwriteln(\"last few A elements: \", A_on_host[lastFewDigits]);\n\nRunning multi-GPU code on Fir\nWhen we run this code on 2 Fir nodes with 2 GPUs per node:\nsource /home/razoumov/startMultiLocaleGPU.sh\nsalloc --time=0:15:0 --nodes=2 --cpus-per-task=1 --mem-per-cpu=3600 --gpus-per-node=v100l:2 \\\n       --account=cc-debug\nchpl --fast primesGPU-distributed.chpl\n./primesGPU-distributed -nl 2\n\nwe get the following output:\nloc = LOCALE0   chunk = 2..501\nloc = LOCALE0   gpu = LOCALE0-GPU0   chunk = 2..251\nloc = LOCALE0   gpu = LOCALE0-GPU1   chunk = 252..501\nloc = LOCALE1   chunk = 502..1000\nloc = LOCALE1   gpu = LOCALE1-GPU0   chunk = 502..751\nloc = LOCALE1   gpu = LOCALE1-GPU1   chunk = 752..1000\nlast few A elements: 90 997 501 46 21\n\n\nDistributed A_on_host array\nIn the code above, the array A_on_host resides entirely in host’s memory on one node. With a sufficiently large problem, you can distribute A_on_host across multiple nodes using block distribution:\nuse BlockDist; // use standard block distribution module to partition the domain into blocks\nconfig const n = 1000;\nconst distributedMesh: domain(1) dmapped new blockDist(boundingBox={2..n}) = {2..n};\nvar A_on_host: [distributedMesh] int;\nThis way, when copying from device to host, you will copy only to the locally stored part of A_on_host."
  },
  {
    "objectID": "chapel-gpu.html#julia-set-problem",
    "href": "chapel-gpu.html#julia-set-problem",
    "title": "GPU computing with Chapel",
    "section": "Julia set problem",
    "text": "Julia set problem\nIn the Julia set problem we need to compute a set of points on the complex plane that remain bound under infinite recursive transformation \\(f(z)\\). We will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\n\npick a point \\(z_0\\in\\mathbb{C}\\)\ncompute iterations \\(z_{i+1}=z_i^2+c\\) until \\(|z_i|&gt;4\\) (arbitrary fixed radius; here \\(c\\) is a complex constant)\nstore the iteration number \\(\\xi(z_0)\\) at which \\(z_i\\) reaches the circle \\(|z|=4\\)\nlimit max iterations at 255\n4.1 if \\(\\xi(z_0)=255\\), then \\(z_0\\) is a stable point\n4.2 the quicker a point diverges, the lower its \\(\\xi(z_0)\\) is\nplot \\(\\xi(z_0)\\) for all \\(z_0\\) in a rectangular region \\(-1&lt;=\\mathfrak{Re}(z_0)&lt;=1\\), \\(-1&lt;=\\mathfrak{Im}(z_0)&lt;=1\\)\n\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\n\n\n\n\n\n\n\nNote\n\n\n\nYou might want to try these values too:\n\\(c = 1.2e^{1.1πi}\\) \\(~\\Rightarrow~\\) original textbook example\n\\(c = -0.4-0.59i\\) and 1.5X zoom-out \\(~\\Rightarrow~\\) denser spirals\n\\(c = 1.34-0.45i\\) and 1.8X zoom-out \\(~\\Rightarrow~\\) beans\n\\(c = 0.34-0.05i\\) and 1.2X zoom-out \\(~\\Rightarrow~\\) connected spiral boots\n\n\nBelow is the serial code juliaSetSerial.chpl:\nuse Time;\n\nconfig const n = 2_000;   // 2000^2 image\nvar watch: stopwatch;\nconfig const save = false;\n\nproc pixel(z0) {\n  const c = 0.355 + 0.355i;\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if z.re**2+z.im**2 &gt;= 16 then // abs(z)&gt;=4 does not work with LLVM\n      return i;\n  }\n  return 255;\n}\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nwatch.start();\nvar stability: [1..n,1..n] int;\nfor i in 1..n {\n  var y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    var point = 2*(j-0.5)/n - 1 + y*1i;\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nchpl  --fast juliaSetSerial.chpl\n./juliaSetSerial\nIt took me 2.34679 seconds to compute a \\(2000^2\\) fractal.\n\nPorting the Julia set problem to a GPU\nLet’s port this problem to a GPU! Copy juliaSetSerial.chpl to juliaSetGPU.chpl and make the following changes.\nStep 1 (optional, will work only on a physical GPU):\n&gt; if here.gpus.size == 0 {\n&gt;   writeln(\"need a GPU ...\");\n&gt;   exit(1);\n&gt; }\nAs of this writing, Chapel 2.2 does not support complex arithmetic on a GPU. Independently of the precision, you will get errors at compilation (if marked with @gpu.assertEligible):\nerror: Loop is marked with @gpu.assertEligible but is not eligible for execution on a GPU\n... function calls out to extern function (_chpl_complex128), which is not marked as GPU eligible\n... function calls out to extern function (_chpl_complex64), which is not marked as GPU eligible\nIf not marked with @gpu.assertEligible, the code compiles with complex arithmetic on a GPU, but it seems to take forever to finish.\nFortunately, we can implement complex arithmetic manually:\nStep 2:\n&lt; proc pixel(z0) {\n---\n&gt; proc pixel(x0,y0) {\nStep 3:\n&lt;   var z = z0*1.2;   // zoom out\n---\n&gt;   var x = x0*1.2;   // zoom out\n&gt;   var y = y0*1.2;   // zoom out\nStep 4:\n&lt;     z = z*z + c;\n---\n&gt;     var xnew = x**2 - y**2 + c.re;\n&gt;     var ynew = 2*x*y + c.im;\n&gt;     x = xnew;\n&gt;     y = ynew;\nStep 5:\n&lt;     if z.re**2+z.im**2 &gt;= 16 then // abs(z)&gt;=4 does not work with LLVM\n---\n&gt;     if x**2+y**2 &gt;= 16 then\nStep 6:\n&lt; var stability: [1..n,1..n] int;\n---\n&gt; on here.gpus[0] {\n&gt;   var stability: [1..n,1..n] int;\n...\n&gt; }\nStep 7:\n&lt; for i in 1..n {\n---\n&gt;   @gpu.assertEligible foreach i in 1..n {\nStep 8:\n&lt;     var point = 2*(j-0.5)/n - 1 + y*1i;\n&lt;     stability[i,j] = pixel(point);\n---\n&gt;       var x = 2*(j-0.5)/n - 1;\n&gt;       stability[i,j] = pixel(x,y);\nHere is the full GPU version of the code juliaSetGPU.chpl:\nuse Time;\n\nconfig const n = 2_000;   // 2000^2 image\nvar watch: stopwatch;\nconfig const save = false;\n\nif here.gpus.size == 0 {\n  writeln(\"need a GPU ...\");\n  exit(1);\n}\n\nproc pixel(x0,y0) {\n  const c = 0.355 + 0.355i;\n  var x = x0*1.2; // zoom out\n  var y = y0*1.2; // zoom out\n  for i in 1..255 {\n    var xnew = x**2 - y**2 + c.re;\n    var ynew = 2*x*y + c.im;\n    x = xnew;\n    y = ynew;\n    if x**2+y**2 &gt;= 16 then\n      return i;\n  }\n  return 255;\n}\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nwatch.start();\non here.gpus[0] {\n  var stability: [1..n,1..n] int;\n  @gpu.assertEligible foreach i in 1..n {\n    var y = 2*(i-0.5)/n - 1;\n    for j in 1..n {\n      var x = 2*(j-0.5)/n - 1;\n      stability[i,j] = pixel(x,y);\n    }\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nIt took 0.017364 seconds on the GPU.\n\n\n\nProblem size\nCPU time in sec\nGPU time in sec\nspeedup factor\n\n\n\n\n\\(2000\\times 2000\\)\n1.64477\n0.017372\n95\n\n\n\\(4000\\times 4000\\)\n6.5732\n0.035302\n186\n\n\n\\(8000\\times 8000\\)\n26.1678\n0.067307\n389\n\n\n\\(16000\\times 16000\\)\n104.212\n0.131301\n794\n\n\n\n\n\nAdding plotting to run on a GPU\nChapel’s Image library lets you write arrays of pixels to a PNG file. The following code – when added to the non-GPU code juliaSetSerial.chpl – writes the array stability to a file 2000.png assuming you\n\ndownload the colour map in CSV and\nadd the file sciplot.chpl (pasted below)\n\nwriteln(\"Plotting ...\");\nuse Image, Math, sciplot;\nwatch.clear();\nwatch.start();\nconst smin = min reduce(stability);\nconst smax = max reduce(stability);\nvar colour: [1..n, 1..n] 3*int;\nvar cmap = readColourmap('nipy_spectral.csv');   // cmap.domain is {1..256, 1..3}\nfor i in 1..n {\n  for j in 1..n {\n    var idx = ((stability[i,j]:real-smin)/(smax-smin)*255):int + 1; //scale to 1..256\n    colour[i,j] = ((cmap[idx,1]*255):int, (cmap[idx,2]*255):int, (cmap[idx,3]*255):int);\n  }\n}\nvar pixels = colorToPixel(colour);               // array of pixels\nwriteImage(n:string+\".png\", imageType.png, pixels);\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n// save this as sciplot.chpl\nuse IO;\nuse List;\n\nproc readColourmap(filename: string) {\n  var reader = open(filename, ioMode.r).reader();\n  var line: string;\n  if (!reader.readLine(line)) then   // skip the header\n    halt(\"ERROR: file appears to be empty\");\n  var dataRows : list(string); // a list of lines from the file\n  while (reader.readLine(line)) do   // read all lines into the list\n    dataRows.pushBack(line);\n  var cmap: [1..dataRows.size, 1..3] real;\n  for (i, row) in zip(1..dataRows.size, dataRows) {\n    var c1 = row.find(','):int;    // position of the 1st comma in the line\n    var c2 = row.rfind(','):int;   // position of the 2nd comma in the line\n    cmap[i,1] = row[0..c1-1]:real;\n    cmap[i,2] = row[c1+1..c2-1]:real;\n    cmap[i,3] = row[c2+1..]:real;\n  }\n  reader.close();\n  return cmap;\n}\nHere are the typical timings on a CPU:\nComputing 2000x2000 Julia set ...\nIt took 0.382409 seconds\nPlotting ...\nIt took 0.192508 seconds\nThis plotting snippet won’t work with the GPU version, as in that version the array stability is defined on the GPU only. You can move the definition of stability to the host, and then the code with plotting on the CPU will work. However, plotting a large heatmap can really benefit from GPU acceleration.\n\n\n\n\n\n\nCautionQuestion Port plotting to a GPU\n\n\n\n\n\nThis is a take-home exercise: try porting this plotting block to a GPU. On my side, I am running into an issue assigning to a tuple element on a GPU with\ncolour[i,j] = ((cmap[idx,1]*255):int, (cmap[idx,2]*255):int, (cmap[idx,3]*255):int);\ngetting a runtime “Error calling CUDA function: an illegal memory access was encountered (Code: 700)”. Image library is still marked unstable, so perhaps this is a bug. No issues running my code on ‘CPU-as-device’.\nIf you succeed in running this block on a real GPU, please send me your solution."
  },
  {
    "objectID": "chapel-gpu.html#reduction-operations",
    "href": "chapel-gpu.html#reduction-operations",
    "title": "GPU computing with Chapel",
    "section": "Reduction operations",
    "text": "Reduction operations\n\nBoth the prime factorization problem and the Julia set problem compute elements of a large array in parallel on a GPU, but they don’t do any reduction (combining multiple numbers into one). It turns out, you can do reduction operations on a GPU with the usual reduce intent in a parallel loop:\nconfig const n = 1e8: int;\nvar total = 0.0;\non here.gpus[0] {\n  forall i in 1..n with (+ reduce total) do\n    total += 1.0 / i**2;\n  writef(\"total = %{###.###############}\\n\", total);\n}\nAlternatively, you can use built-in reduction operations on an array (that must reside in GPU-accessible memory), e.g.\nuse GPU;\nconfig const n = 1e8: int;\non here.gpus[0] {\n  var a: [1..n] real;\n  forall i in 1..n do\n    a[i] = 1.0 / i**2;\n  writef(\"total = %{###.###############}\\n\", gpuSumReduce(a));\n}\nOther supported array reduction operations on GPUs are gpuMinReduce(), gpuMaxReduce(), gpuMinLocReduce() and gpuMaxLocReduce()."
  },
  {
    "objectID": "chapel-gpu.html#links",
    "href": "chapel-gpu.html#links",
    "title": "GPU computing with Chapel",
    "section": "Links",
    "text": "Links\n\nIntroduction to GPU Programming in Chapel\nChapel’s High-Level Support for CPU-GPU Data Transfers and Multi-GPU Programming\nGPU module functions and attributes"
  },
  {
    "objectID": "apptainer1/02-build.html",
    "href": "apptainer1/02-build.html",
    "title": "Creating and running container images",
    "section": "",
    "text": "Blue lines show workflows without modifying a container, whereas grey-line workflows modify system files in your container and typically require root, although there are non-root workarounds in dotted boxes.",
    "crumbs": [
      "Creating and running container images"
    ]
  },
  {
    "objectID": "apptainer1/02-build.html#build-command",
    "href": "apptainer1/02-build.html#build-command",
    "title": "Creating and running container images",
    "section": "Build command",
    "text": "Build command\nThe apptainer build &lt;imagePath&gt; &lt;buildSpec&gt; command is a versatile tool that lets you:\n\ndownload and assemble existing containers from external hubs like Docker Hub (docker://), Singularity Hub (shub://), and the Container Library (library:// - no longer supported by default),\ncreate a container from scratch using an Apptainer definition file customized to your needs,\nconvert containers between different formats supported by Apptainer, e.g. create a container from a sandbox.\n\n\n\n\n\n\napptainer build --help\n\nBuild a Container page in the Apptainer documentation\n\nIn most cases, you would download an existing Docker/Apptainer container image and build/run a container from it, without having to modify the image. Or maybe, the image is already provided by your lab, a research collaborator, or the Alliance. For example, right now we are testing a set of Apptainer images with GPU development tools for different GPU architectures, such as NVIDIA’s CUDA and AMD’s ROCm, as it is often difficult to compile these from source.\n\n\n\n\n\n\n\nNote\n\n\n\nIn most cases apptainer build ... will need root access, so for simple downloading of an online image as a regular user you might prefer apptainer pull ... command.\n\n\n\nBuild consideration 1: modifying system files inside the container (root vs. regular user)\nIn some cases, you might want to modify the image or build your own container image from scratch. To create a container image, you need a machine that:\n\nruns Linux,\nhas Apptainer installed,\nhas Internet access, and\nideally where you have root (or sudo) permissions, otherwise all permissions inside the container will be messed up, and you won’t have root or sudo access  ➜  you won’t be able to install or upgrade packages (this requires root).\n\nIt is possible to install packages and modify system (root-owned) files inside the container without being root, either:\n\nby using --fakeroot to modify system files inside the container as if you were root (covered in this section; this option might be prone to errors, depending on the complexity of your container), or\nby using a --remote build option (covered in this section).\n\nIf installing packages is not important to you, i.e. you are planning to use the container as is for production purposes, you can probably create an image on an HPC cluster. If you run into problems, please ask for help at support@tech.alliancecan.ca.\n\n\n\n\n\n\nNote\n\n\n\nIn Apptainer the user always remains the same inside and outside of the container. In other words, if you enter a container without root privileges, you won’t be able to obtain root privileges within the container.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have access to your own Linux computer, it is best to build your images there (root access and performance from a local drive). Alternatively, you can use a VM where you have root access. In the Alliance, we provide cloud projects to researchers for spinning up VMs.\n\n\n\n\nBuild consideration 2: host’s filesystem limitations\nAnother limitation to keep in mind is the type and bandwitdth of the host’s filesystem in which you are building the container. Parallel filesystems such as Lustre and GPFS (used on our clusters for /home, /scratch, /project) have some limitations that might lead to errors when creating a container – for technical details on one of the issues see this page.\nIn addition, building containers involves a large number of small file operations slowing down parallel filesystems which were not optimized for this type of I/O. For this reason, when building containers on our clusters, we highly recommend doing this inside a Slurm job in $SLURM_TMPDIR (SSD on a compute node).",
    "crumbs": [
      "Creating and running container images"
    ]
  },
  {
    "objectID": "apptainer1/02-build.html#running-pre-packaged-containers-without-modifying-them",
    "href": "apptainer1/02-build.html#running-pre-packaged-containers-without-modifying-them",
    "title": "Creating and running container images",
    "section": "Running pre-packaged containers (without modifying them)",
    "text": "Running pre-packaged containers (without modifying them)\nRun the “Lolcow” container by Apptainer developers:\nmkdir tmp && cd tmp\nmodule load apptainer\nsalloc --time=2:0:0 --mem-per-cpu=3600   # very important!!!\napptainer pull hello-world.sif shub://vsoch/hello-world   # store it as hello-world.sif\nls\napptainer run hello-world.sif   # run its default script\nWhere is this script? What did running the container actually do to result in the displayed output?\napptainer inspect -r hello-world.sif          # it runs the script /rawr.sh\napptainer exec hello-world.sif cat /rawr.sh   # here is what's inside\nWe can also run an image on the fly without putting it into the current directory:\nrm hello-world.sif                        # clear old image\napptainer run shub://vsoch/hello-world    # use the cached image    \nAlternatively, we can run a Docker container directly off Docker Hub. However, it will first convert a Docker image into an Apptainer image, and then run this Apptainer image:\napptainer run docker://godlovedc/lolcow   # random cow message\nLet’s try something more basic:\napptainer run docker://ubuntu   # press Ctrl-D to exit\nWhat happened here in the last example? Well, there was no default script, so it presented the container shell to type commands. You can exit the container with Ctrl-D or exit.\n\nApptainer’s image cache\nIn the last two examples we did not store SIF images in the current directory. Where were they stored?\napptainer cache list\napptainer cache list -v\napptainer cache clean          # clean all; will ask to confirm\napptainer cache clean --help   # more granular control\nBy default, Apptainer cache is stored in $HOME/.apptainer/cache. We can control the cache location with APPTAINER_CACHE environment variable.\n\nExercise\nAre there any APPTAINER* variables in our setup? What do you think they do?\n\n\n\nInspecting image metadata\napptainer inspect /path/to/SIF/file      # some general build information\napptainer inspect -r /path/to/SIF/file   # short for --runscript\napptainer inspect -d /path/to/SIF/file   # short for --deffile\n\nExercise\nCan you find other inspection flags besides -r and -d?",
    "crumbs": [
      "Creating and running container images"
    ]
  },
  {
    "objectID": "apptainer1/02-build.html#modifying-containers",
    "href": "apptainer1/02-build.html#modifying-containers",
    "title": "Creating and running container images",
    "section": "Modifying containers",
    "text": "Modifying containers\n\nDemo: building a development container in a sandbox as root\n\n\n\n\n\n\nNote\n\n\n\nYou need to be root in this section. For this reason, in this section I will run a demo on a machine with root access, and you can simply watch.\n\n\n\nPull an existing Docker image from Docker Hub.\nCreate a modifiable sandbox directory into which you can install packages.\nAdd packages and perhaps your application and data.\nConvert the sandbox into a regular SIF image.\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, Apptainer containers are read-only, i.e. while you can write into bind-mounted directories, normally you cannot modify files inside the container.\n\n\nTo build a writable container into which you can install packages, you need root access. The --sandbox flag below builds a sandbox to which changes can be made, and the --writable flag launches a read-write container. In this example ubuntu.dir is a directory on the host filesystem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI will log in as user centos (with sudo privileges) to the training cluster and run apptainer with the full path (as apptainer from CVMFS will not be accessible to sudo):\n\nssh centos@cass\nmkdir -p tmp && cd tmp\nalias apptainer=/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Core/apptainer/1.2.4/bin/apptainer\nalias sudo='sudo '   # so that the command after sudo is checked for alias\napptainer build --sandbox ubuntu.dir docker://ubuntu   # sudo not yet required\ndu -sh ubuntu.dir                              # 82M, before installing packages\nsudo apptainer shell --writable ubuntu.dir     # start the sandbox in writable mode;\n                                               # sudo needed to install packages as root\nApptainer&gt; apt-get update            # update the package index files\nApptainer&gt; wget                      # not available\nApptainer&gt; whoami                    # I am root, so can install software system-wide\nApptainer&gt; apt-get -y install wget   # will fail here if Apptainer run without root\n\nsudo du -sh ubuntu.dir               # 124M; sudo necessary to scan root directories\n\nAbove you might see a warning when running ... apptainer shell --writable ...\nWARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container\noccurs when you try to mount a file/directory into the container without that destination already inside the container. You can simply ignore this message. We will learn bind-mounting in the next section.\n\nTo convert the sandbox to a regular non-writable SIF container image, I will use:\nsudo apptainer build ubuntu.sif ubuntu.dir\nsudo rm -rf ubuntu.dir\nls -lh ubuntu.sif            # 58M (compressed)\napptainer shell ubuntu.sif   # can now start it as non-root\n\nApptainer&gt; wget\nApptainer&gt; ...\n\nscp ubuntu.sif user01@cass.vastcloud.org:/project/def-sponsor00/shared\nAs a regular user on the training cluster, I should be able to use this image now:\nchmod -R og+rX /project/def-sponsor00/shared   # give read access to all users\nmodule load apptainer\napptainer shell /project/def-sponsor00/shared/ubuntu.sif\n\n\nFakeroot in a sandbox: no need for root!\n\n\n\n\n\n\nNote\n\n\n\nYou need an Apptainer 1.1 or later in this section, as well as fakeroot-sysv utility on the host preceding fakeroot in the path. Our CVMFS modules have been patched to include this utility inside the module’s private bindir, and on standalone Linux systems fakeroot-sysv is typically installed when you install the Apptainer package. This is to say that the approach outlined in this section most likely works both on our production clusters and on most standalone Linux systems with Apptainer 1.1 (or higher). However, it is always preferable to modify system files in the containers as root, and with --fakeroot you might still run into problems with more complex Apptainer images, so always monitor your output for errors.\n\n\nWe will follow the same recipe:\n\nPull an existing Docker image from Docker Hub.\nCreate a modifiable sandbox directory into which you can install packages.\nAdd packages and perhaps your application and data.\nConvert the sandbox into a regular SIF image.\n\nHowever, this time we will start the sandbox shell with the --fakeroot flag that will let us write system files inside the container as a regular (non-root) user. As a reminder, we need the sandbox to be able to modify files inside the container, i.e. we cannot do this inside an immutable SIF image.\nmkdir -p tmp && cd tmp\napptainer build --sandbox ubuntu.dir docker://ubuntu   # create the sandbox in ubuntu.dir/\ndu -sh ubuntu.dir                                      # 82M, before installing packages\napptainer shell --fakeroot --writable ubuntu.dir\n\nExercise\nAt this point you might see the following output:\nINFO:    User not listed in /etc/subuid, trying root-mapped namespace\nINFO:    Using fakeroot command combined with root-mapped namespace\nWARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container\nWARNING: By using --writable, Apptainer can't create /project destination automatically without overlay or underlay\nFATAL:   container creation failed: mount hook function failure: mount /project-&gt;/project error: while mounting /project: destination /project doesn't exist in container\nHow would you solve it?\n\n\n\n\n\n\n\n\nNext, inside the container, we can install the software we need, e.g.\nApptainer&gt; apt-get update\nApptainer&gt; apt-get -y install wget\n\ndu -sh ubuntu.dir                     # 124M\nTo convert the sandbox to a regular non-writable SIF container image, use\napptainer build ubuntu.sif ubuntu.dir\nrm -rf ubuntu.dir\napptainer shell ubuntu.sif\n\nApptainer&gt; wget\nApptainer&gt; ...\n\n\nBuilding a development container from a definition file: root or not\n\n\n\n\n\n\nNote\n\n\n\nDepending on your Apptainer installation, you may or may not need to be root in this section. Try to follow along, if you want, or alternatively simply watch this demo.\n\n\nInside your working directory, create a new file test.def:\ncd ~/tmp\nnano test.def\nBootstrap: docker\nFrom: ubuntu:24.04\n\n%post\n    apt-get -y update && apt-get install -y python3\n\n%runscript\n    python3 -c 'print(\"Hello World! Hello from our custom Apptainer image!\")'\nWe will bootstrap our image from a minimal Ubuntu 20.04 Linux Docker image as then run it as a regular user:\nunset APPTAINER_BIND\napptainer build test.sif test.def   # in Apptainer 1.0 important to run this as root; it can\n                                    # run without sudo but will use /etc/subuid mappings;\n                                    # running this as regular user will result in errors;\n                                    # in properly configured Apptainer 1.1 and higher can run\n                                    # this as regular user\nls -l test.sif           # 69M\napptainer run test.sif   # Hello World! Hello from our custom Apptainer image!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore advanced definition files may have these sections:\n%setup - commands in this section are first executed on the host system outside of the container\n%files - copy files into the container with greater safety\n%environment - define environment variables that will be set at runtime\n%startscript - executed when the `instance start` command is issued\n%test - runs at the very end of the build process to validate the container using a method of your choice\n%labels - add metadata to `/.apptainer.d/labels.json` within your container\n%help - this text can then be displayed with `apptainer run-help ...` in the host\nas described in the user guide.\n\n\n\n\n\n\nImportantKey point\n\n\n\nApptainer definition files are used to define the build process and configuration for an image.\n\n\n\n\n\n\n\n\nNoteDiscussion\n\n\n\nAt this point, how do we install additional packages into this new container? There are several (three?) options.\n\n\n\n\nRemote builder: no need for root!\nIf you have access to a platform with Apptainer installed but you don’t have root access to create containers, you may be able to use the Remote Builder functionality to offload the process of building an image to remote cloud resources. One popular cloud service for this is the Remote Builder from SyLabs, the developers of Apptainer. If you want to use their service, you will need to register for a cloud token via the link on the their page. Here is one possible workflow:\n\nLog in at https://cloud.sylabs.io/builder (can sign in with an existing GitHub account).\nGenerate an access token at https://cloud.sylabs.io/auth/tokens and copy it into your clipboard.\nOn the training cluster create a new local file test.def:\n\nBootstrap: docker\nFrom: ubuntu:24.04\n\n%post\n    apt-get -y update && apt-get install -y python3\n\n%runscript\n    python3 -c 'print(\"Hello World! Hello from our custom Apptainer image!\")'\n\nBuild the image remotely and then run it locally:\n\nmodule load apptainer\napptainer remote login    # enter the token\napptainer remote status   # check that you are able to connect to the services\napptainer build --remote test.sif test.def\nls -l test.sif   # 62M\napptainer run test.sif   # Hello World! Hello from our custom Apptainer image!\nYou can find more information on remote endpoints in the official documentation.",
    "crumbs": [
      "Creating and running container images"
    ]
  },
  {
    "objectID": "apptainer1/02-build.html#running-more-useful-docker-images-via-apptainer-as-regular-user",
    "href": "apptainer1/02-build.html#running-more-useful-docker-images-via-apptainer-as-regular-user",
    "title": "Creating and running container images",
    "section": "Running more useful Docker images via Apptainer as regular user",
    "text": "Running more useful Docker images via Apptainer as regular user\n\nDemo: running client-server ParaView from a container\nYou can pull a Docker image from Docker Hub and convert it to an Apptainer image. For this you typically do not need sudo access. Please build containers only on compute nodes, as this process is CPU-intensive.\nThe following commands require online access and will work only on Cedar (and the training cluster!) where compute nodes can access Internet. Let’s search Docker Hub for “topologytoolkit”. Click on the result and then on Tags – you should see the suggested Docker command “docker pull topologytoolkit/ttk-dev:5.11.0”. From Apptainer, the address will be “docker://topologytoolkit/ttk-dev:5.11.0”.\ncd ~/tmp\nmodule load apptainer\nsalloc --cpus-per-task=1 --time=0:30:0 --mem-per-cpu=3600\napptainer pull topologytoolkit.sif docker://topologytoolkit/ttk-dev:5.11.0\n\n\n\n\n\n\nNote\n\n\n\nOn other production clusters – such as Béluga, Narval or Graham – compute nodes do not have Internet access, so you will have to use the two-step approach there:\ncd ~/scratch\nwget https://raw.githubusercontent.com/moby/moby/master/contrib/download-frozen-image-v2.sh\nsh download-frozen-image-v2.sh build ttk:latest   # download an image\n                                                  # from Docker Hub into build/\ncd build && tar cvf ../ttk.tar * && cd ..\nmodule load apptainer\nsalloc --cpus-per-task=1 --time=0:30:0 --mem-per-cpu=3600 --account=...\napptainer build topologytoolkit.sif docker-archive://ttk.tar   # build the Apptainer image;\n                                                               # wait for `Build complete`\n/bin/rm -rf build topologytoolkit.tar\n\n\nLet’s use this image! While still inside the job (on a compute node):\nunzip /project/def-sponsor00/shared/paraview.zip data/sineEnvelope.nc   # unpack some sample data\napptainer exec -B /home topologytoolkit.sif pvserver\nIf successful, it should say something like “Accepting connection(s): node1.int.cass.vastcloud.org:11111” – write down the node and the port names as you will need them in the next step.\nOn your laptop, set up SSH port forwarding to this compute node and port:\nssh username@cass.vastcloud.org -L 11111:node1:11111\nNext, start ParaView 5.11.x on your computer and connect to localhost:11111, and then load the dataset and visualize it.\n\nExercise: switch to another Linux distribution\nLet’s try creating an image with another Linux distribution:\napptainer pull debian.sif docker://debian:latest\nTry running nano text editor inside one of these containers. If it is not installed, install it yourself: create a writable sandbox directory, enter it with --fakeroot --writable, install nano (google the commands), convert it back to SIF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: latest Python image\nPull the latest Python image from Docker Hub into an Apptainer image. It should take few minutes to build it. 1. How large is the resulting image? 1. Run container’s Python with the apptainer exec command. Which version of Python did it install? 1. Do some math, e.g. print \\(\\pi\\). 1. When you exit Python, you will be back to your host system’s shell. 1. Now try running Python via the apptainer shell command. 1. Which operating system does this container run?\nmodule load apptainer\nsalloc --cpus-per-task=1 --time=3:00:0 --mem-per-cpu=3600\napptainer pull python.sif docker://???\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: CPU-based PyTorch image\nPull a recent CPU-based PyTorch image from Docker Hub into an Apptainer image.\nmodule load apptainer\nsalloc --cpus-per-task=1 --time=3:00:0 --mem-per-cpu=3600\napptainer pull pytorch.sif docker://???\n\nWhich operating system does this container run?\nTry running some basic PyTorch commands inside this image:\n\nimport torch\nA = torch.tensor([[2., 3., -1.], [1., -2., 8.], [6., 1., -3.]])\nprint(A)\nb = torch.tensor([5., 21., -1.])\nprint(b)\nx = torch.linalg.solve(A, b)\nprint(x)\ntorch.allclose(A @ x, b)   # verify the result\nFor more info on working with PyTorch tensors watch our webinar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOn our clusters we have PyTorch installed natively, both for CPUs and GPUs, so no need to rely on a container for this.",
    "crumbs": [
      "Creating and running container images"
    ]
  },
  {
    "objectID": "apptainer1/03-run.html",
    "href": "apptainer1/03-run.html",
    "title": "More on running containers",
    "section": "",
    "text": "As a reminder, we continue working in our ~/tmp directory inside an interactive job with the Apptainer module loaded:\ncd ~/tmp\nmodule load apptainer\nsalloc --cpus-per-task=1 --time=3:00:0 --mem-per-cpu=3600   # only from the login node\nIf you have not done so already, let’s pull the latest Ubuntu container from Docker:\napptainer pull ubuntu.sif docker://ubuntu\nWe already saw some of these commands:\n\napptainer shell ubuntu.sif launches the container and opens an interactive shell inside it\napptainer exec ubuntu.sif &lt;command&gt; launches the container and runs a command inside it\napptainer run ubuntu.sif launches the container and executes the default runscript\n\nApptainer matches users between the container and the host. For example, if you run a container that needs to be root, you also need to be root outside the container.\n\n\napptainer exec ubuntu.sif ls /\napptainer exec ubuntu.sif ls /; whoami\napptainer exec ubuntu.sif bash -c \"ls /; whoami\"   # probably a safer way\napptainer exec ubuntu.sif cat /etc/os-release\n\n\n\nWe’ve already done this! If there is no default script, Apptainer will give you the shell to type in your commands.\n\n\n\nWe’ve already done this!\n$ apptainer shell ubuntu.sif\nApptainer&gt; whoami   # same username as in the host system\nApptainer&gt; groups   # tries to match groups as on the host system\nAt startup Apptainer simply copied the relevant user and group lines from the host system to files /etc/passwd and /etc/group inside the container. Why do this? The container must ensure that you cannot modify anything on the host system that you should not have permission to, i.e. you are restricted to the same user permissions within the container as you are on the host system.",
    "crumbs": [
      "More on running containers"
    ]
  },
  {
    "objectID": "apptainer1/03-run.html#different-ways-to-run-a-container",
    "href": "apptainer1/03-run.html#different-ways-to-run-a-container",
    "title": "More on running containers",
    "section": "",
    "text": "As a reminder, we continue working in our ~/tmp directory inside an interactive job with the Apptainer module loaded:\ncd ~/tmp\nmodule load apptainer\nsalloc --cpus-per-task=1 --time=3:00:0 --mem-per-cpu=3600   # only from the login node\nIf you have not done so already, let’s pull the latest Ubuntu container from Docker:\napptainer pull ubuntu.sif docker://ubuntu\nWe already saw some of these commands:\n\napptainer shell ubuntu.sif launches the container and opens an interactive shell inside it\napptainer exec ubuntu.sif &lt;command&gt; launches the container and runs a command inside it\napptainer run ubuntu.sif launches the container and executes the default runscript\n\nApptainer matches users between the container and the host. For example, if you run a container that needs to be root, you also need to be root outside the container.\n\n\napptainer exec ubuntu.sif ls /\napptainer exec ubuntu.sif ls /; whoami\napptainer exec ubuntu.sif bash -c \"ls /; whoami\"   # probably a safer way\napptainer exec ubuntu.sif cat /etc/os-release\n\n\n\nWe’ve already done this! If there is no default script, Apptainer will give you the shell to type in your commands.\n\n\n\nWe’ve already done this!\n$ apptainer shell ubuntu.sif\nApptainer&gt; whoami   # same username as in the host system\nApptainer&gt; groups   # tries to match groups as on the host system\nAt startup Apptainer simply copied the relevant user and group lines from the host system to files /etc/passwd and /etc/group inside the container. Why do this? The container must ensure that you cannot modify anything on the host system that you should not have permission to, i.e. you are restricted to the same user permissions within the container as you are on the host system.",
    "crumbs": [
      "More on running containers"
    ]
  },
  {
    "objectID": "apptainer1/03-run.html#mounting-external-directories-and-copying-environment",
    "href": "apptainer1/03-run.html#mounting-external-directories-and-copying-environment",
    "title": "More on running containers",
    "section": "Mounting external directories and copying environment",
    "text": "Mounting external directories and copying environment\nBy default, Apptainer containers are read-only, so you cannot write into its directories. However, from inside the container you can organize read-write access to your directories on the host filesystem. The command\napptainer shell -B /home,/project,/scratch ubuntu.sif\nwill bind-mount /home,/project,/scratch inside the container so that these directories can be accessed for both read and write, subject to your account’s permissions, and then will run a shell. Inside the container:\npwd     # most likely your working directory\necho $USER\nls /home\nls /scratch\nls /project\nIf you prefer, you can pass the same bind information via an environment variable APPTAINER_BIND. In fact, by default your APPTAINER_BIND is set to /project,/scratch, so these two directories (along with /home/$USER) will be mounted every time. If you want to stop mounting /project and /scratch, unset the variable:\nunset APPTAINER_BIND\napptainer shell ubuntu.sif\nYou can mount host directories to specific paths inside the container, e.g.\napptainer shell -B /project/def-sponsor00/$USER:/myproject,/home/$USER/scratch:/myscratch ubuntu.sif\nApptainer&gt; ls /myproject\nApptainer&gt; ls /myscratch\nNote that by default Apptainer typically mounts some of the host’s directories (think /home/$USER). The flag -C will hide the host’s filesystems and environment variables, but then you need to explicitly bind-mount the needed paths (to store results), e.g.\napptainer shell -C -B /scratch ubuntu.sif   # from the host see only /scratch\nApptainer&gt; ls /home/$USER     # still there, but does not contain host's files and directories\n\n\nYou can disable specific mounts, e.g. the following will start the container without mounting your home directory, but it’ll mount the current directory:\napptainer shell --no-mount home ubuntu.sif\nAlternatively, you can disable mounting /home with the --no-home flag, which is equivalent to --no-mount home. And you can disable multiple mounts with something like --no-mount tmp,sys,dev.\nIn general, without -C, Apptainer inherits all environment variables and default bind-mounted filesystems. You can add the -e flag to remove only the host’s environment variables from your container but keep the default bind-mounted filesystems, to start in a cleaner environment:\napptainer shell ubuntu.sif\nApptainer&gt; echo $USER $PYTHONPATH         # defined from the host\nApptainer&gt; ls /home/user01                # shows my $HOME content on the host\n\napptainer shell -C ubuntu.sif\nApptainer&gt; echo $USER $PYTHONPATH         # not defined\nApptainer&gt; ls /home/user01                # nothing\n\napptainer shell -e ubuntu.sif\nApptainer&gt; echo $USER $PYTHONPATH         # not defined\nApptainer&gt; ls /home/user01                # shows my $HOME content on the host\nOn the other hand, you can pass variables to your container by prefixing their names:\n$ APPTAINERENV_HI=\"hello\" APPTAINERENV_NAME=\"alex\" apptainer shell ubuntu.sif\nApptainer&gt; echo $HI\nhello\nApptainer&gt; echo $NAME\nalex\nFinally, we already mentioned APPTAINER_BIND: you don’t have to pass the same bind (-B) flags every time – instead you can put them into a variable (that can be stored in your ~/.bashrc file):\nexport APPTAINER_BIND=\"/home,/project/def-sponsor00/${USER}:/project,/scratch/${USER}:/scratch\"\napptainer shell ubuntu.sif\nYou can have more granular control (e.g. specifying read only) with the --mount flag – for details see the official Bind Paths and Mounts documentation.\n\n\n\n\n\n\nImportantKey points\n\n\n\n\nYour current directory and home directory are usually available by default in a container.\nYou have the same username and permissions in a container as on the host system.\nUse -B to mount host’s directories inside the container.\nUse -C to hide both host’s filesystems and environment variables, perhaps while mounting only few specific directories.\nUse -e to hide only the host’s environment variables.",
    "crumbs": [
      "More on running containers"
    ]
  },
  {
    "objectID": "bash-questions.html",
    "href": "bash-questions.html",
    "title": "Answering your Bash questions",
    "section": "",
    "text": "December 19th, 2024, 10:00am–noon Pacific Time\nYou can find this page at   https://folio.vastcloud.org/bash-questions\nAbstract: We host introductory bash sessions several times a year. In this workshop, we aim to cover topics that are not typically included in our curriculum. We asked for your input on which topics should be prioritized, and we really appreciate all your answers! The responses we got mostly fell into two categories: topics that are already covered in our bash, introductory HPC, and beginner’s Python courses, or more specialized topics. Within this two-hour workshop, we will try to balance these suggestions while showing tools that will be useful to a wide range of people who use our HPC clusters or command line independently on their own computers. We will cover, among other things, aliases, scripts, functions, make, tmux, fuzzy finder, bat, color setup for your shell, zsh plugins, and autojump. We will provide a training cluster and guest accounts to try all these tools and will explain how to install them on production clusters."
  },
  {
    "objectID": "bash-questions.html#aliases-and-scripts-external-links",
    "href": "bash-questions.html#aliases-and-scripts-external-links",
    "title": "Answering your Bash questions",
    "section": "Aliases and scripts (external links)",
    "text": "Aliases and scripts (external links)\n\nhttps://mint.westdri.ca/bash/intro_aliases\nhttps://mint.westdri.ca/bash/molecules/intro_script"
  },
  {
    "objectID": "bash-questions.html#functions",
    "href": "bash-questions.html#functions",
    "title": "Answering your Bash questions",
    "section": "Functions",
    "text": "Functions\nFunctions are similar to scripts, but there are some differences. A bash script is an executable file sitting at a given path. A bash function is defined in your shell environment. Therefore, when running a script, you need to prepend its path to its name, whereas a function – once defined in your environment – can be called by its name without a need for a path. Both scripts and functions can take command-line arguments.\nA convenient place to put all your function definitions is ~/.bashrc file which is run every time you start a new shell (local or remote).\nLike in any programming language, in bash a function is a block of code that you can access by its name. The syntax is:\nfunctionName() {\n  command 1\n  command 2\n  ...\n}\nInside functions you can access its arguments with variables $1 $2 … $# $@ – exactly the same as in scripts. Functions are very convenient because you can define them once inside your ~/.bashrc file and then forget about them.\nHere is our first function:\ngreetings() {\n  echo hello\n}\nLet’s write a function combine() that takes all the files we pass to it, moves them into a randomly-named directory and prints that directory to the screen:\ncombine() {\n  if [ $# -eq 0 ]; then\n    echo \"No arguments specified. Usage: combine file1 [file2 ...]\"\n    return 1        # return a non-zero error code\n  fi\n  dir=$RANDOM$RANDOM\n  mkdir $dir\n  mv $@ $dir\n  echo look in the directory $dir\n}\n\n\n\n\n\n\nCautionExercise Swap file names\n\n\n\n\n\nWrite a function to swap two file names. Add a check that both files exist, before renaming them.\n\n\n\n\n\n\n\n\n\nCautionExercise archive()\n\n\n\n\n\nWrite a function archive() to replace directories with their gzipped archives.\n$ mkdir -p chapter{1..5} notes\n$ ls -F\nchapter1/  chapter2/  chapter3/  chapter4/  chapter5/  notes/\n$ archive chapter* notes/\n$ ls\nchapter1.tar.gz  chapter2.tar.gz  notes.tar.gz\n\n\n\nI will leave it to you to write the reverse function unarchive() that replaces a gzipped tarball with a directory.\n\n\n\n\n\n\nCautionExercise countfiles()\n\n\n\n\n\nWrite a function countfiles() to count files in all directories passed to it as arguments (need to loop through all arguments). At the beginning add the check:\nif [ $# -eq 0 ]; then\n    echo \"No arguments given. Usage: countfiles dir1 dir2 ...\"\n    return 1\nfi\n\n\n\n\n\n\nYou can also use functions to expand existing bash commands. Here is a function I wrote recently that adds some functionality to git-annex &lt;verb&gt; command:\nfunction ga() {\n    case $1 in\n    init|status|add|sync|drop|dropunused|unannex|unused|find|whereis|info|copy|move|get)\n        git-annex $@\n        ;;\n    st)\n        git-annex status\n        ;;\n    locate)\n        git-annex whereis --json \"${@:2}\" | jq '.file + \" \" + (.whereis|map(.description)|join(\",\"))' \\\n        | sed -e 's|\\\"||g'\n        ;;\n    here)\n        git-annex find \"${@:2}\"\n        ;;\n    *)\n        echo command not found ...\n        ;;\n    esac\n}"
  },
  {
    "objectID": "bash-questions.html#make",
    "href": "bash-questions.html#make",
    "title": "Answering your Bash questions",
    "section": "Make",
    "text": "Make\nOriginally, make command (first released in 1975) was created for automating compilations. Consider a large software project with hundreds of dependencies. When you compile it, each source file is converted into an object file, and then all of them are linked together to the libraries to form a final executable(s) or a final library.\nDay-to-day, you typically work on a small section of the program, e.g. debug a single function, with much of the rest of the program unchanged. When recompiling, it would be a waste of time to recompile all hundreds of source files every time you want to compile/run the code. You need to recompile just a single source file and then update the final executable.\nA makefile is a build manager to automate this process, i.e. to figure out what is up-to-date and what is not, and only run the commands that are necessary to rebuild the final target. A makefile is essentially a tree of dependencies stored in a text file along with the commands to create these dependencies. It ensures that if some of the source files have been updated, we only run the steps that are necessary to create the target with those new source files.\nMakefiles can be used for any project (not just compilation) with multiple steps producing intermediate results, when some of these steps are compute-heavy. Let’s look at an example! We will store the following text in the file text.md:\n## Part 1\n\nIn this part we cover:\n\n- bash aliases\n- bash scripts\n- bash functions\n- make\n\n\\newpage\n\n## Part 2\n\nIn this part we cover:\n\n- tmux (also in the context of process control)\n- fuzzy finder\n- bat\n- color setup for your shell\n- zsh plugins\n- autojump\nThis is our workflow:\npandoc text.md -t beamer -o text.pdf\n\nwget https://wgpages.netlify.app/img/dolphin.png\nmagick dolphin.png dolphin.pdf\n\nwget https://wgpages.netlify.app/img/penguin.png\nmagick penguin.png penguin.pdf\n\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\n/bin/rm -f dolphin.* penguin.* text.pdf\ncurl -F \"file=@slides.pdf\" https://temp.sh/upload && echo\nFirst version of Makefile automates creating of slides.pdf:\nslides.pdf: text.pdf dolphin.pdf penguin.pdf\n    gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\ntext.pdf: text.md\n    pandoc text.md -t beamer -o text.pdf\ndolphin.pdf: dolphin.png\n    magick dolphin.png dolphin.pdf\npenguin.pdf: penguin.png\n    magick penguin.png penguin.pdf\ndolphin.png:\n    wget https://wgpages.netlify.app/img/dolphin.png\npenguin.png:\n    wget https://wgpages.netlify.app/img/penguin.png\nRunning make will create the target slides.pdf – how many command will it run? That depends on how many intermediate files you have, and their timestamps.\nTest 1: let’s modify text.md, e.g. add a line there. The makefile will figure out what needs to be done to update slides.pdf. How many command will it run?\nTest 2: let’s remove dolphin.png. How many commands will make run?\nTest 3: let’s remove both PNG files. How many commands will make run?\nNow, add three special targets at the end:\nclean:\n    /bin/rm -f dolphin.* penguin.* text.pdf\ncleanall:\n    make clean\n    /bin/rm -f slides.pdf\nupload: slides.pdf\n    curl -F \"file=@slides.pdf\" https://temp.sh/upload && echo\nNext, we can make use of make’s builtin variables:\n\n$@ is the “target of this rule”\n$ˆ is “all prerequisites of this rule”\n$&lt; is “the first prerequisite of this rule”\n$? is “all out-of-date prerequisites of this rule”\n\n&lt; gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\n---\n&gt; gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=$@ $^\n&lt; pandoc text.md -t beamer -o text.pdf\n---\n&gt; pandoc $^ -t beamer -o $@\nThe next simplification makes use of make wildcards to specify patterns:\n&lt; dolphin.pdf: dolphin.png\n&lt;   magick dolphin.png dolphin.pdf\n&lt; penguin.pdf: penguin.png\n&lt;   magick penguin.png penguin.pdf\n---\n&gt; %.pdf: %.png\n&gt;   magick $^ $@"
  },
  {
    "objectID": "bash-questions.html#process-control-with-terminal-ui",
    "href": "bash-questions.html#process-control-with-terminal-ui",
    "title": "Answering your Bash questions",
    "section": "Process control with terminal UI",
    "text": "Process control with terminal UI\nLinux features commands to pause/resume processes, send them into background, and bring them back into foreground. We think that a better alternative to these controls is tmux, a terminal multiplexer that can create persistent virtual terminal sessions inside which you can run background processes. Let’s demo this!\nHere is one exception with fg that I started using recently:\nfunction e() {\n    if pgrep -x \"emacs\" $UU &gt; /dev/null\n    then\n        fg %emacs\n    else\n        emacs -nw $@\n    fi\n}\ncoupled with the following inside my ~/.emacs file:\n(global-set-key (kbd \"\\C-xe\") 'suspend-frame)"
  },
  {
    "objectID": "bash-questions.html#pointers-on-regular-expressions-external-link",
    "href": "bash-questions.html#pointers-on-regular-expressions-external-link",
    "title": "Answering your Bash questions",
    "section": "Pointers on regular expressions (external link)",
    "text": "Pointers on regular expressions (external link)\n\nhttps://mint.westdri.ca/bash/intro_regexp (not teaching them here!)"
  },
  {
    "objectID": "bash-questions.html#fuzzy-finder-fzf",
    "href": "bash-questions.html#fuzzy-finder-fzf",
    "title": "Answering your Bash questions",
    "section": "Fuzzy finder fzf",
    "text": "Fuzzy finder fzf\nfzf – stands for “fuzzy finder” – is an interactive filter for files or for standard input. You can type with errors and omitted characters and still get the correct results.\nIf we call it as is, fzf, it will traverse the file system under the current directory to get the list of files. Type something and then:\n\nuse up/down arrows or pageup/pagedown to navigate the list\nhit return to return the selected line\nhit backspace to modify your search\nhit TAB multiple times to select multiple lines\nhit Shift-TAB to deselect\nhit escape to exit search\n\nIt is a very quick way to find a file interactively.\nYou can control fzf appearance / real estate:\nfzf                # by default take the entire screen\nfzf --height 10%   # start below the cursor with the 10% height\nfzf --tmux         # this flag is not available on clusters\nThere are many other options for formatting fzf’s output – you can find them in the documentation, so I won’t cover them here.\nHere is a useful function to find a file and open it in nano:\nfunction nn() {\n    nano $(fzf)\n}\nHere is a more sophisticated version of this function, with a couple of improvements:\n\nwon’t open nano if you have an empty result (Esc or Ctrl-C)\ncan handle spaces in file names\n\nfunction nn() {\n    fzf --bind 'enter:become(nano {+})'\n}\n\n--bind binds Enter key to running nano\n\nA more interesting use of fzf is to process standard output of other commands:\nhistory | fzf\n\nfind ~/projects/def-sponsor00/shared/ -type f | fzf\nfind ~/projects/def-sponsor00/shared/ -type f | fzf --preview 'more {}'\nfind ~/projects/def-sponsor00/shared/ -type f | fzf --preview 'head -5 {}'\n\ngit show $(git rev-list --all) | fzf   # search through file changes in all previous commits\n\ncat ~/Documents/notes/*.md | wc -l     # 91k lines of text\ncat ~/Documents/notes/*.md | fzf       # just the content (no file names)\ngrep . ~/Documents/notes/*.md | fzf    # if you want to see the file names as well\nYou can act on the results of search:\nfunction playLocalMusic() {\n    dir=${HOME}/Music/Music/Media.localized/Music\n    player=/Applications/Tiny*Player.app/Contents/MacOS/Tiny*Player\n    find $dir -type d | fzf | sed 's| |\\\\ |g' | xargs $player\n}\nYou can use selected fields in the fzf’s output:\nfunction kk() {\n    kill -9 `/bin/ps ux | fzf | awk '{print $2}'`\n}\n\nAuto-completion\n\nIn bash, you can complete commands by pressing  once or twice. If you want to replace that with fzf, i.e. if you want to use fzf inline inside your commands, you can enable fzf completion that will replace a completion trigger (** by default) with the fzf’s output:\n\n\n\n\n\n\nNote\n\n\n\nfzf in CVMFS does not seem to have been compiled with completion support. To enable it on the training cluster, you will need to install fzf into your own directory, which is fortunately very easy:\ngit clone --depth 1 https://github.com/junegunn/fzf.git fzf\n./fzf/install   # answer \"yes\" to enable fuzzy auto-completion\nexport PATH=${HOME}/fzf/bin:${PATH}        # add fzf to PATH\nsource ${HOME}/fzf/shell/completion.bash   # enable completion support\n\n\nbat **&lt;TAB&gt;\nbat &lt;dir&gt;/**&lt;TAB&gt;\nIt is most useful when you run it pointing to a large subdirectory, e.g.\nnano ~/Documents/**&lt;TAB&gt;\ncd ~/training/**&lt;TAB&gt;\nIt is context-aware:\nkill -9 **&lt;TAB&gt;\nssh **&lt;TAB&gt;\nunset **&lt;TAB&gt;\nunalias **&lt;TAB&gt;"
  },
  {
    "objectID": "bash-questions.html#other-tools-external-links",
    "href": "bash-questions.html#other-tools-external-links",
    "title": "Answering your Bash questions",
    "section": "Other tools (external links)",
    "text": "Other tools (external links)\n\nhttps://mint.westdri.ca/bash/intro_modern describes eza, bat, ripgrep, fd, autojump\nhttps://mint.westdri.ca/bash/intro_zsh describes useful plugins, auto-suggestions, syntax highlighting"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Courses in Research Computing",
    "section": "",
    "text": "The Research Computing Group at Simon Fraser University offers workshops, courses, and webinars in high-performance research computing for researchers at Canadian post-secondary institutions. We work in collaboration with other universities from British Columbia, Alberta, Saskatchewan, and Manitoba under the umbrella of the Digital Research Alliance of Canada.\n\nUpcoming training events\nResearch computing courses\n\nIf you want to get informed about upcoming courses and webinars, please join our mailing list below. We will only email you about training events."
  },
  {
    "objectID": "git-rcg-website.html",
    "href": "git-rcg-website.html",
    "title": "Using Git to contribute to the RCG website",
    "section": "",
    "text": "You can find this page at   https://folio.vastcloud.org/git-rcg"
  },
  {
    "objectID": "git-rcg-website.html#links",
    "href": "git-rcg-website.html#links",
    "title": "Using Git to contribute to the RCG website",
    "section": "Links",
    "text": "Links\n\nRCG website https://www.rcg.sfu.ca\n\nmain repo https://github.sfu.ca/its/rcg-website\nmy fork https://github.sfu.ca/alexeir/rcg-website.git\n\nMarie’s excellent Git course\nSome reading on the GitHub’s pull requests workflow"
  },
  {
    "objectID": "git-rcg-website.html#local-git-primer",
    "href": "git-rcg-website.html#local-git-primer",
    "title": "Using Git to contribute to the RCG website",
    "section": "Local Git primer",
    "text": "Local Git primer\nMake sure you have Git installed on the computer where you want to work, whether it’s your own machine or a remote system.\n\nSet up your local Git variables\n\ngit config --global user.name \"&lt;Your Name&gt;\"\ngit config --global user.email \"&lt;your@email.ca&gt;\"\ngit config --global core.editor \"nano\"\ngit config --global core.autocrlf input   # on macOS, Linux, or WSL\ngit config --global core.autocrlf true    # on Windows\ngit config --global alias.st 'status'\ngit config --global alias.one \"log --graph --date-order --date=short --pretty=format:'%C(cyan)%h %C(yellow)%ar %C(auto)%s%+b %C(green)%ae'\"\ngit config --list\n\nCreate a local repository\nLearn the git add + git commit workflow\nIgnoring files\nExploring history\nBranches\n\ngit branch            # list branches\ngit branch -v         # give a little bit more info on the branches\ngit branch -vv        # show remote branches as well\ngit branch test       # create a new branch called \"test\"\ngit switch test       # switch to that branch\ngit checkout test     # same\ngit switch -c dev     # create a new branch called \"dev\" and switch to it\ngit checkout -b dev   # same\n&gt;&gt;&gt; make some local commits in the branch\ngit checkout main && git merge dev && git branch -d dev   # merge dev into main and delete dev\n\nCreate a local conflict with two branches and then resolve it by hand"
  },
  {
    "objectID": "git-rcg-website.html#initial-setup-on-httpsgithub.sfu.ca",
    "href": "git-rcg-website.html#initial-setup-on-httpsgithub.sfu.ca",
    "title": "Using Git to contribute to the RCG website",
    "section": "Initial setup on https://github.sfu.ca",
    "text": "Initial setup on https://github.sfu.ca\n\nSet up your access token:\n\nIn a private browser window log in to SFU’s GitHub https://github.sfu.ca via SFU authentication\nAccount Settings | Developer settings | Personal access tokens | Generate new token\nCheck “repo”, “workflow” boxes\nCopy the token and save it in your password manager, and use it with your SFU’s GitHub username (which is the same as the your SFU Computing ID)"
  },
  {
    "objectID": "git-rcg-website.html#workflow-via-prs-no-direct-access",
    "href": "git-rcg-website.html#workflow-via-prs-no-direct-access",
    "title": "Using Git to contribute to the RCG website",
    "section": "Workflow via PRs (no direct access)",
    "text": "Workflow via PRs (no direct access)\n\nFork the website repository on GitHub\nClone your fork to your computer\nTo keep up with the main repository, pull the changes from it through your Git upstream config\nEdit the code, commit locally\nPush your changes to a branch in the forked repository – ideally a separate branch for each PR (but could be the main branch)\nCreate a PR to the website repository from your forked repository\n\nCreate your own fork of https://github.sfu.ca/its/rcg-website. Let’s assume the fork is called https://github.sfu.ca/username/rcg-website.git. You will have write access to your fork, but not to the main repository.\nClone your fork to your computer:\n&gt;&gt;&gt; make sure you are not inside any Git repo\ngit clone https://github.sfu.ca/username/rcg-website.git\ncd rcg-website\nhugo serve   # assuming you have Hugo installed locally https://gohugo.io/installation\nAdd the website repository as an upstream:\n\ngit remote -v    # shows only origin = your fork\ngit remote add upstream https://github.sfu.ca/its/rcg-website\ngit remote -v    # now shows both origin and upstream\nWhen you are ready to edit the website, collect the latest changes and start editing the code. Eventually, you can create a PR to the RCG website either from your main branch, or from a specially created branch. Let’s consider these two options.\n\nWorking from the main branch\nThis is not a universally approved practice: it can break things for complex edits, and your fork’s main branch can diverge from the upstream if your PR is not approved right away and in its entirety, e.g. because of compatibility issues or other conflicts.\ngit fetch upstream    # collect the latest changes from the upstream\n&gt;&gt;&gt; do some work and create local commits\ngit push              # upload to origin\n\nOpen https://github.sfu.ca/its/rcg-website in your web browser\nPull requests | New pull request | compare across forks\nCompare   base repository: its/rcg-website base:main   to   username/rcg-website compare:main\nCreate pull request\nDescribe your changes\n\n\n\nWorking from a branch (good practice in general)\ngit fetch upstream     # collect the latest changes from the upstream\ngit checkout -b idea   # create a new branch `idea` and switch to it\n&gt;&gt;&gt; do some work and create local commits in `idea` branch\ngit push origin idea\n\nOpen https://github.sfu.ca/its/rcg-website in your web browser\nPull requests | New pull request | compare across forks\nCompare   base repository: its/rcg-website base:main   to   username/rcg-website compare:idea\nCreate pull request\nDescribe your changes\n\nBack on your computer, at some point you want to merge idea into main. Rather than doing it locally, a good practice is to wait to your PR to be approved and then pull into your fork:\ngit checkout main\ngit pull upstream main\n&gt;&gt;&gt; make sure your suggested edits have been merged\ngit branch -d idea   # delete your local branch\n\nOpen https://github.sfu.ca/username/rcg-website/branches and delete idea branch\n\n\n\nWorking directly on GitHub for small edits\nOpen https://github.sfu.ca/its/rcg-website in your web browser and start editing a file. Since most likely you don’t have direct write access, GitHub will automatically clone the repo and start a new branch with your change.\n\nPropose change | Create a pull request\nDescribe your changes"
  },
  {
    "objectID": "plotly.html#links",
    "href": "plotly.html#links",
    "title": "Plot.ly for scientific visualization",
    "section": "Links",
    "text": "Links\n\nOnline gallery with code examples by category\nPlotly Express tutorial, also plotly.express\nPlotly Community Forum\nKeyword index\nGetting started guide\nPlotly vs matplotlib (with video)\nDisplaying Figures (including offscreen into a file)\nSaving static images (PNG, PDF, etc)\nPlotly and IPython / Jupyter notebook with additional plotting examples"
  },
  {
    "objectID": "plotly.html#installation",
    "href": "plotly.html#installation",
    "title": "Plot.ly for scientific visualization",
    "section": "Installation",
    "text": "Installation\n\nYou will need Python 3, along with some Python package manager\nUse your favourite installer:\n\n$ pip install plotly\n$ uv pip install plotly\n$ conda install -c conda-forge plotly\n\nOther recommended libraries to install for today’s session: jupyter, numpy, pandas, networkx, scikit-image, kaleido"
  },
  {
    "objectID": "plotly.html#displaying-plotly-figures",
    "href": "plotly.html#displaying-plotly-figures",
    "title": "Plot.ly for scientific visualization",
    "section": "Displaying Plotly figures",
    "text": "Displaying Plotly figures\n\n\n\nWith Plotly, you can: 1. work inside a Python shell, 2. save your script into a *.py file and then run it, or 3. run code inside a Jupyter Notebook (start a notebook with jupyter notebook or even better jupyter lab).\nPlotly supports a number of renderers, and it will attempt to choose an appropriate renderer automatically (in my experience, not very successfully). You can examine the selected default renderer with:\nimport plotly.io as pio\npio.renderers   # show default and available renderers\nYou can overwrite the default by setting it manually inside your session or inside your code, e.g.\npio.renderers.default = 'browser'    # open each plot in a new browser tab\npio.renderers.default = 'notebook'   # plot inside a Jupyter notebook\nIf you want to have this setting persistent across sessions (and not set it manually or in the code), you can create a file ~/.plotly_startup.py with the following:\ntry:\n    import plotly.io as pio\n    pio.renderers.default = \"browser\"\nexcept ImportError:\n    pass\nand set export PYTHONSTARTUP=~/.plotly_startup.py in your ~/.bashrc file.\n\n\n\n\n\nLet’s create a simple line plot:\nimport plotly.graph_objs as go\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\ny = sin(1/x)\nline = go.Scatter(x=x, y=y, mode='lines+markers', name='sin(1/x)')\nfig = go.Figure([line])\nfig.show()   # should open in your browser\n\nfig.write_image(\"/Users/razoumov/tmp/lines.png\", scale=2)   # static, supports svg, png, jpg/jpeg, webp, pdf\nfig.write_html(\"/Users/razoumov/tmp/lines.html\")            # interactive\nfig.write_json(\"/Users/razoumov/tmp/2007.json\")             # for further editing\nIn general, use fig.show() when working outside of a Jupyter Notebook, or when you want to save your plot to a file. If you want to display plots inline inside a Jupyter notebook, set pio.renderers.default = 'notebook' and use the command\ngo.Figure([line])\nthat should display plots right inside the notebook, without a need for fig.show().\nYou can find more details at https://plotly.com/python/renderers ."
  },
  {
    "objectID": "plotly.html#plotly-express-data-exploration-library",
    "href": "plotly.html#plotly-express-data-exploration-library",
    "title": "Plot.ly for scientific visualization",
    "section": "Plotly Express (data exploration) library",
    "text": "Plotly Express (data exploration) library\nNormally, in this workshop I would teach plotly.graph_objs (Graph Objects) which is the standard module in Plotly.py – you saw its example in the previous section.\nPlotly Express is a higher-level interface to Plotly.py that sits on top of Graph Objects and provides 30+ functions for creating different types of figures in a single function call. It works with NumPy arrays, Xarrays, Pandas dataframes, basic Python iterables, etc.\n\nHere is one way to create a line plot from above in Plotly Express, using just NumPy arrays:\nimport plotly.express as px\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\ny = sin(1/x)\nfig = px.line(x=x, y=y, markers=True)\nfig.show()\nYou can also use feed a dataframe into the plotting function:\nimport plotly.express as px\nfrom numpy import linspace, sin\nimport pandas as pd\nx = linspace(0.01,1,100)\ndf = pd.DataFrame({'col1': x, 'col2': sin(1/x)})\nfig = px.line(df, x='col1', y='col2', markers=True)\nfig.show()\nOr you can feed a dictionary:\nimport plotly.express as px\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\nd = {'key1': x, 'key2': sin(1/x)}\nfig = px.line(d, x='key1', y='key2', markers=True)\nfig.show()\nTo see Plotly Express really shine, we should play with a slightly larger dataset containing several variables. The module px.data comes with several datasets included. Let’s take a look at the Gapminder data that contains one row per country per year.\nimport plotly.express as px\ndf = px.data.gapminder().query(\"year==2007\")\n\npx.line(df, x=\"gdpPercap\", y=\"lifeExp\", markers=True)   # this should be familiar\n# 1. replace df with df.sort_values(by='gdpPercap')\n# 2. add log_x=True\n# 3. change line to scatter, remove markers=True\n# 4. don't actually need to sort now, with no markers\n# 5. add hover_name=\"country\"\n# 6. add size=\"pop\"\n# 7. add size_max=60\n# 8. add color=\"continent\" - can now turn continents off/on\n\npx.strip(df, x=\"lifeExp\")   # single-axis scatter plot\n# 1. add hover_name=\"country\"\n# 2. add color=\"continent\"\n# 3. change strip to histogram\n# 4. can turn continents off/on in the legend\n# 5. add marginal=\"rug\" to show countries in a rug plot\n# 6. add y=\"pop\" to switch from country count to population along the vertical axis\n# 7. add facet_col=\"continent\" to break continents into facet columns\n\npx.bar(df, color=\"lifeExp\", x=\"pop\", y=\"continent\", hover_name=\"country\")\n\npx.sunburst(df, color=\"lifeExp\", values=\"pop\", path=[\"continent\", \"country\"],\n            hover_name=\"country\", height=800)\n\npx.treemap(df, color=\"lifeExp\", values=\"pop\", path=[\"continent\", \"country\"],\n           hover_name=\"country\", height=500)\n\npx.choropleth(df, color=\"lifeExp\", locations=\"iso_alpha\", hover_name=\"country\", height=580)\n\n\n\n\n\nHere is an ternary plot example with Montreal elections data (58 electoral districts, 3 candidates):\ndf = px.data.election()\npx.scatter_ternary(df, a=\"Joly\", b=\"Coderre\", c=\"Bergeron\", color=\"winner\",\n                   size=\"total\", hover_name=\"district\", size_max=15,\n                   color_discrete_map={\"Joly\": \"blue\", \"Bergeron\": \"green\", \"Coderre\": \"red\"})"
  },
  {
    "objectID": "plotly.html#plotting-via-graph-objects",
    "href": "plotly.html#plotting-via-graph-objects",
    "title": "Plot.ly for scientific visualization",
    "section": "Plotting via Graph Objects",
    "text": "Plotting via Graph Objects\nWhile Plotly Express is excellent for quick data exploration, it has some limitations: it supports fewer plot types and does not allow combining different plot types directly in a single figure. Plotly Graph Objects, on the other hand, is a lower-level library that offers greater functionality and customization with layouts. In this section, we’ll explore Graph Objects, starting with 2D plots.\nGraph Objects supports many plot types:\nimport plotly.graph_objs as go\ndir(go)\n['AngularAxis', 'Annotation', 'Annotations', 'Bar', 'Barpolar', 'Box', 'Candlestick', 'Carpet', 'Choropleth', 'Choroplethmap', 'Choroplethmapbox', 'ColorBar', 'Cone', 'Contour', 'Contourcarpet', 'Contours', 'Data', 'Densitymap', 'Densitymapbox', 'ErrorX', 'ErrorY', 'ErrorZ', 'Figure', 'FigureWidget', 'Font', 'Frame', 'Frames', 'Funnel', 'Funnelarea', 'Heatmap', 'Histogram', 'Histogram2d', 'Histogram2dContour', 'Histogram2dcontour', 'Icicle', 'Image', 'Indicator', 'Isosurface', 'Layout', 'Legend', 'Line', 'Margin', 'Marker', 'Mesh3d', 'Ohlc', 'Parcats', 'Parcoords', 'Pie', 'RadialAxis', 'Sankey', 'Scatter', 'Scatter3d', 'Scattercarpet', 'Scattergeo', 'Scattergl', 'Scattermap', 'Scattermapbox', 'Scatterpolar', 'Scatterpolargl', 'Scattersmith', 'Scatterternary', 'Scene', 'Splom', 'Stream', 'Streamtube', 'Sunburst', 'Surface', 'Table', 'Trace', 'Treemap', 'Violin', 'Volume', 'Waterfall', 'XAxis', 'XBins', 'YAxis', 'YBins', 'ZAxis', 'bar', 'barpolar', 'box', 'candlestick', 'carpet', 'choropleth', 'choroplethmap', 'choroplethmapbox', 'cone', 'contour', 'contourcarpet', 'densitymap', 'densitymapbox', 'funnel', 'funnelarea', 'heatmap', 'histogram', 'histogram2d', 'histogram2dcontour', 'icicle', 'image', 'indicator', 'isosurface', 'layout', 'mesh3d', 'ohlc', 'parcats', 'parcoords', 'pie', 'sankey', 'scatter', 'scatter3d', 'scattercarpet', 'scattergeo', 'scattergl', 'scattermap', 'scattermapbox', 'scatterpolar', 'scatterpolargl', 'scattersmith', 'scatterternary', 'splom', 'streamtube', 'sunburst', 'surface', 'table', 'treemap', 'violin', 'volume', 'waterfall']\n\nScatter plots\nWe already saw an example of a Scatter plot with Graph Objects:\nimport plotly.graph_objs as go\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\ny = sin(1/x)\nline = go.Scatter(x=x, y=y, mode='lines+markers', name='sin(1/x)')\ngo.Figure([line])\nLet’s print the dataset line:\ntype(line)\nprint(line)\nIt is a plotly object which is actually a Python dictionary, with all elements clearly identified (plot type, x numpy array, y numpy array, line type, legend line name). So, go.Scatter simply creates a dictionary with the corresponding type element. This variable/dataset line completely describes our plot!* Then we create a list of such objects and pass it to the plotting routine.\n\n\n\n\n\n\nCautionExercise 1\n\n\n\n\n\nPass a list of two objects to the plotting routine with data = [line1,line2]. Let the second dataset line2 contain another mathematical function. The idea is to have multiple objects in the plot.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHovering over each data point will reveal their coordinates. Use the toolbar at the top. Double-clicking on the plot will reset it.\n\n\n\n\n\n\n\n\nCautionExercise 2\n\n\n\n\n\nAdd a bunch of dots to the plot with dots = go.Scatter(x=[.2,.4,.6,.8], y=[2,1.5,2,1.2]). What is default scatter mode?\n\n\n\n\n\n\n\n\n\nCautionExercise 3\n\n\n\n\n\nChange line colour and width by adding the dictionary line=dict(color=('rgb(10,205,24)'),width=4) to dots.\n\n\n\n\n\n\n\n\n\nCautionExercise 3b\n\n\n\n\n\nCreate a scatter plot of 300 random filled blue circles inside a unit square. Their random opacity must anti-correlate with their size (bigger circles should be more transparent) – see the plot below.\n\n\n\n\n\nBar plots\nLet’s try a Bar plot, constructing data directly in one line from the dictionary:\nimport plotly.graph_objs as go\nbar = go.Bar(x=['Vancouver', 'Calgary', 'Toronto', 'Montreal', 'Halifax'],\n               y=[2463431, 1392609, 5928040, 4098927, 403131])\nfig = go.Figure(data=[bar])\nfig.show()\nLet’s plot inner city population vs. greater metro area for each city:\nimport plotly.graph_objs as go\ncities = ['Vancouver', 'Calgary', 'Toronto', 'Montreal', 'Halifax']\nproper = [662_248, 1_306_784, 2_794_356, 1_762_949, 439_819]\nmetro = [3_108_926, 1_688_000, 6_491_000, 4_615_154, 530_167]\nbar1 = go.Bar(x=cities, y=proper, name='inner city')\nbar2 = go.Bar(x=cities, y=metro, name='greater area')\nfig = go.Figure(data=[bar1,bar2])\nfig.show()\nLet’s now do a stacked plot, with outer city population on top of inner city population:\noutside = [m-p for p,m in zip(proper,metro)]   # need to subtract\nbar1 = go.Bar(x=cities, y=proper, name='inner city')\nbar2 = go.Bar(x=cities, y=outside, name='outer city')\nfig = go.Figure(data=[bar1,bar2], layout=go.Layout(barmode='stack'))   # new element!\nfig.show()\nWhat else can we modify in the layout?\nhelp(go.Layout)\nThere are many attributes!\n\n\nHeatmaps\n\ngo.Area() for plotting wind rose charts\ngo.Box() for basic box plots\n\nLet’s plot a heatmap of monthly temperatures at the South Pole:\nimport plotly.graph_objs as go\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Year']\nrecordHigh = [-14.4,-20.6,-26.7,-27.8,-25.1,-28.8,-33.9,-32.8,-29.3,-25.1,-18.9,-12.3,-12.3]\naverageHigh = [-26.0,-37.9,-49.6,-53.0,-53.6,-54.5,-55.2,-54.9,-54.4,-48.4,-36.2,-26.3,-45.8]\ndailyMean = [-28.4,-40.9,-53.7,-57.8,-58.0,-58.9,-59.8,-59.7,-59.1,-51.6,-38.2,-28.0,-49.5]\naverageLow = [-29.6,-43.1,-56.8,-60.9,-61.5,-62.8,-63.4,-63.2,-61.7,-54.3,-40.1,-29.1,-52.2]\nrecordLow = [-41.1,-58.9,-71.1,-75.0,-78.3,-82.8,-80.6,-79.3,-79.4,-72.0,-55.0,-41.1,-82.8]\ndata = [recordHigh, averageHigh, dailyMean, averageLow, recordLow]\nyticks = ['record high', 'aver.high', 'daily mean', 'aver.low', 'record low']\nheatmap = go.Heatmap(z=data, x=months, y=yticks)\nfig = go.Figure([heatmap])\nfig.show()\n\n\n\n\n\n\nCautionExercise 3c\n\n\n\n\n\nTry a few different colourmaps, e.g. ‘Viridis’, ‘Jet’, ‘Rainbow’. What colourmaps are available?\n\n\n\n\n\nContour maps\n\n\n\n\n\n\nCautionExercise 4\n\n\n\n\n\nPretend that our heatmap is defined over a 2D domain and plot the same temperature data as a contour map. Remove the Year data (last column) and use go.Contour to plot the 2D contour map.\n\n\n\n\n\nDownload data\nOpen a terminal window inside Jupyter (New | Terminal) and run these commands:\nwget https://tinyurl.com/pvzip -O paraview.zip\nunzip paraview.zip\nmv data/*.{csv,nc} .\n\n\nGeographical scatterplot\nGo back to your Python Jupyter Notebook. Now let’s do a scatterplot on top of a geographical map:\nimport plotly.graph_objs as go\nimport pandas as pd\nfrom math import log10\ndf = pd.read_csv('cities.csv')   # lists name,pop,lat,lon for 254 Canadian cities and towns\ndf['text'] = df['name'] + '&lt;br&gt;Population ' + \\\n             (df['pop']/1e6).astype(str) +' million' # add new column for mouse-over\n\nlargest, smallest = df['pop'].max(), df['pop'].min()\ndef normalize(x):\n    return log10(x/smallest)/log10(largest/smallest)   # x scaled into [0,1]\n\ndf['logsize'] = round(df['pop'].apply(normalize)*255)   # new column\ncities = go.Scattergeo(\n    lon = df['lon'], lat = df['lat'], text = df['text'],\n    marker = dict(\n        size = df['pop']/5000,\n        color = df['logsize'],\n        colorscale = 'Viridis',\n        showscale = True,   # show the colourbar\n        line = dict(width=0.5, color='rgb(40,40,40)'),\n        sizemode = 'area'))\nlayout = go.Layout(title = 'City populations',\n                       showlegend = False,   # do not show legend for first plot\n                       geo = dict(\n                           scope = 'north america',\n                           resolution = 50,   # base layer resolution of km/mm\n                           lonaxis = dict(range=[-130,-55]), lataxis = dict(range=[44,70]), # plot range\n                           showland = True, landcolor = 'rgb(217,217,217)',\n                           showrivers = True, rivercolor = 'rgb(153,204,255)',\n                           showlakes = True, lakecolor = 'rgb(153,204,255)',\n                           subunitwidth = 1, subunitcolor = \"rgb(255,255,255)\",   # province border\n                           countrywidth = 2, countrycolor = \"rgb(255,255,255)\"))  # country border\nfig = go.Figure(data=[cities], layout=layout)\nfig.show()\n\n\n\n\n\n\nCautionExercise 5\n\n\n\n\n\nModify the code to display only 10 largest cities.\n\n\n\nRecall how we combined several scatter plots in one figure before. You can combine several plots on top of a single map – let’s combine scattergeo + choropleth:\nimport plotly.graph_objs as go\nimport pandas as pd\ndf = pd.read_csv('cities.csv')\ndf['text'] = df['name'] + '&lt;br&gt;Population ' + \\\n             (df['pop']/1e6).astype(str)+' million' # add new column for mouse-over\ncities = go.Scattergeo(lon = df['lon'],\n                       lat = df['lat'],\n                       text = df['text'],\n                       marker = dict(\n                           size = df['pop']/5000,\n                           color = \"lightblue\",\n                           line = dict(width=0.5, color='rgb(40,40,40)'),\n                           sizemode = 'area'))\ngdp = pd.read_csv('gdp.csv')   # read name, gdp, code for 222 countries\nc1 = [0,\"rgb(5, 10, 172)\"]     # define colourbar from top (0) to bottom (1)\nc2, c3 = [0.35,\"rgb(40, 60, 190)\"], [0.5,\"rgb(70, 100, 245)\"]\nc4, c5 = [0.6,\"rgb(90, 120, 245)\"], [0.7,\"rgb(106, 137, 247)\"]\nc6 = [1,\"rgb(220, 220, 220)\"]\ncountries = go.Choropleth(locations = gdp['CODE'],\n                          z = gdp['GDP (BILLIONS)'],\n                          text = gdp['COUNTRY'],\n                          colorscale = [c1,c2,c3,c4,c5,c6],\n                          autocolorscale = False,\n                          reversescale = True,\n                          marker = dict(line = dict(color='rgb(180,180,180)',width = 0.5)),\n                          zmin = 0,\n                          colorbar = dict(tickprefix = '$',title = 'GDP&lt;br&gt;Billions US$'))\nlayout = go.Layout(hovermode = \"x\", showlegend = False)  # do not show legend for first plot\nfig = go.Figure(data=[cities,countries], layout=layout)\nfig.show()\n\n\n3D Topographic elevation\nLet’s plot some tabulated topographic elevation data:\nimport plotly.graph_objs as go\nimport pandas as pd\ntable = pd.read_csv('mt_bruno_elevation.csv')\nsurface = go.Surface(z=table.values)  # use 2D numpy array format\nlayout = go.Layout(title='Mt Bruno Elevation',\n                   width=1200, height=1200,    # image size\n                   margin=dict(l=65, r=10, b=65, t=90))   # margins around the plot\nfig = go.Figure([surface], layout=layout)\nfig.show()\n\n\n\n\n\n\nCautionExercise 6\n\n\n\n\n\nPlot a 2D function f(x,y) = (1−y) sin(πx) + y sin^2(2πx), where x,y ∈ [0,1] on a 100^2 grid.\n\n\n\n\n\nElevated 2D functions\nLet’s define a different colourmap by adding colorscale='Viridis' inside go.Surface(). This is our current code:\nimport plotly.graph_objs as go\nfrom numpy import *\nn = 100   # plot resolution\nx = linspace(0,1,n)\ny = linspace(0,1,n)\nY, X = meshgrid(x, y)   # meshgrid() returns two 2D arrays storing x/y respectively at each mesh point\nF = (1-Y)*sin(pi*X) + Y*(sin(2*pi*X))**2   # array operation\ndata = go.Surface(z=F, colorscale='Viridis')\nlayout = go.Layout(width=1000, height=1000, scene=go.layout.Scene(zaxis=go.layout.scene.ZAxis(range=[-1,2])));\nfig = go.Figure(data=[data], layout=layout)\nfig.show()\n\n\nLighting control\nLet’s change the default light in the room by adding lighting=dict(ambient=0.1) inside go.Surface(). Now our plot is much darker!\n\nambient controls the light in the room (default = 0.8)\nroughness controls amount of light scattered (default = 0.5)\ndiffuse controls the reflection angle width (default = 0.8)\nfresnel controls light washout (default = 0.2)\nspecular induces bright spots (default = 0.05)\n\nLet’s try lighting=dict(ambient=0.1,specular=0.3) – now we have lots of specular light!\n\n\n3D parametric plots\nIn plotly documentation you can find quite a lot of different 3D plot types. Here is something visually very different, but it still uses go.Surface(x,y,z):\nimport plotly.graph_objs as go\nfrom numpy import pi, sin, cos, mgrid\ndphi, dtheta = pi/250, pi/250    # 0.72 degrees\n[phi, theta] = mgrid[0:pi+dphi*1.5:dphi, 0:2*pi+dtheta*1.5:dtheta]\n        # define two 2D grids: both phi and theta are (252,502) numpy arrays\nr = sin(4*phi)**3 + cos(2*phi)**3 + sin(6*theta)**2 + cos(6*theta)**4\nx = r*sin(phi)*cos(theta)   # x is also (252,502)\ny = r*cos(phi)              # y is also (252,502)\nz = r*sin(phi)*sin(theta)   # z is also (252,502)\nsurface = go.Surface(x=x, y=y, z=z, colorscale='Viridis')\nlayout = go.Layout(title='parametric plot')\nfig = go.Figure(data=[surface], layout=layout)\nfig.show()\n\n\n3D scatter plots\nLet’s take a look at a 3D scatter plot using the country index data from http://www.prosperity.com for 142 countries:\nimport plotly.graph_objs as go\nimport pandas as pd\ndf = pd.read_csv('legatum2015.csv')\nspheres = go.Scatter3d(x=df.economy,\n                       y=df.entrepreneurshipOpportunity,\n                       z=df.governance,\n                       text=df.country,\n                       mode='markers',\n                       marker=dict(\n                           sizemode = 'diameter',\n                           sizeref = 0.3,   # max(safetySecurity+5.5) / 32\n                           size = df.safetySecurity+5.5,\n                           color = df.education,\n                           colorscale = 'Viridis',\n                           colorbar = dict(title = 'Education'),\n                           line = dict(color='rgb(140, 140, 170)')))   # sphere edge\nlayout = go.Layout(height=900, width=900,\n                   title='Each sphere is a country sized by safetySecurity',\n                   scene = dict(xaxis=dict(title='economy'),\n                                yaxis=dict(title='entrepreneurshipOpportunity'),\n                                zaxis=dict(title='governance')))\nfig = go.Figure(data=[spheres], layout=layout)\nfig.show()\n\n\n3D graphs\nWe can plot 3D graphs. Consider a Dorogovtsev-Goltsev-Mendes graph: in each subsequent generation, every edge from the previous generation yields a new node, and the new graph can be made by connecting together three previous-generation graphs.\nimport plotly.graph_objs as go\nimport networkx as nx\nimport sys\ngeneration = 5\nH = nx.dorogovtsev_goltsev_mendes_graph(generation)\nprint(H.number_of_nodes(), 'nodes and', H.number_of_edges(), 'edges')\n# Force Atlas 2 graph layout from https://github.com/tpoisot/nxfa2.git\npos = nx.spectral_layout(H,scale=1,dim=3)\nXn = [pos[i][0] for i in pos]   # x-coordinates of all nodes\nYn = [pos[i][1] for i in pos]   # y-coordinates of all nodes\nZn = [pos[i][2] for i in pos]   # z-coordinates of all nodes\nXe, Ye, Ze = [], [], []\nfor edge in H.edges():\n    Xe += [pos[edge[0]][0], pos[edge[1]][0], None]   # x-coordinates of all edge ends\n    Ye += [pos[edge[0]][1], pos[edge[1]][1], None]   # y-coordinates of all edge ends\n    Ze += [pos[edge[0]][2], pos[edge[1]][2], None]   # z-coordinates of all edge ends\n\ndegree = [deg[1] for deg in H.degree()]   # list of degrees of all nodes\nlabels = [str(i) for i in range(H.number_of_nodes())]\nedges = go.Scatter3d(x=Xe, y=Ye, z=Ze,\n                     mode='lines',\n                     marker=dict(size=12,line=dict(color='rgba(217, 217, 217, 0.14)',width=0.5)),\n                     hoverinfo='none')\nnodes = go.Scatter3d(x=Xn, y=Yn, z=Zn,\n                     mode='markers',\n                     marker=dict(sizemode = 'area',\n                                 sizeref = 0.01, size=degree,\n                                 color=degree, colorscale='Viridis',\n                                 line=dict(color='rgb(50,50,50)', width=0.5)),\n                     text=labels, hoverinfo='text')\n\naxis = dict(showline=False, zeroline=False, showgrid=False, showticklabels=False, title='')\nlayout = go.Layout(\n    title = str(generation) + \"-generation Dorogovtsev-Goltsev-Mendes graph\",\n    width=1000, height=1000,\n    showlegend=False,\n    scene=dict(xaxis=go.layout.scene.XAxis(axis),\n               yaxis=go.layout.scene.YAxis(axis),\n               zaxis=go.layout.scene.ZAxis(axis)),\n    margin=go.layout.Margin(t=100))\nfig = go.Figure(data=[edges,nodes], layout=layout)\nfig.show()\n\n\n3D functions\nLet’s create an isosurface of a decoCube function at f=0.03. Isosurfaces are returned as a list of polygons, and for plotting polygons in plotly we need to use plotly.figure_factory.create_trisurf() which replaces plotly.graph_objs.Figure():\nfrom plotly import figure_factory as FF\nfrom numpy import mgrid\nfrom skimage import measure\nX,Y,Z = mgrid[-1.2:1.2:30j, -1.2:1.2:30j, -1.2:1.2:30j] # three 30^3 grids, each side [-1.2,1.2] in 30 steps\nF = ((X*X+Y*Y-0.64)**2 + (Z*Z-1)**2) * \\\n    ((Y*Y+Z*Z-0.64)**2 + (X*X-1)**2) * \\\n    ((Z*Z+X*X-0.64)**2 + (Y*Y-1)**2)\nvertices, triangles, normals, values = measure.marching_cubes(F, 0.03)  # create an isosurface\nx,y,z = zip(*vertices)   # zip(*...) is opposite of zip(...): unzips a list of tuples\nfig = FF.create_trisurf(x=x, y=y, z=z, plot_edges=False,\n                        simplices=triangles, title=\"Isosurface\", height=1200, width=1200)\nfig.show()\nTry switching plot_edges=False to plot_edges=True – you’ll see individual polygons!"
  },
  {
    "objectID": "plotly.html#dash-library-for-making-interactive-web-applications",
    "href": "plotly.html#dash-library-for-making-interactive-web-applications",
    "title": "Plot.ly for scientific visualization",
    "section": "Dash library for making interactive web applications",
    "text": "Dash library for making interactive web applications\nPlotly Dash library is a framework for making interactive data applications.\n\nDash Python User Guide https://dash.plotly.com\nhttps://dash.gallery/Portal has ~100 app examples\n\n\ncan create a dropdown to select data to plot\ncan enter a value into a box to select or interpolate data to plot\nselection in one plot shows in the other plot\nmix and match these into a single web app\ncan create different tabs inside the app, with the render switching between them\ncan make entire website with user guides, plots, code examples, etc."
  },
  {
    "objectID": "ray.html",
    "href": "ray.html",
    "title": "Part 2: distributed computing with Ray",
    "section": "",
    "text": "There is a number of high-level open-source parallel frameworks for Python that are quite popular in data science and beyond:\nHere we’ll focus on Ray, a unified framework for scaling AI and general Python workflows. Since this is not a machine learning workshop, we will not touch Ray’s AI capabilities, but will focus on its core distributed runtime and data libraries. We will learn several different approaches to parallelizing purely numerical (and therefore CPU-bound) workflows, both with and without reduction. We will also look at I/O-bound workflows."
  },
  {
    "objectID": "ray.html#initializing-ray",
    "href": "ray.html#initializing-ray",
    "title": "Part 2: distributed computing with Ray",
    "section": "Initializing Ray",
    "text": "Initializing Ray\nimport ray\nray.init()   # start a Ray cluster  and connect to it\n             # no longer necessary, will run by default when you first use it\nHowever, ray.init() is very useful for passing options at initialization. For example, Ray is quite verbose when you do things in it. To turn off this logging output to the terminal, you can do\nray.init(configure_logging=False)   # hide Ray's copious logging output\nYou can run ray.init() only once. If you want to re-run it, first you need to run ray.shutdown(). Alternatively, you can pass the argument ignore_reinit_error=True to the call.\nYou can specify the number of cores for Ray to use, and you can combine multiple options, e.g.\nray.init(num_cpus=4, configure_logging=False)\nBy default Ray will use all available CPU cores, e.g. on my laptop ray.init() will start 8 ray::IDLE processes (workers), and you can monitor these in a separate shell with htop --filter \"ray::IDLE\" command (you may want to hide threads – typically thrown in green – with Shift+H).\n\nDiscussion\nHow many “ray::IDLE” processes do you see, and why? Recall that you can use srun --jobid=&lt;jobID&gt; --pty bash to open an interactive shell process inside your currently running job, and run htop --filter \"ray::IDLE\" there.\n\n\n\n\n\n\n\nCautionQuestion 11\n\n\n\n\n\nHow would you pass the actual number of processor cores to the Ray cluster? Consider three options:\n1. Using a Slurm environment variable. How would you pass it to ray.init()?\n2. Launching a single-node Ray cluster as described in our Ray documentation.\n3. Not passing anything at all, in which case Ray will try – unsuccessfully – to grab all cores."
  },
  {
    "objectID": "ray.html#ray-tasks",
    "href": "ray.html#ray-tasks",
    "title": "Part 2: distributed computing with Ray",
    "section": "Ray tasks",
    "text": "Ray tasks\nIn Ray you can execute any Python function asynchronously on separate workers. Such functions are called Ray remote functions, and their asynchronous invocations are called Ray tasks:\nimport ray\nray.init(configure_logging=False)   # optional\n\n@ray.remote             # declare that we want to run this function remotely\ndef square(x):\n    return x * x\n\nr = square.remote(10)   # launch/schedule a remote calculation (non-blocking call)\ntype(r)                 # ray._raylet.ObjectRef (object reference)\nray.get(r)              # retrieve the result (=100) (blocking call)\nThe calculation may happen any time between &lt;function&gt;.remote() and ray.get() calls, i.e. it does not necessarily start when you launch it. This is called lazy execution: the operation is often executed when you try to access the result.\na = square.remote(10)   # launch a remote calculation\nray.cancel(a)           # cancel it\nray.get(a)              # either error or 100, depending on whether the calculation\n                        # has finished before cancellation\nYou can launch several Ray tasks at once, to be executed in parallel in the background, and you can retrieve their results either individually or through the list:\nr = [square.remote(i) for i in range(4)]   # launch four parallel tasks (non-blocking call)\nprint([ray.get(r[i]) for i in range(4)])   # retrieve the results (multiple blocking calls)\nprint(ray.get(r))          # more compact way to do the same (single blocking call)\n\nTask output\nConsider a code in which each Ray task sleeps for 10 seconds, prints a message and returns its task ID:\nimport ray\nfrom time import sleep, time\nray.init(num_cpus=4, configure_logging=False)\n\n@ray.remote\ndef nap():\n    sleep(10)\n    print(\"almost done\")\n    return ray.get_runtime_context().get_task_id()\nLet’s run it with timing on:\nstart = time()\nr = [nap.remote() for i in range(4)]\nray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nI get 10.013 seconds since I have enough cores to run all of them in parallel. However, most likely, I see printout (“almost done”) from only one process and a message “repeated 3x across cluster”. To enable print messages from all tasks, you need set the bash shell environment variable export RAY_DEDUP_LOGS=0.\nNotice that Ray task IDs are not integers but 48-character hexadecimal numbers.\n\n\nDistributed progress bars\nfrom ray.experimental.tqdm_ray import tqdm\n\n@ray.remote\ndef busy(name):\n    if \"2\" in name: sleep(2)\n    for x in tqdm(range(100), desc=name):\n        sleep(0.1)\n\n[busy.remote(\"task 1\"), busy.remote(\"task 2\")]\nA side effect of tqdm() is that these tasks start running immediately.\n\n\n\n\n\n\nCautionQuestion 11b\n\n\n\n\n\nImplement the same for 10 tasks using a for loop.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRay’s tqdm() is somewhat buggy, so you might want to restart Python and your Ray cluster, if you don’t want to see artifacts from previous bars sometimes popping up in your session.\n\n\n\n\n\n\n\n\n\n\n\nParallelizing the slow series with Ray tasks\nLet’s perform our slow series calculation as a Ray task. This is our original serial implementation, now with a Ray remote function running on one of the workers:\nfrom time import time\nimport ray\nray.init(num_cpus=4, configure_logging=False)\n\n@ray.remote\ndef slow(n: int):\n    total = 0\n    for i in range(1,n+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    return total\n\nstart = time()\nr = slow.remote(100_000_000)\ntotal = ray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nWe should get the same timing as before (~6-7 seconds). You can call ray.get() on the previously computed result again without having to redo the calculation:\nstart = time()\ntmp = ray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))   # 0.001 seconds\nprint(tmp)\nLet’s speed up this calculation with parallel Ray tasks! Instead of doing the full sum over range(1,n+1), let’s calculate a partial sum on each task:\nfrom time import time\nimport psutil, ray\nray.init(num_cpus=4, configure_logging=False)\n\n@ray.remote\ndef slow(interval):\n    total = 0\n    for i in range(interval[0],interval[1]+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    return total\nThis would be a serial calculation:\nn = 100_000_000\nstart = time()\nr = slow.remote((1, n))   # takes in one argument\ntotal = ray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nTo launch it in parallel, we need to subdivide the interval:\nncores = psutil.cpu_count(logical=False)   # good option on a standalone computer\nncores = 4                                 # on a cluster\n\nsize = n//ncores   # size of each batch\nintervals = [(i*size+1,(i+1)*size) for i in range(ncores)]\nif n &gt; intervals[-1][1]: intervals[-1] = (intervals[-1][0], n)   # add the remainder (if any)\n\nstart = time()\nr = [slow.remote(intervals[i]) for i in range(ncores)]\ntotal = sum(ray.get(r))   # compute total sum\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nOn 8 cores I get the average runtime of 1.282 seconds – not too bad, considering that some of the cores are low-efficiency (slower) cores.\n\n\n\n\n\n\nCautionQuestion 12\n\n\n\n\n\nIncrease n to 1_000_000_000 and run htop --filter \"ray::IDLE in a separate shell to monitor CPU usage of individual processes.\n\n\n\n\n\nRunning Numba-compiled functions as Ray tasks\nWe ended Part 1 with a Numba-compiled version of the slow series code that works almost as well as a Julia/Chapel code. As you just saw, Ray itself can distribute the calculation, speeding up the code with parallel execution, but individual tasks still run native Python code that is slow.\nWouldn’t it be great if we could use Ray to distribute execution of Numba-compiled functions to workers? It turns out we can, but we have to be careful with syntax. We would need to define remote compiled functions, but neither Ray, nor Numba let you combine their decorators (@ray.remote and @numba.jit, respectively) for a single function. You can do this in two steps:\nimport ray\nfrom numba import jit\n\nray.init(num_cpus=4, configure_logging=False)\n\n@jit(nopython=True)\ndef square(x):\n    return x*x\n\n@ray.remote\ndef runCompiled():\n    return square(5)\n\nr = runCompiled.remote()\nray.get(r)\nHere we “jit” the function on the main process and send it to workers for execution. Alternatively, you can “jit” on workers:\nimport ray\nfrom numba import jit\n\nray.init(num_cpus=4, configure_logging=False)\n\ndef square(x):\n    return x*x\n\n@ray.remote\ndef runCompiled():\n    compiledSquare = jit(square)\n    return compiledSquare(5)\n\nr = runCompiled.remote()\nray.get(r)\nIn my tests with more CPU-intensive functions, both versions produce equivalent runtimes.\n\n\n\n\n\n\nCautionQuestion Combining Numba and Ray remotes for the slow series (big one!)\n\n\n\n\n\nWrite a slow series solver with Numba-compiled functions executed as Ray tasks. 1. Numba-compiled combined(k) that returns either 1/x or 0.\n2. Numba-compiled slow(interval) for partial sums.\n3. Ray-enabled remote function runCompiled(interval) to launch partial sums on workers.\n4. Very important step: you must do small runCompiled() runs on workers to copy code over to them – no need to time these runs. Without this “pre-compilation” step you will not get fast execution on workers on the bigger problem.\n5. The rest of the code will look familiar:\nstart = time()\nr = [runCompiled.remote(intervals[i]) for i in range(ncores)]\ntotal = sum(ray.get(r))   # compute total sum\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAveraged (over three runs) times:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n0.439\n0.235\n0.130\n0.098\n\n\n\nUsing a combination of Numba and Ray tasks on 8 cores, we accelerated the calculation by ~68X.\n\n\nGetting partial results from Ray tasks\nConsider the following code:\nfrom time import sleep\nimport ray, random\n\n@ray.remote\ndef longSleep():\n    duration = random.randint(1,100) # random integer from [1,100]\n    sleep(duration)   # sleep for this number of seconds\n    return f\"I slept for {duration} seconds\"\n\nrefs = [longSleep.remote() for x in range(1,21)]   # start 20 tasks\nIf we now call ray.get(refs), that would block until all of these remote tasks finish. If we want to, let’s say, one of them to finish and then continue on the main task, we can do:\nready_refs, remaining_refs = ray.wait(refs, num_returns=1, timeout=None) # wait for one of them to finish\nprint(ready_refs, remaining_refs)   # print the IDs of the finished task, and the other 19 IDs\nray.get(ready_refs)                 # get finished results\nLet’s wait for the first 5 answers:\nready_refs, remaining_refs = ray.wait(refs, num_returns=5, timeout=None) # wait for 5 of them to finish\nray.get(ready_refs)                 # get finished results\nfor i in remaining_refs:\n    ray.cancel(i)   # cancel the unfinished ones\n\n\nMultiple returns from Ray tasks\nSimilar to normal Python functions, Ray tasks can return tuples:\nimport ray\n\n@ray.remote\ndef threeNumbers():\n    return 10, 20, 30\n\nr = threeNumbers.remote()\nprint(ray.get(r)[0])        # get the result (tuple) and show its first element\nAlternatively, you can pipe each number to a separate object ref (tell the decorator!):\n@ray.remote(num_returns=3)\ndef threeNumbers():\n    return 10, 20, 30\n\nr1, r2, r3 = threeNumbers.remote()\nray.get(r1)   # get the first result only\nYou can also create a remote generator that will return only one number at a time, to reduce memory usage:\n@ray.remote(num_returns=3)\ndef threeNumbers():\n    for i in range(3):   # should return 10,20,30\n        yield (i+1)*10\n\nr1, r2, r3 = threeNumbers.remote()\nray.get(r1)   # get the first number only\n\n\nLinking remote tasks\nIn addition to values, object refs can also be passed to remote functions. Define two functions:\nimport ray\n\n@ray.remote\ndef one():\n    return 1\n\n@ray.remote\ndef increment(value):\n    return value + 1\nIn by-now familiar syntax:\nr1 = one.remote()                    # create the first Ray task\nr2 = increment.remote(ray.get(r1))   # pass its result as an argument to another Ray task\nprint(ray.get(r2))\nYou can also shorten this syntax:\nr1 = one.remote()           # create the first Ray task\nr2 = increment.remote(r1)   # pass its object ref as an argument to another Ray task\nprint(ray.get(r2))\nAs the second task depends on the output of the first task, Ray will not execute the second task until the first task has finished.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPersistent storage on Ray workers\n\n\nYou might have noticed that Ray functions (remote tasks) are stateless, i.e. they can run on any processor that happens to be more idle at the time, and they do not store any data on that processor in between the function calls.\nIf you are trying to parallelize a tightly-coupled problem, you might want to store arrays on the workers permanently, and then repeatedly call quick functions to do some computations on these arrays, without copying the arrays back and forth at each step.\nTo do this in Ray, we can use Ray actors (https://docs.ray.io/en/latest/ray-core/actors.html). A Ray actor is essentially a stateful (bound to a processor) worker that is created via a Python class instance with its own persistent variables and methods, and it stays permanently on that worker until we destroy this instance.\n\n\n\n\n\nimport ray\nimport numpy as np\n\nray.init()\n\n@ray.remote\nclass ArrayStorage:         # define an actor (ArrayStorage class) with a persistent array\n    def __init__(self):\n        self.array = None   # persistent array variable\n    def store_array(self, arr):\n        self.array = arr    # store an array in the actor's state\n    def get_array(self):\n        return self.array   # retrieve the stored array\n\nstorage_actor = ArrayStorage.remote()   # create an instance of the actor\n\narr = np.array([1, 2, 3, 4, 5])\nray.get(storage_actor.store_array.remote(arr))   # store an array\n\nr = ray.get(storage_actor.get_array.remote())    # retrieve the stored array\nprint(r)  # Output: [1 2 3 4 5]\n\n\n\n\nTo scale this to multiple workers, we can do the same with an array of workers:\nworkers = [ArrayStorage.remote() for i in range(2)]   # create two instances of the actor\n\nr = [workers[i].store_array.remote(np.ones(5)*(i+1)) for i in range(2)]\nprint(ray.get(r))\n\nprint(ray.get(workers[0].get_array.remote()))   # [1. 1. 1. 1. 1.]\nprint(ray.get(workers[1].get_array.remote()))   # [2. 2. 2. 2. 2.]\n\nr = [workers[i].get_array.remote() for i in range(2)]\nprint(ray.get(r))   # both arrays in one go\nIf we want to make sure that these arrays stay on the same workers, we can retrieve and print their IDs and the node IDs by adding these two functions to the actor class:\n...\n    def get_actor_id(self):\n        return self.actor_id\n    def get_node_id(self):\n        return self.node_id   # the node ID where this actor is running\n...\nprint([ray.get(workers[i].get_actor_id.remote()) for i in range(2)])   # actor IDs\nprint([ray.get(workers[i].get_node_id.remote()) for i in range(2)])    # node IDs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can even use NumPy on workers. For example, if we were to implement a linear algebra solver on a worker and wanted to have the solution array stored there permanently, we could do it this way:\nimport numpy as np\nimport ray\n\nray.init(num_cpus=2, configure_logging=False)\n\nn = 500\nh = 1/(n+1)\nb = np.exp(-(100*(np.linspace(0,1,n)-0.45))**2)*h*h\n\n@ray.remote\nclass ArrayStorage:\n    def __init__(self, n):\n        self.b = None   # persistent variable\n        self.u = None   # persistent variable\n        flatIdentity = np.identity(n).reshape([n*n])\n        a = -2.0*flatIdentity\n        a[1:n*n-1] = a[1:n*n-1] + flatIdentity[:n*n-2] + flatIdentity[2:]\n        self.a = a.reshape([n,n])   # persistent variable\n    def store_b(self, arr):\n        self.b = arr                # store an array in the actor's state\n    def get_u(self):\n        return self.u\n    def localSolve(self):\n        self.u = np.linalg.solve(self.a,self.b)\n\nworker = ArrayStorage.remote(n)\nworker.store_b.remote(b)\nworker.localSolve.remote()\nu = ray.get(worker.get_u.remote())\nprint(\"The solution is\", u)\nFinally, you can delete a Ray actor with:\nray.kill(worker)\n\n\n\n\n\n\nNote\n\n\n\nIs it possible to use JIT-compiled functions inside a Ray actor? The answer is “probably yes”: in Numba there is an experimental feature to compile Python classes with @jitclass. Whether you can use it to compile Ray classes is an open question – I’d be very interested to hear your findings on this."
  },
  {
    "objectID": "ray.html#ray-data",
    "href": "ray.html#ray-data",
    "title": "Part 2: distributed computing with Ray",
    "section": "Ray Data",
    "text": "Ray Data\nRay Data is a parallel data processing library for ML workflows. As you will see in this section, Ray Data can be easily used for non-ML workflows. To process large datasets, Ray Data uses streaming/lazy execution, i.e. processing does not happen until you try to access (“consume” in Ray’s language) the result.\n\nThe core object in Ray Data is a dataset which is a distributed data collection. Ray datasets can store general multidimensional array data that are too large to fit into a single machine’s memory. In Ray, these large arrays will be: 1. distributed in memory across a number of Ray tasks and 2. saved to disk once they are no longer in use.\nRay’s dataset operates over a sequence of Ray object references to blocks. Each block contains a disjoint subset of rows, and Ray Data loads and transforms these blocks in parallel. Each row in Ray’s datasets is a dictionary.\n\n\n\n\n\n\nNote\n\n\n\nRecall: a Python dictionary is a collection of key-value pairs.\n\n\n\nCreating datasets\nimport ray\nray.init(num_cpus=4, configure_logging=False)\n\nds = ray.data.range(1000)   # create a dataset from a range\nds                # Dataset(num_rows=1000, schema={id: int64})\nds.count()        # explicitly count the number of rows; might be expensive (to load the dataset into memory)\nds.take(3)        # return first 3 rows as a list of dictionaries; default keys are often 'id' or 'item'\nlen(ds.take())    # default is 20 rows\nds.show(3)        # first 3 rows in a different format (one row per line)\nUntil recently, you could easily displays the number of blocks in a dataset, but now you have to materialize it first, and the number of blocks can change during execution:\nds.materialize().num_blocks()   # will show the number of blocks; might be expensive\nRows can be generated from arbitrary items:\nray.data.from_items([10,20,30]).take()   # [{'item': 10}, {'item': 20}, {'item': 30}]\nRows can have multiple key-value pairs:\n\n\n\n\n\n\nNote\n\n\n\nRecall: each row is a dictionary, and dictionaries can have multiple entries.\n\n\nlistOfDict = [{\"col1\": i, \"col2\": i ** 2} for i in range(1000)] # each dict contains 2 pairs\nds = ray.data.from_items(listOfDict)\nds.show(5)\nA dataset can also be loaded from a file:\n\n\n\n\n\n\nNote\n\n\n\nThis example works on my computer (pyenv activate hpc-env), but not on the training cluster where arrow/19.0.1 was compiled without S3 support.\n\n\ndd = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")   # load a predefined dataset\ndd\ndd.show()   # might pause to read data, default is 20 rows\n\n\nTransforming datasets\n\n\n\n\n\n\nRay datasets become useful once you start processing them. Let’s initialize a simple dataset from a range:\nimport ray\nray.init(num_cpus=4, configure_logging=False)\n\nds = ray.data.range(1000)   # create a dataset\nds.show(5)\nWe will apply a function to each row in this dataset. This function must return a dictionary that will form each row in the new dataset:\nds.map(lambda row: row).show(3)                  # takes a row, returns the same row\n\nds.map(lambda row: {\"key\": row[\"id\"]}).show(3)   # takes a row, returns a similar row;\n                       # `row[\"id\"]` is needed to refer to the value in each\n                       # original row; `key` is a new, arbitrary key name\n\na = ds.map(lambda row: {\"long\": str(row[\"id\"])*3})  # takes a row, returns a dict with 'id' values\n                                                    # converted to strings and repeated 3X\na   # it is a new dataset\na.show(5)\nWith .map() you can also use familiar non-lambda (i.e. non-anonymous) functions:\ndef squares(row):\n    return {'squares': row['id']**2}\n\nds.map(squares).show(5)\nYour function can also add new key-value pairs to the existing rows, instead of returning brand new rows:\nds = ray.data.range(1000)\ndef addSquares(row):\n    row['square'] = row['id']**2   # add a new entry to each row\n    return row                       \n\nb = ds.map(addSquares)\nds.show(5)  # original dataset\nb.show(5)   # contains both the original entries and the squares\nYou might have already noticed by now that all processing in Ray Data is lazy, i.e. it happens when we request results:\nds = ray.data.range(10_000_000)   # define a bigger dataset\nds.map(addSquares)   # no visible calculation, just a request, not consuming results\nds.map(addSquares).show()   # print results =&gt; start calculation\nds.map(addSquares).show()   # previous results were not stored, so this will re-run the calculation\n\nb = ds.map(addSquares)   # this does not start calculation\nb.show()                 # this starts calculation\nEvery time you print or use b, it’ll re-do the calculation. For this reason, you can think of b not as a variable with a value but as a data stream.\nHow can you do the calculation once and store the results for repeated use? You can convert b into a more permanent (not a data stream) object, e.g. a list:\nb = ds.map(addSquares)   # this does not start calculation\nc = b.take()             # create a list, fast (only first 20 elements)\nc = b.take(10_000_000)   # takes a couple of minutes, runs in parallel\nThis is not very efficient … certainly, computing 10,000,000 squares should not take a couple of minutes in parallel! The problem is that we have too many rows, and – similar to Python lists – Ray datasets perform poorly with too many rows. Think about subdividing your large computation into a number of chunks where each chunk comes with its own computational and communication overhead – you want to keep their number small.\nLet’s rewrite this problem:\n\n\n\n\n\n\n\n\n\nn = 10_000_000\nn1 = n // 2\nn2 = n - n1\n\nfrom time import time\nimport numpy as np\nds = ray.data.from_items([np.arange(n1), n1 + np.arange(n2)])\nds.show()   # 2 rows\n\ndef squares(row):\n    return {'squares': row['item']**2}   # compute element-wise square of an array\n\nstart = time()\nb = ds.map(squares).take()\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(b)\nNow the runtime is 0.072 seconds.\n\n\n\n\n\n\nVectorizable dataset transformations\nThe function .map_batches() will process batches (blocks) of rows, vectorizing operations on NumPy arrays inside each batch. We won’t study it here, as in practical terms it does not solve the specific problems in this course any faster. With .map_batches() you are limited in terms of data types that you can use inside a function passed to .map_batches() – in general it expects vectorizable NumPy arrays. If you are not planning to vectorize via map_batches(), use map() instead, and you will still get parallelization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlow series with Ray Data\nLet’s start with a serial implementation:\nfrom time import time\nimport ray\nray.init(configure_logging=False)\n\nintervals = ray.data.from_items([{\"a\": 1, \"b\": 100_000_000}])   # one item\nintervals.show()\n\ndef slow(row):\n    total = 0\n    for i in range(row['a'], row['b']+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    row['sum'] = total             # add key-value pair to the row\n    return row\n\nstart = time()\npartial = intervals.map(slow)      # define the calculation\ntotal = partial.take()[0]['sum']   # request the result =&gt; start the calculation on 1 CPU core\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nI get the average runtime of 6.978 seconds. To parallelize this, you can redefine intervals:\nintervals = ray.data.from_items([\n    {\"a\": 1, \"b\": 50_000_000},\n    {\"a\": 50_000_001, \"b\": 100_000_000},\n])\n\nstart = time()\npartial = intervals.map(slow)     # define the calculation\ntotal = sum([p['sum'] for p in partial.take()])   # request the result =&gt; start the calculation on 2 CPU cores\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))   # 4.322, 4.857, 4.782 and 3.852 3.812 3.823\nprint(total)\nOn 2 cores I get the average time of 3.765 seconds.\n\n\n\n\n\n\nCautionQuestion 14\n\n\n\n\n\nParallelize this for 4 CPU cores. Hint: programmatically form a list of dictionaries, each containing two key-value pairs (a and b) with the sub-intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn my laptop I am getting:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n6.978\n3.765\n1.675\n1.574\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing Ray datasets with Numba-compiled functions\nSo far we processed Ray datasets with non-Numba functions, either lambda (anonymous) or plain Python functions, and we obtained very good parallel scaling with multiple Ray processes. However, these functions are slower than their Numba-compiled versions.\nIs it possible to process Ray datasets with Numba-compiled functions, similar to how earlier we executed Numba-compiled functions on remote workers? The answer is a firm yes, but I will leave it to you to write an implementation for the slow series problem. Even though I have the solution on my laptop, I am not providing it here, as I feel this would be an excellent take-home exercise.\nA couple of considerations: 1. Numba does not work with Python dictionaries, instead providing its own dictionary type which is not compatible with Ray datasets. You can easily sidestep this problem, but you will have to find the solution yourself. 2. Very important: you must do small runs on workers to copy your functions over to them – no need to time these runs. Without this “pre-compilation” step you will not get fast execution on workers on the bigger problem the first time you run it.\n\nWith this Numba-compiled processing, on my laptop I am getting:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n0.447\n0.234\n0.238\n0.137\n\n\n\nI am not quite sure why going 2 → 4 cores does not result in better runtimes, but there could be some inherent overhead in Ray tasks implementation on my laptop that shows up in this small problem – or more likely the efficiency cores enter the calculation at this time?\nWith the same code on the training cluster I get better scaling:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n1.054\n0.594\n0.306\n0.209"
  },
  {
    "objectID": "ray.html#running-ray-workflows-on-a-single-node",
    "href": "ray.html#running-ray-workflows-on-a-single-node",
    "title": "Part 2: distributed computing with Ray",
    "section": "Running Ray workflows on a single node",
    "text": "Running Ray workflows on a single node\n\n\n\n\n\n\n\nCautionQuestion 15\n\n\n\n\n\nLet’s try running the slow series without Numba-compiled functions on the training cluster as a batch job.\n1. Save the entire Python code for the slow series problem into rayPartialMap.py\n2. Modify the code to take  as a command-line argument:\nimport sys\nncores = int(sys.argv[1])\nray.init(num_cpus=ncores, configure_logging=False)\nand test it from the command line inside your interactive job\npython rayPartialMap.py $SLURM_CPUS_PER_TASK\n\nQuit the interactive job.\n\nBack on the login node, write a Slurm job submission script, in which you launch rayPartialMap.py in the last line of the script.\n\nSubmit your job with sbatch to 1, 2, 4, 8 CPU cores, all on the same node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting on the training cluster:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n15.345\n7.890\n4.264\n2.756\n\n\n\nTesting on Cedar (averaged over 2 runs):\n\n\n\n\n\n\n\n\n\n\n\n\nncores\n1\n2\n4\n8\n16\n32\n\n\nwallclock runtime (sec)\n18.263\n9.595\n5.228\n3.048\n2.069\n1.836\n\n\n\nIn our Ray documentation there are instructions for launching a single-node Ray cluster. Strictly speaking, this is not necessary, as on a single machine (node) a call to ray.init() will start a new Ray cluster and will automatically connect to it."
  },
  {
    "objectID": "ray.html#running-ray-workflows-on-multiple-nodes",
    "href": "ray.html#running-ray-workflows-on-multiple-nodes",
    "title": "Part 2: distributed computing with Ray",
    "section": "Running Ray workflows on multiple nodes",
    "text": "Running Ray workflows on multiple nodes\n\nTo run Ray workflows on multiple cluster nodes, you must create a virtual Ray cluster first. You can find details of Ray’s virtual clusters in the official Ray documentation https://docs.ray.io/en/latest/cluster/getting-started.html.\nHere we’ll take a look at the example which I copied and adapted from our documentation at https://docs.alliancecan.ca/wiki/Ray#Multiple_Nodes. I made several changes in this workflow:\n\nmade it interactive,\nnot creating virtual environments in $SLURM_TMPDIR inside the job, but using the already existing one in /project/def-sponsor00/shared/hpc-env,\nremoved GPUs.\n\nLet’s quit our current Slurm job (if any), back on the login node start the following interactive job, and then run the following commands:\nmodule load python/3.12.4 arrow/19.0.1 scipy-stack/2025a netcdf/4.9.2\nsource /project/def-sponsor00/shared/hpc-env/bin/activate\n\nsalloc --nodes 2 --ntasks-per-node=1 --cpus-per-task=2 --mem-per-cpu=3600 --time=0:60:0\n\nexport HEAD_NODE=$(hostname)   # head node's address -- different from the login node!\nexport RAY_PORT=34567          # a port to start Ray on the head node \nNext, we will start a Ray cluster on the head node as a background process:\nray start --head --node-ip-address=$HEAD_NODE --port=$RAY_PORT --num-cpus=$SLURM_CPUS_PER_TASK --block &\nsleep 10   # wait for the prompt; it'll ask to enable usage stats collection\n...\n# Eventually should say \"Ray runtime started.\"\nThen on each node inside our Slurm job, except the head node, we launch the worker nodes of the Ray cluster:\ncat &lt;&lt; EOF &gt; launchRay.sh\n#!/bin/bash\nmodule load python/3.12.4 arrow/19.0.1 scipy-stack/2025a netcdf/4.9.2\nsource /project/def-sponsor00/shared/hpc-env/bin/activate\nif [[ \"\\$SLURM_PROCID\" -eq \"0\" ]]; then   # if MPI rank is 0\n        echo \"Ray head node already started...\"\n        sleep 10\nelse\n        ray start --address \"${HEAD_NODE}:${RAY_PORT}\" --num-cpus=\"${SLURM_CPUS_PER_TASK}\" --block\n        sleep 5\n        echo \"Ray worker started!\"\nfi\nEOF\nchmod u+x launchRay.sh\nsrun launchRay.sh &\nray_cluster_pid=$!   # get its process ID\nNext, we launch a Python script that connects to the Ray cluster, checks the nodes and all available CPUs:\nimport ray\nimport os\nray.init(address=f\"{os.environ['HEAD_NODE']}:{os.environ['RAY_PORT']}\",_node_ip_address=os.environ['HEAD_NODE'])\n...\n# Eventually should say \"Connected to Ray cluster.\"\nprint(\"Nodes in the Ray cluster:\", ray.nodes())   # should see two nodes with 'Alive' status\nprint(ray.available_resources())                  # should see 4 CPUs and 2 nodes\nFinally, from bash, we shut down the Ray worker nodes:\nkill $ray_cluster_pid\nand terminate the job."
  },
  {
    "objectID": "ray.html#distributed-data-processing-on-ray-and-io-bound-workflows",
    "href": "ray.html#distributed-data-processing-on-ray-and-io-bound-workflows",
    "title": "Part 2: distributed computing with Ray",
    "section": "Distributed data processing on Ray and I/O-bound workflows",
    "text": "Distributed data processing on Ray and I/O-bound workflows\n\nSimple distributed dataset example\nWith multiple CPU cores available, run the following Python code line by line, while watching memory usage in a separate window with htop --filter \"ray::IDLE\":\nimport numpy as np\nimport ray\n\nray.init(configure_logging=False, _system_config={ 'automatic_object_spilling_enabled':False })\n# 1. hide Ray's copious logging output\n# 2. start as many ray::IDLE process as the physical number of cores -- we'll use only two of them below\n# 3. disable automatic object spilling to disk\n\nb = ray.data.from_items([(800,800,800), (800,800,800)])   # 800**3 will take noticeable 3.81GB memory\nb.show()\n\ndef initArray(row):\n    nx, ny, nz = row['item']\n    row['array'] = np.zeros((nx,ny,nz))\n    return row\n\nc = b.map(initArray)\nc.show()\nWe’ll see the two arrays being initialized in memory, on two processes, with a couple of GBs of memory consumed per process. Ray writes (“spills”) objects to storage once they are no longer in use, as it tries to minimize the total number of “materialized” (in-memory) blocks. On Linux and MacOS, the temporary spill folder is /tmp/ray, but you can customize its location as described here.\nWith the automatic object spilling to disk disabled (our second flag to ray.init() above), these arrays will stay in memory.\nWith that flag removed, and hence with the usual automatic object spilling to disk, these array blocks will be automatically written to disk, and the memory usage goes back to zero after a fraction of a second. If next you try to access these arrays, e.g.\nd = c.take()\ntype(d[0]['array'])   # numpy.ndarray\nd[0]['array'].shape   # (800, 800, 800)\nthey will be loaded temporarily into memory, which you can monitor with htop --filter \"ray::IDLE\".\n\n\n\n\n\n\n\nPandas on Ray (Modin)\nYou can run many types of I/O workflows on top of Ray. One famous example is “Modin” (previously called Pandas on Ray) which is a drop-in replacement for Pandas on top of Ray. We won’t study here, but it will run all your pandas workflows, and you don’t need to modify your code, except importing the library, e.g.\nimport pandas as pd\ndata = pd.read_csv(\"filename.csv\", &lt;other flags&gt;)\nwill become\nimport modin.pandas as pd\nimport ray\ndata = pd.read_csv(\"filename.csv\", &lt;other flags&gt;)\nModin will run your workflows using Ray tasks on multiple cores, potentially speeding up large workflows. You can find more information at https://github.com/modin-project/modin\nSimilarly, many other Ray Data read functions will read your data in parallel, distributing it to multiple processes if necessary for larger processing:\n&gt;&gt;&gt; import ray\n&gt;&gt;&gt; ray.data.read_\nray.data.read_api                 ray.data.read_databricks_tables(  ray.data.read_mongo(              ray.data.read_sql(\nray.data.read_bigquery(           ray.data.read_datasource(         ray.data.read_numpy(              ray.data.read_text(\nray.data.read_binary_files(       ray.data.read_images(             ray.data.read_parquet(            ray.data.read_tfrecords(\nray.data.read_csv(                ray.data.read_json(               ray.data.read_parquet_bulk(       ray.data.read_webdataset(\n\n\n\n\nProcessing images\n\n\n\n\n\n\nNote\n\n\n\nThis example won’t work on the training cluster, where arrow/14.0.1 was compiled without proper filesystem support. However, I can demo this on my computer.\n\n\nHere is a simple example of processing a directory with images with Ray Data. Suppose we have a \\(2874\\times\n2154\\) image tuscany.avif. Let’s crop 100 random \\(300\\times 300\\) images out of it:\ntmpdir=${RANDOM}${RANDOM}\nmkdir -p ~/$tmpdir && cd ~/$tmpdir\npyenv activate hpc-env\n\nwget https://wgpages.netlify.app/img/tuscany.jpg\nls -l tuscany.jpg\nmkdir -p images\nwidth=$(identify tuscany.jpg | awk '{print $3}' | awk -Fx '{print $1}')\nheight=$(identify tuscany.jpg | awk '{print $3}' | awk -Fx '{print $2}')\n\nfor num in $(seq -w 00 99); do   # crop it into hundred 300x300 random images\n    echo $num\n    x=$(echo \"scale=8; $RANDOM / 32767 * ($width-300)\" | bc)   # $RANDOM goes from 0 to 32767\n    x=$(echo $x | awk '{print int($1+0.5)}')\n    y=$(echo \"scale=8; $RANDOM / 32767 * ($height-300)\" | bc)\n    y=$(echo $y | awk '{print int($1+0.5)}')\n    convert tuscany.jpg -crop 300x300+$x+$y images/small$num.png\ndone\nNow we will load these images into Ray Data. First, let’s set export RAY_DEDUP_LOGS=0, and then do:\nimport ray\nds = ray.data.read_images(\"images/\")\nds   # 100 rows (one image per row) split into ... blocks\nds.take(1)[0]                  # first image\ntype(ds.take(1)[0][\"image\"])   # stored as a numpy array\nds.take(1)[0][\"image\"].shape   # 300x300 and three channels (RGB)\nds.take(1)[0][\"image\"].max()   # 255 =&gt; they are stored as 8-bit images (0..255)\nLet’s print min/max values for all images:\ndef minmax(row):\n    print(row[\"image\"].min(), row[\"image\"].max())\n    return row  # must return a dictionary, otherwise `map` will fail\n\nb = ds.map(minmax)   # on output; we scheduled the calculation, but have not started it yet\nb.materialize()      # force the calculation =&gt; now we see printouts from individual rows\nLet’s compute their negatives:\ndef negate(row):\n    return {'image': 255-row['image']}\n\nnegative = ds.map(negate)\nnegative.write_images(\"output\", column='image')\nIn output/ subdirectory you will find 100 negative images.\n\n\nCoding parallel I/O by hand\nIn Ray Data you can also write your own parallel I/O workflows by hand, defining functions to process rows that will load certain data/file into a specific row, e.g. with something like this:\na = ray.data.from_items([\n    {'file': '/path/to/file1'},\n    {'file': '/path/to/file2'},\n    {'file': '/path/to/file3'}\n])\ndef readFileInParallel(row):\n    &lt;load data from file row['file']&gt;\n    &lt;process these data&gt;\n    row['status'] = 'done'\n    return row\n\nb = a.map(readFileInParallel)\nb.show()\n\n\nA CPU-intensive problem without reduction\nSo far we’ve been working with problems where calculations from individual tasks add to form a single number (sum of a slow series) – this is called reduction. Let’s now look at a problem without reduction, i.e. where results stay distributed.\nLet’s compute a mathematical Julia set defined as a set of points on the complex plane that remain bound under an infinite recursive transformation \\(z_{i+1}=f(z_i)\\). For the recursive function, we will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\n\npick a point \\(z_0\\in\\mathbb{C}\\)\ncompute iterations \\(z_{i+1}=z_i^2+c\\) until \\(|z_i|&gt;4\\) (arbitrary fixed radius; here \\(c\\) is a complex constant)\nstore the iteration number \\(\\xi(z_0)\\) at which \\(z_i\\) reaches the circle \\(|z|=4\\)\nlimit max iterations at 255\n4.1 if \\(\\xi(z_0)\\lt 255\\), then \\(z_0\\) is a stable point\n4.2 the quicker a point diverges, the lower its \\(\\xi(z_0)\\) is\nplot \\(\\xi(z_0)\\) for all \\(z_0\\) in a rectangular region   \\(-1.2&lt;=\\mathfrak{Re}(z_0)&lt;=1.2\\)   and   \\(-1.2&lt;=\\mathfrak{Im}(z_0)&lt;=1.2\\)\n\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\n\n\n\n\n\n\n\nNote\n\n\n\nYou might want to try these values too:\n\\(c = 1.2e^{1.1πi}\\) \\(~\\Rightarrow~\\) original textbook example\n\\(c = -0.4-0.59i\\) and 1.5X zoom-out \\(~\\Rightarrow~\\) denser spirals\n\\(c = 1.34-0.45i\\) and 1.8X zoom-out \\(~\\Rightarrow~\\) beans\n\\(c = 0.34-0.05i\\) and 1.2X zoom-out \\(~\\Rightarrow~\\) connected spiral boots\n\n\nAs you must be accustomed by now, this calculation runs much faster when implemented in a compiled language. I tried Julia (0.676s) and Chapel (0.489s), in both cases running the code on my laptop in serial (one CPU core), both with exactly the same workflow and the same \\(2000^2\\) image size.\nBelow is the serial implementation in Python – let’s save it as juliaSetSerial.py:\nfrom time import time\nimport numpy as np\n\nnx = ny = 2000   # image size\n\ndef pixel(z):\n    c = 0.355 + 0.355j\n    for i in range(1, 256):\n        z = z**2 + c\n        if abs(z) &gt;= 4:\n            return i\n    return 255\n\nprint(\"Computing Julia set ...\")\nstability = np.zeros((nx,ny), dtype=np.int32)\n\nstart = time()\nfor i in range(nx):\n    for k in range(ny):\n        point = 1.2*((2*(i+0.5)/nx-1) + (2*(k+0.5)/ny-1)*1j) # rescale to -1.2:1.2 in the complex plane\n        stability[i,k] = pixel(point)\n\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\n\nimport netCDF4 as nc\nf = nc.Dataset('test.nc', 'w', format='NETCDF4')\nf.createDimension('x', nx)\nf.createDimension('y', ny)\noutput = f.createVariable('stability', 'i4', ('x', 'y'))   # 4-byte integer\noutput[:,:] = stability\nf.close()\nHere we are timing purely the computational part, not saving the image to a NetCDF file. Let’s run it and look at the result in ParaView! When running this on my laptop, it takes 9.220 seconds.\nHow would you parallelize this problem with ray.data? The main points to remember are:\n\nYou need to subdivide your problem into blocks – let’s do this in the ny (vertical) dimension.\nYou have to construct a dataset with each row containing inputs for a single task. These inputs will be the size of each image block and the offset (the starting row number of this block inside the image).\nYou need to write a function computeStability(row) that acts on each input row in the dataset. The result will be a NumPy array stored as a new entry stability in each row.\nTo write the final image to a NetCDF file, you need to merge these arrays into a single \\(2000\\times 2000\\) array, and then write this square array to disk.\n\nThe solution is in the file juliaSetParallel.py on instructor’s laptop. Here are the runtimes for \\(2000\\times 2000\\) (averaged over three runs):\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n9.220\n4.869\n2.846\n2.210"
  },
  {
    "objectID": "ray.html#quickly-on-parallel-python-in-ml-frameworks-marie",
    "href": "ray.html#quickly-on-parallel-python-in-ml-frameworks-marie",
    "title": "Part 2: distributed computing with Ray",
    "section": "Quickly on parallel Python in ML frameworks (Marie)",
    "text": "Quickly on parallel Python in ML frameworks (Marie)\n\n\n\nMachine learning (ML) frameworks in Python usually come with their own parallelization tools, so you do not need to use general parallel libraries that we discussed in this course.\n\nJAX\n\nSingle-Program Multi-Data\nSeveral parallel computing sections in the advanced guide\n\n\n\nPyTorch\n\nPyTorch Distributed (torch.distributed) Overview\nDistributed and Parallel Training Tutorials\nGetting Started with DistributedDataParallel (DDP) module\nUsing multiple GPUs with DataParallel\n\n\n\nTensorFlow\n\nDistributed training with TensorFlow\nMulti-GPU and distributed training"
  },
  {
    "objectID": "annex.html",
    "href": "annex.html",
    "title": "Distributed file storage with git-annex",
    "section": "",
    "text": "November 26th, 2024, 10:00am–11:00am Pacific Time\nYou can find this page at   https://folio.vastcloud.org/annex\nAbstract: git-annex is a file synchronization tool designed to simplify the management of large (typically data-oriented) files under version control. Unlike Git, git-annex does not track file contents but rather facilitates the organization of data across multiple locations, both online and offline, enabling the creation of multiple copies for backup and redundancy, ensuring data safety and organization.     In the past, we have taught webinars on tools built upon git-annex, such as DataLad. In these tools the core functionality is typically provided by git-annex, so we believe it is crucial to understand how to effectively organize data using git-annex itself, without the distraction of additional features.     Personally, I have been utilizing git-annex for several years to manage my extensive collection of archived files across multiple drives stored on a shelf. git-annex provides built-in redundancy, ensuring that each individual repository or drive is aware of the location of all files on other drives, eliminating the need to power them on just to find a file. git-annex also offers online capabilities, allowing file synchronization across multiple filesystems and clusters to help you manage your research data."
  },
  {
    "objectID": "annex.html#git-shortcomings-for-large-files",
    "href": "annex.html#git-shortcomings-for-large-files",
    "title": "Distributed file storage with git-annex",
    "section": "Git shortcomings for large files",
    "text": "Git shortcomings for large files\nGit was designed for version control of text files (codes, articles, any text-based documents). One major drawback of putting large data files under traditional version control is that it will double the size of your repository. You can use a hack to alleviate this problem, e.g. instead of putting the repository into the working directory, you can put it on a separate drive:\nmkdir large && cd large\ngit init --separate-git-dir=/Volume/drive/repoDir\ndd if=/dev/urandom of=someLargeFile bs=10240 count=$(( RANDOM + 1024 ))\ngit add someLargeFile\ngit commit -m \"...\"\nIn the working directory, this will create a text file .git containing the path to the actual repository. Now, when you commit large files to this repository, a copy will remain in the working directory (first drive), and another copy will go into the repository (second drive). Theoretically, you can even swap between different repositories (i.e. different drives) by swapping the content of .git, so you can have a copy of the same set of files on multiple drives.\nHowever, if you modify/delete a binary data file from the working directory, git will keep the older version in the repository. If this is a multi-GB data file and you no longer need it, you’ll be wasting space by keeping this file in the repository.\ngit-annex solves this problem by replacing data files with symbolic links. When you no longer need a file, you simply run a git-annex drop command to free some disk space. In addition, git-annex provides these features:\n\nIt can distribute files across multiple repositories, to have a different collection of files in each repository. You don’t use multiple bookcases to store the same set of books!\nIt can automate redundant storage, so that you can have multiple copies of files on different drives when you need backups.\nIt can work offline: you can synchronize data between drives without having to mount all of them at the same time. git-annex is fully distributed, i.e. each repository in the collection stores the full history, but it is often convenient to designate the main repository for syncing multiple secondary repositories on external drives. In addition, each repository knows about the location of all files in all other repositories, without having to mount the drives.\n\nIn short, git-annex helps me automate storage of a large number of files across a heterogeneous collection of standalone HDDs and SSDs sitting on a shelf and ensures that all my storage is redundant, i.e. I have at least two copies of all important files. If I ever run out of space, I can add another annex on a new drive, or even throw into the mix a remote server with extra storage.\nYou can read about the history of git-annex on Wikipedia.\nThere is a competing project Git Large File Storage (LFS), and a number of projects built on top of git-annex."
  },
  {
    "objectID": "annex.html#installation",
    "href": "annex.html#installation",
    "title": "Distributed file storage with git-annex",
    "section": "Installation",
    "text": "Installation\ngit-annex is open source and is available as a package in Linux, Mac, Windows. On my Mac I installed it with brew install git-annex. On the Alliance clusters git-annex is provided as a module (module load git-annex), so no need to install it there.\nFor git-annex to function, you also need Git. Sometimes Git development gets ahead of git-annex, and very occasionally things might behave weirdly if you have newer Git and older git-annex (but I never had any data lost). In such cases you might want to check if a new version of git-annex is available."
  },
  {
    "objectID": "annex.html#standalone-workflows",
    "href": "annex.html#standalone-workflows",
    "title": "Distributed file storage with git-annex",
    "section": "Standalone workflows",
    "text": "Standalone workflows\n\nAdding binary files\nFor the purpose of this tutorial, let’s create a bash function to quickly add new binary files to the current directory.\n\nshuf -i 0-9 -n 5 produces 5 unique random digits 0-9\nshuf -i 0-9 -n 5 | tr -d '\\n' puts 5 unique random digits into a singe line\n$RANDOM returns a pseudorandom integer in the range 0..32767\n\nThe following function will take a number and will produce this number of binary files (filled with random bits) of size (1-33)MB.\nfunction populate() {\n    if [ $# -gt 0 ]; then\n    for i in $(seq -w 1 $1); do\n        num=$(shuf -i 0-9 -n 5 | tr -d '\\n')\n        dd if=/dev/urandom of=test\"$num\" bs=1024 count=$(( RANDOM + 1024 ))\n    done\n    fi\n}\n\n\nInitialize annexes\ncd ~/tmp\nmkdir -p {earth,mars,venus}/annex   # placeholders for different drives or filesystems\ntree\ncd earth/annex\n\nalias ga='git-annex'\n\ngit init\nga init \"earth\"\n\npopulate 10\n\nga add .   # (1) move files to .git/annex/objects\n           # (2) create symbolic links pointing to the new location\n           # (3) add these links to the index (git staging area)\ngit commit -m \"add 10 new files\"\n\nIt is very important to use ga add and not git add when adding files to annex, so you need to rewire your git finger memory.\nTo check the repository/annex current status, you can use one of these two commands – both of them report approximately the same information, so in practice I use one or the other:\ngit status   # everything is clean\nga status    # everything is clean\npopulate 5\nls   # 5 new files, 10 old links\n\nga add .\ngit commit -m \"add 5 new files\"\ndu -s .git/annex/objects   # the files are really there\n\n\nUndo accidental add command\nLet’s say we added a file by mistake, but have not committed it yet:\npopulate 1\nls\nexport testfile=test50427\nga add $testfile\n\ngit status   # new file to be comitted\nga status    # added the new file\n\nls -l $testfile\nobject=$(ls -l $testfile | awk '{print $11}')   # store last line\nls -l $object\nHow do we undo this ga add operation? We must do it in two steps.\n\nput the file content back to replace the link:\n\nga unannex $testfile\nls -l $testfile   # the file is back\nls -l $object     # but the object is also still there\n\nremove unlinked file content from the repository:\n\nga unused   # show all unlinked files in the repository (object store)\nga dropunused --force 1   # drop the first unused file\n\nls -l $object     # now the object is gone\nga unused         # no unlinked files left\nWe are now back to having our new, untracked file – you can check that with either git status or ga status.\n\n\nRemove files from annex control\nWhat if we have both added and committed the file – how do we undo that, i.e. how do we remove a file from git-annex control? Follow the same two steps and update the repository:\nga unannex test80145      # unannex one file currently under git-annex control\nga dropunused --force 1   # drop the first unused file\ngit commit -m \"moved test80145 out of the annex\"\nNow if we do either git status or ga status, they will report that now we have a new, untracked file.\nIf you want to remove multiple files, e.g. file1, file2, file3, or an entire subdirectory subdir, you would do something like this:\nga unannex &lt;file1&gt; &lt;file2&gt; &lt;file3&gt;\nga unannex &lt;subdir&gt;\nga unused                      # show all unlinked files\nga dropunused --force {1..5}   # adjust the number based on the previous command's output\n\n\nRemove file completely from the drive\nIf you don’t want to keep the data file, one possible solution is:\n\nreplace the link with the file content: ga unannex &lt;filename&gt;\ndrop the unused object file with a combination of ga unused and ga dropunused --force &lt;number&gt;\nupdate the repository: git commit -m \"moved &lt;filename&gt; out of the annex\"\ndelete the file rm &lt;filename&gt;\n\nAn alternative solution (we’ll see ga drop later when we work with remotes) could be:\nga drop --force &lt;filename&gt;   # drop a local copy (object)\nunlink  &lt;filename&gt;           # remove the link\nga sync                      # does git commit in the background\n\n\nGetting help\n\n`ga –help’ will list all available commands\nga &lt;command&gt; --help will give quick info about the command flags\nonline help pages for individual commands https://git-annex.branchable.com/git-annex-command (replace command with the actual command)"
  },
  {
    "objectID": "annex.html#working-with-remotes",
    "href": "annex.html#working-with-remotes",
    "title": "Distributed file storage with git-annex",
    "section": "Working with remotes",
    "text": "Working with remotes\nThe real power of git-annex lies in its ability to distribute files across multiple annexes in a seamless manner. This is achieved through conventional Git remotes.\n\nRemote via a clone\nLet’s go to another drive and clone our existing repository there:\nga status   # make sure everything is committed\n\ncd ~/tmp/venus\ngit clone ~/tmp/earth/annex\ncd annex\n\nls          # there are symbolic links to non-existent destinations\ndu -s .     # currently no objects (data files)\ngit log     # see the history from earth\n\nga whereis    # all 15 files are on earth (1 copy of each)\n\npopulate 10\nga add .\ngit commit -m \"add 10 new files\"\n\nga whereis   # 15 files on earth, 10 files here\nIn a separate terminal, if we go to earth drive:\ncd ~/tmp/earth/annex\nga whereis   # shows only 15 local files\n\ngit remote add venus ~/tmp/venus/annex\nga sync\nga whereis   # 15 files here, 10 files on venus\nSo, what does ga sync do exactly? It performs these operations:\n\nrun local git commit with a generic message “git-annex in ”\nupdate the current repository with all changes made to its remotes, and\npush any changes in the current repository to its remotes, where you need to run ga sync to get them\n\nAn important innovation of ga sync is that it can work with remotes when they are offline:\n\nwhen pulling from an offline remote, it will check its “Inbox” branch (not actual name) to see if that remote has sync’ed to it earlier\nwhen pushing to an offline remote, it will push updates to its “Outbox” branch (not actual name)\n\nin earth ---\n  git-annex\n* main\n  synced/git-annex\n  synced/main\n\nin venus ---\n  git-annex\n* main\n  synced/main\nLet’s test communicating with offline annexes:\n--- venus annex ---\nga sync             # we made some changes to earth, need to update here\ndd if=/dev/urandom of=venus1 bs=1024 count=$(( RANDOM + 1024 ));\nga add venus1\ngit commit -m \"add venus1\"\nga sync\ncd ../..\nmv venus hidden     # hide it from earth\n\n--- earth annex ---\nga whereis venus1   # cannot find\nga sync             # update from the local branch\nga whereis venus1   # it is in venus\n\ncd ~/tmp\nmv hidden venus\n\n\nRemote from scratch\nLet’s move to another drive and create a new annex there from scratch with 10 new files:\ncd ~/tmp/mars/annex\ngit init\nga init \"mars\"\npopulate 10\nga add .\ngit commit -m \"add 10 new files\"\nga status   # all clean\nAt this point we still have an isolated annex. Let’s add our original repo as a remote for this repo:\ngit remote add earth ~/tmp/earth/annex\nga sync   # now fails, as you have two separate histories\nga sync --allow-unrelated-histories   # should work if no file conflicts\nIf there is a conflict, you may want to either (1) create a new repository via git clone like in the previous section, and then add new files by hand, or (2) try to rename the conflicting file.\nFinally, let’s do this:\n--- earth annex ---\nga sync\ngit remote               # mars is not its remote yet\nga whereis | grep mars   # but we can see the 10 files on mars!\nHere we see another example of offline communication: mars pushed into earth’s inbox earlier, and the last ga sync received those updates, even though we cannot connect to mars directly (no such remote). Let’s correct this by adding the remote:\n--- earth annex ---\ngit remote add mars ~/tmp/mars/annex\n\n\nCreate multiple copies\nThis is easily one of my favourite git-annex features!\n--- earth annex ---\npopulate 2\nexport coupleOfFiles=\"test03985 test69314\"\nga add $coupleOfFiles\ngit commit -m \"add 2 files\"\nga whereis $coupleOfFiles\nThese files have just one copy (here). In fact, in our annexes there are many files with a single copy:\nga whereis | grep -A 1 \"1 copy\"   # show all files with only one copy, in earth + mars + venus\nga whereis . | grep -A 1 \"1 copy\" | grep -B 2 \"\\[here\\]\"   # show local files with only one copy\nga sync\n\n--- mars annex ---\nga sync\nga whereis $coupleOfFiles   # 1 copy on earth\nga get --auto --numcopies=2 $coupleOfFiles\nga whereis $coupleOfFiles   # 2 copies\nga sync\nThe command ga get --auto --numcopies=2 can work with any list of files and/or directories.\n\n\n\n\n\n\n\n\n\n\n\n\nMove files manually between remotes\nga whereis   # pick a file on venus\nga move &lt;filename&gt; --to here   # will likely fail (venus not linked)\ngit remote add venus ~/tmp/venus/annex\nga move &lt;filename&gt; --to here   # should work\nga find   # list all local files =&gt; pick a file\nga whereis test52930   # check its copies\nga move test52930 --to venus   # venus has to be online for this to work\n\n\nDrop a local copy\nga find   # list all local files =&gt; pick a file\nga drop test97806   # delete the object; keep the link: dangerous territory!\nUsually, git-annex will not let you delete a single copy, but you can --force it to permanently delete your data.\nIf you still want to keep a copy elsewhere, a safer approach would be:\nga drop --auto --numcopies=1 &lt;path&gt;   # drop all local copies if there is a remote copy\nga drop --auto --numcopies=2 &lt;path&gt;   # drop all local copies if there are 2 remote copies\n\n\nMove a file out of the annex and drop all annexed copies\n\nLets’ create a new file and put its content into two annexes:\n--- earth annex ---\npopulate 1\nexport testfile=test05873\nga add $testfile\nga copy $testfile --to mars\nga sync\nga whereis $testfile   # two copies: on earth and mars\n\n--- mars annex ---\nga sync\nga whereis $testfile   # two copies: on earth and mars\nobject=$(ls -l $testfile | awk '{print $11}')\nls -l $object          # actual file is stored here\nLet’s copy the file content back to replace the link:\nga unannex $testfile\nls              # the file replaced the link\nga unused       # shows no unlinked files\nls -l $object   # the file is still there, and it is unlinked\nWhat is going on here? It turns out that “ga unused” actually works across linked annexes, and it simply says that there is a link to this file somewhere in the other repository. To unannex this file’s copy everywhere, you need to tell ga unused to ignore that this file is checked out in other annexes:\nga unused --used-refspec=+master\nga dropunused --force 1\nga sync\nls -l $object   # the object is now gone locally\n\n--- earth annex ---\nls -l $testfile   # the file is still under annex here\nga sync\nls -l $testfile   # and now it is not\nga whereis $testfile               # this file is not under annex anywhere\nga unused --used-refspec=+master   # delete its copy here as well\nga dropunused --force 1\n\n--- mars annex ---\nls $testfile      # we have the unannexed copy (and only copy) in mars\n\n\nShow disk usage across all remotes\nga info &lt;path&gt;   # show local+global disk usage in a directory\n\n\nOrganize links into directories\nIn real life you put files into folders / directories. We can do the same with git-controlled links:\n--- mars annex ---\nmkdir data\ngit mv test{0,1,2}* data/\nga sync\n\n--- earth annex ---\nga sync\nls   # test0*,test1*,test2* should be in data/\nga get --auto --numcopies=2 data   # get files in /data with fewer than two copies\nga sync\n\n--- mars annex ---\nga get --auto --numcopies=2 data   # get files in /data with fewer than two copies\nga whereis data                    # all files will likely have two copies now\nga sync"
  },
  {
    "objectID": "annex.html#ssh-remotes",
    "href": "annex.html#ssh-remotes",
    "title": "Distributed file storage with git-annex",
    "section": "SSH remotes",
    "text": "SSH remotes\nIf you want to talk to a remote via ssh, its host server will need to have git and git-annex installed and available via command line when you ssh to that server. However, the Alliance clusters provide git-annex as a module. In this case, you can add module load git-annex to your ~/.bashrc file:\nuser01@vis.vastcloud.org\necho module load git-annex &gt;&gt; .bashrc\necho alias ga=git-annex &gt;&gt; .bashrc\nsource .bashrc\ngit config --global init.defaultBranch main   # sometimes remote git can be old\n\n\n\n\n\n\n\nuser01@vis.vastcloud.org\nmkdir annex && cd annex\ngit init       # --bare\nga init \"cluster\"\ngit commit --allow-empty -m \"first commit\"   # otherwise local \"ga sync\" will fail to merge\n\n--- earth annex ---\ngit remote remove cluster\ngit remote add cluster user01@vis.vastcloud.org:annex\nga sync --allow-unrelated-histories\n\n--- cluster annex ---\nga sync\nls        # all aliases from earth should be there\ndu -s .   # but there are no object files\nWith the two repositories linked, any new change on earth can be sync’ed with the SSH remote:\n--- earth annex ---\ndd if=/dev/urandom of=earth3 bs=1024 count=$(( RANDOM + 1024 ));\nga add earth3\nga unannex earth1\nga sync\n\n--- cluster annex ---\nga sync\nls   # recent updates from earth should be here\nga whereis earth3   # only one copy on earth\nga whereis earth1   # no copies under git-annex control\nHow about going the other way, i.e. syncing a change from the ssh remote?\n--- cluster annex ---\ndd if=/dev/urandom of=cluster1 bs=1024 count=$(( RANDOM + 1024 ));\nga add cluster1\nga sync   # cannot update the repos on my computer: they are behind the firewall;\n          # plus we have not even set up the remotes!\n\n--- earth annex ---\nga sync   # not to worry, this will sync the recent changes in the ssh remote\nls        # cluster1 is now here\nga whereis cluster1\nNote that having SSH remotes will slow down your ga sync commands, so personally – even when I have them – I prefer to temporarily remove them with git remote remove ....\nYou can find another example at https://git-annex.branchable.com/walkthrough/using_ssh_remotes."
  },
  {
    "objectID": "annex.html#check-annexed-data",
    "href": "annex.html#check-annexed-data",
    "title": "Distributed file storage with git-annex",
    "section": "Check annexed data",
    "text": "Check annexed data\nga fsck     # run checksum on all local files\nga unused   # show all unlinked files in this annex"
  },
  {
    "objectID": "annex.html#when-a-drive-fails",
    "href": "annex.html#when-a-drive-fails",
    "title": "Distributed file storage with git-annex",
    "section": "When a drive fails",
    "text": "When a drive fails\nIf a drive goes bad, you want to make sure its content is mirrored in two other places, i.e. you should rebuild your redundant storage:\n--- some other annex ---\nga whereis . | grep -A 1 \"1 copy\" | grep -B 2 &lt;bad annex&gt;    # congratulations, you lost your only copy ...\nga whereis . | grep -A 2 \"2 copies\" | grep -B 3 &lt;bad annex&gt;  # 1 copy left elsewhere\nBack up those single copies to a third annex with one of these commands:\n--- third annex ---\nga get &lt;items&gt;   # get a local copy\nga get --auto --numcopies=2 &lt;items&gt;\n\n--- source annex ---\nga copy &lt;items&gt; --to &lt;third annex&gt;\nNext, you want to mark the failed drive’s annex as “dead”, so that its content stops showing in gitannex whereis:\nga dead &lt;bad annex&gt;\nand remove it from all other repositories:\ngit remote remove &lt;bad annex&gt;   # do this in each annex"
  },
  {
    "objectID": "annex.html#print-one-line-per-file",
    "href": "annex.html#print-one-line-per-file",
    "title": "Distributed file storage with git-annex",
    "section": "Print one line per file",
    "text": "Print one line per file\nSometimes it is tricky to grep the output of ga whereis, as it produces a variable number of lines per file, depending on the number of copies.\nga whereis has the flag --format, but I find its output somewhat lacking.\n\nSimpler, less efficient solution\nThe following commands\necho $(ga whereis)        | sed 's/whereis/\\n/g'\necho $(ga whereis &lt;path&gt;) | sed 's/whereis/\\n/g'\nwill print one line per file, so you can organize searches with something like:\necho $(ga whereis) | sed 's/whereis/\\n/g' | grep &lt;annex&gt;\necho $(ga whereis) | sed 's/whereis/\\n/g' | grep \"1 copy\"\necho $(ga whereis) | sed 's/whereis/\\n/g' | grep &lt;annex&gt; | grep \"1 copy\"\necho $(ga whereis watch) | sed ‘s/whereis//g’ | grep “1 copy”\n\n\nFaster, more elegant solution\nPerhaps, a more elegant solution is to use the --json flag which could be useful for sending (very complete) JSON output to other utilities or even Python scripts. One added benefit: it’ll properly process and display Unicode in file names, e.g. if you use Chinese characters or a non-Latin alphabet.\nLet’s process our output with jq, a command-line JSON processor.\nFirst, let’s list files with at least 2 copies:\nga whereis --copies 2\nand use one of their names below.\nga whereis test43816               # multiple lines\nga whereis --json test43816        # one JSON line per file\nga whereis --json test43816 | jq   # show the multi-line JSON object\nga whereis --json test43816 | jq '.file'   # print the file name only\n\n# print file name and the space-delimited list of repositories\nga whereis --json test43816 | jq '.file + \" \" + (.whereis|map(.description)|join(\",\"))'\nWe can now apply this to any list of items or the entire repository:\nga whereis --json | jq '.file + \" \" + (.whereis|map(.description)|join(\",\"))'"
  },
  {
    "objectID": "annex.html#cheat-sheet",
    "href": "annex.html#cheat-sheet",
    "title": "Distributed file storage with git-annex",
    "section": "Cheat sheet",
    "text": "Cheat sheet\n\n&lt;items&gt; could be any collection of files and/or directories\n&lt;annex&gt; is the name of a git-annex, e.g. earth or mars or venus\n\nga init &lt;name&gt;\nga add &lt;items&gt;\nga sync\n\nga unannex &lt;items&gt;            # replace the link with the actual file content\nga unused                     # show all unlinked files in the annex\nga unused --used-refspec=+master   # same, but ignore these files being checked out in other annexes\nga dropunused --force &lt;num&gt;   # drop the unused file with a given index\nga drop --force &lt;items&gt;       # drop a local copy (object)\n\nga find                       # list all local files\nga whereis &lt;items&gt;            # locate file(s) or dir(s)\nga whereis | grep -A 1 \"1 copy\"                          # show all files with only one copy\nga whereis | grep -A 1 \"1 copy\" | grep -B 2 \"\\[here\\]\"   # show local files with only one copy\necho $(ga whereis) | sed 's/whereis/\\n/g'                # print one file per line\nga whereis --json | jq '.file + \" \" + (.whereis|map(.description)|join(\",\"))' # print name + annex\nga info &lt;items&gt;               # show local + global disk usage\n\nga copy &lt;items&gt; --to here\nga copy &lt;items&gt; --to &lt;annex&gt;\nga move &lt;items&gt; --to here\nga move &lt;items&gt; --to &lt;annex&gt;\nga get --auto --numcopies=2 &lt;items&gt;\n\ngit-annex walkthrough is a good introduction to using git-annex from the command line."
  },
  {
    "objectID": "list.html",
    "href": "list.html",
    "title": "Longer courses",
    "section": "",
    "text": "Longer courses\n\n\n\nLinux command line\n\nIntroduction to Bash command line\n\n\n\n\n\nHigh-performance computing (HPC)\n\nIntroduction to HPC\nWorking with Apptainer containers\n\n\n\n\n\nPython\n\nIntroduction to Python\nScientific Python\nTowards high-performance Python\nDistributed computing with Ray\n\n\n\n\n\nJulia\n\nParallel computing in Julia\n\n\n\n\n\nChapel\n\nParallel programming in Chapel\nGPU computing with Chapel\n\n\n\n\n\nVisualization\n\nScientific visualization with ParaView\nLarge-scale and remote visualization on HPC clusters\nScientific visualization with VisIt\n\n\n\n\n\n\nShorter workshops and webinars\n\n\n\n\nBash scripting for beginners\nLesser known but very useful Bash features\nAnswering your Bash questions\nIntroduction to makefiles\nPlot.ly for scientific visualization\nHands-on with ParaView’s Programmable Filter & Source for scientific visualization\nPython basics for humanists\nDistributed file storage with git-annex\nWhat format to choose to save your data\n\n\n\n\n\n\nCommand-line image processing with ImageMagick\nManaging large hierarchical datasets with PyTables\nDistributed datasets with DataLad\nUsing Git to contribute to the RCG website\nFrom cloud to HPC … building a GPU-enabled container on Arbutus\nRunning parallel Ray workflows\nHigh-level parallel stencil computations on CPUs and GPUs in Julia\nGlobus-cli for file transfer\n\n\n\n\n\n\nPages on the old website\n\n\nhttps://wgpages.netlify.app/hpc-in-summer\nhttps://wgpages.netlify.app/pythonlibraries\nhttps://wgpages.netlify.app/julia-installation\nhttps://wgpages.netlify.app/distributed\nhttps://wgpages.netlify.app/darrays"
  },
  {
    "objectID": "clusterWorkflows.html#abstract",
    "href": "clusterWorkflows.html#abstract",
    "title": "Running parallel Ray workflows",
    "section": "Abstract",
    "text": "Abstract\nRay is a unified framework for scaling AI and general Python workflows. Outside of machine learning (ML), its core distributed runtime and data libraries can be used for writing parallel applications that launch multiple processes, both on the same node and across multiple cluster nodes. These processes can subsequently execute a variety of workloads, e.g. Numba-compiled functions, NumPy array calculations, and even GPU-enabled codes.\nIn this webinar, we will focus on scaling Ray workflows to multiple HPC cluster nodes to speed up various (non-ML) numerical workflows. We will look at both a loosely coupled (embarrassingly parallel) problem with a slowly converging series (the harmonic series with some terms taken out) and a tightly coupled parallel problem."
  },
  {
    "objectID": "clusterWorkflows.html#installation",
    "href": "clusterWorkflows.html#installation",
    "title": "Running parallel Ray workflows",
    "section": "Installation",
    "text": "Installation\nOn my computer, with pyenv installed earlier:\npyenv install 3.12.7\nunlink ~/.pyenv/versions/hpc-env\npyenv virtualenv 3.12.7 hpc-env   # goes into ~/.pyenv/versions/&lt;version&gt;/envs/hpc-env\npyenv activate hpc-env\npip install numpy numba\npip install --upgrade \"ray[default]\"\npip install --upgrade \"ray[data]\"\n...\npyenv deactivate\nOn a production HPC cluster:\nmodule load python/3.12.4 arrow/18.1.0 scipy-stack/2024b netcdf/4.9.2\nvirtualenv --no-download pythonhpc-env\nsource pythonhpc-env/bin/activate\npip install --no-index --upgrade pip\npip install --no-index numba\navail_wheels \"ray\"\npip install --no-index ray\n...\ndeactivate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo use it:\nmodule load StdEnv/2023 python/3.12.4 arrow/17.0.0 scipy-stack/2024a netcdf/4.9.2\nsource /project/def-sponsor00/shared/pythonhpc-env/bin/activate"
  },
  {
    "objectID": "clusterWorkflows.html#initializing-ray",
    "href": "clusterWorkflows.html#initializing-ray",
    "title": "Running parallel Ray workflows",
    "section": "Initializing Ray",
    "text": "Initializing Ray\nimport ray\nray.init()   # start a Ray cluster  and connect to it\n             # no longer necessary, will run by default when you first use it\nHowever, ray.init() is very useful for passing options at initialization. For example, Ray is quite verbose when you do things in it. To turn off this logging output to the terminal, you can do\nray.init(configure_logging=False)   # hide Ray's copious logging output\nYou can run ray.init() only once. If you want to re-run it, first you need to run ray.shutdown(). Alternatively, you can pass the argument ignore_reinit_error=True to the call.\nYou can specify the number of cores for Ray to use, and you can combine multiple options, e.g.\nray.init(num_cpus=4, configure_logging=False)\nIf not specified, Ray will use all available CPU cores, e.g. on my laptop ray.init() will start 8 ray::IDLE processes (workers), and you can monitor these in a separate shell with htop --filter \"ray::IDLE\" command (you may want to hide threads – typically thrown in green – with Shift+H)."
  },
  {
    "objectID": "clusterWorkflows.html#ray-tasks",
    "href": "clusterWorkflows.html#ray-tasks",
    "title": "Running parallel Ray workflows",
    "section": "Ray tasks",
    "text": "Ray tasks\nIn Ray you can execute any Python function asynchronously on separate workers. Such functions are called Ray remote functions, and their asynchronous invocations are called Ray tasks:\nimport ray\nray.init(configure_logging=False)   # optional\n\n@ray.remote             # declare that we want to run this function remotely\ndef square(x):\n    return x * x\n\nr = square.remote(10)   # launch/schedule a remote calculation (non-blocking call)\ntype(r)                 # ray._raylet.ObjectRef (object reference)\nray.get(r)              # retrieve the result (=100) (blocking call)\nThe calculation may happen any time between &lt;function&gt;.remote() and ray.get() calls, i.e. it does not necessarily start when you launch it. This is called lazy execution: the operation is often executed when you try to access the result.\na = square.remote(10)   # launch a remote calculation\nray.cancel(a)           # cancel it\nray.get(a)              # either error or 100, depending on whether the calculation\n                        # has finished before cancellation\nYou can launch several Ray tasks at once, to be executed in parallel in the background, and you can retrieve their results either individually or through the list:\nr = [square.remote(i) for i in range(4)]   # launch four parallel tasks (non-blocking call)\nprint([ray.get(r[i]) for i in range(4)])   # retrieve the results (multiple blocking calls)\nprint(ray.get(r))          # more compact way to do the same (single blocking call)"
  },
  {
    "objectID": "clusterWorkflows.html#loosely-coupled-parallel-problem",
    "href": "clusterWorkflows.html#loosely-coupled-parallel-problem",
    "title": "Running parallel Ray workflows",
    "section": "Loosely coupled parallel problem",
    "text": "Loosely coupled parallel problem\nWhen I teach parallel computing in other languages (Julia, Chapel), the approach is to take a numerical problem and parallelize it using multiple processors, and concentrate on various issues and bottlenecks (variable locks, load balancing, false sharing, messaging overheads, etc.) that lead to less than 100% parallel efficiency. For the numerical problem I usually select something that is very simple to code, yet forces the computer to do brute-force calculation that cannot be easily optimized.\nOne such problem is a slow series. It is a well-known fact that the harmonic series \\(\\sum\\limits_{k=1}^\\infty{1\\over k}\\) diverges. It turns out that if we omit the terms whose denominators in decimal notation contain any digit or string of digits, it converges, albeit very slowly (Schmelzer & Baillie 2008), e.g.\n\nBut this slow convergence is actually good for us: our answer will be bounded by the exact result (22.9206766192…) on the upper side, and we will force the computer to do CPU-intensive work. We will sum all the terms whose denominators do not contain the digit “9”, and evaluate the first \\(10^8\\) terms.\nI implemented and timed this problem running in serial in Julia (356ms) and Chapel (300ms) – both are compiled languages. Here is one possible Python implementation:\nfrom time import time\ndef slow(n: int):\n    total = 0\n    for i in range(1,n+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    return total\n\nstart = time()\ntotal = slow(100_000_000)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nLet’s save this code inside the file slowSeries.py and run it. Depending on the power supplied to my laptop’s CPU (which I find varies quite a bit depending on the environment), I get the average runtime of 6.625 seconds. That’s ~20X slower than in Julia and Chapel!\nNote that for other problems you will likely see an even bigger (100-200X) gap between Python and the compiled languages. In other languages looking for a substring in a decimal representation of a number takes a while, and there you will want to code this calculation using integer numbers. If we also do this via integer operations in Python:\ndef digitsin(num: int):\n    base = 10\n    while 9//base &gt; 0: base *= 10\n    while num &gt; 0:\n        if num%base == 9: return True\n        num = num//10\n    return False\n\ndef slow(n: int):\n    total = 0\n    for i in range(1,n+1):\n        if not digitsin(i):\n            total += 1.0 / i\n    return total\nour code will be ~3-4X slower, so in the native Python code we should use the first version of the code with if not \"9\" in str(i) – it turns out that in this particular case Python’s high-level substring search is actually quite well optimized!\n\nSpeeding up the slow series with Numba\nYou can speed up this problem with an open-source just-in-time compiler called Numba. It uses LLVM underneath, can parallelize your code for multi-core CPUs and GPUs, and often requires only minor code changes.\nI won’t go through all the Numba details – we cover these in our separate HPC Python course. To speed up the slow series with Numba, you have to use the integer-operations version of digitsin(), as Numba is notoriously slow with some high-level Python constructs. Check out this implementation which is almost as fast as with the compiled languages:\nfrom time import time\nfrom numba import jit\n\n@jit(nopython=True)\ndef combined(k):\n    base, k0 = 10, k\n    while 9//base &gt; 0: base *= 10\n    while k &gt; 0:\n        if k%base == 9: return 0.0\n        k = k//10\n    return 1.0/k0\n\n@jit(nopython=True)\ndef slow(n):\n    total = 0\n    for i in range(1,n+1):\n        total += combined(i)\n    return total\n\nstart = time()\ntotal = slow(100_000_000)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nIt finishes in 0.601 seconds, i.e. ~10X faster than a naive Numba implementation with substring search (not shown here).\n\n\nRunning Numba-compiled functions as Ray tasks\nWouldn’t it be great if we could use Ray to distribute execution of Numba-compiled functions to workers? It turns out we can, but we have to be careful with syntax. We would need to define remote compiled functions, but neither Ray, nor Numba let you combine their decorators (@ray.remote and @numba.jit, respectively) for a single function. You can do this in two steps:\nimport ray\nfrom numba import jit\n\nray.init(num_cpus=4, configure_logging=False)\n\n@jit(nopython=True)\ndef square(x):\n    return x*x\n\n@ray.remote\ndef runCompiled():\n    return square(5)\n\nr = runCompiled.remote()\nray.get(r)\nHere we “jit” the function on the main process and send it to workers for execution. Alternatively, you can “jit” on workers:\nimport ray\nfrom numba import jit\n\nray.init(num_cpus=4, configure_logging=False)\n\ndef square(x):\n    return x*x\n\n@ray.remote\ndef runCompiled():\n    compiledSquare = jit(square)\n    return compiledSquare(5)\n\nr = runCompiled.remote()\nray.get(r)\nIn my tests with more CPU-intensive functions, both versions produce equivalent runtimes.\nHere is the combined Numba + Ray remotes code for the slow series (store it as slowSeriesNumbaRayCore.py):\nfrom time import time\nimport ray\nfrom numba import jit\n\nray.init(configure_logging=False)\n\nn = 100_000_000\n\n@jit(nopython=True)\ndef combined(x):\n    base, x0 = 10, x\n    while 9//base &gt; 0: base *= 10\n    while x &gt; 0:\n        if x%base == 9: return 0.0\n        x = x//10\n    return 1.0/x0\n\n@jit(nopython=True)\ndef slow(interval):\n    total = 0\n    for i in range(interval[0],interval[1]+1):\n        total += combined(i)\n    return total\n\n@ray.remote\ndef runCompiled(interval):\n    return slow(interval)\n\nncores = 4\nsize = n//ncores   # size of each batch\nintervals = [(i*size+1,(i+1)*size) for i in range(ncores)]\nif n &gt; intervals[-1][1]: intervals[-1] = (intervals[-1][0], n)   # add the remainder (if any)\n\nr = [runCompiled.remote((1,10)) for i in range(ncores)]   # expose workers to runCompiled function\ntotal = sum(ray.get(r))\n\nstart = time()\nr = [runCompiled.remote(intervals[i]) for i in range(ncores)]\ntotal = sum(ray.get(r))   # compute total sum\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nHere the averaged (over three runs) times on my laptop:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n0.439\n0.235\n0.130\n0.098\n\n\n\nUsing a combination of Numba and Ray tasks on 8 cores, we accelerated the calculation by ~68X.\n\n\nRunning on one cluster node\nLet’s run this Numba + Ray remotes code on several CPU cores on one cluster node. We documented this in our wiki https://docs.alliancecan.ca/wiki/Ray – look for the section “Single Node”. It advises you to start a single-node Ray cluster with multiple CPUs via the ray start ... command.\nStrictly speaking, you don’t have to do this, as Slurm will ensure that any additional processes spawned to run your Ray tasks will run on the allocated CPUs.\ntraining cluster\ncd ~/webinar\nmodule load StdEnv/2023 python/3.12.4 arrow/17.0.0 scipy-stack/2024a netcdf/4.9.2\nsource /project/def-sponsor00/shared/pythonhpc-env/bin/activate\n\nsalloc --time=0:60:0 --mem-per-cpu=3600\n&gt;&gt;&gt; ncores = 1\npython slowSeriesNumbaRayCore.py   # 1.04s\n&gt;&gt;&gt; ncores = 4\npython slowSeriesNumbaRayCore.py   # 1.135s (still running on 1 core)\nexit\n\nsalloc --time=0:60:0 --ntasks=4 --mem-per-cpu=3600\npython slowSeriesNumbaRayCore.py   # 0.264s (running on 4 cores)\nexit\nOn this training cluster we have 8 cores per node, so we can attempt to use all of them on a single node:\n&gt;&gt;&gt; ncores = 8\nsalloc --time=0:60:0 --ntasks-per-node=8 --nodes=1 --mem-per-cpu=3600\npython slowSeriesNumbaRayCore.py   # 0.135 (running on 8 cores)\nexit\n\n\nRunning on multiple cluster nodes\nNow let’s run the same problem on 2 cluster nodes. We’ll try to distribute these 8 workers across 8 cores across 2 nodes:\n&gt;&gt;&gt; ncores = 8\nsalloc --time=0:60:0 --ntasks-per-node=4 --nodes=2 --mem-per-cpu=3600\npython slowSeriesNumbaRayCore.py   # 0.292 (not getting 8X speedup)\nThe problem is that Ray cluster ran all 8 workers on 4 cores on the first node. While still inside the same job, let start an interactive Python shell and then type:\nimport ray\nray.init(num_cpus=8, configure_logging=False)\nray.nodes()   # reports a dictionary with node specs\nprint(len(ray.nodes()))   # 1 node\nprint(ray.cluster_resources().get(\"CPU\"))   # 8 CPU \"cores\"\nSo, we have 1 node in Ray … Ray thinks it has access to 8 CPU cores, but that’s actually the number of workers running on 4 physical CPU cores …\nTo run properly on 2 cluster nodes, we need to (1) start a single-node Ray cluster and then (2) add workers on other nodes to it outside of Python. This is described in our documentation https://docs.alliancecan.ca/wiki/Ray.\n\n\n\n\n\n\nNote\n\n\n\nRay does not play nicely with single-core MPI tasks. Internally, Ray thinks that each MPI rank should be a “Ray node”, inside of which you would utilize multiple CPU cores. We can do this by launching just one MPI rank per cluster node and then specifying --cpus-per-task=4.\n\n\nLet’s do it:\n&gt;&gt;&gt; make sure to go back to the login node!\nsalloc --time=0:60:0 --nodes=2 --ntasks-per-node=1 --cpus-per-task=4 --mem-per-cpu=3600\n\nexport HEAD_NODE=$(hostname)   # store head node's address\nexport RAY_PORT=34567          # choose a port to start Ray on the head node; unique from other users\n\necho \"Starting the Ray head node at $HEAD_NODE\"\nray start --head --node-ip-address=$HEAD_NODE --port=$RAY_PORT \\\n          --num-cpus=$SLURM_CPUS_PER_TASK --block &\nsleep 10\n\necho \"Starting the Ray worker nodes on all other MPI tasks\"\nsrun launchWorkers.sh &\n\nray status   # should report 2 nodes (their IDs) and 8 cores\nHere is our code launchWorkers.sh to launch Ray workers on other nodes:\n#!/bin/bash\nif [[ \"$SLURM_PROCID\" -ne \"0\" ]]; then\n    ray start --address \"${HEAD_NODE}:${RAY_PORT}\" --num-cpus=\"${SLURM_CPUS_PER_TASK}\" --block\n    sleep 5\n    echo \"ray worker started!\"\nfi\nStart Python and type:\nimport ray\nimport os\n\n# connect to our pre-configured Ray cluster\nray.init(address=f\"{os.environ['HEAD_NODE']}:{os.environ['RAY_PORT']}\",_node_ip_address=os.environ['HEAD_NODE'])\n\nprint(\"Nodes in the Ray cluster:\")\nprint(len(ray.nodes()))   # should report 2 nodes\nprint(ray.nodes())        # their details\n\nnodes = ray.nodes()\nfor node in nodes:\n    status = \"Alive\" if node['Alive'] else \"Dead\"\n    print(node['NodeID'], node['NodeManagerAddress'], status, int(node['Resources']['CPU']), \"cores\")\n\nprint(ray.available_resources())\n\n\n\n\n\n\nNote\n\n\n\nIn this setup, each Ray node runs on a separate MPI task, one task per cluster node, and multiple CPU cores inside that task.\n\n\nLet’s go back to our slow series with Numba. We’ll start from scratch and run the job via sbatch.\nStep 1: Prepare the following submit.sh job submission script:\n#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=3600\n#SBATCH --time=0:5:0\n\nmodule load StdEnv/2023 python/3.12.4 arrow/17.0.0 scipy-stack/2024a netcdf/4.9.2\nsource /project/def-sponsor00/shared/pythonhpc-env/bin/activate\n\nexport HEAD_NODE=$(hostname)   # head node's address\nexport RAY_PORT=34567          # port to start Ray on the head node; should be unique\n\necho \"Starting the Ray head node at $HEAD_NODE\"\nray start --head --node-ip-address=$HEAD_NODE --port=$RAY_PORT \\\n          --num-cpus=$SLURM_CPUS_PER_TASK --block &\nsleep 10\n\necho \"Starting the Ray worker nodes on all other MPI tasks\"\nsrun launchWorkers.sh &\n\npython slowSeriesNumbaRayCore.py\nStep 2: Prepare the following launchWorkers.sh script:\n#!/bin/bash\nif [[ \"$SLURM_PROCID\" -ne \"0\" ]]; then\n    ray start --address \"${HEAD_NODE}:${RAY_PORT}\" --num-cpus=\"${SLURM_CPUS_PER_TASK}\" --block\n    sleep 5\n    echo \"ray worker started!\"\nfi\nStep 3: In the code slowSeriesNumbaRayCore.py connect to the existing Ray cluster:\nimport os\nray.init(address=f\"{os.environ['HEAD_NODE']}:{os.environ['RAY_PORT']}\",_node_ip_address=os.environ['HEAD_NODE'])\nRun it with:\nsbatch submit.sh\nRunning it multiple times, I got the following runtimes: 0.188s, 0.208s, 0.153s – these are better than a 4-core, single-node run (0.292s), but worse than an 8-core, single-node run (0.135s). Any idea why?\n\n\n\n\n\n\nNote\n\n\n\nFor longer runtimes, while your job is still running, you can ssh into individual nodes and check Ray’s CPU usage with htop --filter \"ray::\"\n\n\n\n\nAlternative parallel implementation via Ray Data\nIn addition to bare-bones remote functions, Ray provides other tools for parallelizing your calculations. For example, you can use Ray Data to process a table of rows in parallel. We won’t go into the details here – again, these are covered in our HPC Python course – here I will provide an example of parallelizing the slow series with Ray Data without Numba compilation:\nimport ray\nfrom time import time\n\ndef slow(row):   # this function processes a row in the dataset\n    total = 0\n    for i in range(row['a'], row['b']+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    row['sum'] = total\n    return row\n\nn = 100_000_000\nncores = 2\nray.init(num_cpus=ncores, configure_logging=False)\nsize = n // ncores\nedges = [(i*size+1, i*size+size) for i in range(ncores)]\nif edges[-1][1] &lt; n: edges[-1] = (edges[-1][0],n)\n\nintervals = ray.data.from_items([{'a':w[0], 'b':w[1]} for w in edges])  # define the dataset\n\nstart = time()\npartial = intervals.map(slow)                     # define the calculation\ntotal = sum([p['sum'] for p in partial.take()])   # request the result =&gt; start the calculation on 2 CPU cores\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nOf course, you can speed up this code a lot by running Numba-compiled slow() via Ray Data – do this as exercise after the webinar, or attend our HPC Python course."
  },
  {
    "objectID": "clusterWorkflows.html#tightly-coupled-parallel-problem",
    "href": "clusterWorkflows.html#tightly-coupled-parallel-problem",
    "title": "Running parallel Ray workflows",
    "section": "Tightly coupled parallel problem",
    "text": "Tightly coupled parallel problem\nWe’ll be solving a 1D Poisson equation\n\\[{\\partial^2 u(x)\\over\\partial x^2}=\\rho(x)\\] on a unit interval \\(x\\in[0,1]\\). If we set \\(\\rho(x)\\equiv 2\\) and \\(u(0)=u(1)=0\\), the exact solution becomes \\(u(x)=x^2-x\\).\n\nMy original intention for this part of the webinar was to solve this problem via an iterative Schwarz linear solver. In this approach you divide the linear problem into ncores linear problems, each solved with np.linalg.solve() on a separate Ray task. At the end of each iteration you update the first and the last elements of the RHS vector in each linear system with the new solutions at the interfaces.\nHowever, a naive implementation of this solver converges very slowly. You can speed it up by using a staggered linear solver (to accelerate the convergence rate) and pre-computing the coarser solution in serial (to be used as the initial iteration for the finer solution). When you implement these techniques, the 1D parallel solver converges in 3 iterations, which (1) defies the purpose of breaking it up into multiple parts to be parallelized with Ray, and (2) will take a while to explain and code in this webinar which is not on the theory of parallel linear solvers.\nFor this reason, here I will show a brute-force iterative solution to this problem. It is much less efficient, but it will work well here to demonstrate a tightly coupled parallel solver with Ray.\nWe can rewrite the above equation in the finite difference form:\n\\[\n{u_{i-1}-2u_i+u_{i+1}\\over h^2}=\\rho_i\n\\]\nwhere \\(i\\) is the grid index. Starting with the initial guess \\(u_i^0=0\\), we can set up iterations to compute the solution:\n\\[\nu_i^{n+1} = {u_{i-1}^n+u_{i+1}^n-b_i\\over 2}\n\\]\nwhere \\(b_i=\\rho_ih^2\\), and the solution converges as \\(n\\to\\infty\\).\n\nSerial code\nTo prototype a parallel solver, first we’ll write a serial code that solves the problem on 2 intervals, and later we’ll parallelize it. We’ll do all array computations with NumPy. Let’s consider a fairly large 1D problem (total number of inner grid points \\(N=2\\times10^7\\)) and break it into 2 intervals.\n\n\nupdate the solution only at the inner points (filled circles)\nalways have \\(u_0=u_{N-1}=0\\) at the edges (large open circles)\nexchange values at the inner boundary (large filled circles)\n\nLet’s store this code in poissonSerial.py:\n##### this is poissonSerial.py #####\nfrom time import time\nimport numpy as np\n\nn, relativeTolerance = 20_000_000, 1e-4\nn1 = n//2; n2 = n - n1\nh = 1/(n-1)\n\nb = np.ones(n)*2*h*h\n\nu1 = np.zeros(n1)   # the initial guess (first interval)\nu2 = np.zeros(n2)   # the initial guess (second interval)\n\ndiff = 999.0   # difference between two successive values of u1[-1]\ncount = 0\nstart = time()\nwhile abs(diff) &gt; relativeTolerance:\n    count += 1\n    ghostLeft = u1[-1]\n    ghostRight = u2[0]\n    diff = ghostLeft\n    # compute new u1\n    u1[1:n1-1] = (u1[0:n1-2] + u1[2:n1] - b[1:n1-1]) / 2   # update at inner points 1..n1-2\n    u1[n1-1] = (u1[n1-2] + ghostRight - b[n1-1]) / 2       # update at the last point\n    # compute new u2\n    u2[1:n2-1] = (u2[0:n2-2] + u2[2:n2] - b[n1+1:n-1]) / 2 # update at inner points 1..n2-2\n    u2[0] = (ghostLeft + u2[1] - b[n1]) / 2                # update at the first point\n    diff = (diff-u1[-1]) / u1[-1]\n    if count%100 == 0: print(u1[-3:], u2[:3], diff)\n\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(\"Converged after\", count, \"iterations\", \"with diff =\", diff)\nprint(\"Solution:\", u1[-3:], u2[:3])\npyenv activate hpc-env\ncd ~/training/pythonHPC/clusterWorkflows\npython poissonSerial.py\nTime in seconds: 279.032\nConverged after 9961 iterations with diff = -9.999348935138563e-05\nSolution: [-2.50966143e-11 -2.50990843e-11 -2.51015743e-11] [-2.51015743e-11 -2.50990843e-11 -2.50966143e-11]\n\n\n\n\n\n\nCaution\n\n\n\nStrictly speaking, we did not converge here (still very far from the exact solution \\(u(x)=x^2-x\\)) – but this is not important for our parallel scaling purposes.\n\n\n\n\nPersistent storage on Ray workers\nIn the previous serial code u1 and u2 are stored on the same processor. In the parallel code we’ll be computing u1 on processor 1 and u2 on processor 2. At each iteration we’ll be calling several Ray remote functions to do processing, but we need to store the solution u1 on processor 1 and u2 on processor 2 in between these function calls.\n\n\n\n\n\n\nNote\n\n\n\nRay functions (remote tasks) are stateless, i.e. they can run on any processor that happens to be more idle at the time. How do we ensure that we always compute u1 on processor 1 and u2 on processor 2, and how do we store the arrays there permanently, without copying them back and forth at each iteration?\n\n\nTo do this, we need to use Ray actors (https://docs.ray.io/en/latest/ray-core/actors.html). A Ray actor is essentially a stateful (bound to a processor) worker that is created via a Python class instance with its own persistent variables and methods, and it stays permanently on that worker until we destroy this instance.\n\n\n\n\n\nimport ray\nimport numpy as np\n\nray.init()\n\n@ray.remote\nclass ArrayStorage:         # define an actor (ArrayStorage class) with a persistent array\n    def __init__(self):\n        self.array = None   # persistent array variable\n    def store_array(self, arr):\n        self.array = arr    # store an array in the actor's state\n    def get_array(self):\n        return self.array   # retrieve the stored array\n\nstorage_actor = ArrayStorage.remote()   # create an instance of the actor\n\narr = np.array([1, 2, 3, 4, 5])\nray.get(storage_actor.store_array.remote(arr))   # store an array\n\nr = ray.get(storage_actor.get_array.remote())    # retrieve the stored array\nprint(r)  # Output: [1 2 3 4 5]\n\n\n\n\nTo scale this to multiple workers, we can do the same with an array of workers:\nworkers = [ArrayStorage.remote() for i in range(2)]   # create two instances of the actor\n\nr = [workers[i].store_array.remote(np.ones(5)*(i+1)) for i in range(2)]\nprint(ray.get(r))\n\nprint(ray.get(workers[0].get_array.remote()))   # [1. 1. 1. 1. 1.]\nprint(ray.get(workers[1].get_array.remote()))   # [2. 2. 2. 2. 2.]\n\nr = [workers[i].get_array.remote() for i in range(2)]\nprint(ray.get(r))   # both arrays in one go\nIf we want to make sure that these arrays stay on the same workers, we can retrieve and print their IDs and the node IDs by adding these two functions to the actor class:\n...\n    def get_actor_id(self):\n        return self.actor_id\n    def get_node_id(self):\n        return self.node_id   # the node ID where this actor is running\n...\nprint([ray.get(workers[i].get_actor_id.remote()) for i in range(2)])   # actor IDs\nprint([ray.get(workers[i].get_node_id.remote()) for i in range(2)])    # node IDs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can even use NumPy on workers. For example, if we were to implement a linear algebra solver on a worker and wanted to have the solution array stored there permanently, we could do it this way:\nimport numpy as np\nimport ray\n\nray.init(num_cpus=2, configure_logging=False)\n\nn = 500\nh = 1/(n+1)\nb = np.exp(-(100*(np.linspace(0,1,n)-0.45))**2)*h*h\n\n@ray.remote\nclass ArrayStorage:\n    def __init__(self, n):\n        self.b = None   # persistent variable\n        self.u = None   # persistent variable\n        flatIdentity = np.identity(n).reshape([n*n])\n        a = -2.0*flatIdentity\n        a[1:n*n-1] = a[1:n*n-1] + flatIdentity[:n*n-2] + flatIdentity[2:]\n        self.a = a.reshape([n,n])   # persistent variable\n    def store_b(self, arr):\n        self.b = arr                # store an array in the actor's state\n    def get_u(self):\n        return self.u\n    def localSolve(self):\n        self.u = np.linalg.solve(self.a,self.b)\n\nworker = ArrayStorage.remote(n)\nworker.store_b.remote(b)\nworker.localSolve.remote()\nu = ray.get(worker.get_u.remote())\nprint(\"The solution is\", u)\n\n\nBack to the Poisson solver\nIn the parallel code, we’ll be storing u1 and u2 permanently on two workers. On each interval, we will be updating the left and right ghost values from the other processor via remote functions getLastValue and getFirstValue – otherwise the code is functionally identical to the serial version. Let’s store this code in poissonDual.py:\n##### this is poissonDual.py #####\nfrom time import time\nimport numpy as np\nimport ray\n\nn, relativeTolerance = 20_000_000, 1e-4\nn1 = n//2; n2 = n - n1\nh = 1/(n-1)\n\nray.init(num_cpus=2, configure_logging=False)\n\nb = np.ones(n)*2*h*h\n\n@ray.remote\nclass ArrayStorage:\n    def __init__(self, n):\n        self.b = None         # persistent variable\n        self.u = np.zeros(n)  # persistent variable\n    def store_b(self, arr):   # store an array in the actor's state\n        self.b = arr\n    def get_u(self):\n        return self.u\n    def localSolve(self, n, ghostValue, side):\n        self.u[1:n-1] = (self.u[0:n-2] + self.u[2:n] - self.b[1:n-1]) / 2\n        if side == 1: self.u[n-1] = (self.u[n-2] + ghostValue - self.b[n-1]) / 2\n        if side == 2: self.u[0] = (ghostValue + self.u[1] - self.b[0]) / 2\n    def getLastValue(self):\n        return self.u[-1]\n    def getFirstValue(self):\n        return self.u[0]\n\nworker1 = ArrayStorage.remote(n1)\nworker2 = ArrayStorage.remote(n2)\n\nworker1.store_b.remote(b[0:n1])\nworker2.store_b.remote(b[n1:n])\n\ndiff = 999.0   # difference between two successive values of u1[-1]\ncount = 0\nstart = time()\nwhile abs(diff) &gt; relativeTolerance:\n    count += 1\n    ghostLeft = ray.get(worker1.getLastValue.remote()) if count &gt; 1 else 0.0\n    ghostRight = ray.get(worker2.getFirstValue.remote()) if count &gt; 1 else 0.0\n    diff = ghostLeft\n    worker1.localSolve.remote(n1, ghostRight, 1)   # compute new u1\n    worker2.localSolve.remote(n2, ghostLeft,  2)   # compute new u2\n    newLeft = ray.get(worker1.getLastValue.remote())\n    diff = (diff-newLeft) / newLeft\n    if count%100 == 0:\n        u1 = ray.get(worker1.get_u.remote())\n        u2 = ray.get(worker2.get_u.remote())\n        print(u1[-3:], u2[:3], diff)\n\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(\"Converged after\", count, \"iterations\", \"with diff =\", diff)\nu1 = ray.get(worker1.get_u.remote())\nu2 = ray.get(worker2.get_u.remote())\nprint(\"Solution:\", u1[-3:], u2[:3])\nTime in seconds: 184.577\nConverged after 9961 iterations with diff = -9.999348935138563e-05\nSolution: [-2.50966143e-11 -2.50990843e-11 -2.51015743e-11] [-2.51015743e-11 -2.50990843e-11 -2.50966143e-11]\n\n\nScalable version\nLet’s adapt this code to any number of partitions. We will use arrays of workers and cycle through all available cores:\n\na generic interval should now update ghost values on both sides, unless it is the left-most interval (only update the right ghost) or the right-most interval (only update the left ghost); we control this via the logical tuple side = (True/False, True/False) that we pass to localSolve() for each interval\nlet’s save this file in poissonDistributed.py\n\n##### this is poissonDistributed.py #####\nfrom time import time\nimport numpy as np\nimport ray\n\nn, relativeTolerance = 20_000_000, 1e-4\nh = 1/(n-1)\n\nncores = 4\nsize = n//ncores\nintervals = [(i*size,(i+1)*size-1) for i in range(ncores)]\nif n &gt; intervals[-1][1]:\n    intervals[-1] = (intervals[-1][0], n-1)\n\nb = np.ones(n)*2*h*h\n\nray.init(num_cpus=ncores, configure_logging=False, _system_config={ 'automatic_object_spilling_enabled':False })\n\n@ray.remote\nclass ArrayStorage:\n    def __init__(self, n):\n        self.b = None         # persistent variable\n        self.u = np.zeros(n)  # persistent variable\n        self.n = n            # persistent variable\n    def store_b(self, arr):   # store an array in the actor's state\n        self.b = arr\n    def get_u(self):\n        return self.u\n    def localSolve(self, ghostLeft, ghostRight, side):\n        n = self.n\n        self.u[1:n-1] = (self.u[0:n-2] + self.u[2:n] - self.b[1:n-1]) / 2\n        if side[0]: self.u[0] = (ghostLeft + self.u[1] - self.b[0]) / 2\n        if side[1]: self.u[n-1] = (self.u[n-2] + ghostRight - self.b[n-1]) / 2\n    def getLastValue(self):\n        return self.u[-1]\n    def getFirstValue(self):\n        return self.u[0]\n\nworkers = [ArrayStorage.remote(intervals[i][1]-intervals[i][0]+1) for i in range(ncores)]\n[workers[i].store_b.remote(b[intervals[i][0]:intervals[i][1]+1]) for i in range(ncores)]\n\ndiff = 999.0   # difference between two successive values of u1[-1]\ncount = 0\nghostRight = np.zeros(ncores)\nghostLeft = np.zeros(ncores)\nstart = time()\nwhile abs(diff) &gt; relativeTolerance:\n    count += 1\n    ghostLeft[1:ncores] = ray.get([workers[i-1].getLastValue.remote() for i in range(1, ncores)])\n    ghostRight[0:ncores-1] = ray.get([workers[i+1].getFirstValue.remote() for i in range(ncores-1)])\n    diff = ghostLeft[1]\n    workers[0].localSolve.remote(ghostLeft[0], ghostRight[0], (False,True))\n    [workers[i].localSolve.remote(ghostLeft[i], ghostRight[i],\n                                  (True,True)) for i in range(1,ncores-1)]\n    workers[ncores-1].localSolve.remote(ghostLeft[ncores-1], ghostRight[ncores-1], (True,False))\n    newLeft = ray.get(workers[0].getLastValue.remote())\n    diff = (diff-newLeft) / newLeft\n    if count%100 == 0:\n        u1 = ray.get(workers[0].get_u.remote())\n        u2 = ray.get(workers[1].get_u.remote())\n        print(u1[-3:], u2[:3], diff)\n\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(\"Converged after\", count, \"iterations\", \"with diff =\", diff)\nu1 = ray.get(workers[0].get_u.remote())\nu2 = ray.get(workers[1].get_u.remote())\nprint(\"Solution:\", u1[-3:], u2[:3])\nTime in seconds: 153.139\nConverged after 9961 iterations with diff = -9.999348935138563e-05\nSolution: [-2.50966143e-11 -2.50990843e-11 -2.51015743e-11] [-2.51015743e-11 -2.50990843e-11 -2.50966143e-11]\n\n\nImportant performance tip\nWhen launching multiple remote functions, instead of this:\nfor i in range(ncores):\n    ray.get(workers[i].localSolve.remote())   # this will block on each call\n\n\n\n\n\n\nNote\n\n\n\nThis would result in serial execution …\n\n\ndo this:\nsolve_refs = [workers[i].localSolve.remote() for i in range(ncores)]\nray.get(solve_refs)\n\n\n\n\n\n\nNote\n\n\n\nThis would result in parallel execution.\n\n\n\n\nRunning on one cluster node\nWe will start by running interactively on one cluster node:\nmodule load StdEnv/2023 python/3.12.4 arrow/17.0.0 scipy-stack/2024a netcdf/4.9.2\nsource /project/def-sponsor00/shared/pythonhpc-env/bin/activate\n\nsalloc --time=0:60:0 --mem-per-cpu=3600\npython poissonSerial.py   # 2289s, 9961 iterations\nexit\n\nsalloc --time=0:30:0 --ntasks=2 --mem-per-cpu=3600\npython poissonDual.py   # 1454s, 9961 iterations\nexit\n\nsalloc --time=0:30:0 --ntasks=4 --mem-per-cpu=3600\n&gt;&gt;&gt; ncores = 4\npython poissonDistributed.py   # 777s, 9961 iterations\nexit\nWhile it is running, you can probe its CPU/memory usage with srun --jobid=... --pty htop or srun --jobid=... --pty htop --filter \"ray::\".\n\n\nRunning on multiple cluster nodes\nNow let’s run the same problem on two cluster nodes following the steps we used for the slow-series problem on multiple nodes.\ncd ~/webinar\nmodule load StdEnv/2023 python/3.12.4 arrow/17.0.0 scipy-stack/2024a netcdf/4.9.2\nsource /project/def-sponsor00/shared/pythonhpc-env/bin/activate\n\nsalloc --time=0:60:0 --nodes=2 --ntasks-per-node=1 --cpus-per-task=2 --mem-per-cpu=3600\nexport HEAD_NODE=$(hostname)   # head node's address\nexport RAY_PORT=34567          # port to start Ray on the head node; unique from other users\n\necho \"Starting the Ray head node at $HEAD_NODE\"\nray start --head --node-ip-address=$HEAD_NODE --port=$RAY_PORT \\\n          --num-cpus=$SLURM_CPUS_PER_TASK --disable-usage-stats --block &\nsleep 10\n\necho \"Starting the Ray worker nodes on all other MPI tasks\"\nsrun launchWorkers.sh &\n\nray status   # should report 2 nodes, 4 cores\nIn my testing, once connected to a Ray cluster, Ray fails to bind Ray workers to the existing processes. With 2 nodes and 2 CPUs per node, it may start 1 worker on node 1 and 3 workers on node 2, or all 4 workers on one node with 2 CPUs. It seems that specifying the number of CPU cores in the function/class definition somehow fixes that:\n@ray.remote(num_cpus=1)\nclass ArrayStorage:\n...\nLet’s test this! Start Python and then run:\nimport time, ray, os\n\nray.init(address=f\"{os.environ['HEAD_NODE']}:{os.environ['RAY_PORT']}\",_node_ip_address=os.environ['HEAD_NODE'])\n\n@ray.remote\ndef my_task(task_id):\n    return f\"Task {task_id} completed on {ray.get_runtime_context().get_node_id()}\"\n\nr = [my_task.remote(i) for i in range(8)]   # create 4 tasks, one per each CPU\nprint(ray.get(r))\n['Task 0 completed on 37531b127109840b8d4f13c959147dbe9ce25ced5e802ba4ddddf06e',\n 'Task 1 completed on 37531b127109840b8d4f13c959147dbe9ce25ced5e802ba4ddddf06e',\n 'Task 2 completed on 37531b127109840b8d4f13c959147dbe9ce25ced5e802ba4ddddf06e',\n 'Task 3 completed on 37531b127109840b8d4f13c959147dbe9ce25ced5e802ba4ddddf06e']\nNow redefine the remote function:\n@ray.remote(num_cpus=1)  # each task uses 1 CPU\ndef my_task(task_id):\n    return f\"Task {task_id} completed on {ray.get_runtime_context().get_node_id()}\"\n\nr = [my_task.remote(i) for i in range(4)]   # create 4 tasks, one per each CPU\nprint(ray.get(r))\n['Task 0 completed on 37531b127109840b8d4f13c959147dbe9ce25ced5e802ba4ddddf06e',\n 'Task 1 completed on 37531b127109840b8d4f13c959147dbe9ce25ced5e802ba4ddddf06e',\n 'Task 2 completed on b3b0caba7c80a89fc381dad5c9312f1db713b8be18704a79e507412d',\n 'Task 3 completed on b3b0caba7c80a89fc381dad5c9312f1db713b8be18704a79e507412d']\nInside poissonDistributed.py, use the following to connect to the existing Ray cluster:\nimport os\nray.init(address=f\"{os.environ['HEAD_NODE']}:{os.environ['RAY_PORT']}\", _node_ip_address=os.environ['HEAD_NODE'])\nand the following to ensure CPU affinity:\nncores = 4\n\n@ray.remote(num_cpus=1)\nclass ArrayStorage:\n    ...\nNow run the numerical code:\npython -u poissonDistributed.py   # unbuffered stdout output\nWhile it is running, in a separate shell on the cluster:\nssh node1\nhtop --filter=\"ray::\"   # should see 2 busy CPU cores\nexit\nssh node2\nhtop --filter=\"ray::\"   # should see 2 busy CPU cores\nexit\nThe output should end with:\nTime in seconds: 817s   # slightly longer than 777s when using 4 cores on the same node\nConverged after 6901 iterations\n\n\nThis is on a cloud machine with a slow interconnect. On Cedar we get better scaling:\n\n\n\n\nncores\n1\n2\n4\n8\n16\n32\n\n\n\n\n1-node wallclock runtime (sec)\n2521\n1335\n1162\n\n\n\n\n\n2-node wallclock runtime (sec)\n\n\n805\n484\n\n\n\n\n4-node wallclock runtime (sec)\n\n\n\n\n309\n\n\n\n8-node wallclock runtime (sec)\n\n\n\n\n\n308\n\n\n\nAs is often the case with tightly coupled parallel problems, the speedup stalls for our small problem. To achieve further speedup, you need to increase the problem size per each CPU core."
  },
  {
    "objectID": "clusterWorkflows.html#other-ray-workloads",
    "href": "clusterWorkflows.html#other-ray-workloads",
    "title": "Running parallel Ray workflows",
    "section": "Other Ray workloads",
    "text": "Other Ray workloads\n\nOur Ray documentation https://docs.alliancecan.ca/wiki/Ray#Multiple_Nodes describes how to add GPUs into the scheduling mix for distributed Python Ray workflows.\n@ray.remote lets you run CuPy codes, see the discussion at https://github.com/ray-project/ray/issues/48217\nMars is a tensor-based framework to scale NumPy, Pandas, and scikit-learn applications on top of Ray:\n\nhttps://mars-project.readthedocs.io\nhttps://docs.ray.io/en/latest/ray-more-libs/mars-on-ray.html\nmight be worth a separate webinar?"
  },
  {
    "objectID": "clusterWorkflows.html#links",
    "href": "clusterWorkflows.html#links",
    "title": "Running parallel Ray workflows",
    "section": "Links",
    "text": "Links\n\nOfficial Ray documentation\nRay on the Alliance clusters\nDeploying Ray clusters on Slurm\nOur beginner’s Ray course (some overlap with this webinar)\nLaunching an on-premise cluster\nRay on GPUs: Ray docs and Stack Overflow\nAssigning multiple GPUs to a worker"
  },
  {
    "objectID": "julia-menu.html",
    "href": "julia-menu.html",
    "title": "Parallel computing in Julia",
    "section": "",
    "text": "March 20th (Part 1) and 27th (Part 2), 10:00am - noon Pacific Time\nJulia is a high-level programming language well suited for scientific computing and data science. Just-in-time compilation, among other things, makes Julia really fast yet interactive. For heavy computations, Julia supports multi-threaded and multi-process parallelism, both natively and via a number of external packages. It also supports memory arrays distributed across multiple processes either on the same or different nodes. In this hands-on workshop, we will start with Julia’s multi-threading features and then focus on Distributed multi-processing standard library and its large array of tools. We will demo parallelization using two problems: a slowly converging series and a Julia set. We will run examples on a multi-core laptop and an HPC cluster.\nInstructor: Alex Razoumov (SFU)\nPrerequisites: Ideally, some familiarity with the Alliance’s HPC cluster environment, in particular, with the Slurm scheduler. Having some previous serial Julia programming experience would help, but we will start slowly so you will be able to follow up even if you are new to Julia.\nSoftware: There are a couple of options:\n\nYou can run Julia on our training cluster, in which case you will need a remote secure shell (SSH) client installed on your computer. On Mac and Linux computers, SSH is usually pre-installed – try typing ssh in a terminal to make sure it is there. Many versions of Windows also provide an OpenSSH client by default –- try opening PowerShell and typing ssh to see if it is available. If not, then we recommend installing the free Home Edition of MobaXterm from https://mobaxterm.mobatek.net/download.html. We will provide guest accounts on our training cluster, and you would not need to install Julia on your computer in this setup.\nYou can run Julia on your own computer, in which case you can install it from https://julialang.org/downloads – this may take a while so please do this before the class.\n\n\n\n\n\nOur Julia webinars\nSince 2020, we’ve been teaching occasional webinars on parallel programming in Julia – watch the recordings here.\n\n\nExternal links\n\nThink Julia: How to Think Like a Computer Scientist by Ben Lauwens and Allen Downey is a very thorough introduction to non-parallel Julia for beginners\nJulia at Scale forum\nBaolai Ge’s (SHARCNET) November 2020 webinar Julia: Parallel computing revisited\nJulia performance tips\nHow to optimise Julia code: A practical guide\nA Deep Introduction to JIT Compilers",
    "crumbs": [
      "Front page"
    ]
  },
  {
    "objectID": "apptainer1/01-intro.html",
    "href": "apptainer1/01-intro.html",
    "title": "What is Apptainer (formerly Singularity)",
    "section": "",
    "text": "Until recently Apptainer was called Singularity. In November 2021 the guidance of parts of Singularity was transferred to the Linux Foundation, and that fully open-source component has been renamed Apptainer, while the commercial fork is still called Singularity.",
    "crumbs": [
      "What is Apptainer / Singularity"
    ]
  },
  {
    "objectID": "apptainer1/01-intro.html#why-use-a-container",
    "href": "apptainer1/01-intro.html#why-use-a-container",
    "title": "What is Apptainer (formerly Singularity)",
    "section": "Why use a container",
    "text": "Why use a container\nIdea: package and distribute the software environment along with the application, i.e. create a portable software environment.\nWhy: 1. avoid compiling complex software chains from scratch for the host’s Linux OS 1. run software in the environment where it might not be available as a package, or run older software 1. use a familiar software environment everywhere where you can run Apptainer, e.g. across different HPC centres - create a consistent testing environment independently of the underlying system - transfer pipelines from a test environment to a production environment 1. popular, but somewhat dubious reason: data reproducibility (use the same software environment as the authors  ➜  same result)",
    "crumbs": [
      "What is Apptainer / Singularity"
    ]
  },
  {
    "objectID": "apptainer1/01-intro.html#whywhen-not-to-use-a-container",
    "href": "apptainer1/01-intro.html#whywhen-not-to-use-a-container",
    "title": "What is Apptainer (formerly Singularity)",
    "section": "Why/when not to use a container",
    "text": "Why/when not to use a container\nDo not use Apptainer if your software is already installed on the Alliance clusters. Learning and understanding Apptainer is more difficult than learning how to use our software modules or pre-compiled Python packages.\nCreate your own Apptainer images only if you have a compelling reason to require a custom image. In my experience, 95% of those who think they need one actually don’t.",
    "crumbs": [
      "What is Apptainer / Singularity"
    ]
  },
  {
    "objectID": "apptainer1/01-intro.html#installingrunning-apptainer-on-your-own-computer",
    "href": "apptainer1/01-intro.html#installingrunning-apptainer-on-your-own-computer",
    "title": "What is Apptainer (formerly Singularity)",
    "section": "Installing/running Apptainer on your own computer",
    "text": "Installing/running Apptainer on your own computer\nApptainer was really developed for use on HPC cluster, but there are ways to run it on your own computer:\n\nOn Linux install and use Apptainer software. When running a longer version of this course after a Cloud course, we install Apptainer as a package inside our VM.\nOn any host OS: in a VM running Linux.\nOn Windows or MacOS: inside Vagrant.\nOn Windows or MacOS: inside Docker (download a Docker image with Apptainer installed).",
    "crumbs": [
      "What is Apptainer / Singularity"
    ]
  },
  {
    "objectID": "apptainer1/01-intro.html#glossary",
    "href": "apptainer1/01-intro.html#glossary",
    "title": "What is Apptainer (formerly Singularity)",
    "section": "Glossary",
    "text": "Glossary\nAn image is a bundle of files including an operating system, software and potentially data and other application-related files. Apptainer uses the Singularity Image Format (SIF), and images are provided as single .sif files.\nA container is a virtual environment that is based on an image. You can start multiple container instances from an image.\nAn operating system (OS) is all the software that let you interact with a computer, run applications, UI, etc, consists of the “kernel” and “userland” parts.\nA kernel is the central piece of software that manages hardware and provides resources (CPU, I/O, memory, devices, filesystems) to the processes it is running.\nA filesystem is an organized collection of files. Under UNIX/Linux, there is a single hierarchy under /, and additional filesystems are “mounted” somewhere under that hierarchy.",
    "crumbs": [
      "What is Apptainer / Singularity"
    ]
  },
  {
    "objectID": "apptainer1/01-intro.html#containers-vs-virtual-machines",
    "href": "apptainer1/01-intro.html#containers-vs-virtual-machines",
    "title": "What is Apptainer (formerly Singularity)",
    "section": "Containers vs virtual machines",
    "text": "Containers vs virtual machines\n\nContainer = the OS-level mechanism to isolate some parts of the OS along with a given application.\n\nvirtualizes an operating system\nlets you run an application compiled for a specific Linux OS on another Linux OS\nalmost no performance overhead\n\nVirtual machine (VM) = complete isolation from the host OS via virtualized hardware\n\nvirtualizes hardware\nmaximum flexibility, can mix any combination of host and guest OS’s\nsignificant performance overhead, as you run on simulated hardware\n\n\nDocker: container platform for services, runs as root on the host system, uses cgroups for resource management between different VMs on a given node, very popular with software developers, can’t really use it on HPC systems (no root or sudo possible for users on clusters + cgroups resource management will conflict with HPC resource managers).\nApptainer: run containers entirely in user space, as a user, can use existing Docker containers (Apptainer will convert them to proper SIF images for you), works seamlessly with the schedulers.\nThere are few other container engines focusing on specific features.",
    "crumbs": [
      "What is Apptainer / Singularity"
    ]
  },
  {
    "objectID": "apptainer1/01-intro.html#apptainer-on-hpc-systems",
    "href": "apptainer1/01-intro.html#apptainer-on-hpc-systems",
    "title": "What is Apptainer (formerly Singularity)",
    "section": "Apptainer on HPC systems",
    "text": "Apptainer on HPC systems\n\n\n\n\n\n\nCautionTraining cluster\n\n\n\nWe will now distribute usernames and passwords for our training cluster.\n\n\nLet’s log in to the training cluster cass.vastcloud.org and try loading Apptainer:\nmodule load apptainer/1.2.4   # the default version at the time of writing\napptainer --version\napptainer                     # see the list of available commands\n\n\n\n\n\n\nNote\n\n\n\nApart from this short example, please do not run Apptainer on a cluster’s login node. Apptainer can be quite resource-demanding, so we will run on a compute node inside a Slurm job. I will explain how to do that in the next section. The same applies to our production clusters: always schedule either an interactive or a batch job to run Apptainer workflows.",
    "crumbs": [
      "What is Apptainer / Singularity"
    ]
  },
  {
    "objectID": "apptainer1/04-advanced.html",
    "href": "apptainer1/04-advanced.html",
    "title": "Advanced Apptainer usage",
    "section": "",
    "text": "Apptainer supports Nvidia GPUs through bind-mounting the GPU drivers and the base CUDA libraries. The --nv flag does it transparently to the user, e.g.\napptainer pull tensorflow.sif docker://tensorflow/tensorflow\nls -l tensorflow.sif   # 415M\nsalloc --gres=gpu:p100:1 --cpus-per-task=8 --mem=40Gb --time=2:0:0 --account=...\napptainer exec --nv -B /scratch/${USER}:/scratch tensorflow.sif python my-tf.py\n\n\n\n\n\n\nImportantKey point\n\n\n\nUse --nv to expose the NVIDIA hardware devices to the container.\n\n\n\n\n\nNVIDIA NGC (NVIDIA’s hub for GPU-optimized software) provides prebuilt containers for a large number of HPC applications. Try searching for TensorFlow, GAMESS (quantum chemistry), GROMACS and NAMD (molecular dynamics), VMD, ParaView, NVIDIA IndeX (visualization). Their GPU-accelerated containers are quite large, so it might take a while to build them, e.g.\napptainer pull tensorflow-22.06-tf1-py3.sif docker://nvcr.io/nvidia/tensorflow:22.06-tf1-py3\nls -l tensorflow-22.06-tf1-py3.sif   # 5.9G (very large!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecently, I built a container for using GPUs from Chapel programming language. There are some issues compiling Chapel with LLVM and GPU support on InfiniBand clusters, so I bypassed them by creating a container that can be used on any of our clusters, whether built on InfiniBand, or Ethernet, or OmniPath interconnect.\n\nthe container can be used only on one node (no multi-node parallelism), but it does support multiple GPUs\ninstalled Chapel into an overlay image (we’ll study overlays below)\n\nHere is how I would use it on Cedar:\ncd ~/scratch\nsalloc --time=0:30:0 --nodes=1 --cpus-per-task=1 --mem-per-cpu=3600 --gpus-per-node=v100l:1 \\\n       --account=cc-debug --reservation=asasfu_756\nnvidia-smi   # verify that we can see the GPU on the node\ngit clone ~/chapelBare $SLURM_TMPDIR/\n\nmodule load apptainer\nexport SRC=/project/6003910/razoumov/apptainerImages/chapelGPU20240826\napptainer shell --nv -B $SLURM_TMPDIR --overlay $SRC/extra.img:ro $SRC/almalinux.sif\n\nnvidia-smi   # verify that we can see the GPU inside the container\nsource /extra/c4/chapel-2.1.0/util/setchplenv.bash\nexport CHPL_GPU=nvidia\nexport CHPL_CUDA_PATH=/usr/local/cuda-12.4\nexport PATH=$PATH:/usr/local/cuda-12.4/bin\ncd $SLURM_TMPDIR/gpu\nchpl --fast probeGPU.chpl -L/usr/local/cuda-12.4/targets/x86_64-linux/lib/stubs\n./probeGPU\nLet me know if you are interested in playing with it on a machine with an NVIDIA GPU, and I can share both container files with you.",
    "crumbs": [
      "Advanced Apptainer usage"
    ]
  },
  {
    "objectID": "apptainer1/04-advanced.html#running-gpu-programs-using-cuda",
    "href": "apptainer1/04-advanced.html#running-gpu-programs-using-cuda",
    "title": "Advanced Apptainer usage",
    "section": "",
    "text": "Apptainer supports Nvidia GPUs through bind-mounting the GPU drivers and the base CUDA libraries. The --nv flag does it transparently to the user, e.g.\napptainer pull tensorflow.sif docker://tensorflow/tensorflow\nls -l tensorflow.sif   # 415M\nsalloc --gres=gpu:p100:1 --cpus-per-task=8 --mem=40Gb --time=2:0:0 --account=...\napptainer exec --nv -B /scratch/${USER}:/scratch tensorflow.sif python my-tf.py\n\n\n\n\n\n\nImportantKey point\n\n\n\nUse --nv to expose the NVIDIA hardware devices to the container.\n\n\n\n\n\nNVIDIA NGC (NVIDIA’s hub for GPU-optimized software) provides prebuilt containers for a large number of HPC applications. Try searching for TensorFlow, GAMESS (quantum chemistry), GROMACS and NAMD (molecular dynamics), VMD, ParaView, NVIDIA IndeX (visualization). Their GPU-accelerated containers are quite large, so it might take a while to build them, e.g.\napptainer pull tensorflow-22.06-tf1-py3.sif docker://nvcr.io/nvidia/tensorflow:22.06-tf1-py3\nls -l tensorflow-22.06-tf1-py3.sif   # 5.9G (very large!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecently, I built a container for using GPUs from Chapel programming language. There are some issues compiling Chapel with LLVM and GPU support on InfiniBand clusters, so I bypassed them by creating a container that can be used on any of our clusters, whether built on InfiniBand, or Ethernet, or OmniPath interconnect.\n\nthe container can be used only on one node (no multi-node parallelism), but it does support multiple GPUs\ninstalled Chapel into an overlay image (we’ll study overlays below)\n\nHere is how I would use it on Cedar:\ncd ~/scratch\nsalloc --time=0:30:0 --nodes=1 --cpus-per-task=1 --mem-per-cpu=3600 --gpus-per-node=v100l:1 \\\n       --account=cc-debug --reservation=asasfu_756\nnvidia-smi   # verify that we can see the GPU on the node\ngit clone ~/chapelBare $SLURM_TMPDIR/\n\nmodule load apptainer\nexport SRC=/project/6003910/razoumov/apptainerImages/chapelGPU20240826\napptainer shell --nv -B $SLURM_TMPDIR --overlay $SRC/extra.img:ro $SRC/almalinux.sif\n\nnvidia-smi   # verify that we can see the GPU inside the container\nsource /extra/c4/chapel-2.1.0/util/setchplenv.bash\nexport CHPL_GPU=nvidia\nexport CHPL_CUDA_PATH=/usr/local/cuda-12.4\nexport PATH=$PATH:/usr/local/cuda-12.4/bin\ncd $SLURM_TMPDIR/gpu\nchpl --fast probeGPU.chpl -L/usr/local/cuda-12.4/targets/x86_64-linux/lib/stubs\n./probeGPU\nLet me know if you are interested in playing with it on a machine with an NVIDIA GPU, and I can share both container files with you.",
    "crumbs": [
      "Advanced Apptainer usage"
    ]
  },
  {
    "objectID": "apptainer1/04-advanced.html#running-mpi-programs-from-within-a-container",
    "href": "apptainer1/04-advanced.html#running-mpi-programs-from-within-a-container",
    "title": "Advanced Apptainer usage",
    "section": "Running MPI programs from within a container",
    "text": "Running MPI programs from within a container\nMPI (message passing interface) is the industry standard for distributed-memory parallel programming. There are several implementations: OpenMPI, MPICH, and few others.\nMPI libraries on HPC systems usually depend on various lower-level runtime libraries – interconnect, RDMA (Remote Direct Memory Access), PMI (process management interface) and others – that vary from one HPC cluster to another, so they are hard to containerize. Thus no generic --mpi flag could be implemented for containers that would work across the network on different HPC clusters.\nThe official Apptainer documentation provides a good overview of running MPI codes inside containers. There are 3 possible modes of running MPI programs with Apptainer:\n\n1. Rely on MPI inside the container\n\nIn the MPI-inside-the-container mode you would start a single Apptainer process on the host:\napptainer exec -B ... --pwd ... container.sif mpirun -np $SLURM_NTASKS ./mpicode\nThe cgroup limitations from the Slurm job are passed into the container which sets the number of available CPU cores inside the container. In this setup the command mpirun uses all available (to this job) CPU cores.\n\n limited to a single node …\n\n no need to adapt container’s MPI to the host; just install SSH into the container\n\n can build a generic container that will work across multiple HPC clusters (each with a different setup)\n\n\n\n2. Hybrid mode\nHybrid mode uses host’s MPI to spawn MPI processes, and MPI inside the container to compile the code and provide runtime MPI libraries. In this mode you would start a separate Apptainer process for each MPI rank:\n\nmpirun -np $SLURM_NTASKS apptainer exec -B ... --pwd ... container.sif ./mpicode\n\n can span multiple nodes\n\n container’s MPI should be configured to support the same process management mechanism and version (e.g. PMI2 / PMIx) as the host – not that difficult with a little bit of technical knowledge (reach to us for help)\n\n\n\n\n\n\n3. Bind mode\nBind mode bind-mount host’s MPI libraries and drivers into the container and use exclusively them, i.e. there is no MPI inside the container. The MPI code will need to be compiled (when building the container) with a version of MPI similar to that of the host – typically that MPI will reside on the build node used to build the container, but will not be installed inside the container.\nI have zero experience with this mode, so I won’t talk about it here in detail.\n\n\n\n\nExample: hybrid-mode MPI\nFor this course I have already built an MPI container that can talk to MPI on the training cluster. For those interested, you can find the detailed instructions here, but in nutshell I created a definition file that:\n\nbootstraps from docker://ubuntu:22.04\ninstalls the necessary fabric and PMI packages, Slurm client, few others requirements\n\nand then used it to build mpi.sif which I copied over to the training cluster into /project/def-sponsor00/shared.\n\n\n\n\n\ncd ~/tmp\nmodule load openmpi apptainer\nunzip /project/def-sponsor00/shared/introHPC.zip codes/distributedPi.c\ncd codes\nmkdir -p ~/.openmpi\necho \"btl_vader_single_copy_mechanism=none\" &gt;&gt; ~/.openmpi/mca-params.conf\nexport PMIX_MCA_psec=native   # allow mpirun to use host's PMI\nexport CONTAINER=/project/def-sponsor00/shared/mpi.sif\napptainer exec $CONTAINER mpicc -O2 distributedPi.c -o distributedPi\nsalloc --ntasks=4 --time=0:5:0 --mem-per-cpu=1200\nmpirun -np $SLURM_NTASKS ./distributedPi   # error: compiled for Ubuntu, cannot run on Rocky Linux\nmpirun -np $SLURM_NTASKS apptainer exec $CONTAINER ./distributedPi\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: WRF container with self-contained MPICH\nThese instructions describe building a WRF Apptainer image following this build script. This container is large (8.1GB compressed SIF file, 47GB uncompressed sandbox) and includes everything but the kitchen sink, including multiple perl and Python 3 libraries and 3rd-party packages. It was created for a support ticket, but what’s important for us is that it also installs MPICH entirely inside the container, not relying on host’s OpenMPI. This means that we’ll be limited to MPI runs on one node.\nTo run an MPI code inside this container, it is important to pass -e to Apptainer to avoid loading MPI from the host:\ncd ~/scratch\nmodule load apptainer/1.2.4\nsalloc --time=1:0:0 --ntasks=4 --mem-per-cpu=3600 --account=def-razoumov-ac\nexport APPTAINERENV_NTASKS=$SLURM_NTASKS\napptainer shell -e --pwd $PWD wrf.sif\n\nexport PATH=/data/WRF/Libs/MPICH/bin:$PATH\n\ncat &lt;&lt; EOF &gt; distributedPi.c\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n#include &lt;mpi.h&gt;\nint main(int argc, char *argv[])\n{\n  double total, h, sum, x;\n  long long int i, n = 1e10;\n  int rank, numprocs;\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  h = 1./n;\n  sum = 0.;\n  if (rank == 0)\n    printf(\"Calculating PI with %d processes\\n\", numprocs);\n  printf(\"process %d started\\n\", rank);\n  for (i = rank+1; i &lt;= n; i += numprocs) {\n    x = h * ( i - 0.5 );    //calculate at center of interval\n    sum += 4.0 / ( 1.0 + pow(x,2));\n  }\n  sum *= h;\n  MPI_Reduce(&sum,&total,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n  if (rank == 0)\n    printf(\"%.17g\\n\", total);\n  MPI_Finalize();\n  return 0;\n}\nEOF\n\nmpicc distributedPi.c -o mpi\nmpirun -np $NTASKS ./mpi",
    "crumbs": [
      "Advanced Apptainer usage"
    ]
  },
  {
    "objectID": "apptainer1/04-advanced.html#overlays",
    "href": "apptainer1/04-advanced.html#overlays",
    "title": "Advanced Apptainer usage",
    "section": "Overlays",
    "text": "Overlays\nIn the container world, an overlay image is a file formatted as a filesystem. To the host filesystem it is a single file. When you mount it into a container, the container will see a filesystem with many files.\nAn overlay mounted into an immutable SIF image lets you store files without rebuilding the image. For example, you can store your computation results, or compile/install software into an overlay.\nAn overlay can be:\n\na standalone writable ext3 filesystem image (most useful),\na sandbox directory,\na writable ext3 image embedded into the SIF file.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you write millions of files, do not store them on a cluster filesystem – instead, use an Apptainer overlay file for that. Everything inside the overlay will appear as a single file to the cluster filesystem.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe direct apptainer overlay command requires Singularity 3.8 / Apptainer 1.0 or later and a relatively recent set of filesystem tools, e.g. it won’t work in a CentOS7 VM. It should work on a VM or a cluster running Rocky Linux 8.5 or later.\n\n\n\ncd ~/tmp\napptainer pull ubuntu.sif docker://ubuntu:latest\nmodule load apptainer\nsalloc --time=0:30:0 --mem-per-cpu=3600\napptainer overlay create --size 512 small.img   # create a 0.5GB overlay image file\napptainer shell --overlay small.img ubuntu.sif\n\n\n\n\n\n\nInside the container any newly-created top-level directory will go into the overlay filesystem:\nApptainer&gt; df -kh             # the overlay should be mounted inside the container\nApptainer&gt; mkdir -p /data     # by default this will go into the overlay image\nApptainer&gt; cd /data\nApptainer&gt; df -kh .           # using overlay; check for available space\nApptainer&gt; for num in $(seq -w 00 19); do\n             echo $num\n             # generate a binary file (1-33)MB in size\n             dd if=/dev/urandom of=test\"$num\" bs=1024 count=$(( RANDOM + 1024 ))\n           done\nApptainer&gt; df -kh .     # should take ~300-400 MB\nIf you exit the container and then mount the overlay again, your files will be there:\napptainer shell --overlay small.img ubuntu.sif\n\nApptainer&gt; ls /data     # here is your data\nYou can also create a new overlay image with a directory inside with something like:\napptainer overlay create --create-dir /data --size 512 overlay.img   # create an overlay with a directory\nIf you want to mount the overlay in the read-only mode:\napptainer shell --overlay small.img:ro ubuntu.sif\nApptainer&gt;  touch /data/test.txt    # error: read-only file system\n\n\n\n\n\n\nNote\n\n\n\nInto the same container at the same time, you can mount many read-only overlays, but only one writable overlay.\n\n\nTo see the help page on overlays (these two commands are equivalent):\napptainer help overlay create\napptainer overlay create --help\n\nSparse overlay images\nSparse images use disk more efficiently when blocks allocated to them are mostly empty. As you add more data to a sparse image, it can grow (but not shrink!) in size. Let’s create a sparse overlay image:\napptainer overlay create --size 512 --sparse sparse.img\nls -l sparse.img                   # its apparent size is 512MB\ndu -h --apparent-size sparse.img   # same\ndu -h sparse.img                   # its actual size is much smaller (17MB)\nLet’s mount it and fill with some data, now creating fewer files:\napptainer shell --overlay sparse.img ubuntu.sif\n\nApptainer&gt; mkdir -p /data && cd /data\nApptainer&gt; for num in $(seq -w 0 4); do\n             echo $num\n             # generate a binary file (1-33)MB in size\n             dd if=/dev/urandom of=test\"$num\" bs=1024 count=$(( RANDOM + 1024 ))\n           done\nApptainer&gt; df -kh .     # should take ~75-100 MB, pay attention to \"Used\"\n\ndu -h sparse.img        # shows actual usage\n\n\n\n\n\n\nNote\n\n\n\nBe careful with sparse images: not all tools (e.g. backup/restore, scp, sftp, gunzip) recognize sparsefiles ⇒ this can potentially lead to data loss and other bad things …\n\n\n\n\nExample: installing Conda into an overlay\nInstalling native Anaconda on HPC clusters is a bad idea for a number of reasons. Instead of Conda, we recommend using virtualenv together with our pre-compiled Python wheels to install Python packages into your own virtual environments.\nOne of the reasons we do not recommend Conda is that it creates a large number of files in your directories. You can alleviate this problem by hiding Conda files inside an overlay image:\n\ntakes a couple of minutes, results in 22k+ files that are hidden from the host\nno need for root, as you don’t modify the container image\nstill might not be the most efficient use of resources (non-optimized binaries)\n\nHere is one way you could install Conda into an overlay image:\ncd ~/tmp\napptainer pull ubuntu.sif docker://ubuntu:latest\napptainer overlay create --size 1200 conda.img   # create a 1200M overlay image\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n\napptainer shell --overlay conda.img -B /home ubuntu.sif\n\nApptainer&gt; mkdir /conda && cd /conda\nApptainer&gt; df -kh .\nApptainer&gt; bash /home/${USER}/tmp/miniconda.sh\n  agree to the license\n  use /conda/miniconda3 for the installation path\n  no to initialize Miniconda3\nApptainer&gt; find /conda/miniconda3/ -type f | wc -l   # 22,637 files\nApptainer&gt; df -kh .   # uses ~695M once finished, but more was used during installation\nThese 22k+ files appear as a single file to the Lustre metadata server (which is great!).\napptainer shell ubuntu.sif\nApptainer&gt; ls /conda                            # no such file or directory\n\napptainer shell --overlay conda.img ubuntu.sif\nApptainer&gt; source /conda/miniconda3/bin/activate\n(base) Apptainer&gt; type python   # /conda/miniconda3/bin/python\n(base) Apptainer&gt; python        # works\nIf you want to install large Python packages, you probably want to resize the image:\ne2fsck -f conda.img         # check your overlay's filesystem first (required step)\nresize2fs -p conda.img 2G   # resize your overlay\nls -l conda.img             # should be 2GB\nNext mount the resized overlay into the container, and make sure to pass the -C flag to force writing config files locally (and not to the host):\napptainer shell -C --overlay conda.img ubuntu.sif   # /home/$USER won't be available\n\nApptainer&gt; cd /conda/miniconda3\nApptainer&gt; source bin/activate\n(base) Apptainer&gt; conda install numpy\n(base) Apptainer&gt; df -kh .   # so far used 1.8G out of 2.0G\n(base) Apptainer&gt; python\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.pi",
    "crumbs": [
      "Advanced Apptainer usage"
    ]
  },
  {
    "objectID": "apptainer1/04-advanced.html#running-container-instances",
    "href": "apptainer1/04-advanced.html#running-container-instances",
    "title": "Advanced Apptainer usage",
    "section": "Running container instances",
    "text": "Running container instances\nYou can also run backgrounded processes within your container while not being inside your container. You can start/terminate these with instance start/instance stop. All these processes will terminate once your job ends.\nmodule load apptainer\nsalloc --cpus-per-task=1 --time=0:30:0 --mem-per-cpu=3600\napptainer instance start ubuntu.sif test01     # start a container instance test01\napptainer shell instance://test01   # start an interactive shell in that instance\nbash -c 'for i in {1..60}; do sleep 1; echo $i; done' &gt; dump.txt &   # start a 60-sec background process\nexit        # and then exit; the instance and the process are still running\napptainer exec instance://test01 tail -3 dump.txt   # check on the process in that instance\napptainer exec instance://test01 tail -3 dump.txt   # and again\napptainer shell instance://test01                   # poke around the instance\napptainer instance list\napptainer instance stop test01",
    "crumbs": [
      "Advanced Apptainer usage"
    ]
  },
  {
    "objectID": "apptainer1/04-advanced.html#best-practices-on-production-clusters",
    "href": "apptainer1/04-advanced.html#best-practices-on-production-clusters",
    "title": "Advanced Apptainer usage",
    "section": "Best practices on production clusters",
    "text": "Best practices on production clusters\n\nDo not build containers on networked filesystems\nDon’t use /home or /scratch or /project to build a container – instead, always use a local disk, e.g. /localscratch on login nodes or $SLURM_TMPDIR inside a Slurm job. After having built it, you can move the container to a regular filesystem.\n\n\nThe importance of temp space when running large workflows\nBy default, for its internal use Apptainer allocates some temporary space in /tmp which is often in RAM and is very limited. When it becomes full, Apptainer will stop working, so you might want to give it another, larger temporary space via the -W flag. In practice, this would mean doing something like:\n\non your own computer or on a production cluster’s login node:\n\nmkdir /localscratch/tmp\napptainer shell/exec/run ... -W /localscratch/tmp &lt;image.sif&gt;\n\ninside a Slurm job:\n\nmkdir $SLURM_TMPDIR/tmp\napptainer shell/exec/run ... -W $SLURM_TMPDIR/tmp &lt;image.sif&gt;\n\n\n\n\n\n\nNote\n\n\n\nRegard /tmp inside the container as temporary space. Any files you put there will disappear the next time you start the container.\n\n\nYou can use an environment variable in lieu of -W:\nexport APPTAINER_TMPDIR=$SLURM_TMPDIR/tmp\napptainer shell/exec/run ... &lt;image.sif&gt;\n\n\nSample job submission script\nOf all clusters in the Alliance only Cedar has Internet access from compute nodes – this might limit your options of where to build a container. You can move your SIF file to other clusters after having built it.\n#!/bin/bash\n#SBATCH --time=...\n#SBATCH --mem=...\n#SBATCH --account=def-...\ncd $SLURM_TMPDIR\nmkdir -p tmp cache\nexport APPTAINER_TMPDIR=${PWD}/tmp\nexport APPTAINER_CACHEDIR=${PWD}/cache   # replaces default `$HOME/.apptainer/cache`\n&lt;build the container in this directory&gt;  # run on Cedar if docker:// access is needed\n&lt;run your workflow inside the container&gt;\n&lt;copy out your results&gt;",
    "crumbs": [
      "Advanced Apptainer usage"
    ]
  },
  {
    "objectID": "apptainer1/04-advanced.html#placeholder-running-multi-locale-chapel-from-a-container",
    "href": "apptainer1/04-advanced.html#placeholder-running-multi-locale-chapel-from-a-container",
    "title": "Advanced Apptainer usage",
    "section": "Placeholder: running multi-locale Chapel from a container",
    "text": "Placeholder: running multi-locale Chapel from a container\n\nUseful if Chapel is not installed natively at your HPC centre.\nSomewhat tricky for multi-locale Chapel due to its dependence on the cluster’s parallel launcher and interconnect.\nPiece of cake for single-locale Chapel and for emulated multi-locale Chapel.",
    "crumbs": [
      "Advanced Apptainer usage"
    ]
  },
  {
    "objectID": "meetup20250910.html",
    "href": "meetup20250910.html",
    "title": "Couple of side training projects that use Chapel",
    "section": "",
    "text": "September 10th, 2025\nThese sessions are not focused on teaching Chapel itself, but rather on providing an opportunity to introduce the audience – academic researchers from diverse fields and our HPC users – to Chapel while covering other topics."
  },
  {
    "objectID": "meetup20250910.html#generating-a-large-ensemble-of-solutions-for-model-training",
    "href": "meetup20250910.html#generating-a-large-ensemble-of-solutions-for-model-training",
    "title": "Couple of side training projects that use Chapel",
    "section": "Generating a large ensemble of solutions for model training",
    "text": "Generating a large ensemble of solutions for model training\nThe goal of this 4-hour ML course is to train a generative AI model on an ensemble of solutions from a 2D advection solver, and then use the model to predict solutions based on initial conditions. I am using a Chapel code to generate the solutions for model training.\n\n\n\n\n\n\nCautionNumerical problem\n\n\n\n Consider the following acoustic wave equation with \\(c=1\\) (speed of sound), written as a system separately for velocity and pressure: \\[\n\\begin{cases}\n   \\partial\\vec{v}/\\partial t = -\\nabla p\\\\\\\n   \\partial p/\\partial t = -\\nabla\\cdot\\vec{v}\n\\end{cases}\n\\] \n\n\nWriting a full 3D solver is straightforward, but to reduce the model training time, we will implement it in 2D. Here is a multi-threaded implementation of this solver in Chapel acoustic2D.chpl:\nuse Image, Math, IO, sciplot, Time;\n\nconfig const n = 501, nt = 350, nout = 5; // resolution, max time steps, plotting frequency\nconfig const ANIMATION = true;\n\nvar h = 1.0 / (n-1);\nconst mesh = {1..n, 1..n};\nconst largerMesh = {0..n+1, 0..n+1};\nconst a = 0.1;   // 0.1 - thick front, no post-wave oscillations; 0.5 - narrow front, large oscillations\nvar Vx: [largerMesh] real;\nvar Vy: [largerMesh] real;\n\nvar P: [largerMesh] real = [(i,j) in largerMesh] exp(-(a/h)**2*(((j-1)*h-0.5)**2));\n\nvar colour: [1..n, 1..n] 3*int;\nvar cmap = readColourmap('viridis.csv');   // cmap.domain is {1..256, 1..3}\n\nif ANIMATION then plotPressure(0);\n\nvar dt = h / 1.6;\nvar watch: stopwatch;\nwatch.start();\nfor count in 1..nt {\n  forall (i,j) in mesh {\n    Vx[i,j] -= dt * (P[i,j]-P[i-1,j]) / h;\n    Vy[i,j] -= dt * (P[i,j]-P[i,j-1]) / h;\n  }\n  forall (i,j) in mesh do\n    P[i,j] -= dt * (Vx[i+1,j] - Vx[i,j] + Vy[i,j+1] - Vy[i,j]) / h;\n  periodic(P);\n  periodic(Vx);\n  periodic(Vy);\n  if count%nout == 0 && ANIMATION then plotPressure(count/nout);\n}\nwatch.stop();\nwriteln('Simulation took ', watch.elapsed(), ' seconds');\n\nproc periodic(A) {\n  A[0,1..n] = A[n,1..n]; A[n+1,1..n] = A[1,1..n]; A[1..n,0] = A[1..n,n]; A[1..n,n+1] = A[1..n,1];\n}\n\nproc plotPressure(counter) {\n  var smin = min reduce(P);\n  var smax = max reduce(P);\n  for i in 1..n {\n    for j in 1..n {\n      var idx = ((P[i,j]-smin)/(smax-smin)*255):int + 1; //scale to 1..256\n      colour[i,j] = ((cmap[idx,1]*255):int, (cmap[idx,2]*255):int, (cmap[idx,3]*255):int);\n    }\n  }\n  var pixels = colorToPixel(colour);   // array of pixels\n  var filename: string;\n  if counter &gt;= 1000 then filename = counter:string;\n  else if counter &gt;= 100 then filename = \"0\"+counter:string;\n  else if counter &gt;= 10 then filename = \"00\"+counter:string;\n  else filename = \"000\"+counter:string;\n  writeln(\"writing \", filename);\n  writeImage(\"frame\"+filename+\".png\", imageType.png, pixels);\n}\nwith the following sciplot.chpl that reads a colour map from a CSV file:\nuse IO;\nuse List;\n\nproc readColourmap(filename: string) {\n  var reader = open(filename, ioMode.r).reader();\n  var line: string;\n  if (!reader.readLine(line)) then   // skip the header\n    halt(\"ERROR: file appears to be empty\");\n  var dataRows : list(string); // a list of lines from the file\n  while (reader.readLine(line)) do   // read all lines into the list\n    dataRows.pushBack(line);\n  var cmap: [1..dataRows.size, 1..3] real;\n  for (i, row) in zip(1..dataRows.size, dataRows) {\n    var c1 = row.find(','):int;    // position of the 1st comma in the line\n    var c2 = row.rfind(','):int;   // position of the 2nd comma in the line\n    cmap[i,1] = row[0..c1-1]:real;\n    cmap[i,2] = row[c1+1..c2-1]:real;\n    cmap[i,3] = row[c2+1..]:real;\n  }\n  reader.close();\n  return cmap;\n}\nTo run it:\nchpl --fast acoustic2D.chpl\n./acoustic2D --nt=5000\nFor a single point source:\n\nVideo\n\nFor two point sources:\n\nVideo\n\n\nSome representative solutions\nHere are solutions for point sources at the same fixed time:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince advection proceeds in the direction of the pressure gradient, a 1D initial condition produces a flat 1D wave:\n\n\n\n\n\n\n\nHere combining 0D (point-source) and 1D features:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plan is to feed thousands of these pairs of images – the initial conditions and the solution at the same fixed time – to a deep neural network, to train it to produce the solution from arbitrary initial conditions.\n\n\n\n\n\n\nNote\n\n\n\nCould also demo a \\(4001^2\\) solution clip ~/training/jax/large.mkv with 10_000 time steps.\n\n\n\n\nBenchmarking Chapel vs. Julia\nOut of curiosity, I implemented exactly the same solver in Julia using ParallelStencil.jl, state-of-the-art Julia package that can run in parallel using either multi-core CPUs (on top of Base.Threads) or GPUs (on top of CUDA.jl, AMDGPU.jl, or Metal.jl). I ran both codes on the same machine (M1 MacBook Pro), for the same problem size (\\(501^2\\), 5000 time steps), turning off animation in both cases.\n\n\n\ncode\n1 CPU thread\n8 CPU threads\nMetal\n\n\n\n\nChapel\n1.716s\n0.471s\n\n\n\nJulia\n7.29s\n2.60s\n23.59s   (problem too small)\n\n\n\nFor the record, in Julia you see a speedup on the M1 GPU in 3D at \\(512^3\\): with 9000 time steps going from 8 CPU threads to Metal reduces the runtime from 39m to 11m40s."
  },
  {
    "objectID": "meetup20250910.html#gpu-efficiency-on-nvidias-h100-cards",
    "href": "meetup20250910.html#gpu-efficiency-on-nvidias-h100-cards",
    "title": "Couple of side training projects that use Chapel",
    "section": "GPU efficiency on NVIDIA’s H100 cards",
    "text": "GPU efficiency on NVIDIA’s H100 cards\nThese GPUs can be challenging to utilize effectively: most off-the-shelf software packages achieve well below 75% utilization on a full GPU. I plan to use a simple Chapel code to demonstrate profiling steps. The challenge is that my current code is too efficient, maintaining 100% GPU usage. I’ve manually degraded its efficiency for demonstration purposes, but I’d prefer to start with a less efficient version and then optimize it, so perhaps I need to consider a different numerical problem.\n\n\n\n\n\n\nCautionNumerical problem\n\n\n\n Prime factorization of an integer number is finding all its prime factors. For example, the prime factors of 60 are 2, 2, 3, and 5. Let’s write a function that takes an integer number and returns the sum of all its prime factors. For example, for 60 it will return 2+2+3+5 = 12, for 91 it will return 7+13 = 20, for 101 it will return 101, and so on.    Since 1 has no prime factors, we start computing from 2, and then apply this function to all integers in the range 2..n, where n is a larger number. We will do all computations separately from scratch for each number, i.e. we will not cache our results (caching can significantly speed up our calculations but the point here is to focus on brute-force computing). \n\n\nHere is the GPU version of the code primesGPU.chpl (the CPU version is very similar, with obvious simplifications):\nuse Time;\nvar watch: stopwatch;\n\nproc primeFactorizationSum(n: int) {\n  var num = n, total = 0, count = 0;\n  while num % 2 == 0 {\n    num /= 2;\n    count += 1;\n  }\n  for j in 1..count do total += 2;\n  for i in 3..(sqrt(n:real)):int by 2 {\n    count = 0;\n    while num % i == 0 {\n      num /= i;\n      count += 1;\n    }\n    for j in 1..count do total += i;\n  }\n  if num &gt; 2 then total += num;\n  return total;\n}\n\nconfig const a = 2, n = 10;\non here.gpus[0] {\n  var A: [a..n] int;\n  watch.start();\n  @assertOnGpu foreach i in a..n do\n    A[i] = primeFactorizationSum(i);\n  watch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\n  var lastFewDigits = if n &gt; 5 then n-4..n else a..n;   // last 5 or fewer digits\n  writeln(\"A = \", A[lastFewDigits]);\n}\nRunning it on a full H100 card on the cluster:\nmodule load gcc/12.3 cuda/12.2 chapel-ucx-cuda/2.4.0\nchpl --fast primesGPU.chpl\nsalloc --time=... --mem-per-cpu=3600 --gpus-per-node=h100:1 --account=...\n./primesGPU -nl 1 --n=1_000_000\n\n\n\n\n\n\n\n\n\n\nn\nsingle-core CPU time in sec\nGPU time in sec\nspeedup factor\nGPU memory in MiB\n\n\n\n\n1e6\n0.5330\n0.000717\n743\n\n\n\n10e6\n16.49\n0.017904\n921\n\n\n\n100e6\n516.8\n0.5456\n947\n\n\n\n1e9\n\n16.91\n\n\n\n\n2e9\n\n48.10\n\n\n\n\n3e9\n\n88.31\n\n\n\n\n5e9\n\n270.8\n\n38670\n\n\n10e9\n\n1357\n\n76816\n\n\n\nRunning nvidia-smi on the GPU node, we see GPU utilization at 100%, and power consumption of 263W / 316W/ 318W / 267W (different stages of the same job) out of max 700W.\n\nMoving some of the calculations to the host\nDivide the entire range 2..n into 100 groups. Next, divide each group into 10 items (each still being a range), of which the first 9 items are processed on the GPU, and the last one on the CPU in serial – a nice demo of Amdahl’s Law.\nuse Time;\nimport RangeChunk.chunks;\nvar watch: stopwatch;\n\nproc primeFactorizationSum(n: int) {\n  var num = n, total = 0, count = 0;\n  while num % 2 == 0 {\n    num /= 2;\n    count += 1;\n  }\n  for j in 1..count do total += 2;\n  for i in 3..(sqrt(n:real)):int by 2 {\n    count = 0;\n    while num % i == 0 {\n      num /= i;\n      count += 1;\n    }\n    for j in 1..count do total += i;\n  }\n  if num &gt; 2 then total += num;\n  return total;\n}\n\nconfig const a = 2, n = 5_000_000_000;\non here.gpus[0] {\n  var A: [a..n] int;\n  watch.start();\n  for group in chunks(a..n, 100) {\n    for item in chunks(group, 10) {\n      if item.last &lt; group.last then\n        @assertOnGpu foreach i in item do   // first 9 items in each group are processed on the GPU\n          A[i] = primeFactorizationSum(i);\n      else\n        for i in item do                    // last item in each group is processed on the CPU\n          A[i] = primeFactorizationSum(i);\n    }\n  }\n  watch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\n  var lastFewDigits = if n &gt; 5 then n-4..n else a..n;   // last 5 or fewer digits\n  writeln(\"A = \", A[lastFewDigits]);\n}\n\n\n\nOn a full H100, this runs at 10% GPU efficiency, consuming 112W out of max 700W.\nIn the actual workshop, in addition to nvidia-smi, I am hoping to use NVIDIA’s Nsight profilers (if those can work with Chapel-compiled binaries) and our cluster’s monitoring portal (not yet in production). Combine this with MIG partitions and MPS scheduling.\n\n\n\n\n\n\nCautionQuestion\n\n\n\nHow do I go in the opposite direction, starting from an inefficient code? Need a simple numerical problem for which a naive implementation results in a poorly-performing GPU code which would be simple to improve. Here are a couple of things to try:\n1. the 3D version of the acoustic wave solver should run inefficiently for smaller problems on a GPU\n2. 2D acoustic wave solver for very large problems\n3. a Julia set should suffer from load imbalance, but not sure if it’ll show when running on a GPU"
  },
  {
    "objectID": "solutions-plotly.html",
    "href": "solutions-plotly.html",
    "title": "Various courses",
    "section": "",
    "text": "import plotly.graph_objs as go\nfrom numpy import linspace, sin\nx1 = linspace(0.01,1,100)\ny1 = sin(1/x1)\nline1 = go.Scatter(x=x1, y=y1, mode='lines+markers', name='sin(1/x)')\nx2 = linspace(0.4,1,500)\ny2 = 0.3*sin(0.5/(x2-0.39)) - 0.5\nline2 = go.Scatter(x=x2, y=y2, mode='lines+markers', name='0.3*sin(0.5/(x2-0.39))-0.5')\nfig = go.Figure([line1, line2])\nfig.show()\nNow we have two lines in the plot, each in its own colour, along with a legend in the corner!"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-1",
    "href": "solutions-plotly.html#solution-to-exercise-1",
    "title": "Various courses",
    "section": "",
    "text": "import plotly.graph_objs as go\nfrom numpy import linspace, sin\nx1 = linspace(0.01,1,100)\ny1 = sin(1/x1)\nline1 = go.Scatter(x=x1, y=y1, mode='lines+markers', name='sin(1/x)')\nx2 = linspace(0.4,1,500)\ny2 = 0.3*sin(0.5/(x2-0.39)) - 0.5\nline2 = go.Scatter(x=x2, y=y2, mode='lines+markers', name='0.3*sin(0.5/(x2-0.39))-0.5')\nfig = go.Figure([line1, line2])\nfig.show()\nNow we have two lines in the plot, each in its own colour, along with a legend in the corner!"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-2",
    "href": "solutions-plotly.html#solution-to-exercise-2",
    "title": "Various courses",
    "section": "Solution to Exercise 2",
    "text": "Solution to Exercise 2\nThe default mode is ‘lines+markers’ as you can see from the plot. You’ll need to update the list to [line1, line2, dots]. You can see that we don’t need numpy arrays for data: dots just has two lists of numbers."
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-3",
    "href": "solutions-plotly.html#solution-to-exercise-3",
    "title": "Various courses",
    "section": "Solution to Exercise 3",
    "text": "Solution to Exercise 3\ndots = go.Scatter(x=[.2,.4,.6,.8], y=[2,1.5,2,1.2], line=dict(color=('rgb(10,205,24)'),width=4))\nfig = go.Figure([line1, line2, dots])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-3b",
    "href": "solutions-plotly.html#solution-to-exercise-3b",
    "title": "Various courses",
    "section": "Solution to Exercise 3b",
    "text": "Solution to Exercise 3b\nimport plotly.graph_objs as go\nimport numpy as np\nn = 300\nopacity = np.random.rand(n)\nsc = go.Scatter(x=np.random.rand(n), y=np.random.rand(n), mode='markers',\n                marker=dict(color='rgb(0,0,255)', opacity=(1-opacity), size=80*opacity))\nfig = go.Figure(data=[sc])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-3c",
    "href": "solutions-plotly.html#solution-to-exercise-3c",
    "title": "Various courses",
    "section": "Solution to Exercise 3c",
    "text": "Solution to Exercise 3c\nheatmap = go.Heatmap(z=data, x=months, y=yticks, colorscale='Viridis')\nfig = go.Figure([heatmap])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-4",
    "href": "solutions-plotly.html#solution-to-exercise-4",
    "title": "Various courses",
    "section": "Solution to Exercise 4",
    "text": "Solution to Exercise 4\ndata = [recordHigh[:-1], averageHigh[:-1], dailyMean[:-1], averageLow[:-1], recordLow[:-1]]\nyticks = ['record high', 'aver.high', 'daily mean', 'aver.low', 'record low']\nheatmap = go.Contour(z=data, x=months[:-1], y=yticks)\nfig = go.Figure([heatmap])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-5",
    "href": "solutions-plotly.html#solution-to-exercise-5",
    "title": "Various courses",
    "section": "Solution to Exercise 5",
    "text": "Solution to Exercise 5\nImmediately fater reading the dataframe from the file, ad the following:\ndf.sort_values(by='pop', ascending=False, inplace=True)\ndf = df.iloc[:10]"
  },
  {
    "objectID": "solutions-plotly.html#exercise-6",
    "href": "solutions-plotly.html#exercise-6",
    "title": "Various courses",
    "section": "Exercise 6",
    "text": "Exercise 6\nOne possible solution is with lambda-functions, with i=0..n-1,     j=0..n-1,     x=i/(n-1)=0..1,     y=j/(n-1)=0..1:\nimport plotly.graph_objs as go\nfrom numpy import fromfunction, sin, pi\nn = 100   # plot resolution\nF = fromfunction(lambda i, j: (1-j/(n-1))*sin(pi*i/(n-1)) + \\\n                 j/(n-1)*(sin(2*pi*i/(n-1)))**2, (n,n), dtype=float)\nsurface = go.Surface(z=F)\nlayout = go.Layout(width=1200, height=1200)\nfig = go.Figure([surface], layout=layout)\nfig.show()\nIf you don’t like lambda-functions, you can replace F = fromfunction(...) line with:\nfrom numpy import zeros, linspace\nF = zeros((n,n))\nfor i, x in enumerate(linspace(0,1,n)):\n    for j, y in enumerate(linspace(0,1,n)):\n        F[i,j] = (1-y)*sin(pi*x) + y*(sin(2*pi*x))**2\nAs a third option, you can use array operations to compute f:\nfrom numpy import linspace, meshgrid\nx = linspace(0,1,n)\ny = linspace(0,1,n)\nY, X = meshgrid(x, y)   # meshgrid() returns two 2D arrays storing x/y respectively at each point\nF = (1-Y)*sin(pi*X) + Y*(sin(2*pi*X))**2   # array operation\nIf we want to scale the z-range, we can add scene=go.layout.Scene(zaxis=go.ZAxis(range=[-1,2])) inside go.Layout()."
  },
  {
    "objectID": "julia1/julia-09-persistent-arrays.html",
    "href": "julia1/julia-09-persistent-arrays.html",
    "title": "Persistent storage on workers",
    "section": "",
    "text": "So far, our remote functions have been stateless in the sense that each remote function runs independently on the assigned worker and does not retain any state between calls. In other words, there is no persistent storage associated with the worker. Consider this example:\nThere are several ways to allocate persistent storage on workers in between the function calls:",
    "crumbs": [
      "PART 2",
      "Persistent storage on workers"
    ]
  },
  {
    "objectID": "julia1/julia-09-persistent-arrays.html#tightly-coupled-parallel-codes",
    "href": "julia1/julia-09-persistent-arrays.html#tightly-coupled-parallel-codes",
    "title": "Persistent storage on workers",
    "section": "Tightly coupled parallel codes",
    "text": "Tightly coupled parallel codes\nFor tightly coupled parallel codes, you almost always want to have persistent storage on the workers, as allocating/deallocating/sending data from scratch at each step will become too expensive. In this workshop we don’t explore tightly coupled parallel problems, but these can be implemented using one of the 3 methods above. Perhaps, I should do a webinar on this topic.\nAlternatively, you can go back to the basics and use MPI.jl, or ParallelStencil.jl if you need a parallel PDE solver.",
    "crumbs": [
      "PART 2",
      "Persistent storage on workers"
    ]
  },
  {
    "objectID": "julia1/julia-09-persistent-arrays.html#channels",
    "href": "julia1/julia-09-persistent-arrays.html#channels",
    "title": "Persistent storage on workers",
    "section": "Channels",
    "text": "Channels\nBefore we study remote channels, let’s first look into Julia’s local Channels. A Channel is a data structure that facilitates communication between tasks; they act like pipes/channels. Consider a local channel on the control process:\nch = Channel(2)     # buffered channel of size 2; can take objects of any type\nput!(ch, \"hello\")   # append \"hello\" to the channel\nput!(ch, \"world\")   # append \"world\" to the channel\nput!(ch, \"5\")       # will hang (no more space); break with Ctrl-C\nch                  # 2 items available\nch.data             # show current data in the channel\nprintln(take!(ch))  # remove and return \"hello\" from the channel\nprintln(take!(ch))  # remove and return \"world\" from the channel\nprintln(take!(ch))  # will hang (no more items); break with Ctrl-C\nYou can declare Channels only for specific data types:\nch = Channel{Vector{Float64}}(1)   # buffered channel of size 1 to hold 1D arrays of Float64's\nput!(ch, 100)         # error: need Vector{Float64}, not Float64\nput!(ch, zeros(20))   # this works\nch.data     # see our 1D array\ntake!(ch)   # remove and return the 1D array\nch.data     # no more data left in the channel\ntake!(ch)   # will hang (no more items); break with Ctrl-C",
    "crumbs": [
      "PART 2",
      "Persistent storage on workers"
    ]
  },
  {
    "objectID": "julia1/julia-09-persistent-arrays.html#remote-channels",
    "href": "julia1/julia-09-persistent-arrays.html#remote-channels",
    "title": "Persistent storage on workers",
    "section": "Remote channels",
    "text": "Remote channels\nA channel is local to a process; in the examples above they are local to the control process, and they be accessed only from the control process. Similarly, if we define a channel on worker 2, the control process or worker 3 cannot directly refer to that channel.\nHowever, they can do this through a RemoteChannel that can send/fetch values to/from other workers. Consider this example:\nusing Distributed\naddprocs(1)\n@everywhere function createPersistentArray()   # function to create a persistent array on a worker\n    return RemoteChannel(() -&gt; Channel{Vector{Float64}}(1))   # remote channel of size 1\nend\nr1 = @fetchfrom 2 createPersistentArray()   # create a persistent array\nr2 = @fetchfrom 2 createPersistentArray()   # create another persistent array\nr1.where, r2.where   # both stored on worker 2\nr1.id, r2.id         # their ID's (1,2)\nr1.data              # error: cannot get to the remote data this way\nput!(r1, [float(i^2) for i in 1:10])        # store an array inside channel r1\nput!(r1, [float(i^3) for i in 1:10])        # blocks: no space in r1\nput!(r2, [float(i^3) for i in 1:10])        # store an array inside channel r2\n\nr1          # RemoteChannel{Channel{Vector{Float64}}}(2,2,1)\nfetch(r1)   # retrieve the array without removing it\ntake!(r1)   # fetch the array and remove it\nTo print or use a RemoteChannel’s content on its host worker, you can also use fetch(), but now all processing happens on worker 2:\n@spawnat 2 println(fetch(r1))\n\n\n\n\n\n\nNote\n\n\n\nRemote channels also let you send data directly between workers, without using the control process.",
    "crumbs": [
      "PART 2",
      "Persistent storage on workers"
    ]
  },
  {
    "objectID": "julia1/julia-13-nbody.html",
    "href": "julia1/julia-13-nbody.html",
    "title": "Parallelizing N-body",
    "section": "",
    "text": "In this section I will describe a project that you can work at home: the direct N-body solver.\nImagine that you place \\(N\\) identical particles randomly into a unit cube, with zero initial velocities. Then you turn on gravity, so that the particles start attracting each other. There are no boundary conditions: these are the only particles in the Universe, and they can fly to \\(\\infty\\).\n\n\n\n\n\n\nCautionQuestion\n\n\n\nWhat do you expect these particles will do?\n\n\nWe will adopt the following numerical method:\n\nforce evaluation via direct summation\nsingle variable (adaptive) time step: smaller \\(\\Delta t\\) when any two particles are close\ntime integration: more accurate than simple forward Euler + one force evaluation per time step\ntwo parameters: softening length and Courant number (I will explain these when we study the code)\n\nIn a real simulation, you would replace:\n\ndirect summation with a tree- or mesh-based \\(O(N\\log N)\\) code\ncurrent integrator with a higher-order scheme, e.g. Runge-Kutta\ncurrent timestepping with hierarchical particle updates\nfor long-term stable evolution with a small number of particles, use a symplectic orbit integrator\n\nExpected solutions:\n\n2 particles: should pass through each other, infinite oscillations\n3 particles: likely form a close binary + distant 3\\(^{\\rm rd}\\) particle (hierarchical triple system)\nmany particles: likely form a gravitationally bound system, with occasional ejection\n\nIn these clips below the time arrow is not physical time but the time step number. Consequently, the animations slow down when any two particles come close to each other.\n\n \n\nBelow you will find the serial code nbodySerial.jl. I removed all parts related to plotting the results, as it’s slow in Julia, and you would need to install Plots package (takes a while with many dependencies!).\nusing ProgressMeter\n\nnpart = 20\nniter = Int(1e5)\ncourant = 1e-3\nsofteningLength = 0.01\n\nx = rand(npart, 3);   # uniform [0,1]\nv = zeros(npart, 3);\n\nsoft = softeningLength^2;\n\nprintln(\"Computing ...\");\nforce = zeros(Float32, npart, 3);\noldforce = zeros(Float32, npart, 3);\n@showprogress for iter = 1:niter\n    tmin = 1.e10\n    for i = 1:npart\n        force[i,:] .= 0.\n        for j = 1:npart\n            if i != j\n                distSquared = sum((x[i,:] .- x[j,:]).^2) + soft;\n                force[i,:] -= (x[i, :] .- x[j,:]) / distSquared^1.5;\n                tmin = min(tmin, sqrt(distSquared / sum((v[i,:] .- v[j,:]).^2)));\n            end\n        end\n    end\n    dt = min(tmin*courant, 0.001);   # limit the initial step\n    for i = 1:npart\n        x[i,:] .+= v[i,:] .* dt .+ 0.5 .* oldforce[i,:] .* dt^2;\n        v[i,:] .+= 0.5 .* (oldforce[i,:] .+ force[i,:]) .* dt;\n        oldforce[i,:] .= force[i,:];\n    end\nend\nTring running this code with julia nbodySerial.jl; the main loop takes ~6m on the training cluster. Obviously, the most CPU-intensive part is force evaluation – this is what you want to accelerate.\nThere are many small arrays in the code – let’s use SharedArrays and fill them in parallel, e.g. you would replace\nforce = zeros(Float32, npart, 3);\nwith\nforce = SharedArray{Float32}(npart,3);\nWhen updating shared arrays, you have a choice: either update array[localindices(array)] on each worker, or use a parallel for loop with reduction. I suggest the latter. What do you want to reduce? Hint: what else do you compute besides the force in that loop? For code syntax, check parallelFor.jl code in this earlier section.\n\n\n\n\nResults\nWith default 20 particles and \\(10^5\\) steps the code runs slower in parallel on the training cluster:\n\n\n\nCode\nTime\n\n\n\n\njulia nbodySerial.jl (serial runtime)\n340s\n\n\njulia -p 1 nbodyDistributedShared.jl (2 processes)\n358s\n\n\n\nThis is the same problem we discussed in the Chapel course: with a fine-grained parallel code the communication overhead dominates the problem. As we increase the problem size, we should see the benefit from parallelization. E.g. with 1000 particles and 10 steps:\n\n\n\nCode\nTime\n\n\n\n\njulia nbodySerial.jl (serial runtime)\n83.7s\n\n\njulia -p 1 nbodyDistributedShared.jl (2 processes)\n47.9s\n\n\n\nHere is what I got on Cedar with 100 particles and \\(10^3\\) steps:\n\n\n\nRun\nTime\n\n\n\n\nserial\n1m23s\n\n\n2 cores\n46s\n\n\n4 cores\n29s\n\n\n8 cores\n22s\n\n\n16 cores\n18s\n\n\n32 cores\n19s",
    "crumbs": [
      "SUPPLEMENTAL MATERIALS",
      "Parallelizing the N-body problem"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html",
    "href": "julia1/julia-03-threads-slow-series.html",
    "title": "Multi-threading with Base.Threads",
    "section": "",
    "text": "Important\n\n\n\nToday we are working on a compute node inside an interactive job scheduled with salloc. Do not run Julia on the login node!\nLet’s start Julia by typing julia in bash:\nIf instead we start with julia -t 2 (or prior to v1.5 with JULIA_NUM_THREADS=2 julia):\nWhen launched from this interface, these two threads will run on two CPU cores on a compute node.\nLet’s run our first multi-threaded code:\nThis would split the loop between 2 threads running on two CPU cores: each core would be running one thread.\nLet’s now fill an array with values in parallel:\nHere we are filling this array in parallel, and no thread will overwrite another thread’s result. In other words, this code is thread-safe.\nLet’s initialize a large floating array and fill it with values in serial and time the loop. We’ll package the code into a function to get more accurate timing with/without @threads.\nOn the training cluster in three runs I get 2.52s, 2.45s, 2.43s.\nThe multi-threaded code performs faster:\n– on the same cluster I see 1.49s, 1.54s, 1.41s. If I reschedule the job with --cpus-per-task=4 and use julia -t 4, the times change to 0.84s, 0.93s, 0.79s.",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html#lets-add-reduction",
    "href": "julia1/julia-03-threads-slow-series.html#lets-add-reduction",
    "title": "Multi-threading with Base.Threads",
    "section": "Let’s add reduction",
    "text": "Let’s add reduction\nWe will compute the sum \\(\\sum_{i=1}^{10^6}i\\) with multiple threads. Consider this code:\ntotal = 0\n@threads for i = 1:Int(1e6)\n    global total += i          # use `total` from global scope\nend\nprintln(\"total = \", total)\nThis code is not thread-safe:\n\nrace condition: multiple threads updating the same variable at the same time\na new result every time\nunfortunately, @threads does not have built-in reduction support\n\nLet’s make it thread-safe (one of many possible solutions) using an atomic variable total. Only one thread can update an atomic variable at a time; all other threads have to wait for this variable to be released before they can write into it.\ntotal = Atomic{Int64}(0)\n@threads for i in 1:Int(1e6)\n    atomic_add!(total, i)\nend\nprintln(\"total = \", total[])   # need to use [] to access atomic variable's value\nNow every time we get the same result. This code is supposed to be much slower: threads are waiting for others to finish updating the variable, so with let’s say 4 threads and one variable there should be a lot of waiting … Atomic variables were not really designed for this type of usage … Let’s do some benchmarking!",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html#benchmarking-in-julia",
    "href": "julia1/julia-03-threads-slow-series.html#benchmarking-in-julia",
    "title": "Multi-threading with Base.Threads",
    "section": "Benchmarking in Julia",
    "text": "Benchmarking in Julia\nWe already know that we can use @time macro for timing our code. Let’s do summation of integers from 1 to Int64(1e8) using a serial code:\nn = Int64(1e8)\ntotal = Int128(0)   # 128-bit for the result!\n@time for i in 1:n\n    global total += i\nend\nprintln(\"total = \", total)\nOn the training cluster I get 10.87s, 10.36s, 11.07s. Here @time also includes JIT compilation time (marginal here). Let’s switch to @btime from BenchmarkTools: it runs the code several times, reports the shortest time, and prints the result only once. Therefore, with @btime you don’t need to precompile the code.\nusing BenchmarkTools\nn = Int64(1e8)\n@btime begin\n    total = Int128(0)   # 128-bit for the result!\n    for i in 1:n\n        total += i\n    end\nend\nprintln(\"total = \", total)\n10.865 s\nNext we’ll package this code into a function:\nfunction quick(n)\n    total = Int128(0)   # 128-bit for the result!\n    for i in 1:n\n        total += i\n    end\n    return(total)\nend\n@btime quick(Int64(1e8))    # correct result, 1.826 ns runtime\n@btime quick(Int64(1e9))    # correct result, 1.825 ns runtime\n@btime quick(Int64(1e15))   # correct result, 1.827 ns runtime\nIn all these cases we see ~2 ns running time – this can’t be correct! What is going on here? It turns out that Julia is replacing the summation with the exact formula \\[\\frac{n(n+1)}{2}.\\]\nWe want to: 1. force computation \\(~\\Rightarrow~\\) we’ll compute something more complex than simple integer summation, so that it cannot be replaced with a formula 1. exclude compilation time \\(~\\Rightarrow~\\) we’ll package the code into a function + precompile it 1. make use of optimizations for type stability \\(~\\Rightarrow~\\) package into a function + precompile it 1. time only the CPU-intensive loops",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html#slow-series",
    "href": "julia1/julia-03-threads-slow-series.html#slow-series",
    "title": "Multi-threading with Base.Threads",
    "section": "Slow series",
    "text": "Slow series\nWe could replace integer summation \\(\\sum_{i=1}^\\infty i\\) with the harmonic series, however, the traditional harmonic series \\(\\sum\\limits_{k=1}^\\infty{1\\over k}\\) diverges. It turns out that if we omit the terms whose denominators in decimal notation contain any digit or string of digits, it converges, albeit very slowly (Schmelzer & Baillie 2008), e.g.\n\nBut this slow convergence is actually good for us: our answer will be bounded by the exact result (22.9206766192…) on the upper side. We will sum all the terms whose denominators do not contain the digit “9”.\nWe will have to check if “9” appears in each term’s index i. One way to do this would be checking for a substring in a string:\nif !occursin(\"9\", string(i))\n    &lt;add the term&gt;\nend\nIt turns out that integer exclusion is ∼4X faster (thanks to Paul Schrimpf from the Vancouver School of Economics @UBC for this code!):\nfunction digitsin(digitSequence::Int, num)   # decimal representation of `digitSequence` has N digits\n    base = 10\n    while (digitSequence ÷ base &gt; 0)   # `digitSequence ÷ base` is same as `floor(Int, digitSequence/base)`\n        base *= 10\n    end\n    # `base` is now the first Int power of 10 above `digitSequence`, used to pick last N digits from `num`\n    while num &gt; 0\n        if (num % base) == digitSequence     # last N digits in `num` == digitSequence\n            return true\n        end\n        num ÷= 10                     # remove the last digit from `num`\n    end\n    return false\nend\nif !digitsin(9, i)\n    &lt;add the term&gt;\nend\nLet’s rewrite this function in a more compact form:\nfunction digitsin(digitSequence::Int, num)\n    base = 10\n    while (digitSequence ÷ base &gt; 0); base *= 10; end\n    while num &gt; 0; (num % base) == digitSequence && return true; num ÷= 10; end\n    return false\nend\nLet’s now do the timing of our serial summation code with \\(10^8\\) terms:\nfunction slow(n::Int64, digitSequence::Int)\n    total = Float64(0)    # this time 64-bit is sufficient!\n    for i in 1:n\n        if !digitsin(digitSequence, i)\n            total += 1.0 / i\n        end\n    end\n    return total\nend\n@btime slow(Int64(1e8), 9)   # total = 13.277605949858103, runtime 2.986 s",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html#st-multi-threaded-version-using-an-atomic-variable",
    "href": "julia1/julia-03-threads-slow-series.html#st-multi-threaded-version-using-an-atomic-variable",
    "title": "Multi-threading with Base.Threads",
    "section": "1st multi-threaded version: using an atomic variable",
    "text": "1st multi-threaded version: using an atomic variable\nRecall that with an atomic variable only one thread can write to this variable at a time: other threads have to wait before this variable is released, before they can write. With several threads running in parallel, there will be a lot of waiting involved, and the code should be relatively slow.\nusing Base.Threads\nusing BenchmarkTools\nfunction slow(n::Int64, digitSequence::Int)\n    total = Atomic{Float64}(0)\n    @threads for i in 1:n\n        if !digitsin(digitSequence, i)\n            atomic_add!(total, 1.0 / i)\n        end\n    end\n    return total[]\nend\n@btime slow(Int64(1e8), 9)\n\n\n\n\n\n\nCautionExercise Threads.1\n\n\n\n\n\nPut this version of slow() along with digitsin() into the file atomicThreads.jl and run it from the bash terminal (or from from REPL). First, time this code with 1e8 terms using one thread (serial run julia atomicThreads.jl). Next, time it with 2 or 4 threads (parallel run julia -t 2 atomicThreads.jl). Did you get any speedup? Make sure you obtain the correct numerical result.\n\n\n\nWith one thread I measured 2.838 s. The runtime stayed essentially the same (now we are using atomic_add()) which makes sense: with one thread there is no waiting for the variable to be released.\n\nWith 2 threads, I measured XXX – let’s discuss! Is this what we expected?\nWith 4 threads, I measured YYY – let’s discuss! Is this what we expected?\n\n\n\n\n\n\n\nCautionExercise Threads.2\n\n\n\n\n\nLet’s run the previous exercise as a batch job with sbatch. Hint: you will need to go to the login node and submit a multi-core job with sbatch shared.sh. When finished, do not forget to go back to (or restart) your interactive job.",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html#nd-version-alternative-thread-safe-implementation",
    "href": "julia1/julia-03-threads-slow-series.html#nd-version-alternative-thread-safe-implementation",
    "title": "Multi-threading with Base.Threads",
    "section": "2nd version: alternative thread-safe implementation",
    "text": "2nd version: alternative thread-safe implementation\nIn this version each thread is updating its own sum, so there is no waiting for the atomic variable to be released? Is this code faster?\nusing Base.Threads\nusing BenchmarkTools\nfunction slow(n::Int64, digitSequence::Int)\n    total = zeros(Float64, nthreads())\n    @threads for i in 1:n\n        if !digitsin(digitSequence, i)\n            total[threadid()] += 1.0 / i\n        end\n    end\n    return sum(total)\nend\n@btime slow(Int64(1e8), 9)\n\nUpdate: Pierre Fortin brought to our attention the false sharing effect. It arises when several threads are writing into variables placed close enough to each other to end up in the same cache line. Cache lines (typically ~32-128 bytes in size) are chunks of memory handled by the cache. If any two threads are updating variables (such as two neighbouring elements in our total array here) that end up in the same cache line, the cache line will have to migrate between the two threads’ caches, reducing the performance.\nIn general, you want to align shared global data (thread partitions in the array total in our case) to cache line boundaries, or avoid storing thread-specific data in an array indexed by the thread id or rank. Pierre suggested a solution using the function space() which introduces some spacing between array elements so that data from different threads do not end up in the same cache line:\nusing Base.Threads\nusing BenchmarkTools\n\nfunction digitsin(digitSequence::Int, num)\n    base = 10\n    while (digitSequence ÷ base &gt; 0); base *= 10; end\n    while num &gt; 0; (num % base) == digitSequence && return true; num ÷= 10; end\n    return false\nend\n\n # Our initial function:\nfunction slow(n::Int64, digitSequence::Int)\n    total = zeros(Float64, nthreads())\n    @threads for i in 1:n\n        if !digitsin(digitSequence, i)\n            total[threadid()] += 1.0 / i\n        end\n    end\n    return sum(total)\nend\n\n # Function optimized to prevent false sharing:\nfunction space(n::Int64, digitSequence::Int)\n    space = 8 # assume a 64-byte cache line, hence 8 Float64 elements per cache line\n    total = zeros(Float64, nthreads()*space)\n    @threads for i in 1:n\n        if !digitsin(digitSequence, i)\n            total[threadid()*space] += 1.0 / i\n        end\n    end\n    return sum(total)\nend\n\n@btime slow(Int64(1e8), 9)\n@btime space(Int64(1e8), 9)\nHere are the timings from two successive calls to slow() and space() on the the training cluster:\n[~/tmp]$ julia separateSums.jl \n  2.836 s (7 allocations: 656 bytes)\n  2.882 s (7 allocations: 704 bytes)\n[~/tmp]$ julia -t 4 separateSums.jl \n  935.609 ms (23 allocations: 2.02 KiB)\n  687.972 ms (23 allocations: 2.23 KiB)\n[~/tmp]$ julia -t 10 separateSums.jl\n  608.226 ms (53 allocations: 4.73 KiB)\n  275.662 ms (54 allocations: 5.33 KiB)\nThe speedup is substantial!\nWe see similar speedup with space = 4, but not quite with space = 2, suggesting that we are dealing with 32-byte cache lines on our system.",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html#rd-multi-threaded-version-using-heavy-loops",
    "href": "julia1/julia-03-threads-slow-series.html#rd-multi-threaded-version-using-heavy-loops",
    "title": "Multi-threading with Base.Threads",
    "section": "3rd multi-threaded version: using heavy loops",
    "text": "3rd multi-threaded version: using heavy loops\nThis version is classical task parallelism: we divide the sum into pieces, each to be processed by an individual thread. For each thread we explicitly compute the start and finish indices it processes.\nusing Base.Threads\nusing BenchmarkTools\nfunction slow(n::Int64, digitSequence::Int)\n    numthreads = nthreads()\n    threadSize = floor(Int64, n/numthreads)   # number of terms per thread (except last thread)\n    total = zeros(Float64, numthreads);\n    @threads for threadid in 1:numthreads\n        local start = (threadid-1)*threadSize + 1\n        local finish = threadid &lt; numthreads ? (threadid-1)*threadSize+threadSize : n\n        println(\"thread $threadid: from $start to $finish\");\n        for i in start:finish\n            if !digitsin(digitSequence, i)\n                total[threadid] += 1.0 / i\n            end\n        end\n    end\n    return sum(total)\nend\n@btime slow(Int64(1e8), 9)\nLet’s time this version together with heavyThreads.jl: 984.076 ms – is this the fastest version?\n\n\n\n\n\n\nCautionExercise Threads.3\n\n\n\n\n\nWould the runtime be different if we use 2 threads instead of 4?\n\n\n\nFinally, below are the timings on Cedar with heavyThreads.jl. Note that the times reported here were measured with 1.6.2. Going from 1.5 to 1.6, Julia saw quite a big improvement (~30%) in performance, plus a CPU on Cedar is different from a vCPU on the training cluster, so treat these numbers only as relative to each other.\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=...\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\n#SBATCH --account=def-someuser\nmodule load julia\njulia -t $SLURM_CPUS_PER_TASK heavyThreads.jl\n\n\n\nCode\nserial\n2 cores\n4 cores\n8 cores\n16 cores\n\n\nTime\n7.910 s\n4.269 s\n2.443 s\n1.845 s\n1.097 s",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-03-threads-slow-series.html#task-parallelism-with-base.threads-building-a-dynamic-scheduler",
    "href": "julia1/julia-03-threads-slow-series.html#task-parallelism-with-base.threads-building-a-dynamic-scheduler",
    "title": "Multi-threading with Base.Threads",
    "section": "Task parallelism with Base.Threads: building a dynamic scheduler",
    "text": "Task parallelism with Base.Threads: building a dynamic scheduler\nIn addition to @threads (automatically parallelize a loop with multiple threads), Base.Threads includes Threads.@spawn that runs a task (an expression / function) on any available thread and then immediately returns to the main thread.\nConsider this:\nusing Base.Threads\nimport Base.Threads: @spawn # has to be explicitly imported to avoid potential conflict with Distributed.@spawn\nnthreads()                  # make sure you have access to multiple threads\nthreadid()                  # always shows 1 = local thread\nfetch(@spawn threadid())    # run this function on another available thread and get the result\nEvery time you run this, you will get a semi-random reponse, e.g.\nfor i in 1:30\n    print(fetch(@spawn threadid()), \" \")\nend\n\n\n\nYou can think of @spawn as a tool to dynamically offload part of your computation to another thread – this is classical task parallelism, unlike @threads which is data parallelism.\nWith @spawn it is up to you to write an algorithm to subdivide your computation into multiple threads. With a large loop, one possibility is to divide the loop into two pieces, offload the first piece to another thread and run the other one locally, and then recursively subdivide these pieces into smaller chunks. With N subdivisions you will have 2^N tasks running on a fixed number of threads, and only one of these tasks will not be scheduled with @spawn.\nusing Base.Threads\nimport Base.Threads: @spawn\nusing BenchmarkTools\n\nfunction digitsin(digitSequence::Int, num)\n    base = 10\n    while (digitSequence ÷ base &gt; 0); base *= 10; end\n    while num &gt; 0; (num % base) == digitSequence && return true; num ÷= 10; end\n    return false\nend\n\n@doc \"\"\"\na, b are the left and right edges of the current interval;\nnumsubs is the number of subintervals, each will be assigned to a thread;\nnumsubs will be rounded up to the next power of 2,\ni.e. setting numsubs=5 will effectively use numsubs=8\n\"\"\" -&gt;\nfunction slow(n::Int64, digitSequence::Int, a::Int64, b::Int64, numsubs=16)\n    if b-a &gt; n/numsubs    # (n/numsubs) is our iteration target per thread\n        mid = (a+b)&gt;&gt;&gt;1   # shift by 1 bit to the right\n        finish = @spawn slow(n, digitSequence, a, mid, numsubs)\n        t2 = slow(n, digitSequence, mid+1, b, numsubs)\n        return fetch(finish) + t2\n    end\n    t = Float64(0)\n    println(\"computing on thread \", threadid())\n    for i in a:b\n        if !digitsin(digitSequence, i)\n            t += 1.0 / i\n        end\n    end\n    return t\nend\n\nn = Int64(1e8)\n@btime slow(n, 9, 1, n, 1)   # run the code in serial (one interval, use one thread)\n@btime slow(n, 9, 1, n, 4)   # 4 intervals, all but one spawned on one of `nthreads()` threads\nThere are two important considerations here:\n\nDepending on the number of subintervals, Julia might decide not to use all four threads! To ensure best load balancing, consider using a very large number of subintervals, to fully saturate all cores, e.g. numsubs=128 with 4 threads.\nThe println line might significantly slow down the code (depending on your processor architecture), as all threads write into an output buffer and – with many threads – take turns doing this, with some waiting involved. You might want to comment out println to get the best performance.\n\n\n\n\n\n\n\nCautionExercise Threads.4\n\n\n\n\n\nWith these two points in mind, try to get 100% parallel efficiency.",
    "crumbs": [
      "PART 1",
      "Multi-threading with Base.Threads (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-04-threadsx-slow-series.html",
    "href": "julia1/julia-04-threadsx-slow-series.html",
    "title": "Multi-threading with ThreadsX",
    "section": "",
    "text": "As you saw in the previous section, Base.Threads does not have a built-in parallel reduction. You can implement it yourself by hand, but all solutions are somewhat awkward, and you can run into problems with thread safety and performance (slow atomic variables, false sharing, etc) if you don’t pay close attention.\nEnter ThreadsX, a multi-threaded Julia library that provides parallel versions of some of the Base functions. To see the list of supported functions, use the double-TAB feature inside REPL:\nAs you see in this example, not all functions in ThreadsX are well-documented, but this is exactly the point: they reproduce the functionality of their Base serial equivalents, so you can always look up help on a corresponding serial function.\nConsider this Base function:\nThis function allows an alternative syntax:\nTo parallelize either snippet, replace mapreduce with ThreadsX.mapreduce, assuming you are running Julia with multiple threads. Do not time this code, as this computation is very fast, and the timing will mostly likely be dominated by an overhead from launching and terminating multiple threads. Instead, let’s parallelize and time the slow series.",
    "crumbs": [
      "PART 1",
      "Multi-threading with ThreadsX (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-04-threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.mapreduce",
    "href": "julia1/julia-04-threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.mapreduce",
    "title": "Multi-threading with ThreadsX",
    "section": "Parallelizing the slow series with ThreadsX.mapreduce",
    "text": "Parallelizing the slow series with ThreadsX.mapreduce\nSave the following as mapreduce.jl:\nusing BenchmarkTools, ThreadsX\n\nfunction digitsin(digitSequence::Int, num)\n    base = 10\n    while (digitSequence ÷ base &gt; 0); base *= 10; end\n    while num &gt; 0; (num % base) == digitSequence && return true; num ÷= 10; end\n    return false\nend\n\nfunction slow(n::Int64, digitSequence::Int)\n    total = ThreadsX.mapreduce(+,1:n) do i\n        if !digitsin(digitSequence, i)\n            1.0 / i\n        else\n            0.0\n        end\n    end\n    return total\nend\n\ntotal = @btime slow(Int64(1e8), 9)\nprintln(\"total = \", total)   # total = 13.277605949855294\nWith 4 CPU cores, I see:\n$ julia mapreduce.jl        # runtime with 1 thread: 2.200 s\n$ julia -t 4 mapreduce.jl   # runtime with 4 threads: 543.949 ms\n$ julia -t 8 mapreduce.jl   # what should we expect?\n\n\n\n\n\n\nCautionExercise ThreadsX.1\n\n\n\n\n\nUsing the compact (one-line) if-else notation, shorten this code by four lines. Time the new, shorter code with one and several threads.\nHint: the syntax is 1 &gt; 2 ? \"1 is greater than 2\" : \"1 is not greater than 2\"",
    "crumbs": [
      "PART 1",
      "Multi-threading with ThreadsX (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-04-threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.sum",
    "href": "julia1/julia-04-threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.sum",
    "title": "Multi-threading with ThreadsX",
    "section": "Parallelizing the slow series with ThreadsX.sum",
    "text": "Parallelizing the slow series with ThreadsX.sum\n?sum\nsum(x-&gt;x^2, 1:10)\nThreadsX.sum(x-&gt;x^2, 1:10)\nThreadsX.sum(x^2 for x in 1:10)   # alternative syntax\nThe expression in the last round brackets is a generator. It generates a sequence on the fly without storing individual elements, thus taking very little memory.\n(i for i in 1:10)          # generator\ncollect(i for i in 1:10)   # construct a vector (this one takes more space) from it\n[i for i in 1:10]          # functionally the same (vector via an array comprehension)\nLet’s use a generator with \\(10^8\\) elements to compute our slow series sum:\nusing BenchmarkTools\n@btime sum(!digitsin(9, i) ? 1.0/i : 0 for i in 1:100_000_000)\n   # serial code: 2.183 s, prints 13.277605949858103\nIt is very easy to parallelize:\nusing BenchmarkTools, ThreadsX\n@btime ThreadsX.sum(!digitsin(9, i) ? 1.0/i : 0 for i in 1:100_000_000)\n   # with 4 threads: 527.573 ms, prints 13.277605949854381\n\n\n\n\n\n\nCautionExercise ThreadsX.2\n\n\n\n\n\nThe expression [i for i in 1:10 if i%2==1] produces an array of odd integers between 1 and 10. Using this syntax, remove zero terms from the last generator, i.e. write a parallel code for summing the slow series with a generator that contains only non-zero terms. It should run slightly faster than the code with the original generator. (I get 527.159 ms runtime.)\n\n\n\n\n\n\nFinally, let’s rewrite our code applying a function to all integers in a range:\nfunction numericTerm(i)\n    !digitsin(9, i) ? 1.0/i : 0\nend\n@btime ThreadsX.sum(numericTerm, 1:Int64(1e8))   # 571.915 ms, same result\n\n\n\n\n\n\nCautionExercise ThreadsX.3\n\n\n\n\n\nRewrite the last code replacing sum with mapreduce. Hint: look up help for mapreduce().",
    "crumbs": [
      "PART 1",
      "Multi-threading with ThreadsX (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-04-threadsx-slow-series.html#other-parallel-functions",
    "href": "julia1/julia-04-threadsx-slow-series.html#other-parallel-functions",
    "title": "Multi-threading with ThreadsX",
    "section": "Other parallel functions",
    "text": "Other parallel functions\nThreadsX provides various parallel functions for sorting. Sorting is intrinsically hard to parallelize, so do not expect 100% parallel efficiency. Let’s take a look at sort() and sort!():\nn = Int64(1e8)\nr = rand(Float32, (n));   # random floats in [0, 1]\nr[1:10]      # first 20 elements, same as first(r,10)\nlast(r,10)   # last 10 elements\n\n?sort              # underneath uses QuickSort (for numeric arrays) or MergeSort\n@btime sort(r);    # 10.421 s, serial sorting\n@btime sort!(r);   # 1.707 s, in-place serial sorting\n\nr = rand(Float32, (n));\n@btime ThreadsX.sort(r);    # 2.950 ms, parallel sorting with 4 threads\n@btime ThreadsX.sort!(r);   # 1.115 ms, in-place parallel sorting with 4 threads\n?ThreadsX.sort!             # there is actually a good manual page\n\n# similar speedup for integers\nr = rand(Int32, (n));\n@btime sort!(r);   # 1.065 ms in serial\n\nr = rand(Int32, (n));\n@btime ThreadsX.sort!(r);   # 1.058 ms with 4 threads\nSearching for extrema is much more parallel-friendly:\nn = Int64(1e9)\nr = rand(Int32, (n));        # make sure we have enough memory\n@btime maximum(r)            # 328.375 ms\n@btime ThreadsX.maximum(r)   # 82.562 ms with 4 threads\nFinally, another useful function is ThreadsX.map() without reduction – we will take a closer look at it in one of the following sections.\nTo sum up this section, ThreadsX.jl provides a super easy way to parallelize some of the Base library functions. It includes multi-threaded reduction and shows very impressive parallel performance. To list the supported functions, use ThreadsX.&lt;TAB&gt;, and don’t forget to use the built-in help pages.",
    "crumbs": [
      "PART 1",
      "Multi-threading with ThreadsX (slow series)"
    ]
  },
  {
    "objectID": "julia1/julia-14-asm.html",
    "href": "julia1/julia-14-asm.html",
    "title": "Parallelizing iterative additive Schwarz method",
    "section": "",
    "text": "In this section we will implement the iterative additive Schwarz method (ASM) 1 in Julia, starting with a serial version. We will then parallelize it with DistributedArrays.jl.\nWe will be solving the 1D Poisson problem:\nDiscretizing this equation on a uniform grid of \\(m\\) points, we have\nor in matrix notation \\(AU=F\\), where\nLet’s break our grid into two domains \\(\\Omega=\\Omega_1\\bigcup\\Omega_2\\), where we are looking for the solution\nIn matrix notation the solution can be written as \\(U_i=R_iU\\), where the restriction operator \\(R_1\\) is a \\(m_s\\times\nm\\) matrix consisting of two parts of sizes \\(m_s\\times m_s\\) and \\(m_s\\times(m-m_s)\\), and \\(R_2\\) is a \\((m-m_s)\\times m\\) matrix consisting of two parts of sizes \\((m-m_s)\\times m_s\\) and \\((m-m_s)\\times(m-m_s)\\), respectively:\nThe iterative additive Schwarz method (eq. 1.30 of 2) lets you compute the next iteration of the solution as\nwhere the matrix\nis called the ASM preconditioner.",
    "crumbs": [
      "SUPPLEMENTAL MATERIALS",
      "Parallelizing the additive Schwarz method"
    ]
  },
  {
    "objectID": "julia1/julia-14-asm.html#footnotes",
    "href": "julia1/julia-14-asm.html#footnotes",
    "title": "Parallelizing iterative additive Schwarz method",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“An Introduction to Domain Decomposition Methods” by Victorita Dolean, Pierre Jolivet, and Frédéric Nataf↩︎\n“An Introduction to Domain Decomposition Methods” by Victorita Dolean, Pierre Jolivet, and Frédéric Nataf↩︎",
    "crumbs": [
      "SUPPLEMENTAL MATERIALS",
      "Parallelizing the additive Schwarz method"
    ]
  },
  {
    "objectID": "julia1/julia-12-shared-arrays.html",
    "href": "julia1/julia-12-shared-arrays.html",
    "title": "SharedArrays.jl",
    "section": "",
    "text": "Unlike distributed DArray from DistributedArrays.jl, a SharedArray object is stored in full on the control process, but it is shared across all workers on the same node, with a significant cache on each worker. SharedArrays package is part of Julia’s Standard Library (comes with the language).\n\nSimilar to DistributedArrays, you can read elements using their global indices from any worker.\nUnlike with DistributedArrays, with SharedArrays you can write into any part of the array from any worker using their global indices. This makes it very easy to parallelize any serial code, but this comes with caveats.\n\nThere are downsides to SharedArray when compared to DistributedArrays.jl:\n\nThe ability to write into the same array elements from multiple processes creates the potential for a race condition and indeterminate outcome with a poorly written code!\nYou are limited to a set of workers on the same node – does SharedArrays predate DistributedArrays?\nYou will have very skewed (non-uniform across processes) memory usage.\n\nLet’s start with serial Julia (julia command) and initialize a 1D shared array:\nusing Distributed\naddprocs(4)\nusing SharedArrays    # important to run after adding workers\na = SharedArray{Float64}(30);\na[:] .= 1.0           # assign from the control process\n@fetchfrom 2 sum(a)   # correct (30.0)\n@fetchfrom 3 sum(a)   # correct (30.0)\nsum(a)                # correct (30.0)\n@sync @spawnat 2 a .= 2.0   # can assign from any worker!\n@fetchfrom 3 sum(a)         # correct (60.0)\na .= 3.0;                   # can assign from the control process as well\n@fetchfrom 2 sum(a)         # correct (90.0)\nYou can use a function to initialize an array, however, pay attention to the result:\nb = SharedArray{Int64}((1000), init = x -&gt; x .= 0);    # use a function to initialize `b`\nlength(b)\nb = SharedArray{Int64}((1000), init = x -&gt; x .+= 1)   # each worker updates the entire array in-place!\n\n\n\n\n\n\nImportantKey idea\n\n\n\nEach worker runs this function!\n\n\nHere is another demo with a problem – let’s fill each element with its corresponding myid() value:\n@everywhere println(myid())     # let's use these IDs in the next function\nc = SharedArray{Int64}((20), init = x -&gt; x .= myid())   # indeterminate outcome! each time a new result\nEach worker updates every element, but the order in which they do this varies from one run to another, producing indeterminate outcome.\n\nAvoiding a race condition: use localindices\nWith a SharedArray, there is an implicit partitioning for processing on workers (although the array itself is stored on the control process):\n@everywhere using SharedArrays   # otherwise `localindices` won't be found on workers\nfor i in workers()\n    @spawnat i println(localindices(c))   # this block is assigned for processing on worker `i`\nend\nWhat we really want is for each worker to fill only its assigned block (parallel init, same result every time):\nc = SharedArray{Int64}((20), init = x -&gt; x[localindices(x)] .= myid())\n\n\nAnother way to avoid a race condition: use the parallel for loop\nLet’s initialize a 2D SharedArray:\na = SharedArray{Float64}(100,100);\n@distributed for i in 1:100   # parallel for loop split across all workers\n    for j in 1:100\n        a[i,j] = myid()       # ID of the worker that initialized this element\n    end\nend\nfor i in workers()\n    @spawnat i println(localindices(a))   # weird: shows 1D indices for 2D array\nend\nfor i in a[1:100,1]         # first element in each row\n    println(i)\nend\na[1:10,1:10]                # on the control process\n@fetchfrom 2 a[1:10,1:10]   # on worker 2",
    "crumbs": [
      "PART 2",
      "SharedArrays.jl"
    ]
  },
  {
    "objectID": "julia1/julia-02-intro-parallel.html",
    "href": "julia1/julia-02-intro-parallel.html",
    "title": "Parallel Julia",
    "section": "",
    "text": "In Unix a process is the smallest independent unit of processing, with its own memory space – think of an instance of a running application. The operating system tries its best to isolate processes so that a problem with one process doesn’t corrupt or cause havoc with another process. Context switching between processes is relatively expensive.\nA process can contain multiple threads, each running on its own CPU core (parallel execution), or sharing CPU cores if there are too few CPU cores relative to the number of threads (parallel + concurrent execution). All threads in a Unix process share the virtual memory address space of that process, e.g. several threads can update the same variable, whether it is safe to do so or not (we’ll talk about thread-safe programming in this course). Context switching between threads of the same process is less expensive.\n \n\nThreads within a process communicate via shared memory, so multi-threading is always limited to shared memory within one node.\nProcesses communicate via messages (over the cluster interconnect or via shared memory). Multi-processing can be in shared memory (one node, multiple CPU cores) or distributed memory (multiple cluster nodes). With multi-processing there is no scaling limitation, but traditionally it has been more difficult to write code for distributed-memory systems. Julia tries to simplify it with high-level abstractions.\n\nIn Julia you can parallelize your code with multiple threads, or with multiple processes, or both (hybrid parallelization) – see an example below:\n\n\n\n\n\n\n\nNoteDiscussion\n\n\n\nWhat are the benefits of each type of parallelism: multi-threading vs. multi-processing? Consider:\n1. context switching, e.g. starting and terminating or concurrent execution on the same CPU core,\n2. communication,\n3. scaling up.",
    "crumbs": [
      "PART 1",
      "Intro to parallelism"
    ]
  },
  {
    "objectID": "julia1/julia-02-intro-parallel.html#threads-vs.-processes",
    "href": "julia1/julia-02-intro-parallel.html#threads-vs.-processes",
    "title": "Parallel Julia",
    "section": "",
    "text": "In Unix a process is the smallest independent unit of processing, with its own memory space – think of an instance of a running application. The operating system tries its best to isolate processes so that a problem with one process doesn’t corrupt or cause havoc with another process. Context switching between processes is relatively expensive.\nA process can contain multiple threads, each running on its own CPU core (parallel execution), or sharing CPU cores if there are too few CPU cores relative to the number of threads (parallel + concurrent execution). All threads in a Unix process share the virtual memory address space of that process, e.g. several threads can update the same variable, whether it is safe to do so or not (we’ll talk about thread-safe programming in this course). Context switching between threads of the same process is less expensive.\n \n\nThreads within a process communicate via shared memory, so multi-threading is always limited to shared memory within one node.\nProcesses communicate via messages (over the cluster interconnect or via shared memory). Multi-processing can be in shared memory (one node, multiple CPU cores) or distributed memory (multiple cluster nodes). With multi-processing there is no scaling limitation, but traditionally it has been more difficult to write code for distributed-memory systems. Julia tries to simplify it with high-level abstractions.\n\nIn Julia you can parallelize your code with multiple threads, or with multiple processes, or both (hybrid parallelization) – see an example below:\n\n\n\n\n\n\n\nNoteDiscussion\n\n\n\nWhat are the benefits of each type of parallelism: multi-threading vs. multi-processing? Consider:\n1. context switching, e.g. starting and terminating or concurrent execution on the same CPU core,\n2. communication,\n3. scaling up.",
    "crumbs": [
      "PART 1",
      "Intro to parallelism"
    ]
  },
  {
    "objectID": "julia1/julia-02-intro-parallel.html#parallelism-in-julia",
    "href": "julia1/julia-02-intro-parallel.html#parallelism-in-julia",
    "title": "Parallel Julia",
    "section": "Parallelism in Julia",
    "text": "Parallelism in Julia\nThe main goal of this course is to teach you the basic tools for parallel programming in Julia, targeting both multi-core PCs and distributed-memory clusters. We will cover the following topics:\n\nmulti-threading with Base.Threads and ThreadsX.jl\nmulti-processing with Distributed.jl\nClusterManagers.jl (very briefly)\nDistributedArrays.jl – distributing large arrays across multiple processes\nSharedArrays.jl – shared-memory access to large arrays from multiple processes\n\nWe will not be covering the following topics today (we have covered some of these in our past webinars):\n\nMPI.jl – a port of the standard MPI library to Julia\nMPIArrays.jl\nParallelStencil.jl – high-level library for fast parallel stencil computations on CPUs (Base.Threads, can also interoperate with ImplicitGlobalGrid.jl built upon MPI.jl) and GPUs (NVIDIA, AMD, Apple Silicon)\nDagger.jl – a task graph scheduler heavily inspired by Python’s Dask\nConcurrent function calls (“lightweight threads” for suspending/resuming computations)\nLoopVectorization.jl\nFLoops.jl\nTransducers.jl\nDistributedData.jl\nother GPU-related packages",
    "crumbs": [
      "PART 1",
      "Intro to parallelism"
    ]
  },
  {
    "objectID": "julia1/julia-06-threadsx-julia-set.html",
    "href": "julia1/julia-06-threadsx-julia-set.html",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "",
    "text": "So far with ThreadsX, most of our parallel codes featured reduction – recall the functions ThreadsX.mapreduce() and ThreadsX.sum(). However, in the Julia set problem we want to fill an array without reduction.\nLet’s first modify the serial code! We will use another function from Base library:\nLet’s modify our serial code juliaSetSerial.jl:\nand remove the original definition of the stability array: it is now defined in the line above.\nRunning this new, vectorized version of the serial code on my laptop, I see @btime report 1.011 s.",
    "crumbs": [
      "PART 1",
      "Parallelizing the Julia set with ThreadsX"
    ]
  },
  {
    "objectID": "julia1/julia-06-threadsx-julia-set.html#parallelizing-the-vectorized-code",
    "href": "julia1/julia-06-threadsx-julia-set.html#parallelizing-the-vectorized-code",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "Parallelizing the vectorized code",
    "text": "Parallelizing the vectorized code\n\nLoad ThreadsX library.\nReplace map() with ThreadsX.map().\n\nWith 8 threads on my laptop, the runtime went down to 180.815 – 5.6X speedup. On 8 cores on the training cluster I see 6.5X speedup.",
    "crumbs": [
      "PART 1",
      "Parallelizing the Julia set with ThreadsX"
    ]
  },
  {
    "objectID": "julia1/julia-06-threadsx-julia-set.html#alternative-parallel-solution",
    "href": "julia1/julia-06-threadsx-julia-set.html#alternative-parallel-solution",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "Alternative parallel solution",
    "text": "Alternative parallel solution\nJeremiah O’Neil suggested an alternative, slightly faster (and not requiring the point array) implementation using ThreadsX.foreach (not covered in this workshop):\nfunction juliaSet(height, width)\n    stability = zeros(Int32, height, width)\n    ThreadsX.foreach(1:height) do i\n        for j = 1:width\n            point = (2*(j-0.5)/width-1) + (2*(i-0.5)/height-1)im\n            stability[i,j] = pixel(point)\n        end\n    end\n    return stability\nend",
    "crumbs": [
      "PART 1",
      "Parallelizing the Julia set with ThreadsX"
    ]
  },
  {
    "objectID": "julia1/julia-06-threadsx-julia-set.html#running-multi-threaded-julia-codes-on-a-production-cluster",
    "href": "julia1/julia-06-threadsx-julia-set.html#running-multi-threaded-julia-codes-on-a-production-cluster",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "Running multi-threaded Julia codes on a production cluster",
    "text": "Running multi-threaded Julia codes on a production cluster\nBefore we jump to multi-processing in Julia, let us remind you how to run multi-threaded Julia codes on an HPC cluster.\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=...\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\n#SBATCH --account=def-user\nmodule load julia\njulia -t $SLURM_CPUS_PER_TASK juliaSetThreadsX.jl\nThere may be some other lines after loading the Julia module, e.g. setting some variables, if you have installed packages into a non-standard location (see our introduction.\nRunning the last example on Cedar cluster with julia/1.7.0, @btime reported 2.467 s (serial) and 180.003 ms (16 cores) – 13.7X speedup.",
    "crumbs": [
      "PART 1",
      "Parallelizing the Julia set with ThreadsX"
    ]
  },
  {
    "objectID": "apptainer-menu.html",
    "href": "apptainer-menu.html",
    "title": "Working with Apptainer containers",
    "section": "",
    "text": "October 10th (Part 1) and October 17th (Part 2), 10am–noon Pacific Time\nInstructor: Alex Razoumov (SFU)\nTarget audience: general\nLevel: beginner\nPrerequisites: none\nThis course is a hands-on introduction to working with Apptainer containers in an HPC environment. Apptainer (formerly Singularity) is a great tool to create custom Linux environments and run off-the-shelf Docker containers on an HPC cluster. We will look at creating new container images, modifying existing images and installing software into them and – if time permits – running parallel codes inside a container.\nSoftware: All attendees will need a remote secure shell (SSH) client installed on their computer in order to participate in the course exercises. On Mac and Linux computers SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there). Many versions of Windows also provide an OpenSSH client by default – try opening PowerShell and typing ssh to see if it is available. If not, then we recommend installing the free Home Edition of MobaXterm.",
    "crumbs": [
      "Front page"
    ]
  },
  {
    "objectID": "apptainer-menu.html#links",
    "href": "apptainer-menu.html#links",
    "title": "Working with Apptainer containers",
    "section": "Links",
    "text": "Links\n\nAlliance’s Apptainer wiki page\nOfficial Apptainer User Guide\nOur recent webinars:\n\nHiding large numbers of files in container overlays by Alex Razoumov (SFU) recorded on 2023-Jan-17\nApptainer by Paul Preney (SHARCNET) recorded on 2022-Apr-06: many best practices and a couple of complete workflows on our clusters using /localscratch, setting $APPTAINER_CACHEDIR and $APPTAINER_TMPDIR for best performance, using overlays for storing millions of small files\nContainer-based approach to bioinformatics applications by Tannistha Nandi (UofCalgary) recorded on 2021-Oct-13\n\nSome great tutorials:\n\nContainers in HPC from Pawsey Centre\nContainers on HPC Resources from Lehigh University\nCreating and running software containers with Singularity by Eduardo Arango-Gutierrez\nSingularity & MPI Applications from Harvard University\nCarpentries incubator tutorial – some material in our course was borrowed from this tutorial",
    "crumbs": [
      "Front page"
    ]
  },
  {
    "objectID": "badSpeedupSolution.html",
    "href": "badSpeedupSolution.html",
    "title": "Fixing poor parallel scaling in the Julia set",
    "section": "",
    "text": "False sharing has no effect in this problem, as for most part individual threads are writing into array elements that are well separated in memory.\nThe row-major vs. column-major order does not matter in the Julia set problem, as it is dominated by computation of individual pixels: computing a single pixel on average takes much longer than writing to a random array element. The timings are virtually identical for row-major vs. column-major, whether in serial or in parallel with @threads. If you were to fill in these elements without computing, then the order would matter, e.g. consider this code:\nusing BenchmarkTools\nfunction testOrder(n::Int, order::Bool)\n    stability = zeros(Int32, n, n)\n    for i in 1:n\n        for j in 1:n\n            order ? stability[i,j] = i+j : stability[j,i] = i+j\n        end\n    end\n    return stability\nend\n@btime testOrder(30_000, true);    # 2.753 s\n@btime testOrder(30_000, false);   # 721.484 ms\n@btime testOrder(1000, true);    # 714.958 μs\n@btime testOrder(1000, false);   # 681.333 μs\nThe real culprit in the Julia set’s bad parallel scaling is the problem’s less-than-perfect load balancing. We know that @threads subdivides the large outer loop equally between the threads. However, some threads have fewer iterations to compute and finish faster, but they have to wait for the threads with more iterations in their pixels. If you modify the code in such a way that all pixels do the same large number of iterations, you will see perfect linear speedup with multi-threading.\nWith the unbalanced load, it is possible to improve performance somewhat with dynamic scheduling using either channels (not covered in our workshop) or Threads.@spawn but it’ll make the code a little bit more complex.\nThe following asynchronous solution was written by Jeremiah O’Neil (University of Ottawa):\nfunction juliaSet(height, width)\n    stability = zeros(Int32, height, width)\n    c = Channel{Int64}(nthreads()) do chnl\n        for i in 1:height\n            put!(chnl, i)\n        end\n    end\n    Threads.foreach(c) do i\n        for j in 1:width\n            point = (2*(j-0.5)/width-1) + (2*(i-0.5)/height-1)im\n            stability[i,j] = pixel(point)\n        end\n    end\n    return stability\nend\nshows better (but still less than linear) speedup."
  },
  {
    "objectID": "imagemagick.html",
    "href": "imagemagick.html",
    "title": "Command-line image processing with ImageMagick",
    "section": "",
    "text": "January 9th, 2024\nYou can find this page at   https://folio.vastcloud.org/imagemagick\nWhat we are not covering today: - APIs - GraphicsMagick (fork of ImageMagick) - Using ImageMagick for animations – for that I recommend ffmpeg - More complex workflows (layers, mathematical algorithms, …)"
  },
  {
    "objectID": "imagemagick.html#installation",
    "href": "imagemagick.html#installation",
    "title": "Command-line image processing with ImageMagick",
    "section": "Installation",
    "text": "Installation\nBinary packages for Linux/Mac/Windows are available from https://imagemagick.org/script/download.php, or you can compile from source. On a Mac you can use brew:\nbrew install ghostscript   # Ghostscript fonts needed for ImageMagick\nbrew install imagemagick\nOn the Alliance clusters, as of this writing, ImageMagick 7.0.10-7 is part of gentoo/2020 (loaded by default). I’ll run a demo on Cedar at the very end of this workshop."
  },
  {
    "objectID": "imagemagick.html#basic-usage",
    "href": "imagemagick.html#basic-usage",
    "title": "Command-line image processing with ImageMagick",
    "section": "Basic usage",
    "text": "Basic usage\nmagick tool [ {option} | {image} ... ] {output_image}\nwhere &lt;tool&gt; is one of the 11 commands: animate, compare, composite, conjure, convert, display, identify, import, mogrify, montage, stream. I’ll show where to find this list in a few minutes.\nStarting with version 6, the operators (options) will always be applied in the command line order given by the user.\nmagick convert ...      # full version of a command\nconvert ...             # shorter version of the same command\nmagick {options} ...    # occasionally we will use this syntax\nLet’s start with a very simple example:\nconvert https://images.pexels.com/photos/35600/road-sun-rays-path.jpg forest.jpg\nidentify forest.jpg\nidentify -verbose forest.jpg   # a lot more additional information\nidentify -format '%f %wx%h %[channels] %[bit-depth]-bit %Q\\n' forest.jpg   # only specific fields\nHere we are showing the filename (%f), image’s width and height (%wx%h), colour space and the number of channels (%[channels]), its bit depth(%[bit-depth]), and the compression quality (%Q). More details on the -format option at https://imagemagick.org/script/escape.php\nYou might find it useful to define an alias:\nalias size=\"identify -format '%f %wx%h\\n'\"\nsize forest.jpg\nconvert forest.jpg forest.png\nconvert -list format   # see the list of support image formats\n\nconvert forest.jpg forest.avif   # modern open-source image format released in 2019-Feb\n                                 # by the Alliance for Open Media (lossless and lossy)\nconvert forest.jpg forest.webp   # Google's open-source format from 2011 (lossless and lossy)\n\n\nls -l $(which convert)\nwhich convert | xargs realpath | xargs dirname | xargs ls   # check tools in /opt/homebrew/Cellar/imagemagick/7.1.1-24/bin\n\nconvert https://upload.wikimedia.org/wikipedia/commons/2/2c/NZ_Landscape_from_the_van.jpg hills.avif\nconvert https://upload.wikimedia.org/wikipedia/commons/5/5e/Deserto_libico_-_Driving_-_panoramio.jpg desert.avif\nconvert https://upload.wikimedia.org/wikipedia/commons/e/e0/Clouds_over_the_Atlantic_Ocean.jpg ocean.avif\n\nman convert   # over a 100 flags! convert + resize, blur, crop, despeckle, dither, draw on, flip, join, re-sample, and much more\nNotice that the manual page also links to the offline (local file) HTML-formatted documentation:\nopen file:///opt/homebrew/Cellar/imagemagick/7.1.1-24/share/doc/ImageMagick-7/www/convert.html"
  },
  {
    "objectID": "imagemagick.html#resizing",
    "href": "imagemagick.html#resizing",
    "title": "Command-line image processing with ImageMagick",
    "section": "Resizing",
    "text": "Resizing\nconvert forest.avif -resize 50% f1.avif\n\n-resize 50%           # keep the aspect ratio\n-resize 1600x1600     # convert to 1600x1067 keeping the aspect ratio, i.e. keep each side 1600 or smaller\n-resize 1600x1600\\!   # same and then stretch to 1600x1600\n-resize 1600x         # convert to 1600x1067 (specify width)\n-resize x1600         # convert to 2400x1600 (specify height)\n-resize 50% -grayscale Rec709Luminance   # example of using two flags; use Rec709Luminance grayscale"
  },
  {
    "objectID": "imagemagick.html#cropping-and-regions",
    "href": "imagemagick.html#cropping-and-regions",
    "title": "Command-line image processing with ImageMagick",
    "section": "Cropping and regions",
    "text": "Cropping and regions\nTo crop to a single smaller image, you must specify an offset:\nconvert forest.avif -crop 500x500+800+1050 f1.avif   # crop a 500x500 region at a specific offset\nconvert forest.avif -crop +800+1050 f1.avif   # crop starting at an offset to the lower right corner\nconvert forest.avif -crop 2656x1254+0+0 f1.avif   # crop from the upper left corner to +2656+1254\n\n\n\n\n\n\nCautionTake-home exercise\n\n\n\n\n\nWrite a bash script to crop ten random 500x500 images out of forest.avif. There is a solution somewhere on this page, but check it only after you write your own script.\n\n\n\nWithout an offset, crop will segment the original image:\nconvert -crop 30%x100% forest.avif pieces.png    # crop the image into 30%+30%+30%+10% pieces\nconvert -crop 30%x100% forest.avif pieces.avif   # all four images get written to one AVIF\nconvert -crop 30% forest.avif pieces.avif        # will use 30%+30%+30%+10% in both dimensions =&gt; 16 images\nconvert -crop 512x512 forest.avif pieces.avif    # crop into 512x512 pieces =&gt; 35 images\nconvert -crop 512x forest.avif pieces.avif       # crop horizontally only =&gt; 7 images\nYou can apply operators to a portion of an image:\nconvert forest.avif -negate f1.avif   # negate the entire image\nconvert forest.avif -gravity Center -region 300x300 -negate f1.avif   # negate a 300x300 region\n                                                             # in the center (order is important!)\n                             NorthEast                       # upper right corner\nconvert -list gravity                                        # list all 11 options\nconvert forest.avif -region 300x300+1600+100 -blur 0,10 f1.avif   # specify exact position; blur {radius}x{sigma}\n\n\n\n\n\n\nCautionQuestion\n\n\n\n\n\nHow do we blur an entire image?"
  },
  {
    "objectID": "imagemagick.html#whole-image-transforms",
    "href": "imagemagick.html#whole-image-transforms",
    "title": "Command-line image processing with ImageMagick",
    "section": "Whole-image transforms",
    "text": "Whole-image transforms\n-flip        # vertically\n-flop        # horizontally\n-rotate 90   # could be any number, not just 90/180/270 - will introduce borders\n-transpose   # flip vertically + rotate 90 degrees\n-transverse  # flip horizontally + rotate 270 degrees\n\n-border 10   # surround the image with a light gray border of width=10 on all four sides\n-bordercolor yellow -border 10    # same, with yellow border (order is important!)\n\nconvert Screenshot*.png -transparent white -fuzz 3% f1.avif"
  },
  {
    "objectID": "imagemagick.html#in-place-and-batch-processing",
    "href": "imagemagick.html#in-place-and-batch-processing",
    "title": "Command-line image processing with ImageMagick",
    "section": "In-place and batch processing",
    "text": "In-place and batch processing\nThe command mogrify is similar to convert, but it overwrites the original image by default (in-place processing), so it can be used on a batch of images.\nmogrify -resize 80% ocean.avif        # overwrite the original\nmogrify -format png -resize 1000x -depth 8 *.avif   # convert all AVIFs to 8-bit colour 1000x PNGs\n\ncp ~/talks/2024/01a-imagemagick/hybridParallelism.svg .\nmogrify -background \"#d3d3d3\" -format png hybridParallelism.svg   # set the background color\nmogrify -transparent \"#d3d3d3\" hybridParallelism.png              # make the background transparent\nmogrify -background none -format png hybridParallelism.svg        # PNG without a background\nThe end result is the same in both cases (-transparent \"#d3d3d3\" and -background none), but the image with the transparent background has more information in it (check with ls -l) which is hidden by the alpha (opacity) channel. We’ll explore this when we talk about channels."
  },
  {
    "objectID": "imagemagick.html#joining-images",
    "href": "imagemagick.html#joining-images",
    "title": "Command-line image processing with ImageMagick",
    "section": "Joining images",
    "text": "Joining images\n\nJoining horizontally:\nsize desert.avif forest.avif\nmontage -geometry x1200 desert.avif forest.avif mosaic1.avif   # create a composite image from two AVIFs\nconvert -geometry x1200 desert.avif forest.avif +append mosaic2.avif   # same but without a gap\nconvert -geometry x1200 -border 30 desert.avif forest.avif +append mosaic2.avif   # add border around each image\nmogrify -trim mosaic2.avif   # trim at the end (removes any edges of the same color as the corner pixels)\nThe flag -trim does not work very well in this two-step process. You can get better results by combining the last two commands into one:\nconvert -geometry x1200 -border 30 desert.avif forest.avif +append -trim mosaic2.avif\nHere processing happens from left to right.\n\n\nJoining vertically:\nconvert -geometry 1200x desert.avif forest.avif -append mosaic2.avif   # merge vertically without a gap\nconvert -geometry 1200x -border 10 desert.avif forest.avif -append mosaic2.avif   # merge with a border around each image\nmogrify -shave 10x10 mosaic2.avif   # remove the outer border; more reliable than -trim\nconvert -geometry 1200x -border 10 desert.avif forest.avif -append -shave 10x10 mosaic2.avif # combine the two commands into one\n\n\nAutomating these with bash functions\nI find it difficult to remember all this syntax, so I like automating these commands with functions, e.g.\nfunction mergeImagesVertically() {\n    if [ $# == 0 ]; then\n        echo Usage: mergeImagesVertically -o outputImage sharedSideSize inputImage1 inputImage2 ...\n        return 1\n    fi\n    size=$3\n    output=$2\n    shift 3\n    convert -geometry ${size}x $@ -append $output\n}\nfunction mergeImagesHorizontally() {\n    if [ $# == 0 ]; then\n        echo Usage: mergeImagesHorizontally -o outputImage sharedSideSize inputImage1 inputImage2 ...\n        return 1\n    fi\n    size=$3\n    output=$2\n    shift 3\n    convert -geometry x${size} $@ +append $output\n}\nThen the command\nconvert -geometry 1200x desert.avif forest.avif -append mosaic2.avif   # merge vertically without a gap\nbecomes\nmergeImagesVertically -o mosaic2.avif 1200 desert.avif forest.avif\n\n\n2D joining\nLet’s create a 2x2 mosaic with a gap between images:\nconvert -geometry 1200x -border 10 desert.avif forest.avif -append -shave 10x10 mosaic1.avif\nconvert -geometry 1200x -border 10 hills.avif ocean.avif -append -shave 10x10 mosaic2.avif\nidentify mosaic{1,2}.avif\nconvert -geometry x1620 -border 10 mosaic{1,2}.avif +append -shave 10x10 mosaic3.avif\nAlternatively, you can create a 2x2 mosaic in one command with montage\nmontage -geometry x1200 desert.avif forest.avif hills.avif ocean.avif mosaic4.avif   # all the same height\nmontage -geometry 1200x desert.avif forest.avif hills.avif ocean.avif mosaic4.avif   # all the same width\nmontage -geometry 1200x1200 desert.avif forest.avif hills.avif ocean.avif mosaic4.avif # still all the same width; wider gaps\nmontage -geometry 1200x1200\\! desert.avif forest.avif hills.avif ocean.avif mosaic4.avif   # all stretched to same size\nI much prefer mosaic3.avif, as it (1) keeps all original aspect ratios and (2) does not waste space.\n\n\nAdding images to a new canvas\nLet’s place desert.avif into a new 520x300 canvas:\nconvert desert.avif -resize 10% -gravity center -background white -extent 520x300 overlap.avif\nAlternatively, you can obtain exactly the same result with a border:\nconvert desert.avif -resize 10% -bordercolor white -border 51x37 overlap.avif\nNow let’s add a second image using \\\\( ... \\\\) to apply resize only to the second image:\nconvert overlap.avif \\( forest.avif -resize 100x100 \\) -gravity northeast -composite overlap.avif\n\n\n\n\n\n\nNote\n\n\n\nWithout \\\\( ... \\\\) the flag -resize 100x100 would apply to both images.\n\n\nAlternatively, we can start with an empty canvas and add images one-by-one:\nmagick -size 520x300 canvas:skyblue overlap.avif  # create an empty canvas\nconvert overlap.avif \\( desert.avif -resize 10% \\) -gravity center -composite overlap.avif\nconvert overlap.avif \\( forest.avif -resize 100x100 \\) -gravity northeast -composite overlap.avif\nconvert overlap.avif \\( ocean.avif -resize 100x100 \\) -geometry +100+220 -composite overlap.avif # use -geometry for position\nconvert overlap.avif hills.avif -geometry 100x100+250+220 -composite overlap.avif   # use -geometry for both size and position\nThe flag -geometry 100x100+250+220 applies only to the preceding (second) image, so no need for the brackets."
  },
  {
    "objectID": "imagemagick.html#drawing",
    "href": "imagemagick.html#drawing",
    "title": "Command-line image processing with ImageMagick",
    "section": "Drawing",
    "text": "Drawing\n\nBasic shapes\nDrawing falls into more advanced ImageMagick usage, and it can get complicated very quickly. Here I will show just a few shorter commands. For more examples see https://imagemagick.org/Usage/draw.\nIn these examples xc:&lt;colour&gt; produces a window fill colour:\nmagick -size 500x300 xc:skyblue empty.avif   # recall: creates an empty canvas\nmagick -size 500x300 xc:skyblue -fill blue -stroke black -strokewidth 5 \\\n       -draw \"line 180,180 390,170\" -draw \"line 160,130 370,210\" drawing.avif\n\nmagick -size 500x300 xc:skyblue -fill white -stroke black \\\n       -draw \"                    rectangle  25,50  75,250 \" \\\n       -draw \"fill-opacity 0.8    rectangle 100,50 150,250 \" \\\n       -draw \"fill-opacity 0.6    rectangle 175,50 225,250 \" \\\n       -draw \"fill-opacity 0.4    rectangle 250,50 300,250 \" \\\n       -draw \"fill-opacity 0.2    rectangle 325,50 375,250 \" \\\n       -draw \"fill-opacity  0     rectangle 400,50 450,250 \" \\\n       drawing.avif\n  \nmagick -size 500x300 xc:skyblue -fill white -stroke black \\\n       -draw \"path 'M 200,50 100,250 450,50 350,200 Z'\" drawing.avif # M starts the path, Z closes it\n\nmagick -size 500x300 xc:skyblue -fill white -stroke black \\\n       -draw \"fill-rule evenodd \\\n                 path 'M 200,50 100,100 350,250 Z\n                       M 100,200 350,200 450,50 Z\n                       M 230,270 290,60 310,250 Z'\" drawing.avif\nmagick -list fill-rule\n\nmagick -size 500x300 xc:skyblue -fill white -draw 'circle 250,150 250,50' drawing.avif # centre and a point on the circumference\n\n\nText\nNow let’s try adding text to images. First, check the available fonts:\nconvert -list font   # list all fonts, 2171 on my laptop\nThe flag -pointsize &lt;size&gt; below sets the font size. Let’s pick one of the fonts:\n\nmagick -size 700x300 xc:#71928C -draw 'text 100,200  \"Hello!\"' hello.avif\nmagick -size 700x300 xc:#71928C -pointsize 210 -font Times-New-Roman-Italic \\\n       -fill white -stroke black -strokewidth 2 -draw 'text 100,200  \"Hello!\"' hello.avif\nNote that the order of the flags is very important here!\nWe can also compare multiple fonts:\nfor font in AvantGarde-Book AvantGarde-BookOblique AvantGarde-Demi \\\n                            AvantGarde-DemiOblique Bookman-Demi; do\n    magick -size 1200x120 xc:lightblue  -pointsize 50 -font $font \\\n           -fill black -draw \"text 30,80 'Hello! with $font'\" font-${font}.avif\ndone\nconvert font*.avif -append someFonts.avif\n\nconvert -list font | grep \"Font:\" &gt; fonts.txt\nfor font in $(awk -F\": \" '{print $2}' fonts.txt ); do\n    magick -size 1200x120 xc:lightblue  -pointsize 50 -font $font \\\n           -fill black -draw \"text 30,80 'Hello! with $font'\" font-${font}.avif\ndone\n\nmagick -size 680x950 xc:#71928C -pointsize 180 -font Apple-Chancery \\\n      -fill white -stroke none                 -draw 'text 30,180  \"Stroke -\"' \\\n      -fill white -stroke black -strokewidth 0 -draw 'text 30,360 \"Stroke 0\"' \\\n      -fill white -stroke black -strokewidth 2 -draw 'text 30,540 \"Stroke 2\"' \\\n      -fill white -stroke black -strokewidth 4 -draw 'text 30,720 \"Stroke 4\"' \\\n      -fill white -stroke black -strokewidth 6 -draw 'text 30,900 \"Stroke 6\"' \\\n      strokes.jpg\nHow about adding text to an existing image?\nidentify -format '%wx%h\\n' forest.avif   # get the size\nconvert forest.avif -pointsize 60 -fill white -font Apple-Chancery -draw \"text 3100,2250 'forest road'\" f1.avif\nconvert forest.avif -pointsize 60 -fill white -gravity southeast -annotate 0 \"forest road\" f1.avif # 0 is rotation in degrees"
  },
  {
    "objectID": "imagemagick.html#working-with-channels",
    "href": "imagemagick.html#working-with-channels",
    "title": "Command-line image processing with ImageMagick",
    "section": "Working with channels",
    "text": "Working with channels\n\n\n\nStart with some large text on a canvas, and add a smaller line with a different colour:\nmagick -size 700x300 xc:#71928C -pointsize 210 -font Times-New-Roman-Italic \\\n    -fill white -draw 'text 100,200 \"Hello!\"' hello.png\nmagick hello.png -pointsize 40 -fill \"#71739C\" -draw 'text 100,260 \"This text will be hidden.\"' hello.png\nMake the background transparent – this will also apply to the smaller line:\nconvert -transparent \"#71928C\" -fuzz 10% hello.png helloTransparent.png\nNext, we overlay over a checked background; dst_over means “destination is composited over the source”:\ncomposite -compose dst_over -tile pattern:checkerboard helloTransparent.png helloChecked.png   # no hidden text\nThe hidden text is still present in the transparent image, even though you cannot see it. Let’s turn the alpha channel off in the transparent image:\nconvert helloTransparent.png -alpha off alphaOff.png   # hidden text still there; transparency data not in alphaOff.png\nconvert helloTransparent.png helloTransparent.jpg    # JPG does not support transparency; so the hidden text is there\nconvert helloTransparent.png helloTransparent.avif && convert helloTransparent.avif -alpha off alphaOff.avif\n         # AVIF supports transparency; note the AVIF compression of the hidden text!\nExtract the alpha channel:\nconvert helloTransparent.png -alpha extract alphaOnly.png   # alpha channel only\n                             -channel alpha -separate       # same\nIn fact, we can extract any channel from an image:\n# separate the alpha+RGB channels\nconvert helloTransparent.png -channel alpha -separate helloAlpha.avif\nconvert helloTransparent.png -channel red -separate helloRed.avif\nconvert helloTransparent.png -channel green -separate helloGreen.avif\nconvert helloTransparent.png -channel blue -separate helloBlue.avif\nopen hello{Red,Green,Blue,Alpha}.avif\nNow, recall helloChecked.png (we ran this command earlier):\ncomposite -compose dst_over -tile pattern:checkerboard helloTransparent.png helloChecked.png\n\nconvert helloTransparent.png -channel alpha -negate h1.png   # negate the alpha channel\ncomposite -compose dst_over -tile pattern:checkerboard h1.png helloCheckedReverse.png\nThe image helloTransparent.png has alpha either at 0% or 100% (only two values). Let’s add 75% to alpha (it’ll truncate values over 100%):\nconvert helloTransparent.png -channel alpha -evaluate add 75% h2.png   # now 75% &lt; alpha &lt; 100%\nopen hello.png h2.png\ncomposite -compose dst_over -tile pattern:checkerboard h2.png helloCheckedPartialTransparency.png\nopen helloChecked.png helloCheckedPartialTransparency.png\n\nconvert -list evaluate   # print all -evaluate operators\n\nconvert forest.avif -channel blue -evaluate multiply 0.1 f1.avif   # reduce blue colour by 10X\nconvert forest.avif -channel red -evaluate set 0 f1.avif           # remove red colour completely\nLet’s superimpose two images with 40% transparency, keeping all original colours:\nconvert ocean.avif -geometry 1600x1000\\! -alpha set -channel A -evaluate set 40% o40.avif\nconvert desert.avif -geometry 1600x1000\\! -alpha set -channel A -evaluate set 40% d40.avif\ncomposite -compose dst_over o40.avif d40.avif oceanDesert.avif"
  },
  {
    "objectID": "imagemagick.html#replacing-colours-in-place",
    "href": "imagemagick.html#replacing-colours-in-place",
    "title": "Command-line image processing with ImageMagick",
    "section": "Replacing colours in-place",
    "text": "Replacing colours in-place\nYou can also modify colours without using channels.\nconvert -size 500x500 xc:black squares.png                         # empty image\nmogrify -fill red -draw \"rectangle 0,0 250,250 \" squares.png       # add red quadrant\nmogrify -fill green -draw \"rectangle 251,0 500,250 \" squares.png   # add green quadrant\nmogrify -fill blue -draw \"rectangle 0,251 250,500 \" squares.png    # add blue quadrant\nconvert squares.png -fill turquoise -opaque black adjusted.png   # change \"opaque\" color to \"fill\" color\nLet’s try the same technique on an actual photo, picking #094C9A (deep blue) colour and changing just those pixels:\nconvert ocean.avif -fill turquoise -opaque \"#094C9A\" o1.avif\nconvert ocean.avif -fuzz 10% -fill turquoise -opaque \"#094C9A\" o1.avif   # widen our colour match"
  },
  {
    "objectID": "imagemagick.html#display",
    "href": "imagemagick.html#display",
    "title": "Command-line image processing with ImageMagick",
    "section": "Display",
    "text": "Display\nOn MacOS, X11 support is not built into precompiled ImageMagick, so I can’t demo it for you locally on my laptop. I can demo it on Cedar:\nconvert ocean.avif -resize 30% ocean.jpg\nscp ocean.jpg cedar:\nssh -X cedar\ndisplay ocean.jpg   # for this you need an X11 server on your computer (XQuartz on MacOS);\n                    # click on the image for menu; press Q to quit"
  },
  {
    "objectID": "imagemagick.html#other",
    "href": "imagemagick.html#other",
    "title": "Command-line image processing with ImageMagick",
    "section": "Other",
    "text": "Other\nconvert -size 500x500 xc:white -evaluate gaussian-noise 10 noise.avif    # produce Gaussian noise\nconvert -size 500x500 xc:white -evaluate PoissonNoise 0.01 noise.avif    # produce Poisson noise\nconvert -size 500x500 xc:white -evaluate uniform-noise 1000 noise.avif   # produce uniform noise\nconvert -list color   # list all 678 named colours"
  },
  {
    "objectID": "imagemagick.html#links",
    "href": "imagemagick.html#links",
    "title": "Command-line image processing with ImageMagick",
    "section": "Links",
    "text": "Links\n\nhttps://usage.imagemagick.org\nhttps://en.wikipedia.org/wiki/ImageMagick\nhttps://imagemagick.org/script/command-line-tools.php\nhttp://www.imagemagick.org/script/command-line-options.php\nhttps://imagemagick.org/script/stream.php\nImagemagick cheatsheet\nhttps://github.com/yangboz/imagemagick-cheatsheet"
  },
  {
    "objectID": "make.html",
    "href": "make.html",
    "title": "Introduction to makefiles",
    "section": "",
    "text": "Originally, make command (first released in 1975) was created for automating compilations. Consider a large software project with hundreds of dependencies. When you compile it, each source file is converted into an object file, and then all of them are linked together to the libraries to form a final executable(s) or a final library.\nDay-to-day, you typically work on a small section of the program, e.g. debug a single function, with much of the rest of the program unchanged. When recompiling, it would be a waste of time to recompile all hundreds of source files every time you want to compile/run the code. You need to recompile just a single source file and then update the final executable.\nA makefile is a build manager to automate this process, i.e. to figure out what is up-to-date and what is not, and only run the commands that are necessary to rebuild the final target. A makefile is essentially a tree of dependencies stored in a text file along with the commands to create these dependencies. It ensures that if some of the source files have been updated, we only run the steps that are necessary to create the target with those new source files.\nMakefiles can be used for any project (not just compilation) with multiple steps producing intermediate results, when some of these steps are compute-heavy. Let’s look at an example! We will store the following text in the file text.md:\n## Design\n\nMakefiles can be used for automating any multiple-step workflow\nwhen there is a need to update only some targets, as opposed to\nrunning the entire workflow from start to finish.\n\n\\newpage\n\n## Make's builtin variables\n\n- `$@` is the \"target of this rule\"\n- `$ˆ` is \"all prerequisites of this rule\"\n- `$&lt;` is \"the first prerequisite of this rule\"\n- `$?` is \"all out-of-date prerequisites of this rule\"\nThis is our workflow to automate:\npandoc text.md -t beamer -o text.pdf\n\nwget https://wgpages.netlify.app/img/dolphin.png\nmagick dolphin.png dolphin.pdf\n\nwget https://wgpages.netlify.app/img/penguin.png\nmagick penguin.png penguin.pdf\n\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\n/bin/rm -f dolphin.* penguin.* text.pdf\ncurl -F \"file=@slides.pdf\" https://temp.sh/upload && echo\nFirst version of Makefile automates creating of slides.pdf:\nslides.pdf: text.pdf dolphin.pdf penguin.pdf\n    gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\ntext.pdf: text.md\n    pandoc text.md -t beamer -o text.pdf\ndolphin.pdf: dolphin.png\n    magick dolphin.png dolphin.pdf\npenguin.pdf: penguin.png\n    magick penguin.png penguin.pdf\ndolphin.png:\n    wget https://wgpages.netlify.app/img/dolphin.png\npenguin.png:\n    wget https://wgpages.netlify.app/img/penguin.png\nRunning make will create the target slides.pdf – how many command will it run? That depends on how many intermediate files you have, and their timestamps.\n\n\n\n\n\n\nCautionTest 1\n\n\n\nLet’s modify text.md, e.g. add a line there. The makefile will figure out what needs to be done to update slides.pdf. How many command will it run?\n\n\n\n\n\n\n\n\nCautionTest 2\n\n\n\nLet’s remove dolphin.png. How many commands will make run?\n\n\n\n\n\n\n\n\nCautionTest 3\n\n\n\nLet’s remove both PNG files. How many commands will make run?\n\n\nNow, add three special targets at the end:\nclean:\n    /bin/rm -f dolphin.* penguin.* text.pdf\ncleanall:\n    make clean\n    /bin/rm -f slides.pdf\nupload: slides.pdf\n    curl -F \"file=@slides.pdf\" https://temp.sh/upload && echo\nNext, we can make use of make’s builtin variables:\n\n$@ is the “target of this rule”\n$ˆ is “all prerequisites of this rule”\n$&lt; is “the first prerequisite of this rule”\n$? is “all out-of-date prerequisites of this rule”\n\n&lt; gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\n---\n&gt; gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=$@ $^\n&lt; pandoc text.md -t beamer -o text.pdf\n---\n&gt; pandoc $^ -t beamer -o $@\nThe next simplification makes use of make wildcards to specify patterns:\n&lt; dolphin.pdf: dolphin.png\n&lt;   magick dolphin.png dolphin.pdf\n&lt; penguin.pdf: penguin.png\n&lt;   magick penguin.png penguin.pdf\n---\n&gt; %.pdf: %.png\n&gt;   magick $^ $@"
  },
  {
    "objectID": "stencil.html",
    "href": "stencil.html",
    "title": "High-level parallel stencil computations",
    "section": "",
    "text": "Abstract: In this webinar, we cover parallel stencil computations in Julia using the ParallelStencil.jl package. This package enables you to write high-level code for fast computations on CPUs and GPUs. These computations are common in all numerical simulations involving the solution of discretized partial differential equations (PDEs) on a grid. ParallelStencil.jl provides high-level functions for computing derivatives and updating arrays. You can execute the same code on a single CPU, multiple CPUs with multithreading via Base.Threads, or on GPUs using either CUDA.jl (NVIDIA GPUs), AMDGPU.jl (AMD GPUs), or Metal.jl (Apple Silicon GPUs).\nRegardless of the underlying parallel hardware, all low-level communication between threads is hidden behind ParallelStencil.jl’s macro calls, ensuring that it remains invisible in the simulation code. This framework makes it highly accessible to domain scientists. The same high-level code can seamlessly run on CPUs and GPUs without any modifications.\nFurthermore, you can extend this framework to multiple processes, integrating ParallelStencil.jl with ImplicitGlobalGrid.jl (built upon MPI.jl). This combination facilitates easy scaling to multiple cluster nodes, with further parallelization on multiple cores and GPUs on each node. This architecture has been shown to scale efficiently to hundreds of GPUs and hundreds of cluster nodes."
  },
  {
    "objectID": "stencil.html#on-cpus-and-gpus-in-julia",
    "href": "stencil.html#on-cpus-and-gpus-in-julia",
    "title": "High-level parallel stencil computations",
    "section": "",
    "text": "Abstract: In this webinar, we cover parallel stencil computations in Julia using the ParallelStencil.jl package. This package enables you to write high-level code for fast computations on CPUs and GPUs. These computations are common in all numerical simulations involving the solution of discretized partial differential equations (PDEs) on a grid. ParallelStencil.jl provides high-level functions for computing derivatives and updating arrays. You can execute the same code on a single CPU, multiple CPUs with multithreading via Base.Threads, or on GPUs using either CUDA.jl (NVIDIA GPUs), AMDGPU.jl (AMD GPUs), or Metal.jl (Apple Silicon GPUs).\nRegardless of the underlying parallel hardware, all low-level communication between threads is hidden behind ParallelStencil.jl’s macro calls, ensuring that it remains invisible in the simulation code. This framework makes it highly accessible to domain scientists. The same high-level code can seamlessly run on CPUs and GPUs without any modifications.\nFurthermore, you can extend this framework to multiple processes, integrating ParallelStencil.jl with ImplicitGlobalGrid.jl (built upon MPI.jl). This combination facilitates easy scaling to multiple cluster nodes, with further parallelization on multiple cores and GPUs on each node. This architecture has been shown to scale efficiently to hundreds of GPUs and hundreds of cluster nodes."
  },
  {
    "objectID": "stencil.html#parallelstencil.jl-basics",
    "href": "stencil.html#parallelstencil.jl-basics",
    "title": "High-level parallel stencil computations",
    "section": "ParallelStencil.jl basics",
    "text": "ParallelStencil.jl basics\n\nI will start the demo on my laptop’s GPU:\nusing ParallelStencil\n\n@init_parallel_stencil(Metal, Float32, 3);   # creates Data module with all the correct types\nData.Number   # Float32\nData.Array    # Metal.MtlArray{Float32}\nThe call to @init_parallel_stencil creates Data module for the selected parallel method, precision and dimensionality, with Data.Number, Data.Array, Data.CellArray inside.\n\n1st argument specifies your parallel package: Threads, Polyester, CUDA, AMDGPU, or Metal\n2nd argument specifies precision: Float16, Float32, Float64, ComplexF16, ComplexF32, or ComplexF64\n\nInt not supported by design, as we are solving PDEs\nfor demos I’ll be using ...f0 and Float32 to enforce single precision, as double precision is not supported in Metal.jl\n\n3rd argument specifies dimensionality for the stencil computations: 1, 2, or 3\n\nYou can initialize Data.Array objects via @zeros, @ones, @rand, @fill, e.g.\nn = 5;\nA = @zeros(n, n, n);\ntypeof(A)   # Metal.MtlArray{Float32, 3, Metal.PrivateStorage}, or Array{Float32, 3}, or ...\nor by converting from Julia’s usual arrays:\nB = [i+j for i=1:5, j=1:5, k=1:n]   # array comprehension\ntypeof(B)   # usual Julia's 2D Matrix{Int64} = Array{Int64, 2}\nsize(B)     # (5, 5, 5)\nA = Data.Array(B)\ntypeof(A)   #  MtlMatrix{Float32, Metal.PrivateStorage}\nYou can also take an initialized Data.Array and assign to it element-wise from another Data.Array:\nn = 5;\nA = @zeros(n, n, n);\nA .= 0.1f0 .+ Data.Array([i+j+k for i=1:n, j=1:n, k=1:n]);\nsize(A)          # (5, 5, 5)\nA[1:3,1:3,1:3]   # you can use the usual subsetting\n\nParallel stencil processing\nLet’s switch to 2D (need to restart Julia!) and try some parallel stencil calculations.\nusing ParallelStencil\n@init_parallel_stencil(Metal, Float32, 2);   # create Data module with all the correct types\nusing ParallelStencil.FiniteDifferences2D    # also 1D and 3D available\n?@inn   # select the inner elements of A\nYou cannot just say @inn(A) in the main scope – instead you need to use it from a parallel function (that would launch a GPU kernel if running on a GPU):\n@parallel function inner!(A, B)\n    @inn(B) = @inn(A);\n    return\nend\n\nour function will launch a GPU kernel (if running on a GPU)\nthe last statement must be return or return nothing or nothing\n@parallel must contain only array assignments using macros from FiniteDifferences{1|2|3}D\n@parallel_indices supports any kind of statements in the kernels (less efficient)\n\nThis is how you would use it:\nn = 4;\nA = Data.Array([i+j for i=1:n, j=1:n])\nB = @zeros(n, n);\n@parallel inner!(A, B)\nB\n4×4 Metal.MtlMatrix{Float32, Metal.PrivateStorage}:\n 0.0  0.0  0.0  0.0\n 0.0  4.0  5.0  0.0\n 0.0  5.0  6.0  0.0\n 0.0  0.0  0.0  0.0\nNow let’s assign to all elements of B with @all(B) by redefining a new parallel function:\n\n\n\n\n\n\nNote\n\n\n\nThis last example works on Metal but breaks on Threads, for the reasons that will become clear in a minute.\n\n\n@parallel function inner2all!(A, B)\n    @all(B) = @inn(A);\n    return\nend\n\nB = @zeros(n, n);\n@parallel inner2all!(A, B)\nB\n4×4 Metal.MtlMatrix{Float32, Metal.PrivateStorage}:\n 4.0  5.0  6.0  0.0\n 5.0  6.0  7.0  0.0\n 6.0  7.0  8.0  0.0\n 4.0  5.0  0.0  0.0\nIt starts by doing the right thing (copying the inner elements A at the start of B), but then it keeps going past the end of @inn(A) in y-dimension:\n\nArray() copy a GPU Data.Array to a CPU\n\nArray(B)[1,3] == Array(A)[2,4]\nArray(B)[2,3] == Array(A)[3,4]\nArray(B)[3,3] == Array(A)[4,4]\nand then wraps around into the x-dimension with a shift in y-dimension:\nArray(B)[4,1] == Array(A)[1,3]   # true\nArray(B)[4,2] == Array(A)[1,4]   # true\nTo me, this is counter-intuitive – I would prefer for it to copy zeros into these last 5 elements – but I guess it makes sense from the performance standpoint, especially on a GPU (where you want to keep things simple), as it just keep incrementing the memory indices. This means that you as a programmer have to be careful when assigning a smaller array to a larger array with these parallel constructs on a GPU, or even better never do this!\n\n\nFirst-order derivatives\nLet’s compute a 1st-order derivative in x-direction:\n?@d_xa   # compute differences between adjacent elements of A along the x-dimension\n\n@parallel function ddx!(A, B, h)\n    @inn(B) = @d_xa(A) / h;\n    return\nend\n\nn = 4\nh = 1f0/(n-1);   # Float32\nA = @rand(n,n)\n\nB = @zeros(n, n);\n@parallel ddx!(A, B, h);\nB\nAs expected, this code results only in the inner elements of B getting assigned the derivatives:\n4×4 Metal.MtlMatrix{Float32, Metal.PrivateStorage}:\n 0.0  0.0        0.0       0.0\n 0.0  0.385376  -1.69729   0.0\n 0.0  1.13448    0.613478  0.0\n 0.0  0.0        0.0       0.0\nUpon closer examination, the first non-zero derivative element turns out to be:\nArray(B)[2,2] == (Array(A)[2,1]-Array(A)[1,1]) / h   # true\ni.e. we get a weird derivative B[i,j] == (A[i,j-1] - A[i-1,j-1]) / h.\nTo get the backward Euler derivative B[i,j] == (A[i,j] - A[i-1,j]) / h, you can use:\n@d_xi(A): Compute differences between adjacent elements of A along the\ndimension x and select the inner elements of A in the remaining dimension.\nCorresponds to @inn_y(@d_xa(A)).\n@parallel function ddx!(A, B, h)\n    @inn(B) = @d_xi(A) / h;\n    return\nend\n\nB = @zeros(n, n);\n@parallel ddx!(A, B, h);\nB\n\nArray(B)[2,2] == (Array(A)[2,2]-Array(A)[1,2]) / h   # true\nTo get the forward Euler derivative B[i,j] == (A[i+1,j] - A[i,j]) / h, on a GPU (breaks on Threads) you could use:\n@parallel function ddx!(A, B, h)\n    @all(B) = @d_xa(A) / h;\n    return\nend\n\nB = @zeros(n, n);\n@parallel ddx!(A, B, h);\nB\n\nC = Array(A)\n[Array(B)[i,j] == (C[i+1,j] - C[i,j]) / h for i=2:n-1, j=2:n-1]   # array of 1==true\nalong with setting the proper boundary conditions, as assigning to @all will give you weird “wrap-around” derivatives along the edges as we saw earlier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond-order derivatives\nLet’s compute the Laplacian of a Gaussian function centred on (0.5, 0.5):\n@parallel function laplacian!(A, B, h)\n    @inn(B) = (@d2_xi(A) + @d2_yi(A))/h^2;\n    return\nend\n\nn = 5;\nh = 1f0/(n-1);   # Float32\nA = Data.Array([exp(-2*(((i-0.5)/n-0.5)^2+((j-0.5)/n-0.5)^2)) for i=1:n, j=1:n])\nB = @zeros(n, n); \n@parallel laplacian!(A, B, h);\nB\n\n# check (should all return true)\nC = Array(A)\nfor i=2:n-1\n    for j=2:n-1\n        println(i, \" \", j, \" \",\n        isapprox(Array(B)[i,j], (C[i-1,j]-2*C[i,j]+C[i+1,j])/h^2 + (C[i,j-1]-2*C[i,j]+C[i,j+1])/h^2))\n    end\nend\n\n\nPlotting a 2D array\nMakie package can plot a 2D array:\nn = 100\nh = 1.0f0/(n-1);   # Float32\nA = Data.Array([exp(-2*(((i-0.5)/n-0.5)^2+((j-0.5)/n-0.5)^2)) for i=1:n, j=1:n]);\nB = @zeros(n, n);\n@parallel laplacian!(A, B, h);\nB\n\ncpuArray = Array(B);   # only needed if running on a GPU\n\nusing CairoMakie\nfig = Figure(resolution=(3*size(cpuArray)[1]+100, 3*size(cpuArray)[2]+100))\nax = Axis(fig[1, 1], xlabel = \"x\", ylabel = \"y\", title = \"Laplacian\")\nheatmap!(ax, cpuArray, colormap = :viridis)\nsave(\"laplacian.png\", fig)"
  },
  {
    "objectID": "stencil.html#d-and-3d-heat-diffusion-solver",
    "href": "stencil.html#d-and-3d-heat-diffusion-solver",
    "title": "High-level parallel stencil computations",
    "section": "2D and 3D heat diffusion solver",
    "text": "2D and 3D heat diffusion solver\nIn this demo we’ll be solving the equation \\(\\partial T/\\partial t = \\nabla^2 T\\) on a 2D grid.\nLet’s study the code heatDiffusionSolver2D.jl:\n\ntwo explicit switch variables USE_GPU and ANIMATION\ndual Gaussian initial state\nzero boundary at the left/right sides\nnon-zero boundary at the top and bottom\n\nconst USE_GPU = true\nconst ANIMATION = true\nusing ParallelStencil\nusing ParallelStencil.FiniteDifferences2D\nusing CairoMakie\nusing Printf\n\n@static if USE_GPU\n    @init_parallel_stencil(Metal, Float32, 2);\nelse\n    @init_parallel_stencil(Threads, Float32, 2);\nend\n\n@parallel function computeNewTemperature!(Tnew, T, dt, h)\n    @inn(Tnew) = @inn(T) + dt*(@d2_xi(T)/h^2 + @d2_yi(T)/h^2);\n    return\nend\n\n@parallel function updateCurrentTemperature!(Tnew, T)\n    @inn(T) = @inn(Tnew)\n    return\nend\n\nfunction plotArray(A, counter)\n    fig = Figure(size=(size(A)[1]+100, size(A)[2]+100))\n    ax = Axis(fig[1, 1], title = \"Heat diffusion\")\n    heatmap!(ax, A, colormap = :viridis)\n    save(\"frame\"*@sprintf(\"%05d\", counter)*\".png\", fig)\n    return\nend\n\nfunction diffusion2D()\n    n, nt, nout = 8192, 100, 10;   # resolution, max time steps, plotting frequency\n    h = 1f0 / (n-1);   # grid step\n\n    # initial conditions\n    T = Data.Array([100f0 * exp(-30*(((i-1)*h-0.5)^2+((j-1)*h-0.5)^2)) +\n        100f0 * exp(-15*(((i-1)*h-0.25)^2+((j-1)*h-0.25)^2)) for i=1:n, j=1:n]);\n\n    Tnew = @zeros(n, n);\n\n    # boundary conditions\n    T[1,:] .= 0f0;    # left\n    T[n,:] .= 0f0;    # right\n    T[:,n] .= 80f0;   # non-zero boundary at the top\n    T[:,1] .= 80f0;   # non-zero boundary at the bottom\n\n    ANIMATION && plotArray(Array(T), 0);   # Array() converts to a CPU array\n\n    dt = h^2 / 4f0;   # Courant condition: h^2/4 for 2D, h^2/6 for 3D\n    @time for it = 1:nt\n        # need two separate steps so that we finish computing Tnew before updating T\n        @parallel computeNewTemperature!(Tnew, T, dt, h);\n        @parallel updateCurrentTemperature!(Tnew, T);\n        if mod(it,nout)==0\n            ANIMATION && plotArray(Array(T), Int32(it/nout));\n            println(it, \"  \", sum(T))\n        end\n    end\nend\n\ndiffusion2D();\nFirst, let’s run this problem with the following parameters:\nconst USE_GPU = true\nconst ANIMATION = true\nn, nt, nout = 500, 50_000, 100;   # resolution, max time steps, plotting frequency\njulia heatDiffusionSolver2D.jl\nThe animation gaussian.mp4 with 501 frames.\n\n2D benchmarking\nThis 2D problem runs very fast. Let’s increase the resolution to \\(8192^2\\) and run on CPU without animation:\nconst USE_GPU = false\nconst ANIMATION = false\nn, nt, nout = 8192, 100, 10;   # resolution, max time steps, plotting frequency\njulia heatDiffusionSolver2D.jl\njulia -t 2 heatDiffusionSolver2D.jl\n...\n\n17.51 seconds on 1 CPU core\n9.08 seconds on 2 CPU cores\n4.73 seconds on 4 CPU cores\n3.28 seconds on 6 CPU cores\n3.93 seconds on 8 CPU cores\n\n\n\n\n\n\nMoving to the M1 GPU gives a small speed-up:\nconst USE_GPU = true\n\n3.32 seconds on M1’s GPU\n\nIf we add cpuArray = Array(T); println(cpuArray[2,2]); line right after calling the two GPU functions, the runtime grows to 5.72 seconds reflecting data transfer from GPU to CPU (and printing).\n\n\n3D benchmarking\nCopy the 2D code heatDiffusionSolver2D.jl to heatDiffusionSolver3D.jl and make the following changes:\n&lt; using ParallelStencil.FiniteDifferences2D\n---\n&gt; using ParallelStencil.FiniteDifferences3D\n\n\n&lt;     @init_parallel_stencil(Metal, Float32, 2);\n---\n&gt;     @init_parallel_stencil(Metal, Float32, 3);\n\n&lt;     @init_parallel_stencil(Threads, Float32, 2);\n---\n&gt;     @init_parallel_stencil(Threads, Float32, 3);\n\n\n&lt;     @inn(Tnew) = @inn(T) + dt*(@d2_xi(T)/h^2 + @d2_yi(T)/h^2);\n---\n&gt;     @inn(Tnew) = @inn(T) + dt*(@d2_xi(T)/h^2 + @d2_yi(T)/h^2 + @d2_zi(T)/h^2);\n\n\n&lt; function diffusion2D()\n&lt;     n, nt, nout = 8192, 100, 10;   # resolution, max time steps, plotting frequency\n---\n&gt; function diffusion3D()\n&gt;     n, nt, nout = 512, 100, 10;   # resolution, max time steps, plotting frequency\n\n\n&lt; diffusion2D();\n---\n&gt; diffusion3D();\n\n\n&lt;     T = Data.Array([100f0 * exp(-30*(((i-1)*h-0.5)^2+((j-1)*h-0.5)^2)) +\n&lt;         100f0 * exp(-15*(((i-1)*h-0.25)^2+((j-1)*h-0.25)^2)) for i=1:n, j=1:n]);\n---\n&gt;     T = Data.Array([100f0 * exp(-30*(((i-1)*h-0.5)^2+((j-1)*h-0.5)^2+((k-1)*h-0.5)^2)) +\n&gt;         100f0 * exp(-15*(((i-1)*h-0.25)^2+((j-1)*h-0.25)^2+((k-1)*h-0.25)^2)) for i=1:n, j=1:n, k=1:n]);\n\n\n&lt;     Tnew = @zeros(n, n);\n---\n&gt;     Tnew = @zeros(n, n, n);\n\n\n&lt;     T[1,:] .= 0f0;    # left\n&lt;     T[n,:] .= 0f0;    # right\n&lt;     T[:,n] .= 80f0;   # non-zero boundary at the top\n&lt;     T[:,1] .= 80f0;   # non-zero boundary at the bottom\n---\n&gt;     T[1,:,:] .= 0f0;    # left\n&gt;     T[n,:,:] .= 0f0;    # right\n&gt;     T[:,n,:] .= 80f0;   # non-zero boundary at the top\n&gt;     T[:,1,:] .= 80f0;   # non-zero boundary at the bottom\n&gt;     T[:,:,1] .= 0f0;\n&gt;     T[:,:,n] .= 0f0;\n\n\n&lt;     ANIMATION && plotArray(Array(T), 0);   # Array() converts to a CPU array\n---\n&gt;     ANIMATION && plotArray(Array(T[:,:,Int32(n/2)]), 0);   # Array() converts to a CPU array\n\n\n&lt;     dt = h^2 / 4f0;   # Courant condition: h^2/4 for 2D, h^2/6 for 3D\n---\n&gt;     dt = h^2 / 6f0;   # Courant condition: h^2/4 for 2D, h^2/6 for 3D\n\n\n&lt;             ANIMATION && plotArray(Array(T), Int32(it/nout));\n---\n&gt;             ANIMATION && plotArray(Array(T[:,:,Int32(n/2)]), Int32(it/nout));\nOn my laptop, at \\(512^3\\) I get the following runtimes:\n\n46.51 seconds on 1 CPU core\n23.99 seconds on 2 CPU cores\n12.47 seconds on 4 CPU cores\n9.71 seconds on 8 CPU cores\n6.88 seconds on the GPU\n\n\n\n\n\n\n\n\n\n3D benchmarking on a cluster\nSetting up Julia with CUDA may require some work – you can find more information at https://docs.alliancecan.ca/wiki/Julia#Using_GPUs_with_Julia\nHere is what worked for me on the training cluster on Arbutus:\nsed -i -e 's|Metal|CUDA|' heatDiffusionSolver3D.jl\n\nmodule load gcc/13.3\nmodule load cudacore/.12.2.2\nmodule load julia/1.10.0\n\njulia\n]\nadd ParallelStencil   # goes into ~/.julia; 11 dependencies\nadd CairoMakie        # optional; takes a while; 222 dependencies\nbackspace\nexit()\n\nsalloc --time=3:0:0 --mem-per-cpu=15000 --gpus-per-node=1\n\njulia\n]\nadd CUDA    # do this from a GPU node; 33+235 dependencies\nbackspace\nusing CUDA\nCUDA.set_runtime_version!(v\"12.2.2\", local_toolkit=true)   # configure Julia to use the local CUDA\nexit()\n\njulia\nusing CUDA\nCUDA.versioninfo()\nThe last command will report the device (GRID V100D-8C) and the software versions for NVIDIA drivers, CUDA libraries, Julia, and LLVM. When running CUDA codes, if you will get warnings:\nWarning: You are using a non-official build of Julia.\nThis may cause issues with CUDA.jl.\nbut things seem to run Ok.\nGo back on the login node, copy heatDiffusionSolver3D.jl to the cluster and set:\nUSE_GPU = false\nRun the code with Base.Threads:\nsalloc --time=0:15:0 --mem-per-cpu=3600 --cpus-per-task=4\njulia heatDiffusionSolver3D.jl\njulia -t 2 heatDiffusionSolver3D.jl\njulia -t 4 heatDiffusionSolver3D.jl\nand then with CUDA:\nsed -i -e 's|USE_GPU = false|USE_GPU = true|' heatDiffusionSolver3D.jl\nsalloc --time=0:15:0 --mem-per-cpu=3600 --gpus-per-node=1\nnvidia-smi   # running on V100D-8C\njulia heatDiffusionSolver3D.jl\nAt \\(512^3\\) I get the following runtimes:\n\n140.9 seconds on 1 CPU core\n72.59 seconds on 2 CPU cores\n37.24 seconds on 4 CPU cores\n2.97 seconds on the GPU"
  },
  {
    "objectID": "stencil.html#d-acoustic-wave-solver",
    "href": "stencil.html#d-acoustic-wave-solver",
    "title": "High-level parallel stencil computations",
    "section": "3D acoustic wave solver",
    "text": "3D acoustic wave solver\nOur next demo code, adapted from https://github.com/omlins/ParallelStencil.jl, solves the 3D acoustic wave equation for \\(c=1\\) (speed of sound), written as a system separately for velocity and pressure:\n\\[\n\\begin{cases}\n   \\partial\\vec{v}/\\partial t = -\\nabla p\\\\\\\n   \\partial p/\\partial t = -\\nabla\\cdot\\vec{v}\n\\end{cases}\n\\]\nLet’s take a look at the code in acoustic3D.jl!\nTo run it:\njulia acoustic3D.jl   # 16m4s seconds on M1's GPU\nThe movie advection.mp4 shows the numerical solution."
  },
  {
    "objectID": "stencil.html#distributed-parallelization-multi-node",
    "href": "stencil.html#distributed-parallelization-multi-node",
    "title": "High-level parallel stencil computations",
    "section": "Distributed parallelization (multi-node)",
    "text": "Distributed parallelization (multi-node)\nFor distributed-memory parallelism, ParallelStencil.jl can interoperate with 3rd-party frameworks to parallelize array computations on many processes. While it might be possible to run ParallelStencil.jl alongside DistributedArrays.jl or SharedArrays.jl, these packages use specialized data structures that are not directly portable to ParallelStencil.jl’s parallel frameworks. A better approach is combining ParallelStencil.jl with ImplicitGlobalGrid.jl, which provides implicit distributed arrays built out of regular local arrays on each MPI rank.\n\nIntro to ImplicitGlobalGrid.jl\nLet’s study the code globalGridDemo.jl:\n\nusing formatted @printf instead of println as it separates output from different ranks into different lines; println tends to mix them somewhere in the output buffers somewhat randomly\nby default, init_global_grid will initialize MPI and finalize_global_grid will finalize MPI; here doing this by hand, as we might want to run test() many times, and MPI can be initialized only once\nour computational domain is a unit square\nthere is no global array per se, in the sense provided by such packages as DistributedArrays.jl and SharedArrays.jl – instead, local arrays cover the domain with some overlap at the inner boundaries, and together they emulate a global grid; you can only explicitly read/write array elements on the processor rank where these elements are hosted in the respective local array\nnx_g(), ny_g(), nz_g provide the global grid size (cumulative over all local arrays minus the borders)\nbased on the current processor rank:\n\n[x_g(i, dx, A) for i in 1:nx] gives the global x-coordinates of all elements in the local array A\n[y_g(j, dy, A) for j in 1:ny] gives the global y-coordinates of all elements in the local array A\n[z_g(k, dz, A) for k in 1:nz] gives the global z-coordinates of all elements in the local array A\n\non each rank we compute only the internal array elements [2:end-1] in each dimension\nthe inner boundary zones are updated via MPI, by calling update_halo!\nthe outer boundary zones are not updated in this example at all; in a real problem they will be used for outer boundary conditions\n\n\n\n\nusing ImplicitGlobalGrid\nusing MPI\nusing Printf\n\nMPI.Init()   # finalize by hand, as we want to run test() many times,\n             # and MPI can be initialized only once\n\nfunction test()\n    nx, ny, nz= 5, 5, 1;   # must be 3D\n    # initialize a Cartesian grid of MPI processes, each computing an (nx, ny, nz) array\n    me, dims, nprocs, coords, comm_cart = init_global_grid(nx, ny, nz, init_MPI=false);\n    @printf(\"me=%d  nprocs=%d  dims=%s  coords=%s\\n\", me, nprocs, string(dims), string(coords))\n\n    me == 0 && @printf(\"global grid size = %d %d %d\\n\", nx_g(), ny_g(), nz_g())\n\n    dx = 1/(nx_g()-1);\n    dy = 1/(ny_g()-1);\n\n    A = fill(-1f0, nx, ny);\n    # x,y,z-coordinates of the first and last local elements\n    @printf(\"me=%d &gt;&gt; from %f %f to %f %f\\n\", me,\n            x_g(1,dx,A), y_g(1,dy,A),\n            x_g(nx,dx,A), y_g(ny,dy,A));\n\n    A[2:end-1,2:end-1] = [\n        exp(-10*((x_g(i,dx,A)-0.4)^2+(y_g(j,dy,A)-0.4)^2))\n        for i=2:nx-1, j=2:ny-1];\n\n    sleep(0.25*me)\n    if me == 1 || me ==3\n        print(\"     \");\n    end\n    display(A);\n\n    update_halo!(A);\n\n    me == 0 && println(\"----------\");\n    sleep(0.25*me)\n    display(A);\n\n    finalize_global_grid(finalize_MPI=false);\nend\n\ntest()\n\nMPI.Finalize()\nRunning this code on 4 cores:\nexport PATH=$PATH:$HOME/.julia/packages/MPI/TKXAj/bin\nunalias julia\nexport PATH=$PATH:/Applications/Julia-1.11.app/Contents/Resources/julia/bin\nmpiexecjl -n 4 julia globalGridDemo.jl\nwill produce the following output:\nme=1  nprocs=4  dims=[2, 2, 1]  coords=[0, 1, 0]\nme=0  nprocs=4  dims=[2, 2, 1]  coords=[0, 0, 0]\nme=3  nprocs=4  dims=[2, 2, 1]  coords=[1, 1, 0]\nme=2  nprocs=4  dims=[2, 2, 1]  coords=[1, 0, 0]\n\nglobal grid size = 8 8 1\n\nme=1 &gt;&gt; from 0.000000 0.428571 to 0.571429 1.000000\nme=3 &gt;&gt; from 0.428571 0.428571 to 1.000000 1.000000\nme=2 &gt;&gt; from 0.428571 0.000000 to 1.000000 0.571429\nme=0 &gt;&gt; from 0.000000 0.000000 to 0.571429 0.571429\n\n     5×5 Matrix{Float32}:\n -1.0  -1.0       -1.0       -1.0       -1.0\n -1.0   0.266482   0.453012   0.512022  -1.0\n -1.0   0.453012   0.770108   0.870423  -1.0\n -1.0   0.512022   0.870423   0.983806  -1.0\n -1.0  -1.0       -1.0       -1.0       -1.0\n5×5 Matrix{     Float32}:\n -1.0  -1.0       -1.0       -1.0        -1.0\n -1.0   0.384773   0.192246   0.0638627  -1.0\n -1.0   0.654103   0.326813   0.108565   -1.0\n -1.0   0.739308   0.369384   0.122707   -1.0\n -1.0  -1.0       -1.0       -1.0        -1.0\n5×5 Matrix{Float32}:\n -1.0  -1.0        -1.0       -1.0       -1.0\n -1.0   0.384773    0.654103   0.739308  -1.0\n -1.0   0.192246    0.326813   0.369384  -1.0\n -1.0   0.0638627   0.108565   0.122707  -1.0\n -1.0  -1.0        -1.0       -1.0       -1.0\n5×5 Matrix{Float32}:\n -1.0  -1.0        -1.0        -1.0        -1.0\n -1.0   0.555573    0.277584    0.0922112  -1.0\n -1.0   0.277584    0.138691    0.0460719  -1.0\n -1.0   0.0922112   0.0460719   0.0153048  -1.0\n -1.0  -1.0        -1.0        -1.0        -1.0\n\n----------\n\n5×5 Matrix{Float32}:\n -1.0  -1.0       -1.0       -1.0       -1.0\n -1.0   0.266482   0.453012   0.512022   0.384773\n -1.0   0.453012   0.770108   0.870423   0.654103\n -1.0   0.512022   0.870423   0.983806   0.739308\n -1.0   0.384773   0.654103   0.739308   0.555573\n5×5 Matrix{Float32}:\n -1.0       -1.0       -1.0       -1.0        -1.0\n  0.512022   0.384773   0.192246   0.0638627  -1.0\n  0.870423   0.654103   0.326813   0.108565   -1.0\n  0.983806   0.739308   0.369384   0.122707   -1.0\n  0.739308   0.555573   0.277584   0.0922112  -1.0\n5×5 Matrix{Float32}:\n -1.0   0.512022    0.870423   0.983806   0.739308\n -1.0   0.384773    0.654103   0.739308   0.555573\n -1.0   0.192246    0.326813   0.369384   0.277584\n -1.0   0.0638627   0.108565   0.122707   0.0922112\n -1.0  -1.0        -1.0       -1.0       -1.0\n5×5 Matrix{Float32}:\n  0.983806   0.739308    0.369384    0.122707   -1.0\n  0.739308   0.555573    0.277584    0.0922112  -1.0\n  0.369384   0.277584    0.138691    0.0460719  -1.0\n  0.122707   0.0922112   0.0460719   0.0153048  -1.0\n -1.0       -1.0        -1.0        -1.0        -1.0\nIn each subdivided dimension we have two overlapping elements, one on each side of the boundary. Running update_halo!(A) copies the array elements (via MPI) into the ghost values across the boundary.\n\n\nExtending the 2D heat diffusion solver to multiple MPI ranks\nCopy the 2D code heatDiffusionSolver2D.jl to distrubuted2D.jl and make the following changes:\n\nset USE_GPU = false (we’ll be solving on multiple MPI ranks, each with one CPU – no GPUs for now)\nremove animation to keep the code compact (some extra work needed to animate multiple local arrays)\nwe are solving the equation on the same physical domain (unit square); assuming fixed local array size, the global grid resolution depends on the number of ranks stacked in each dimension, so now we have variables grid steps dx and dy\n\n&lt;     h = 1f0 / (n-1);   # grid step\n---\n&gt;     dx = 1/(nx_g()-1);   # grid step in x\n&gt;     dy = 1/(ny_g()-1);   # grid step in y\n\n\n&lt; @parallel function computeNewTemperature!(Tnew, T, dt, h)\n&lt;     @inn(Tnew) = @inn(T) + dt*(@d2_xi(T)/h^2 + @d2_yi(T)/h^2);\n---\n&gt; @parallel function computeNewTemperature!(Tnew, T, dt, dx, dy)\n&gt;     @inn(Tnew) = @inn(T) + dt*(@d2_xi(T)/dx^2 + @d2_yi(T)/dy^2);\n\n\n&lt;         @parallel computeNewTemperature!(Tnew, T, dt, h);\n---\n&gt;         @parallel computeNewTemperature!(Tnew, T, dt, dx, dy);\n\nfew other small changes:\n\n&gt; using ImplicitGlobalGrid\n&gt; import MPI\n\n\n&lt;     n, nt, nout = 8192, 100, 10;   # resolution, max time steps, plotting frequency\n---\n&gt;     n, nt, nout = 256, 100_000, 1000;   # resolution, max time steps, plotting frequency\n&gt;     me, dims, nprocs, coords, comm_cart = init_global_grid(n, n, 1);\n\n\nreplace (i-1)*h with x_g(i,dx,T)\nreplace (j-1)*h with y_g(j,dy,T)\n\n\n&lt;     # boundary conditions\n&lt;     T[1,:] .= 0f0;    # left\n&lt;     T[n,:] .= 0f0;    # right\n&lt;     T[:,n] .= 80f0;   # non-zero boundary at the top\n&lt;     T[:,1] .= 80f0;   # non-zero boundary at the bottom\n---\n&gt;     # domain's outer boundary\n&gt;     coords[2] == 0         ? T[1,:] .= 0f0 : nothing    # if 1st rank in y =&gt; left boundary\n&gt;     coords[2] == dims[2]-1 ? T[n,:] .= 0f0 : nothing    # if last rank in y =&gt; right boundary\n&gt;     coords[1] == dims[1]-1 ? T[:,n] .= 80f0 : nothing   # if last rank in x =&gt; non-zero boundary at the top\n&gt;     coords[2] == 0         ? T[:,1] .= 80f0 : nothing   # if 1st rank in x =&gt; non-zero boundary at the bottom\n\n\n&lt;     dt = h^2 / 4f0;   # Courant condition: h^2/4 for 2D, h^2/6 for 3D\n---\n&gt;     dt = min(dx^2,dy^2) / 4f0;   # Courant condition: h^2/4 for 2D, h^2/6 for 3D\n\n\n&gt;         update_halo!(T);\n\n\n&lt;             println(it, \"  \", sum(T))\n---\n&gt;             total = MPI.Reduce(sum(T), MPI.SUM, 0, MPI.COMM_WORLD)\n&gt;             me == 0 && @printf(\"%d steps total = %f\\n\", it, total)\n\n\n&gt;     finalize_global_grid();\nTo run this code with 4 MPI ranks:\nexport PATH=$PATH:$HOME/.julia/packages/MPI/TKXAj/bin\nunalias julia\nexport PATH=$PATH:/Applications/Julia-1.11.app/Contents/Resources/julia/bin\nmpiexecjl -n 4 julia distributed2D.jl\nTo run this code on the cluster, you need to configure Julia’s MPI to use system’s MPI libraries as described in https://docs.alliancecan.ca/wiki/Julia#Running_Julia_with_MPI.\n\n\nFurther parallelization with multiple threads on each MPI rank\nShould be automatic: no change to the code.\n\n\nFurther parallelization with GPUs on each MPI rank\nupdate_halo!(T) will fail, as it will try to update the inner ghost zones of objects of type Data.Array that it knows nothing about.\nIf you have a CUDA-aware MPI (for Nvidia GPUs), then you can turn it on with the following:\nexport JULIA_CUDA_MEMORY_POOL=none   # disable the memory pool\nexport IGG_CUDAAWARE_MPI=1\nThis should make ImplicitGlobalGrid.jl also aware of GPU arrays. I have not tested it myself (most Alliance’s production clusters have been under maintenance for the past couple of weeks) – let me know if this something you would like to explore."
  },
  {
    "objectID": "stencil.html#links",
    "href": "stencil.html#links",
    "title": "High-level parallel stencil computations",
    "section": "Links",
    "text": "Links\n\nhttps://github.com/omlins/ParallelStencil.jl\nhttps://github.com/eth-cscs/ImplicitGlobalGrid.jl"
  },
  {
    "objectID": "python3/python-10-libraries.html",
    "href": "python3/python-10-libraries.html",
    "title": "Libraries",
    "section": "",
    "text": "Most of the power of a programming language is in its libraries. This is especially true for Python which is an interpreted language and is therefore very slow (compared to compiled languages). However, the libraries are often compiled (can be written in compiled languages such as C/C++) and therefore offer much faster performance than native Python code.\nA library is a collection of functions that can be used by other programs. Python’s standard library includes many functions we worked with before (print, int, round, …) and is included with Python. There are many other additional modules in the standard library such as math:\nYou can also import math’s items directly:\nYou can also create an alias from the library:",
    "crumbs": [
      "DAY 2",
      "Libraries, virtual environments and packaging"
    ]
  },
  {
    "objectID": "python3/python-10-libraries.html#virtual-environments-and-packaging",
    "href": "python3/python-10-libraries.html#virtual-environments-and-packaging",
    "title": "Libraries",
    "section": "Virtual environments and packaging",
    "text": "Virtual environments and packaging\n\n\nTo install a 3rd-party library into the current Python environment from inside a Jupyter notebook, simply do (you will probably need to restart the kernel before you can use the package):\n%pip install &lt;packageName&gt;   # e.g. try bson\nIn Python you can create an isolated environment for each project, into which all of its dependencies will be installed. This could be useful if your several projects have very different sets of dependencies. On the computer running your Jupyter notebooks, open the terminal and type:\n(Important: on a cluster you must do this on the login node, not inside the JupyterLab terminal.)\nmodule load python/3.9.6    # specific to HPC clusters\npip install virtualenv\nvirtualenv --no-download climate   # create a new virtual environment in your current directory\nsource climate/bin/activate\nwhich python && which pip\npip install --no-index netcdf4 ...\n...\ndeactivate\nTo use this environment in the terminal, you would do:\nsource climate/bin/activate\n...\ndeactivate\nOptionally, you can add your environment to Jupyter:\npip install --no-index ipykernel    # install ipykernel (IPython kernel for Jupyter) into this environment\npython -m ipykernel install --user --name=climate --display-name \"My climate project\" # add your env to Jupyter\n...\ndeactivate\nQuit all your currently running Jupyter notebooks and the Jupyter dashboard, and then restart. One of the options in New below Python 3 should be climate.\nTo delete the environment, in the terminal type:\njupyter kernelspec list                  # `climate` should be one of them\njupyter kernelspec uninstall climate     # remove your environment from Jupyter\n/bin/rm -rf climate",
    "crumbs": [
      "DAY 2",
      "Libraries, virtual environments and packaging"
    ]
  },
  {
    "objectID": "python3/python-10-libraries.html#quick-overview-of-some-of-the-libraries",
    "href": "python3/python-10-libraries.html#quick-overview-of-some-of-the-libraries",
    "title": "Libraries",
    "section": "Quick overview of some of the libraries",
    "text": "Quick overview of some of the libraries\n\n\n\n\n\nPython lists are very general and flexible, which is great for high-level programming, but it comes at a cost. The Python interpreter can’t make any assumptions about what will come next in a list, so it treats everything as a generic object with its own type and size. As lists get longer, eventually performance takes a hit.\nPython does not have any mechanism for a uniform/homogeneous list, where – to jump to element #1000 – you just take the memory address of the very first element and then increment it by (element size in bytes) x 999. NumPy library fills this gap by adding the concept of homogenous collections to python – numpy.ndarrays – which are multidimensional, homogeneous arrays of fixed-size items (most commonly numbers, but could be strings too). This brings huge performance benefits!\nTo speed up calculations with NumPy, typically you perform operations on entire arrays, and this by extension applies the same operation to each array element. Since NumPy was written in C, it is much faster for processing multiple data elements than manually looping over these elements in Python.\nLearning NumPy is outside the scope of this introductory workshop, but there are many packages built on top of NumPy that could be used in HSS:\n\npandas is a library for working with 2D tables / spreadsheets, built on top of numpy\nscikit-image is a collection of algorithms for image processing, built on top of numpy\nMatplotlib and Plotly are two plotting packages for Python\nxarray is a library for working with labelled multi-dimensional arrays and datasets in Python\n\n“pandas for multi-dimensional arrays”\ngreat for large scientific datasets; writes into NetCDF files\nwe won’t study it in this workshop\n\n\nWe’ll also take a look at these two libraries (not based on NumPy): - requests is an HTTP library to download HTML data from the web - Beautiful Soup is a library to parse these HTML data",
    "crumbs": [
      "DAY 2",
      "Libraries, virtual environments and packaging"
    ]
  },
  {
    "objectID": "python3/python-11-pandas.html",
    "href": "python3/python-11-pandas.html",
    "title": "Pandas dataframes",
    "section": "",
    "text": "Pandas is a widely-used Python library for working with tabular data, borrows heavily from R’s dataframes, built on top of NumPy. Let’s try reading some public-domain data about Jeopardy questions with pandas (31MB file, so it might take a while):\nimport pandas as pd\ndata = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndata.shape      # shape is a member variable inside data =&gt; 216930 rows, 7 columns\n\nprint(data)\ndata            # this prints out the table nicely in Jupyter Notebook!\n\ndata.info()     # info is a *member method inside data*\ndata.head(10)   # first 10 rows\ndata.tail()     # last 5 rows\ndata.columns    # names of the columns\nLet’s download the same online data as a file into our current directory:\n!wget https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\nThis time let’s read from this file naming the rows by their respective answer:\ndata = pd.read_csv(\"jeopardy.csv\", index_col='Answer')\ndata\ndata.shape   # one fewer column\ndata.columns\ndata.index   # row names",
    "crumbs": [
      "DAY 3",
      "Pandas dataframes"
    ]
  },
  {
    "objectID": "python3/python-11-pandas.html#reading-csv-tabular-data",
    "href": "python3/python-11-pandas.html#reading-csv-tabular-data",
    "title": "Pandas dataframes",
    "section": "",
    "text": "Pandas is a widely-used Python library for working with tabular data, borrows heavily from R’s dataframes, built on top of NumPy. Let’s try reading some public-domain data about Jeopardy questions with pandas (31MB file, so it might take a while):\nimport pandas as pd\ndata = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndata.shape      # shape is a member variable inside data =&gt; 216930 rows, 7 columns\n\nprint(data)\ndata            # this prints out the table nicely in Jupyter Notebook!\n\ndata.info()     # info is a *member method inside data*\ndata.head(10)   # first 10 rows\ndata.tail()     # last 5 rows\ndata.columns    # names of the columns\nLet’s download the same online data as a file into our current directory:\n!wget https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\nThis time let’s read from this file naming the rows by their respective answer:\ndata = pd.read_csv(\"jeopardy.csv\", index_col='Answer')\ndata\ndata.shape   # one fewer column\ndata.columns\ndata.index   # row names",
    "crumbs": [
      "DAY 3",
      "Pandas dataframes"
    ]
  },
  {
    "objectID": "python3/python-11-pandas.html#subsetting",
    "href": "python3/python-11-pandas.html#subsetting",
    "title": "Pandas dataframes",
    "section": "Subsetting",
    "text": "Subsetting\nPandas lets you subset elements using either their numerical indices or their row/column names. Long time ago Pandas used to have a single function to do both. Now there are two separate functions, .iloc and .loc. Let’s print one element:\ndata.iloc[0,5]                                    # using row/column numbers\ndata.loc[\"Sinking of the Titanic\", \"Question\"]    # using row/column names\nPrinting a row:\ndata.loc['Sinking of the Titanic',:]   # usual Python's slicing notation - show all columns in that row\ndata.loc['Sinking of the Titanic']     # exactly the same\ndata.loc['Sinking of the Titanic',]    # exactly the same\nPrinting a column:\ndata.loc[:,'Category']   # show all rows in that column\ndata['Category']         # exactly the same; single index refers to columns\ndata.Category            # most compact notation; does not work with numerical-only names\nCombining .iloc and .loc – let’s say we want to retrieve a cell by its row number and its column name:\ndata.iloc[0]     # returns a pandas series (with many rows) =&gt; use .loc to select its row\ndata.iloc[0].loc[\"Question\"]   # combining numbers and names\ndata.iloc[0][\"Question\"]       # the same\nPrinting a range:\ndata.iloc[10:15]     # rows 10,11,12,13,14\ndata.columns\ndata.iloc[10:15].loc[:,'Category':'Question']     # rows 10-14, columns 'Category' - 'Question'\ndata.iloc[10:15].loc[:,['Category','Question']]   # rows 10-14, columns 'Category' and 'Question'\ndata.loc['Copernicus', 'Air Date':'Category']     # a range of columns ... why so many lines?\ndata.loc['Copernicus', ['Air Date','Question']]   # print two selected columns\ndata['Category']              # many different categories, truncated output ...\nlist(data['Category'])        # print all categories ... huge list\nlen(set(data['Category']))    # 27,983 unique categories\ndata['Category']=='HISTORY'   # return either True or False for each row\nYou can use the last expression as a mask to return only those rows where Category is “HISTORY”:\ndata.loc[data['Category']=='HISTORY']\ndata.loc[data['Category']=='HISTORY'].shape   # 349 matches\ndata.loc[data['Category']=='HISTORY'].to_csv(\"history.csv\")   # write to a file\nLet’s take a look at the value column:\nlist(data['Value'])   # some of these contain nan\n\ndata.shape              # original table: 216,930 rows\nclean = data.dropna()   # drop rows with nan's\nclean.shape             # after dropping rows with missing values: 213,144 rows\n\nclean['Value']   # one column\nclean['Value'].apply(lambda x: type(x))   # show the type of each element (fixed for each column)\nclean['Value'].apply(lambda x: x.replace('$',''))   # remove all $ signs\nvalues = clean['Value'].apply(lambda x: int(x.replace('$','').replace(',','')))\n\nmask = values&gt;5000\nclean[mask]         # only show rows with Value &gt; $5000\nclean[mask].shape   # thera are 345 such rows\nLet’s replace the “Values” column in-place – here using the same expression on the right-hand side as before:\nclean.loc[:,'Value'] = clean['Value'].apply(lambda x: int(x.replace('$','').replace(',','')))\nclean\nNow clean’s Value column is all numerical.\n\n\n\n\n\n\nCautionExercise 11.1\n\n\n\n\n\nExplain in simple terms what .idxmin() and .idxmax() do in the short program below. When would you use these methods?\nclean.idxmin()\nclean.idxmax()\nHint: Try running these:\nclean.loc[\"Freddie And The Dreamers\"]\nclean.loc[\"Suriname\"]\nor use help pages. This simpler example could also help:\ncol1 = [\"1\",\"a\",\"A\"]\ncol2 = [\"4\",\"b\",\"B\"]\ndata = pd.DataFrame({'a': col1, 'b': col2})   # dataframe from a dictionary\ndata.idxmin()\n\n\n\n\n\n\n\nFinally, let’s check what time period is covered by these data:\ndata[\"Air Date\"]\ndata[\"Air Date\"][0][-2:]   # first row, last two digits is the year\nyear = data[\"Air Date\"].apply(lambda x: x[-2:])   # last two digits of the year from all rows\nyear.min(); year.max()     # '00' and '99' - not very informative, wraps at the turn of the century\n\nfor y in range(100):\n    twoDigits = str(y).zfill(2)\n    print(twoDigits, sum(year==twoDigits))\nThis shows that this table covers years from 1984 to 2012.",
    "crumbs": [
      "DAY 3",
      "Pandas dataframes"
    ]
  },
  {
    "objectID": "python3/python-11-pandas.html#creating-a-dataframe-from-scratch",
    "href": "python3/python-11-pandas.html#creating-a-dataframe-from-scratch",
    "title": "Pandas dataframes",
    "section": "Creating a dataframe from scratch",
    "text": "Creating a dataframe from scratch\nHow do you create a dataframe from scratch? There are many ways; perhaps, the easiest is by defining columns (as you saw in the last exercise):\ncol1 = [1,2,3]\ncol2 = [4,5,6]\npd.DataFrame({'a': col1, 'b': col2})       # dataframe from a dictionary\nWe can index (assign names to) the rows with this syntax:\npd.DataFrame({'a': col1, 'b': col2}, index=['a1','a2','a3'])",
    "crumbs": [
      "DAY 3",
      "Pandas dataframes"
    ]
  },
  {
    "objectID": "python3/python-11-pandas.html#three-solutions-to-a-classification-problem",
    "href": "python3/python-11-pandas.html#three-solutions-to-a-classification-problem",
    "title": "Pandas dataframes",
    "section": "Three solutions to a classification problem",
    "text": "Three solutions to a classification problem\n\nFizz buzz is a children’s game to practice divisions. Players take turn counting out loud while replacing: - any number divisible by 3 with the word “Fizz”, - any number divisible by 5 with the word “Buzz”, - any number divisible by both 3 and 5 with the word “FizzBuzz”.\nLet’s implement this in pandas! First, create a simple dataframe from scratch:\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame()\nsize = 10_000\ndf['number'] = np.arange(1, size+1)   # create a column called \"number\" containing 1,2,...,size\nDefine for pretty printing:\ndef show(frame):\n    print(df.tail(15).to_string(index=False))   # print last 15 rows without the row index\n\nshow(df)\nLet’s built a new column response containing either “Fizz” or “Buzz” or “FizzBuzz” or the original number, based on the number value in that row. Let’s start by defining a function to process a row:\ndef count(row):\n    if (row['number'] % 3 == 0) and (row['number'] % 5 == 0):\n        return 'FizzBuzz'\n    elif row['number'] % 3 == 0:\n        return 'Fizz'\n    elif row['number'] % 5 == 0:\n      return 'Buzz'\n    else:\n      return str(row['number'])\nThis is how you would use this function:\ncount(df.iloc[2])    # returns 'Fizz'\ncount(df.iloc[14])   # returns 'FizzBuzz'\n\nWe can apply this function to each row in a loop:\n\n%%timeit\nfor index, row in df.iterrows():\n    df.loc[index, 'response'] = count(row)\n413 ms ± 11.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nshow(df)\n\nWe can use df.apply() to apply this function to each row:\n\n%%timeit\ndf['response'] = df.apply(count, axis=1)   # axis=1 means apply along the column\n69.1 ms ± 380 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nshow(df)\n\nOr we could use a mask to only assign correct responses to the corresponding rows:\n\n%%timeit\ndf['response'] = df['number'].astype(str)\ndf.loc[df['number'] % 3==0, 'response'] = 'Fizz'\ndf.loc[df['number'] % 5==0, 'response'] = 'Buzz'\ndf.loc[(df['number'] % 3==0) & (df['number'] % 5==0), 'response'] = 'FizzBuzz'\n718 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nshow(df)",
    "crumbs": [
      "DAY 3",
      "Pandas dataframes"
    ]
  },
  {
    "objectID": "python3/python-01-setup.html",
    "href": "python3/python-01-setup.html",
    "title": "Setup and running Jupyter notebooks",
    "section": "",
    "text": "NoteDisclaimer\n\n\n\nThese notes started number of years ago from the official SWC lesson but then evolved quite a bit to include other topics.",
    "crumbs": [
      "DAY 1",
      "Setup and running Jupyter notebooks"
    ]
  },
  {
    "objectID": "python3/python-01-setup.html#why-python",
    "href": "python3/python-01-setup.html#why-python",
    "title": "Setup and running Jupyter notebooks",
    "section": "Why Python?",
    "text": "Why Python?\nPython is a free, open-source programming language first developed in the late 1980s and 90s that became really popular for scientific computing in the past 15 years. With Python in a few minutes you can: - analyze thousands of texts, - process tables with billions of records, - manipulate thousands of images, - restructure and process data any way you want.\n\nPython vs. Excel\n\nUnlike Excel, Python can read any type of data, both structured and unstructured.\nPython is free and open-source, so no artificial limitations on where/how you run it.\nPython works on all platforms: Windows, Mac, Linux, Android, etc.\nData manipulation is much easier in Python. There are hundreds of data processing, machine learning, and visualization libraries.\nPython can handle much larger amounts of data: limited not by Python, but by your available computing resources. In addition, Python can run at scale (in parallel) on larger systems.\nPython is more reproducible (rerun / modify the script).\n\n\n\n\n\n\n\n\n\n\n\nPython vs. other programming languages\n\n\n\n\n\n\n\nPython pros\nPython cons\n\n\n\n\nelegant scripting language\nslow (interpreted, dynamically typed)\n\n\neasy to write and read code\nuses indentation for code blocks\n\n\npowerful, compact constructs for many tasks\n\n\n\nvery popular across all fields\n\n\n\nhuge number of external libraries",
    "crumbs": [
      "DAY 1",
      "Setup and running Jupyter notebooks"
    ]
  },
  {
    "objectID": "python3/python-01-setup.html#installing-python-locally",
    "href": "python3/python-01-setup.html#installing-python-locally",
    "title": "Setup and running Jupyter notebooks",
    "section": "Installing Python locally",
    "text": "Installing Python locally\nToday we’ll be running Python in the cloud, so you can skip this section. I am listing these options in case you want to install Python on your computer after the workshop.\nOption 1: Install Python from https://www.python.org/downloads making sure to check the option “Add Python to PATH” during the installation.\nOption 2: Install Python and the packages via Anaconda from https://www.anaconda.com/download.\nOption 3: Install Python via your favourite package manager, e.g. in MacOS – assuming you have Homebrew installed – run the command brew install python.\nPost-installation: Install 3rd-party Python packages in the Command Prompt / terminal via pip install &lt;packageName&gt;, e.g. to be able to run Python inside a Jupyter Notebook run pip install jupyter.",
    "crumbs": [
      "DAY 1",
      "Setup and running Jupyter notebooks"
    ]
  },
  {
    "objectID": "python3/python-01-setup.html#starting-python",
    "href": "python3/python-01-setup.html#starting-python",
    "title": "Setup and running Jupyter notebooks",
    "section": "Starting Python",
    "text": "Starting Python\nThere are many ways to run Python commands:\n\nfrom a Unix shell you can start a Python shell and type commands there,\nyou can launch Python scripts saved in plain text *.py files,\nyou can execute Python cells inside Jupyter notebooks; the code is stored inside JSON files, displayed as HTML",
    "crumbs": [
      "DAY 1",
      "Setup and running Jupyter notebooks"
    ]
  },
  {
    "objectID": "python3/python-01-setup.html#todays-setup",
    "href": "python3/python-01-setup.html#todays-setup",
    "title": "Setup and running Jupyter notebooks",
    "section": "Today’s setup",
    "text": "Today’s setup\nToday we’ll be using JupyterHub on our training cluster. Point your browser to https://hss.c3.ca and log in with your username and password, then launch a JupyterHub server with time = 2.5 hours, 1 CPU core, memory = 3712 MB, GPU configuration = None, user interface = JupyterLab. Finally, start a new Python 3 notebook.\n\nAfter you log in, in the dashboard start a new Python 3 notebook.",
    "crumbs": [
      "DAY 1",
      "Setup and running Jupyter notebooks"
    ]
  },
  {
    "objectID": "python3/python-01-setup.html#navigating-jupyter-interface",
    "href": "python3/python-01-setup.html#navigating-jupyter-interface",
    "title": "Setup and running Jupyter notebooks",
    "section": "Navigating Jupyter interface",
    "text": "Navigating Jupyter interface\n\nFile | Save Notebook As - to rename your notebook\nFile | Download - download the notebook to your computer\nFile | New Launcher - to open a new launcher dashboard, e.g. to start a terminal\nFile | Log Out - to terminate your job (everything is running inside a Slurm job!)\n\n\n\n\n\n\n\nCautionNote to self\n\n\n\nExplain: tab completion, annotating code, displaying figures inside the notebook.\n\n\n\nEsc - leave the cell (border changes colour) to the control mode\nA - insert a cell above the current cell\nB - insert a cell below the current cell\nX - delete the current cell\nM - turn the current cell into the markdown cell\nH - to display help\nEnter - re-enter the cell (border becomes green) from the control mode\n\nYou can enter Latex expressions in a markdown cell, e.g. try typing \\int_0^\\infty f(x)dx inside two dollar signs.\nprint(1/2)   # to run all commands in the cell, either use the Run button, or press shift+return",
    "crumbs": [
      "DAY 1",
      "Setup and running Jupyter notebooks"
    ]
  },
  {
    "objectID": "python3/python-06-loops.html",
    "href": "python3/python-06-loops.html",
    "title": "Loops",
    "section": "",
    "text": "For loops are very common in Python and are similar to for in other languages, but one nice twist with Python is that you can iterate over any collection, e.g., a list, a character string, etc.\nThis is equivalent to:\nWhat will this do:\nLet’s sum numbers 1 to 10:",
    "crumbs": [
      "DAY 2",
      "Loops"
    ]
  },
  {
    "objectID": "python3/python-06-loops.html#while-loops",
    "href": "python3/python-06-loops.html#while-loops",
    "title": "Loops",
    "section": "While loops",
    "text": "While loops\nSince we talk about loops, we should also briefly mention while loops, e.g.\nx = 2\nwhile x &gt; 1.:\n    x /= 1.1\n    print(x)\n\n\n\n\n\n\nCautionExercise 6.4\n\n\n\n\n\nRemove all occurrences of a specific item in a list, e.g. in this case number 20 in the list [5, 20, 15, 20, 25, 50, 20], so it becomes [5, 15, 25, 50].\nHint: use while to check if 20 is still in the list.",
    "crumbs": [
      "DAY 2",
      "Loops"
    ]
  },
  {
    "objectID": "python3/python-06-loops.html#more-on-lists-in-loops",
    "href": "python3/python-06-loops.html#more-on-lists-in-loops",
    "title": "Loops",
    "section": "More on lists in loops",
    "text": "More on lists in loops\nYou can also form a zip object of tuples from two lists of the same length:\nfor i, j in zip(a,b):\n    print(i,j)\nAnd you can create an enumerate object from a list:\nfor i, j in enumerate(b):    # creates a list of tuples with an iterator as the first element\n    print(i,j)\n\n\n\n\n\n\n\n\n\nCautionExercise 6.5\n\n\n\n\n\nWrite a program to add two lists index-wise, e.g. ['M', 'na', 'i', 'Stu'] and ['y', 'me', 's', 'art'] should produce a single list ['My', 'name', 'is', 'Stuart'].\n\n\n\n\n\n\n\n\n\n\n\nList comprehensions\nIt’s a compact way to create new lists based on existing lists/collections. Let’s list squares of numbers from 1 to 10:\n[x**2 for x in range(1,11)]\nOf these, list only odd squares:\n[x**2 for x in range(1,11) if x%2==1]\nThe first list in the previous section was also generated via a list comprehension:\nevents = [random.randint(0,2024) for i in range(10)]\nYou can also use list comprehensions to combine information from two or more lists:\nweek = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\nweekend = ['Sat', 'Sun']\nprint([day for day in week])                         # the entire week\nprint([day for day in week if day not in weekend])   # only the weekdays\nprint([day for day in week if day in weekend])       # in both lists\nThe syntax is:\n[something(i) for i in list1 if i [not] in list2 if i [not] in list3 ...]\n\n\n\n\n\n\nCautionExercise 6.6\n\n\n\n\n\nRemove empty strings from the list of strings, e.g. [\"Mike\", \"\", \"Emma\", \"Kelly\", \"\", \"Brad\"] should become [\"Mike\", \"Emma\", \"Kelly\", \"Brad\"].\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise 6.7\n\n\n\n\n\nWrite a one-line code to sum up the squares of numbers from 1 to 100.\n\n\n\n\n\n\n\n\n\nCautionExercise 6.8\n\n\n\n\n\nWrite a script to build a list of words that are shorter than n characters from a given list of words ['red', 'green', 'white', 'black', 'pink', 'yellow'].\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise 6.9\n\n\n\n\n\nWrite a program to flatten a nested list, e.g. [[11, 21.0, 3.5], ['Mercury', 'Venus', 'Earth'], 'hello'] should become [11, 21.0, 3.5, 'Mercury', 'Venus', 'Earth', 'hello'].\nHint: try two nested loops, or maybe extend two nested loops to a nested list comprehension.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise 6.10\n\n\n\n\n\nWrite a program to convert a list of multiple integers into a single integer with all their digits combined, e.g. a list [11, 33, 50] should become 113350.",
    "crumbs": [
      "DAY 2",
      "Loops"
    ]
  },
  {
    "objectID": "python3/python-08-functions.html",
    "href": "python3/python-08-functions.html",
    "title": "Writing functions",
    "section": "",
    "text": "functions encapsulate complexity so that we can treat it as a single thing\nfunctions enable re-use: write one time, use many times\n\nFirst define:\ndef greeting():\n    print('Hello!')\nand then we can run it:\ngreeting()\ndef printDate(year, month, day):\n    joined = str(year) + '/' + str(month) + '/' + str(day)\n    print(joined)\nprintDate(1871, 3, 19)\nEvery function returns something, even if it’s None.\na = printDate(1871, 3, 19)\nprint(a)\nHow do we actually return a value from a function?\ndef average(values):   # the argument is a list\n    if len(values) == 0:\n        return None\n    return sum(values) / len(values)\nprint('average of actual values:', average([1, 3, 4]))\nHere is an example of a more complex calendar function returning an alphabetical day of the week:\ndef dayOfTheWeek(year, month, day):\n    import datetime\n    week = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n    return week[datetime.datetime(year, month, day).weekday()]\ndayOfTheWeek(2022, 11, 10)   # 'Thu'\n\n\n\n\n\n\nCautionExercise 8.1\n\n\n\n\n\nWrite a function to convert from Fahrenheit to Celsius, e.g. typing celsius(77) would produce 25.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise 8.2\n\n\n\n\n\nWrite a function to convert from Celsius to Fahrenheit. Test it with celcius(), e.g. by converting Fahrenheit → Celsius → Fahrenheit, or Celsius → Fahrenheit → Celsius.\n\n\n\n\n\n\n\n\n\nCautionExercise 8.3\n\n\n\n\n\nNow modify celsius() to take a list of Fahrenheit temperatures, e.g., celcius([70,80,90,100]), to return a list of Celsius temperatures.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCautionExercise 8.4\n\n\n\n\n\nWrite a function that takes two lists and returns True if they have at least one common member.\n\n\n\nFunction arguments in Python can take default values becoming optional:\ndef addNumber(a, b=1):\n    return a+b\nprint(addNumber(5))\nprint(addNumber(5,3))\nWith several optional arguments it is important to be able to differentiate them:\ndef modify(a, b=1, coef=1):\n    return a*coef + b\nprint(modify(10))\nprint(modify(10, 1))   # which argument did we add?\nprint(modify(10, coef=2))\nprint(modify(10, coef=2, b=5))\nAny complex python function will have many optional arguments, for example:\n?print",
    "crumbs": [
      "DAY 2",
      "Writing functions"
    ]
  },
  {
    "objectID": "python3/python-07-dictionaries.html",
    "href": "python3/python-07-dictionaries.html",
    "title": "Dictionaries",
    "section": "",
    "text": "As you just saw, Python’s lists are ordered sets of objects that you access via their position/index. Dictionaries are unordered sets in which the objects are accessed via their keys. In other words, dictionaries are unordered key-value pairs.\nConsider two lists:\nThere is nothing connecting these two lists, as far as figuring a person’s favourite colour goes. You could do something like this using indices:\nbut this is a little too convoluted … A dictionary can help you connect the two datasets directly:\nYou can also cycle using .keys(), .values() and .items() methods:\nThere are other ways to organize the same information using dictionaries. For example, you can create a list of dictionaries, one dictionary per person:\nThe benefit of this approach is that you can have many more attributes per person than just name and colour, and this is a very common way to organize structured and/or hierarchical data in Python. The downside is that – to search for by name – you have to do it explicitly:\nor in a single line:\nFinally, if you want performance, you might want to consider the following approach:\nHere we: 1. apply the lambda (anonymous) function  lambda x: x%2 == 0  to each item in range(1,11); it returns True or False, 2. create an iterator yielding only those items in range(1,11) for which the lambda function produced True, and 3. cycle through this iterator.\nUsing this approach, we can create an iterator of all people matching a name:\nHere we: 1. apply the lambda function lambda person: person[\"name\"] == \"Jeff\" to each item in the list data; it returns True or False, 2. create an iterator yielding only those items in data for which the lambda function produced True, and 3. create a list from this iterator, in this case containing only one element.\nGoing back to the basics, you can see where dictionary got its name:\nLet’s modify values:\nDeleting dictionary items:\nValues can also be numerical:\nAnd so can be the keys:",
    "crumbs": [
      "DAY 2",
      "Dictionaries"
    ]
  },
  {
    "objectID": "python3/python-07-dictionaries.html#sorting-dictionary-items",
    "href": "python3/python-07-dictionaries.html#sorting-dictionary-items",
    "title": "Dictionaries",
    "section": "“Sorting” dictionary items",
    "text": "“Sorting” dictionary items\nLet’s go back to our original dictionary:\nfav = {'Mary': 'orange', 'John': 'green', 'Eric': 'turquoise', 'Jeff': 'burgundy', 'Anne': 'turquoise'}\nsorted(fav)             # returns the sorted list of keys\nsorted(fav.keys())      # the same\nsorted(fav.values())    # returns the sorted list of values\nfor k in sorted(fav):\n    print(k, fav[k])    # full dictionary sorted by key\n\n\n\n\n\n\nCautionExercise 7.3\n\n\n\n\n\nWrite a script to print the full dictionary (keys and values) sorted by the value.\nHint: create a list comprehension looping through all (key,value) pairs and then try sorting the result.",
    "crumbs": [
      "DAY 2",
      "Dictionaries"
    ]
  },
  {
    "objectID": "python3/python-07-dictionaries.html#dictionary-comprehensions",
    "href": "python3/python-07-dictionaries.html#dictionary-comprehensions",
    "title": "Dictionaries",
    "section": "Dictionary comprehensions",
    "text": "Dictionary comprehensions\nSimilar to list comprehensions, we can form a dictionary comprehension:\n{k:'.'*k for k in range(10)}\n{k:v*2 for (k,v) in zip(range(10),range(10))}\n{j:c for j,c in enumerate('computer')}",
    "crumbs": [
      "DAY 2",
      "Dictionaries"
    ]
  },
  {
    "objectID": "python3/python-05-lists.html",
    "href": "python3/python-05-lists.html",
    "title": "Lists",
    "section": "",
    "text": "Python has a number of built-in composite data structures: tuples, lists, sets, dictionaries. Here we take a look at lists and dictionaries.\nA list stores many values in a single structure.\nevents = [267, 1332, 1772, 1994, 493, 1373, 1044, 156, 1515, 1788]  # array of years\nprint('events:', events)\nprint('length:', len(events))\nprint('first item of events is', events[0])   # indexing starts witgh 0\nevents[2] = 1773                              # individual elements are mutable\nprint('events is now:', events)\nevents.append(1239)   # append at the end\nevents.append(606)    # append at the end\nprint('events is now:', events)\nevents.pop(4)         # remove element #4\nprint('events is now:', events)\nevents.remove(267)    # remove by value (first occurrence)\na = []   # start with an empty list\na.append('Vancouver')\na.append('Toronto')\na.append('Kelowna')\nprint(a)\na[3]   # will give an error message (past the end of the array)\na[-1]   # display the last element; what's the other way?\na[:]    # will display all elements\na[1:]   # all elements starting from #1\na[:1]   # ending with but not including #1\nLists can be heterogeneous and nested:\na = [11, 21., 3.5]\nb = ['Mercury', 'Venus', 'Earth']\nc = 'hello'\nnestedList = [a, b, c]\nprint(nestedList)\n\n\n\n\n\n\nCautionExercise 5.1: double subsetting\n\n\n\n\n\nHow would you extract element “Earth” from nestedList?\n\n\n\nYou can search inside a list:\n'Venus' in b                   # returns True\n'Pluto' in b                   # returns False\nb.index('Venus')               # returns 1 (positional index)\nnestedList[1].index('Venus')   # same\nAnd you sort lists alphabetically:\nb.sort()\nb             # returns ['Earth', 'Mercury', 'Venus']\n\n\n\n\n\n\nCautionExercise 5.2\n\n\n\n\n\nWrite a script to find the second largest number in the list [77,9,23,67,73,21].",
    "crumbs": [
      "DAY 1",
      "Lists"
    ]
  },
  {
    "objectID": "globus.html",
    "href": "globus.html",
    "title": "Globus-cli for file transfer",
    "section": "",
    "text": "Thursday, October 14th, 2025\nAbstract: Globus is a widely used platform for secure, reliable, and high-performance data transfer across research systems. While the web interface is intuitive and remains our primary recommendation for most users, the command-line interface (CLI) offers advanced capabilities for automation, scripting, and managing large-scale workflows. In this webinar, we will present the globus-cli tool, show how to authenticate and configure your environment, and demonstrate key commands for transferring, monitoring, and managing files between endpoints. You will also learn tips for integrating globus-cli into scripts for repeatable and efficient data movement."
  },
  {
    "objectID": "globus.html#advantages-of-globus-transfer-over-traditional-scpsftprsync",
    "href": "globus.html#advantages-of-globus-transfer-over-traditional-scpsftprsync",
    "title": "Globus-cli for file transfer",
    "section": "Advantages of Globus transfer over traditional scp/sftp/rsync",
    "text": "Advantages of Globus transfer over traditional scp/sftp/rsync\nIn April, we hosted a webinar “Introduction to Globus”. Here is a quick recap of why you might choose Globus over traditional file transfer tools:\n\nParallel, multi-stream transfers: Globus uses the GridFTP protocol with multiple TCP streams, tuning them for wide-area networks, which can dramatically outperform single-threaded scp or sftp. This makes it ideal for transferring large-scale (multi-GB and multi-PB) datasets where scp/sftp typically fail or crawl. In short, Globus will try to maximize bandwidth between any two endpoints.\nAutomatic retrying and resuming: If a transfer is interrupted (network drop, machine reboot, etc), Globus resumes where it left off without user intervention.\nChecksum verification.\nFile transfer happens in the background, i.e. you can close your laptop and the transfer continues server-to-server, unlike with scp/sftp where the session must stay open.\n\nyou’ll get an email notification when a task completes or fails\ncan also check status with interactive commands\n\nProvides logs, progress tracking, notifications, and retry logic automatically.\n\nmany research institutions use Globus for secure, auditable data movement that meets funding agency or privacy requirements\n\nYou can share data with collaborators who don’t have direct system accounts."
  },
  {
    "objectID": "globus.html#different-ways-to-use-globus",
    "href": "globus.html#different-ways-to-use-globus",
    "title": "Globus-cli for file transfer",
    "section": "Different ways to use Globus:",
    "text": "Different ways to use Globus:\n\nBy default, Globus uses a Web-based interface to manage transfers via a web browser. However, this requires a lot of mouse clicking and sometimes navigating very large directory trees.\nThere is also a REST-style Application Programming Interface (API) that supports submitting and monitoring file transfers and managing Globus Connect Personal collections. This could be used in scripting languages like Python and Ruby to integrate Globus into web portals. This is not covered in this presentation.\nIn this webinar we will focus on using Globus from the command line to automate your transfers with shell commands and scripts/functions.\n\n\n\n\n\n\n\n\n\nCautionCaution\n\n\n\nFiles aren’t stored on Globus – they reside on the underlying filesystem. If you don’t intend to remove files from your directories, avoid deleting them through the Globus interface."
  },
  {
    "objectID": "globus.html#install-globus-cli-tools",
    "href": "globus.html#install-globus-cli-tools",
    "title": "Globus-cli for file transfer",
    "section": "Install Globus CLI tools",
    "text": "Install Globus CLI tools\n\nOn your computer\nuv venv ~/env-globus --python 3.12   # create a new virtual environment\nsource ~/env-globus/bin/activate\nuv pip install globus-cli\n...\ndeactivate\n\n\nOn a cluster\nnibi\nmodule avail python                # several versions available\nmodule load python/3.12.4\npython -m venv ~/env-globus        # install Python tools in your $HOME/env-globus\nsource ~/env-globus/bin/activate   # load the environment\npython -m pip install --upgrade pip --no-index\npython -m pip install --no-index globus_cli setuptools   # all these will go into your $HOME/env-globus\n...\ndeactivate\n\n\n\n\n\n\nCautionUpdating on a cluster\n\n\n\nAs of this moment, the default version of globus-cli on our clusters is slightly old, so some functionality mentioned today is not available there. Fortunately, you can upgrade globus-cli in place very easily:\nsource ~/env-globus/bin/activate\nglobus version   # 3.2.0, latest is 3.38.0\nglobus update"
  },
  {
    "objectID": "globus.html#getting-help",
    "href": "globus.html#getting-help",
    "title": "Globus-cli for file transfer",
    "section": "Getting help",
    "text": "Getting help\n\nsource ~/env-globus/bin/activate\neval \"$(globus --bash-completer)\"   # enable tab completion in bash version 4.4 or newer\nglobus --help             # or -h\nglobus &lt;command&gt; --help   # or -h\nglobus ls --help\nglobus --help\nglobus list-commands\n\nCLI reference  🡐  index of all commands with detailed manual pages"
  },
  {
    "objectID": "globus.html#globus-terminology",
    "href": "globus.html#globus-terminology",
    "title": "Globus-cli for file transfer",
    "section": "Globus terminology",
    "text": "Globus terminology\n\nEndpoint is a server or system where data can be transferred to/from, e.g. an HPC cluster or your own computer.\nCollection is a named access point to a specific directory or dataset on an endpoint, with its own permissions and sharing settings.\n\nSometimes the two are used interchangeably as you can you can transfer data to/from both, but they mean different things.\nThere are two main types of collections:\n\nMapped collections expose a specific directory for authenticated users on a multi-user system, e.g. your home folder on a cluster. You typically have read+write access to your own home/scratch/project directory mapped collection, but other Globus users don’t.\nGuest collections are shared subsets of mapped collections, can be shared with other Globus users without giving full system access. An example would be a subdirectory in your home directory on a cluster that you open for read (or maybe write) access to your collaborator."
  },
  {
    "objectID": "globus.html#starting-up-and-authenticating",
    "href": "globus.html#starting-up-and-authenticating",
    "title": "Globus-cli for file transfer",
    "section": "Starting up and authenticating",
    "text": "Starting up and authenticating\nIn Globus there are two levels of authentication.\n\nFirst, you need to log in to Globus itself:\n\n\nglobus login   # log in to a Globus session through your browser\nglobus whoami\nglobus whoami --verbose\nglobus session show   # show all active sessions\n# globus logout   # might run this at some point if you no longer need Globus\nOne person can have multiple Globus accounts, e.g. one with the Alliance, one with Google, one directly with Globus ID.\n\nSecond, when you access an endpoint (e.g. globus ls ..., globus transfer ...) inside a session, you will need to give Globus permission to access that endpoint / filesystem on your behalf. Theoretically, these “consents” can be managed via https://auth.globus.org/consents where you can see and revoke them individually. In practice I find it somewhat difficult to tell which endpoint is which (since it does not name them!), so the only way I can effectively use this site is by removing all existing consents.\n\nLet’s see how consents work in practice. We define a couple of Globus tutorial collections through their IDs:\nexport T1=6c54cade-bde5-45c1-bdea-f4bd71dba2cc   # provided by Globus\nexport T2=31ce9ba0-176d-45a5-add3-f37d233ba47d   # another mirror of the same collection\nglobus ls $T1\nIf this is your first time accessing this collection after globus login, you will likely get “The collection you are trying to access data on requires you to grant consent for the Globus CLI to access it” along with a suggested command to run:\nglobus session consent &lt;authentication_scope&gt;   # opens a browser page\nglobus ls $T1                      # empty\nglobus ls $T1:/home/share/godata   # three small files\n\n\n\n\n\n\n\nNoteFrom Globus manual\n\n\n\n\nWhen the Globus CLI is used to interact with a collection, it may find that it has not been granted sufficient permissions to access the collection. In these cases, the CLI will prompt you to run a login command to grant it the necessary consent.\nThere are also other cases, such as strict authentication policies, in which the CLI dynamically discovers requirements which require a new login flow.\n\n\n\n\n\n\n\n\n\nNoteInfinite authentication loop\n\n\n\nSometimes you might get stuck in an infinite authentication loop. After having logged into Globus, when trying to list or transfer files, if you are missing that endpoint’s consent, Globus should tell you to run a very specific command globus session consent &lt;authentication_scope&gt;. Sometimes, if some Globus machinery behind the scenes is not updated properly, you might get instead:\nThe resource you are trying to access requires you to re-authenticate with specific identities.\nmessage: Missing required data_access consent\nPlease use \"globus session update\" to re-authenticate with specific identities\nIf you follow this advice, you will get stuck in an infinite authentication loop. In my experience, waiting for a day or so should automatically resolve the problem. Alternatively, email us at globus@tech.alliancecan.ca to open a support ticket."
  },
  {
    "objectID": "globus.html#other-synchronous-blocking-commands",
    "href": "globus.html#other-synchronous-blocking-commands",
    "title": "Globus-cli for file transfer",
    "section": "Other synchronous (blocking) commands",
    "text": "Other synchronous (blocking) commands\nglobus ls \"$COLLECTION\" is an example of a blocking Globus command that waits for the completion of the task before exiting. Other blocking command examples are:\nglobus ls $T1                      # empty: this is my own playground\nglobus mkdir $T1:test              # create a new directory\nglobus mkdir $T1:test/today        # create a subdirectory inside\n\nglobus rename $T1 test/today test/tomorrow   # diff from Unix\n                                             # not moving files via Globus!\nglobus ls $T1:test\n\nglobus rename $T1 test/tomorrow tomorrow   # can move objects across directories\nglobus ls $T1\n\nglobus rm --recursive $T1:test             # --recursive only needed when not empty\nglobus rm --recursive $T1:tomorrow\nglobus ls $T1\n\n\n\n\n\n\nNote\n\n\n\nglobus delete is a non-blocking version of globus rm, with mostly the same flags."
  },
  {
    "objectID": "globus.html#single-file-transfer",
    "href": "globus.html#single-file-transfer",
    "title": "Globus-cli for file transfer",
    "section": "Single file transfer",
    "text": "Single file transfer\n\nTesting within the tutorial endpoint\nglobus mkdir $T1:test\nglobus ls $T1:/home/share/godata   # file1.txt file2.txt file3.txt\nglobus transfer $T1:/home/share/godata/file1.txt $T1:test/file1.txt\n\n\n\n\n\n\nNoteThe most common error with globus-cli\n\n\n\nWith globus transfer for a single file, it is very important to specify the full file path in the destination that includes the file name, not just the target directory. Without the file name, the transfer will eventually fail, after trying for 1 hour.\n\n\nThis command is non-blocking, i.e. it will exit back to the command prompt, while transfer will take place in the background. Note that you will receive an email when transfer is complete. If you want to block until transfer completion in the terminal, run:\nglobus task wait &lt;taskID_from_the_previous_line&gt;\nglobus ls -l $T1:test\nglobus rm -r $T1:test\n\n\n\n\n\n\nNote\n\n\n\nglobus ls works on directories, not files, e.g. try running globus ls $T1:test/file1.txt. On the other hand, globus transfer works with both directories and files.\n\n\n\n\nTransfer a tutorial collection file to Fir\nglobus endpoint search alliance\nglobus endpoint search alliance | grep fir   # use the ID of alliancecan#fir-globus in the next line\n\nexport FIR=8dec4129-9ab4-451d-a45f-5b4b8471f7a3\nglobus ls $FIR   # might or might not have access; using a mapped collection\n\n# globus session consent &lt;authentication_scope&gt;\n\nglobus ls $FIR           # should see my Fir home directory\nglobus ls $FIR:scratch   # using another mapped collection\n\nglobus transfer $T1:/home/share/godata/file1.txt $FIR:scratch\nglobus task show &lt;taskID_from_the_previous_line&gt;\nglobus task show &lt;taskID_from_the_previous_line&gt; | grep -E 'Status|Faults'\nglobus task event-list --filter-errors &lt;taskID&gt;       # shows the actual error\nglobus task cancel &lt;taskID&gt;\n\nglobus transfer $T1:/home/share/godata/file1.txt $FIR:scratch/f1.txt\nglobus task list --filter-status ACTIVE   # show all my active tasks\nglobus task wait &lt;taskID&gt;\n\n\n\n\nTransfer a large (8.1GB) file from Fir to Nibi\nglobus endpoint search alliance | grep nibi\nexport NIBI=07baf15f-d7fd-4b6a-bf8a-5b5ef2e229d3\nexport FIR=8dec4129-9ab4-451d-a45f-5b4b8471f7a3\n\nglobus transfer $FIR:projects/def-razoumov-ac/razoumov/ieeevis2017-clouds/2d_lonlat_20.nc $NIBI:scratch/2d_lonlat_20.nc\nglobus ls -l $NIBI:scratch | grep 2d_lonlat_20.nc\n\nglobus task show &lt;taskID&gt; | grep -E 'Status|Faults'\nThis command will fail if one or more endpoints require authentication. Fortunately, in this case you will be prompted to run the following command:\nglobus session consent &lt;authentication_scope&gt;   # in browser; select razoumov@computecanada.ca\nThen you can rerun the transfer command:\nglobus transfer $FIR:projects/def-razoumov-ac/razoumov/ieeevis2017-clouds/2d_lonlat_20.nc $NIBI:scratch/2d_lonlat_20.nc\nglobus task show &lt;taskID&gt; | grep -E 'Status|Faults'"
  },
  {
    "objectID": "globus.html#multiple-files",
    "href": "globus.html#multiple-files",
    "title": "Globus-cli for file transfer",
    "section": "Multiple files",
    "text": "Multiple files\nGlobus CLI does not support Unix shell wildmasks in the source path. However, there are several workarounds:\n\nPack your multiple files into an archive.\n, copy a directory with a --include flag, e.g. this will copy only *.sh files:\n\nglobus transfer --recursive --include '*.sh' --exclude '*' $FIR:syncHPC/testEGL/ $NIBI:scratch\n\n\n\n\n\n\nNote\n\n\n\n\n--include requires --recursive\nin my tests, --exclude '*' is needed to make sure no other files are copied\nyou can specify many filters in a row, e.g. --include '*.txt' --include '*.md'\n\n\n\n\nAlternatively, you can use --batch to list multiple specific files with their source and destination paths:\n\n# store this as include.txt: a list of source paths followed by destination paths\nalexWithUpdatedDriver.sh alexWithUpdatedDriver.sh\nbartWithCorrection.sh bartWithCorrection.sh\ntest.sh test.sh   # Slurm scripting for testing\nglobus transfer $FIR:syncHPC/testEGL/ $NIBI:scratch --batch include.txt\nWith this approach, you can mix multiple paths in a single file, e.g.\nalexWithUpdatedDriver.sh a/alexWithUpdatedDriver.sh\nbartWithCorrection.sh b/bartWithCorrection.sh\ntest.sh test.sh   # Slurm scripting for testing\nwould create the subdirectories a/ and b/ inside ~/tmp/ as part of the transfer.\nYou can also delete files in batch:\nglobus delete \"COLLECTION:path/\" --batch delete.txt\n\nFinally, you can sync two directories – this is described in a separate section below."
  },
  {
    "objectID": "globus.html#globus-connect-personal",
    "href": "globus.html#globus-connect-personal",
    "title": "Globus-cli for file transfer",
    "section": "Globus Connect Personal",
    "text": "Globus Connect Personal\nGlobus Connect Personal (GCP) turns a local machine into a Globus endpoint. This way you can transfer files with Globus from/to your computer.\n\nDownload Globus Connect Personal from https://app.globus.org/collections/gcp.\nInstall and run “Globus Connect Personal” application.\nClick Log in, follow in the browser, create a unique name for your endpoint (I used “alexrsfu”), provide details, save, exit setup.\nIn the menu bar click Globus Connect Personal | Preferences…\nAccess tab, specify directory access details\nInfo tab | write down your Endpoint ID \nContinue running Globus Connect Personal in the background.\n\n\nTransfer a large file from Fir to my laptop\nglobus endpoint search alexrsfu   # might take a while to refresh\nexport FIR=8dec4129-9ab4-451d-a45f-5b4b8471f7a3\nexport LAPTOP=f81086c2-a870-11f0-bd08-027493648695\n\nglobus ls $LAPTOP:tmp       # my local files\nls ~/tmp                    # should be the same\n\nglobus ls -l $FIR:data/deepImpact\nglobus ls -l $FIR:data/deepImpact | grep yB31_oneblock_46521.vti   # pick one of the files (590MB in size)\n\nglobus transfer $FIR:data/deepImpact/yB31_oneblock_46521.vti $LAPTOP:tmp/   # won't succeed (destination != filename)\nglobus task show &lt;taskID&gt; | grep -E 'Status|Faults'   # shows that it's ACTIVE but there are some Faults\nglobus task event-list --filter-errors &lt;taskID&gt;       # shows the actual error\nglobus task cancel &lt;taskID&gt;\n\nglobus transfer $FIR:data/deepImpact/yB31_oneblock_46521.vti $LAPTOP:tmp/46521.vti   # this one works great!\nYou can create endpoints for any number of personal computers. However, transfer between two personal endpoints is not enabled by default. If you need this capability, please contact open a Globs ticket to set up a Globus Plus account."
  },
  {
    "objectID": "globus.html#capturing-task-id",
    "href": "globus.html#capturing-task-id",
    "title": "Globus-cli for file transfer",
    "section": "Capturing task ID",
    "text": "Capturing task ID\n\nOption 1: Capture at scheduling time\nLet’s copy a file from a Globus tutorial collection to Fir’s /scratch:\nexport T1=6c54cade-bde5-45c1-bdea-f4bd71dba2cc\nexport FIR=8dec4129-9ab4-451d-a45f-5b4b8471f7a3\ntaskid=$(globus transfer $T1:/home/share/godata/file1.txt $FIR:scratch/file1.txt | grep \"Task ID\" | awk '{print $3}')\nglobus task show $taskid | grep -E 'Status|Faults'\nglobus task wait $taskid\n\n\nOption 2: Use labels to filter current tasks\n\nassign a label to a Globus task\nfilter all your Globus tasks by this label\nextract the taskID and use it for getting details on the task\n\nlabel=\"copying a Globus tutorial file\"\nlabel=$RANDOM$RANDOM$RANDOM   # labels don't have to be unique, but having a unique label helps\nglobus transfer --label $label $T1:/home/share/godata/file1.txt $FIR:scratch/file1.txt\ntaskid=$(globus task list --filter-label $label | tail -1 | awk '{print $1}')\nglobus task show $taskid | grep -E 'Status|Faults'\nglobus task wait $taskid"
  },
  {
    "objectID": "globus.html#automating-workflows",
    "href": "globus.html#automating-workflows",
    "title": "Globus-cli for file transfer",
    "section": "Automating workflows",
    "text": "Automating workflows\nUsing one of these two approaches to handle task IDs, you can create a shell script to automate Globus transfers. However, be aware that you may occasionally encounter authentication or other issues, so treat these scripts as a convenience rather than a fully reliable black box.\nEveryone’s workflow is different, so there is no one-size-fits-all script for every purpose. Instead, here are a few interactive functions that simplify my own day-to-day Globus workflows: gcp(), gshow(), gwait(), gerror() and gcancel().\nThe command gcp:\n\nprovides a scp-like syntax, where source and dest could be “fir”, “nibi”, “rorqual”, “trillium”, “laptop”\nstores endpoint IDs (don’t need to define these separately)\nchecks if the source and destination extensions are the same  ⮕  forces you to enter a valid file name in the destination (providing just a directory will result in failed transfer)\n\nfunction gcp() {\n    if [ $# -eq 0 ]; then\n    echo \"No arguments specified ... Usage: gcp source:/path/to/src/file.ext dest:/path/to/dest/file.ext\"\n    return 1\n    fi\n    SRC=\"${1%%:*}\"    # endpoint name before the colon in the 1st argument\n    DEST=\"${2%%:*}\"   # endpoint name before the colon in the 2nd argument\n    PATH1=\"${1#*:}\"   # file path after the colon in the 1st argument\n    PATH2=\"${2#*:}\"   # file path after the colon in the 2nd argument\n    echo $SRC $DEST\n    rename() {\n    SRC=\"${1//$3/$4}\"\n    DEST=\"${2//$3/$4}\"\n    }\n    rename \"$SRC\" \"$DEST\" fir 8dec4129-9ab4-451d-a45f-5b4b8471f7a3\n    rename \"$SRC\" \"$DEST\" nibi 07baf15f-d7fd-4b6a-bf8a-5b5ef2e229d3\n    rename \"$SRC\" \"$DEST\" trillium ad462f99-8436-42b4-adc6-3644e36c1b67\n    rename \"$SRC\" \"$DEST\" rorqual 93b2625f-a4ba-47ac-90d4-0bbb5ae19451\n    rename \"$SRC\" \"$DEST\" laptop f81086c2-a870-11f0-bd08-027493648695\n    extension() {\n    filename=$(basename \"$1\")\n    if [[ \"$filename\" == *.* ]]; then\n        extension=\"${filename##*.}\"\n    else\n        extension=\"\"\n    fi\n    echo \"$extension\"\n    }\n    EXT1=$(extension \"$PATH1\")\n    EXT2=$(extension \"$PATH2\")\n    if [[ \"$EXT1\" != \"$EXT2\" ]]; then\n    echo \"Extension mismatch ($EXT1 and $EXT2) ... exiting\"\n    return 1\n    fi\n    globus transfer $SRC:$PATH1 $DEST:$PATH2\n}\n\nfunction gshow() {\n    if [ $# -eq 0 ]; then\n    echo \"No arguments specified ... Usage: gv &lt;taskID1&gt; &lt;taskID2&gt; ...\"\n    echo \"Therefore, showing all active tasks:\"\n    globus task list --filter-status ACTIVE\n    return 1\n    fi\n    for task in $@; do\n    globus task show $task | grep -E 'Status|Faults'\n    done\n}\n\nfunction gwait() {\n    globus task wait $1\n}\n\nfunction gerror() {\n    globus task event-list --filter-errors $1\n}\n\nfunction gcancel() {\n    globus task cancel $1\n}\nWith these functions defined, the two earlier transfers can be written as:\ngcp fir:projects/def-razoumov-ac/razoumov/ieeevis2017-clouds/2d_lonlat_20.nc nibi:scratch/2d_lonlat_20.nc\ngcp fir:data/deepImpact/yB31_oneblock_46521.vti laptop:tmp/46521.vti\ngshow                          # show all active tasks\ngshow taskID1 taskID2          # show details of these tasks\ngwait taskID1                  # block until taskID1 completes"
  },
  {
    "objectID": "globus.html#syncing-directories",
    "href": "globus.html#syncing-directories",
    "title": "Globus-cli for file transfer",
    "section": "Syncing directories",
    "text": "Syncing directories\nGlobus sync makes the destination match the source:\nglobus transfer --sync-level &lt;level&gt; sourceEndpointID:/path/to/source/ destEndpointID:/path/to/dest/\nwhere &lt;level&gt; is one of:\n\nexists  →  only send if destination file does not exist\nsize  →  send if size differs\nmtime  →  send if modification time differs\nchecksum  →  compute checksums and send if they differ (slowest)\n\n\n\n\n\n\n\nNoteOptional flag\n\n\n\n--recursive will include subdirectories.\n\n\n\n\n\n\n\n\nNote\n\n\n\nGlobus sync doesn’t delete extra files on the destination, i.e. there is no rsync --delete equivalent.\n\n\nHere is a complete example:\nexport FIR=8dec4129-9ab4-451d-a45f-5b4b8471f7a3\nexport LAPTOP=f81086c2-a870-11f0-bd08-027493648695\nmkdir -p ~/tmp   # no need to create ~/tmp/testEGL\nglobus transfer --sync-level size --recursive $FIR:syncHPC/testEGL/ $LAPTOP:tmp/testEGL/\nTo automate this, I would probably create a separate gsync() function."
  },
  {
    "objectID": "globus.html#running-globus-cli-on-a-cluster",
    "href": "globus.html#running-globus-cli-on-a-cluster",
    "title": "Globus-cli for file transfer",
    "section": "Running globus-cli on a cluster",
    "text": "Running globus-cli on a cluster\nRefer to an earlier section to install globus-cli in a virtual environment on a cluster.\nsource ~/env-globus/bin/activate   # load the environment\n\nglobus login    # log in by pasting the URL into a browser; copy the code back into the terminal\nglobus whoami   # returns razoumov@computecanada.ca\n\nglobus endpoint search alliance | grep -e fir -e nibi\nexport FIR=8dec4129-9ab4-451d-a45f-5b4b8471f7a3\nexport NIBI=07baf15f-d7fd-4b6a-bf8a-5b5ef2e229d3\nglobus ls $FIR   # prompts to run the following command\nglobus session consent &lt;authentication_scope&gt;   # log in by pasting the URL into a browser; copy the code back\nglobus ls $FIR   # works now!\n\nglobus ls -l $FIR:projects/def-razoumov-ac/razoumov/ieeevis2017-clouds | grep 2d_lonlat_20.nc   # 8.03GB\nglobus transfer $FIR:projects/def-razoumov-ac/razoumov/ieeevis2017-clouds/2d_lonlat_20.nc $NIBI:scratch/2d_lonlat_20.nc\nglobus task show 95ca1d32-9ee8-11f0-afcf-0e1cc5cf4f03 | grep -E 'Status|Faults'\nInterestingly, if you are running Globus Connect Personal on your machine which is behind the firewall, you can still initialize transfer to/from your computer on the cluster, which will never work with scp/sftp/rsync:\nexport LAPTOP=f81086c2-a870-11f0-bd08-027493648695\nglobus transfer $NIBI:data/mandelbulb800.nc $LAPTOP:tmp/800.nc"
  },
  {
    "objectID": "globus.html#globus-sharing",
    "href": "globus.html#globus-sharing",
    "title": "Globus-cli for file transfer",
    "section": "Globus sharing",
    "text": "Globus sharing\nGlobus sharing enables people to access files stored on your account on an Alliance cluster even if they don’t have an account on that system. You can find some Web interface-specific details in our documentation https://docs.alliancecan.ca/wiki/Globus#Globus_sharing. You can share with individual Globus users and with groups. Here we’ll demo sharing with users.\n\n\n\n\n\n\nNoteGetting your collaborator’s Globus userID\n\n\n\nYou will need your collaborator’s Globus userID to give them access. You can look it up yourself using their Globus username, e.g.\nglobus get-identities razoumov@computecanada.ca\nor you can ask them for it. They can look it up via the web interface:\n\nlog in to https://globus.alliancecan.ca\nSettings | Account | select Identity from the list | click the ↓ arrow to see your ID\n\nor in many different ways in the CLI:\nglobus get-identities &lt;collaboratorUserName&gt;\nglobus whoami --verbose | grep ID\nglobus session show\nE.g., when I run these commands myself, all three return the same useID. \n\n\nTo demo sharing, I will use my alternative Globus ID (log in via https://www.globus.org and organization = Globus ID):\nglobus get-identities razoumov@globusid.org                  # userName ⟹ userID\nglobus get-identities ae739fc4-d274-11e5-b957-ab0e5eadc80e   # userID ⟹ userName\n\nexport COLLABORATOR=ae739fc4-d274-11e5-b957-ab0e5eadc80e  # could be user or group ID\nexport FIR=8dec4129-9ab4-451d-a45f-5b4b8471f7a3\n\n\n\n\n\n\nImportantGuest collections inside GCP\n\n\n\nCreating guest collections on a Globus Connect Personal mapped collection is supported, but it requires a paid subscription. Without it, you cannot share files hosted on your own computer. See the details here. However, you can host a guest collection on a Globus Connect Server mapped collection on an Alliance cluster where we already have a subscription.\n\n\nYou can share your files stored on an Alliance cluster with any Globus user in the world, even if that user does not have an account on that cluster.\n\n/home – yes on all general-purpose clusters (not sure on Trillium)\n/scratch – yes on Beluga, Narval, Rorqual, no on other clusters\n/project – on PI’s request only (to prevent sharing other users’ files)\n\nHere we will create a guest collections on Fir’s /home and then we’ll share it with our collaborator.\n\n\n\n\n\n\nCautionRunning this workflow on the cluster\n\n\n\nAs of this moment, the default version of globus-cli on our clusters is slightly old, so the command globus collection create ... does not exist there. Therefore, you can either upgrade globus-cli in place yourself:\nsource ~/env-globus/bin/activate\nglobus version   # 3.2.0, latest is 3.38.0\nglobus update\nor you can run this workflow on your own computer where presumably you have the latest version installed.\n\n\nOn Fir, let’s create ~/share and place a file (e.g. noise.silo) there. In Globus, Fir is already registered as a Globus Connect Server v5 Mapped Collection, so you can create a new guest collection inside it:\nglobus collection create guest $FIR /home/razoumov/share project004   # create a new guest collection\nAt this point Globus might ask you to run globus login --gcs ... to verify who you are, and then you can rerun the command:\nglobus collection create guest $FIR /home/razoumov/share project004   # create a new guest collection\n# Display Name:                     project004\n# Owner:                            razoumov@computecanada.ca\n# ID:                               2342c760-b98b-40cc-b3f7-9550dce48c83\n# Collection Type:                  guest\n# ...\n# Public:                           True\n“Public” means that anyone with a Globus account can search for this collection with globus endpoint search project004, but they cannot see the files inside it. For file access, give your collaborator permission (r or rw) to access your collection, e.g. at its root level (that will point to ~/share):\nCOLLECTION=2342c760-b98b-40cc-b3f7-9550dce48c83\nglobus endpoint permission create --identity $COLLABORATOR --permissions rw $COLLECTION:/\n# Message: Access rule created successfully.\n# Rule ID: 228e0e90-a07f-11f0-b159-0affca67c55f\nIf instead you want to share with all logged-in Globus users:\nglobus endpoint permission create --permissions r --all-authenticated $COLLECTION:/\nIf you want to share with anyone (no login needed):\nglobus endpoint permission create --permissions r --anonymous $COLLECTION:/\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep the permission’s ruleID, as you will need it later to modify/revoke access.\n\n\nAt this point your collaborator should be able to see the files inside your collection:\nglobus endpoint search project004   # if they have just the name =&gt; returns collectionID\nglobus ls 0a13e15f-4848-4b63-aa87-31202640eff4   # should see noise.silo\nand transfer files (download with r and download/upload with rw permissions) with globus transfer ... or via the web interface.\nSince this is your own collection, you should be able to use it as well, e.g. – after starting Globus Connect Personal – you can transfer files between this collection and your computer:\nexport LAPTOP=f81086c2-a870-11f0-bd08-027493648695\nglobus transfer $COLLECTION:noise.silo $LAPTOP:tmp/n.silo   # download a file\ndate &gt; ~/tmp/test.txt\nglobus transfer $LAPTOP:tmp/test.txt $COLLECTION:/test.txt   # upload a file\nor, after adding “project004” to the gcp() shell function:\ngcp project004:noise.silo laptop:tmp/n.silo    # download a file\ngcp laptop:tmp/test.txt project004:/test.txt   # upload a file\nAfter your project is done, you might want to remove permissions:\nglobus endpoint permission list $COLLECTION   # show all permissions for this collection with ruleIDs\nglobus endpoint permission delete $COLLECTION 228e0e90-a07f-11f0-b159-0affca67c55f   # collectionID ruleID\nand eventually delete the guest collection:\nglobus collection delete $COLLECTION\n\n\n\n\n\n\nCautionDelete order\n\n\n\nIf you delete the collection before removing its permissions, the permissions will get orphaned, and you will still see them listed (without ruleIDs) for any future collections created with the same shared directory – I don’t know how to delete these permissions in this case. Therefore, make sure to delete any associated permissions before deleting the collection."
  },
  {
    "objectID": "globus.html#globus-flows",
    "href": "globus.html#globus-flows",
    "title": "Globus-cli for file transfer",
    "section": "Globus flows",
    "text": "Globus flows\nFlow is a series of steps that are performed in a specified order, e.g. you can run tar, delete files, add some other processing before / after transfer, share files, etc. A flow definition is a JSON document.\n\nhow to create a flow https://docs.globus.org/guides/tutorials/flow-automation/create-a-flow\n\ncd ~/tmp\ngit clone https://github.com/globus/globus-flows-trigger-examples.git\nglobus flows create \"My Transfer and Share Flow Example\" transfer_share/definition.json --input-schema transfer_share/schema.json\n\ntheir own example contains deprecated flow commands … someone needs to update the tutorial script\ntoo advanced for this webinar"
  },
  {
    "objectID": "globus.html#links",
    "href": "globus.html#links",
    "title": "Globus-cli for file transfer",
    "section": "Links",
    "text": "Links\n\nCLI front page\nCLI QuickStart Guide  🡐  good starting point\nCLI examples  🡐  nice examples\nFAQ in all categories  🡐  a must-read\nCLI reference  🡐  index of all commands\nOpen a ticket with any Globus issue on an Alliance system\nHigh Assurance Collections for Protected Data enable a subset of stricter security and compliance features\n\ncan enable MFA and other provider restriction for stricter endpoint access control\ndetailed logging and auditing\ngood for medical research institutions\nseparate commercial subscription tier at an extra cost\nnot covered today (except for Globus Connect Personal)"
  },
  {
    "objectID": "solutions-chapel.html",
    "href": "solutions-chapel.html",
    "title": "Solutions for Chapel course",
    "section": "",
    "text": "To see the evolution of the temperature at the top right corner of the plate, we just need to modify iout and jout. This corner correspond to the first row (iout=1) and the last column (jout=cols) of the plate.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 1.48171\nTemperature at iteration 40: 0.767179\n...\nTemperature at iteration 460: 0.068973\nTemperature at iteration 480: 0.0661081\nTemperature at iteration 500: 0.0634717\n\n\n\nTo get the linear distribution, the 80 degrees must be divided by the number of rows or columns in our plate. So, the following couple of for loops at the start of time iteration will give us what we want:\n// boundary conditions\nfor i in 1..rows do\n  T[i,cols+1] = i*80.0/rows;   // right side\nfor j in 1..cols do\n  T[rows+1,j] = j*80.0/cols;   // bottom side\nNote that 80 degrees is written as a real number 80.0. The division of integers in Chapel returns an integer, then, as rows and cols are integers, we must have 80 as real so that the result is not truncated.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nThe idea is simple: after each iteration of the while loop, we must compare all elements of Tnew and T, find the greatest difference, and update delta with that value. The following nested for loops should do the job:\n// update delta, the greatest difference between Tnew and T\ndelta = 0;\nfor i in 1..rows do {\n  for j in 1..cols do {\n    tmp = abs(Tnew[i,j] - T[i,j]);\n    if tmp &gt; delta then delta = tmp;\n  }\n}\nClearly there is no need to keep the difference at every single position in the array, we just need to update delta if we find a greater one.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nFor example, lets use a 650 x 650 grid and observe the evolution of the temperature at the position (200,300) for 10000 iterations or until the difference of temperature between iterations is less than 0.002; also, let’s print the temperature every 1000 iterations.\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ ./baseSolver --rows=650 --cols=650 --iout=200 --jout=300 --niter=10000 --tolerance=0.002 --nout=1000\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position after 7750 iterations is: 24.9671\nThe greatest difference in temperatures between the last two iterations was: 0.00199985\n\n\n\nWithout --fast the calculation will become slower by ~95X."
  },
  {
    "objectID": "solutions-chapel.html#part-1-basic-language-features",
    "href": "solutions-chapel.html#part-1-basic-language-features",
    "title": "Solutions for Chapel course",
    "section": "",
    "text": "To see the evolution of the temperature at the top right corner of the plate, we just need to modify iout and jout. This corner correspond to the first row (iout=1) and the last column (jout=cols) of the plate.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 1.48171\nTemperature at iteration 40: 0.767179\n...\nTemperature at iteration 460: 0.068973\nTemperature at iteration 480: 0.0661081\nTemperature at iteration 500: 0.0634717\n\n\n\nTo get the linear distribution, the 80 degrees must be divided by the number of rows or columns in our plate. So, the following couple of for loops at the start of time iteration will give us what we want:\n// boundary conditions\nfor i in 1..rows do\n  T[i,cols+1] = i*80.0/rows;   // right side\nfor j in 1..cols do\n  T[rows+1,j] = j*80.0/cols;   // bottom side\nNote that 80 degrees is written as a real number 80.0. The division of integers in Chapel returns an integer, then, as rows and cols are integers, we must have 80 as real so that the result is not truncated.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nThe idea is simple: after each iteration of the while loop, we must compare all elements of Tnew and T, find the greatest difference, and update delta with that value. The following nested for loops should do the job:\n// update delta, the greatest difference between Tnew and T\ndelta = 0;\nfor i in 1..rows do {\n  for j in 1..cols do {\n    tmp = abs(Tnew[i,j] - T[i,j]);\n    if tmp &gt; delta then delta = tmp;\n  }\n}\nClearly there is no need to keep the difference at every single position in the array, we just need to update delta if we find a greater one.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nFor example, lets use a 650 x 650 grid and observe the evolution of the temperature at the position (200,300) for 10000 iterations or until the difference of temperature between iterations is less than 0.002; also, let’s print the temperature every 1000 iterations.\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ ./baseSolver --rows=650 --cols=650 --iout=200 --jout=300 --niter=10000 --tolerance=0.002 --nout=1000\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position after 7750 iterations is: 24.9671\nThe greatest difference in temperatures between the last two iterations was: 0.00199985\n\n\n\nWithout --fast the calculation will become slower by ~95X."
  },
  {
    "objectID": "solutions-chapel.html#part-2-task-parallelism",
    "href": "solutions-chapel.html#part-2-task-parallelism",
    "title": "Solutions for Chapel course",
    "section": "Part 2: task parallelism",
    "text": "Part 2: task parallelism\n\nSolution to “Task.1”\nThe following code is a possible solution:\nvar x = 1;\nconfig var numthreads = 2;\nvar messages: [1..numthreads] string;\nwriteln('This is the main thread: x = ', x);\ncoforall threadid in 1..numthreads do {\n  var c = threadid**2;\n  messages[threadid] = 'this is thread ' + threadid:string + ': my value of c is ' + c:string + ' and x is ' + x:string;  // add to a string\n}\nwriteln('This message will not appear until all threads are done ...');\nfor i in 1..numthreads do  // serial loop, will be printed in sequential order\n  writeln(messages[i]);\n$ chpl consecutive.chpl -o consecutive\n$ sed -i -e 's|coforall --numthreads=5|consecutive --numthreads=5|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main thread: x = 10\nThis message will not appear until all threads are done ...\nthis is thread 1: my value of c is 1 and x is 10\nthis is thread 2: my value of c is 4 and x is 10\nthis is thread 3: my value of c is 9 and x is 10\nthis is thread 4: my value of c is 16 and x is 10\nthis is thread 5: my value of c is 25 and x is 10\n\n\nSolution to “Task.2”\nconfig const numthreads = 12;     // let's pretend we have 12 cores\nconst n = nelem / numthreads;     // number of elements per thread\nconst r = nelem - n*numthreads;   // these did not fit into the last thread\nvar lmax: [1..numthreads] real;   // local maximum for each thread\n\ncoforall threadid in 1..numthreads do {   // each iteration processed by a separate thread\n  var start, finish: int;\n  start  = (threadid-1)*n + 1;\n  finish = (threadid-1)*n + n;\n  if threadid == numthreads then finish += r;    // add r elements to the last thread\n  for i in start..finish do\n    if x[i] &gt; lmax[threadid] then lmax[threadid] = x[i];\n }\n\nfor threadid in 1..numthreads do     // no need for a parallel loop here\n  if lmax[threadid] &gt; gmax then gmax = lmax[threadid];\n\n$ chpl --fast gmax.chpl -o gmax\n$ sed -i -e 's|coforall --numthreads=5|gmax|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nthe maximum value in x is: 1.0\nWe use coforall to spawn threads that work concurrently in a fraction of the array. The trick here is to determine, based on the threadid, the initial and final indices that the thread will use. Each thread obtains the maximum in its fraction of the array, and finally, after the coforall is done, the main thread obtains the maximum of the array from the maximums of all threads.\n\n\nSolution to “Task.3”\nvar x = 0;\nwriteln('This is the main thread, my value of x is ', x);\n\nsync {\n  begin {\n     var x = 5;\n     writeln('this is thread 1, my value of x is ', x);\n  }\n  begin writeln('this is thread 2, my value of x is ', x);\n}\n\nwriteln('this message will not appear until all threads are done...');\n\n\nSolution to “Task.4”\nThe code most likely will lock (although sometimes it might not), as we’ll be hitting a race condition. Refer to the diagram for explanation.\n\n\nSolution to “Task.5”\nYou need two separate locks, and for simplicity increment them both:\nvar lock1, lock2: atomic int;\nconst numthreads = 5;\nlock1.write(0);   // the main thread set lock to zero\nlock2.write(0);   // the main thread set lock to zero\ncoforall id in 1..numthreads {\n  writeln('greetings form thread ', id, '... I am waiting for all threads to say hello');\n  lock1.add(1);              // thread id says hello and atomically adds 1 to lock\n  lock1.waitFor(numthreads);   // then it waits for lock=numthreads (which will happen when all threads say hello)\n  writeln('thread ', id, ' is done ...');\n  lock2.add(1);\n  lock2.waitFor(numthreads);\n  writeln('thread ', id, ' is really done ...');\n}"
  },
  {
    "objectID": "solutions-chapel.html#part-3-data-parallelism",
    "href": "solutions-chapel.html#part-3-data-parallelism",
    "title": "Solutions for Chapel course",
    "section": "Part 3: data parallelism",
    "text": "Part 3: data parallelism\n\nSolution to “Data.1”\nChange the line\nfor i in 1..n {\nto\nforall i in 1..n with (+ reduce total) {\n\n\nSolution to “Data.2”\nRun the code with\n$ ./test -nl 4 --n=3\n$ ./test -nl 4 --n=20\nFor n=3 we get fewer threads (7 in my case), for n=20 we still get 12 threads (the maximum available number of cores inside our job).\n\n\nSolution to “Data.3”\nSomething along the lines of m = here.id:string + '-' + m.locale.id:string; should work.\nIn most cases m.locale.id should be the same as here.id (computation follows data distribution).\n\n\nSolution to “Data.4”\nIt should be forall (i,j) in largerMesh[1..rows,1..cols] do (run on multiple locales in parallel) instead of forall (i,j) in mesh do (run in parallel on locale 0 only).\nAnother possible solution is forall (i,j) in Tnew.domain[1..rows,1..cols] do (run on multiple locales in parallel).\nAlso, we cannot have forall (i,j) in largerMesh do (run in parallel on multiple locales) as this would overwrite the boundaries.\n\n\nSolution to “Data.5”\nJust before temperature output (if count%nout == 0), insert the following:\n  var total = 0.0;\n  forall (i,j) in largerMesh[1..rows,1..cols] with (+ reduce total) do\n    total += T[i,j];\nand add total to the temperature output. It is decreasing as energy is leaving the system:\n$ chpl --fast parallel3.chpl -o parallel3\n$ ./parallel3 -nl 1 --rows=30 --cols=30 --niter=2000   # run this from inside distributed.sh\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 3.49566   21496.5\nTemperature at iteration 40: 2.96535   21052.6\n...\nTemperature at iteration 1100: 2.5809   18609.5\nTemperature at iteration 1120: 2.58087   18608.6\nTemperature at iteration 1140: 2.58085   18607.7\nFinal temperature at the desired position [1,30] after 1148 iterations is: 2.58084\nThe largest temperature difference was 9.9534e-05\nThe simulation took 0.114942 seconds\n\n\nSolution to “Data.6”\nHere is one possible solution examining the locality of the finite-difference stencil:\nvar message: [largerMesh] string = 'empty';\nand in the next line after computing Tnew[i,j] put\n    message[i,j] = \"%i\".format(here.id) + message[i,j].locale.id + message[i-1,j].locale.id +\n      message[i+1,j].locale.id + message[i,j-1].locale.id + message[i,j+1].locale.id + '  ';\nand before the end of the while loop\n  writeln(message);\n  assert(1&gt;2);\nThen run it\n$ chpl --fast parallel3.chpl -o parallel3\n$ ./parallel3 -nl 4 --rows=8 --cols=8   # run this from inside distributed.sh"
  },
  {
    "objectID": "python2/python-10-libraries.html",
    "href": "python2/python-10-libraries.html",
    "title": "Libraries",
    "section": "",
    "text": "Note\n\n\n\nThere are many ways to run Python: in the shell, via scripts, in a Jupyter notebook. In this session we’ll be running Python in the terminal inside an interactive job. We have preinstalled all libraries for today into a virtual environment /project/def-sponsor00/shared/scientificpython-env accessible to all users on the training cluster, so our workflow will be:\nmodule load StdEnv/2023 python/3.11.5 arrow/14.0.1 scipy-stack/2023b netcdf/4.9.2\nsource /project/def-sponsor00/shared/scientificpython-env/bin/activate\nsalloc --time=2:00:0 --mem-per-cpu=3600\npython\n...\nMost of the power of a programming language is in its libraries. This is especially true for Python which is an interpreted language and is therefore very slow (compared to compiled languages). However, libraries are often compiled, as they were written in compiled languages such as C/C++, and therefore offer much faster performance than native Python code.\nA library is a collection of functions that can be used by other programs. Python’s standard library includes many functions we worked with before (print, int, round, …) and is included with Python. There are many other additional modules in the standard library such as math:\nYou can also import math’s items directly:\nYou can also create an alias from the library:",
    "crumbs": [
      "MAIN PART",
      "Libraries, virtual environments and packaging"
    ]
  },
  {
    "objectID": "python2/python-10-libraries.html#virtual-environments-and-packaging",
    "href": "python2/python-10-libraries.html#virtual-environments-and-packaging",
    "title": "Libraries",
    "section": "Virtual environments and packaging",
    "text": "Virtual environments and packaging\n\n\n\n\n\n\n\nIn Python you can create an isolated environment for each project, into which all of its dependencies will be installed. This could be useful if your several projects have very different sets of dependencies. On the computer running your Jupyter notebooks, open the terminal and type:\n(Important: on a cluster you must do this on the login node, not inside the JupyterLab terminal.)\nmodule load python/3.10.2          # specific to HPC clusters\npip install virtualenv\nvirtualenv --no-download climate   # create a new virtual environment in your current directory\nsource climate/bin/activate\nwhich python && which pip\npip install --no-index numpy netcdf4 ...\npip install --no-index ipykernel   # install ipykernel (IPython kernel for Jupyter) into this environment\npython -m ipykernel install --user --name=climate --display-name \"My climate project\"   # add your environment to Jupyter\n...\ndeactivate\nQuit all your currently running Jupyter notebooks and the Jupyter dashboard. Reopen the notebook dashboard, and one of the options in New below Python 3 should be climate.\n\n\nTo delete the environment, in the terminal type:\njupyter kernelspec list                  # `climate` should be one of them\njupyter kernelspec uninstall climate     # remove your environment from Jupyter\n/bin/rm -rf climate",
    "crumbs": [
      "MAIN PART",
      "Libraries, virtual environments and packaging"
    ]
  },
  {
    "objectID": "python2/python-10-libraries.html#quick-overview-of-some-of-the-libraries",
    "href": "python2/python-10-libraries.html#quick-overview-of-some-of-the-libraries",
    "title": "Libraries",
    "section": "Quick overview of some of the libraries",
    "text": "Quick overview of some of the libraries\n\nPandas (and its more efficient replacemet Polars) is a library for working with 2D tables / spreadsheets.\nNumPy is a library for working with large, multi-dimensional arrays, along with a large collection of linear algebra functions.\n\nprovides missing uniform collections (arrays) in Python, along with a large number of ways to quickly process these collections ⮕ great for speeding up calculations in Python\n\nMatplotlib and Plotly are two plotting packages for Python.\nScikit-image is a collection of algorithms for image processing.\nXarray is a library for working with labelled multi-dimensional arrays and datasets in Python.\n\nbeen called “pandas for multi-dimensional arrays”\ngreat for large scientific datasets; writes into NetCDF files",
    "crumbs": [
      "MAIN PART",
      "Libraries, virtual environments and packaging"
    ]
  },
  {
    "objectID": "python2/python-13-pandas.html",
    "href": "python2/python-13-pandas.html",
    "title": "Dataframes",
    "section": "",
    "text": "In this section we will be reading datasets from data-python. If you have not downloaded it in the previous section, in the terminal run:\nwget http://bit.ly/pythfiles -O pfiles.zip\nunzip pfiles.zip && rm pfiles.zip        # this should unpack into the directory data-python/\n\n\n\n\n\nPandas is a widely-used Python library for working with tabular data. It borrows heavily from R’s dataframes and is built on top of NumPy. We will be reading the data we downloaded a minute ago into a pandas dataframe:\nimport pandas as pd\ndata = pd.read_csv('data-python/gapminder_gdp_oceania.csv')\nprint(data)\ndata   # this prints out the table nicely in Jupyter Notebook!\ndata.shape    # shape is a *member variable inside data*\ndata.info()   # info is a *member method inside data*\n\n\n\n\n\n\nCautionExercise 13.1\n\n\n\n\n\nTry reading a much bigger Jeopardy dataset from https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv. There are two ways to do this:\n1. you can first download it using wget and then read a local file, or\n2. you can try doing this without downloading the file.\nHow many lines and columns does it have? What are the column names?\n\n\n\n\nUse dir(data) to list all dataframe’s variables and methods. Then call one of them without (), and if it’s a method it’ll tell you, so you’ll need to use ().\n\nYou can think of rows as observations, and columns as the observed variables. You can add new observations at any time.\nCurrently the rows are indexed by number. Let’s index by country:\ndata = pd.read_csv('data-python/gapminder_gdp_oceania.csv', index_col='country')\ndata\ndata.shape        # now 12 columns\ndata.info()       # it's a dataframe! show row/column names, precision, memory usage\nprint(data.columns)   # list all the columns\nprint(data.T)     # this will transpose the dataframe; curously this is a variable\ndata.describe()   # will print some statistics of numerical columns (very useful for 1000s of rows!)\n\n\n\n\n\n\nCautionQuick question\n\n\n\n\n\nHow would you list all country names?\nHint: try data.T.columns\n\n\n\n\n\n\n\n\n\nCautionExercise 13.3\n\n\n\n\n\nRead the data in gapminder_gdp_americas.csv (which should be in the same directory as gapminder_gdp_oceania.csv) into a variable called americas and display its summary statistics.\n\n\n\n\n\n\n\n\n\nCautionExercise 13.4\n\n\n\n\n\nWrite a command to display the first three rows of the americas data frame. What about the last three columns of this data frame?\n\n\n\n\n\n\n\n\n\nCautionExercise 13.5\n\n\n\n\n\nThe data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis:\nyour_home_directory\n├── fieldData\n│   └── microbes.csv\n└── thesis\n    └── analysis.ipynb\nWhat value(s) should you pass to read_csv() to read microbes.csv in analysis.ipynb?\n\n\n\n\n\n\n\n\n\nCautionExercise 13.6\n\n\n\n\n\nAs well as the pd.read_csv() function for reading data from a file, Pandas provides a to_csv() function to write data frames to files. Applying what you’ve learned about reading from files, write one of your data frames to a file called processed.csv. You can use help to get information on how to use to_csv().\n\n\n\n\n\n\ndata = pd.read_csv('data-python/gapminder_gdp_europe.csv', index_col='country')\ndata.head()\nLet’s rename the first column:\ndata.rename(columns={'gdpPercap_1952': 'y1952'})   # this renames only one but does not change `data`\nNote: we could also name the column ‘1952’, but some Pandas operations don’t work with purely numerical column names.\nLet’s go through all columns and assign the new names:\nfor col in data.columns:\n    print(col, col[-4:])\n    data = data.rename(columns={col: 'y'+col[-4:]})\n\ndata\nPandas lets you subset elements using either their numerical indices or their row/column names. Long time ago Pandas used to have a single function to do both. Now there are two separate functions, iloc() and loc(). Let’s print one element:\ndata.iloc[0,0]                # the very first element by position\ndata.loc['Albania','y1952']   # exactly the same; the very first element by label\nPrinting a row:\ndata.loc['Albania',:]   # usual Python's slicing notation - show all columns in that row\ndata.loc['Albania']     # exactly the same\ndata.loc['Albania',]    # exactly the same\nPrinting a column:\ndata.loc[:,'y1952']   # show all rows in that column\ndata['y1952']         # exactly the same; single index refers to columns\ndata.y1952            # most compact notation; does not work with numerical-only names\nPrinting a range:\ndata.loc['Italy':'Poland','y1952':'y1967']   # select multiple rows/columns\ndata.iloc[0:2,0:3]\nResult of slicing can be used in further operations:\ndata.loc['Italy':'Poland','y1952':'y1967'].max()   # max for each column\ndata.loc['Italy':'Poland','y1952':'y1967'].min()   # min for each column\nUse comparisons to select data based on value:\nsubset = data.loc['Italy':'Poland', 'y1962':'y1972']\nprint(subset)\nprint(subset &gt; 1e4)\nUse a Boolean mask to print values (meeting the condition) or NaN (not meeting the condition):\nmask = (subset &gt; 1e4)\nprint(mask)\nprint(subset[mask])   # will print numerical values only if the corresponding elements in mask are True\nNaN’s are ignored by statistical operations which is handy:\nsubset[mask].describe()\nsubset[mask].max()\n\n\n\n\n\n\nCautionExercise 13.7\n\n\n\n\n\nAssume Pandas has been imported into your notebook and the Gapminder GDP data for Europe has been loaded:\ndf = pd.read_csv('data-python/gapminder_gdp_europe.csv', index_col='country')\nWrite an expression to find the per capita GDP of Serbia in 2007.\n\n\n\n\n\n\n\n\n\nCautionExercise 13.8\n\n\n\n\n\nExplain what each line in the following short program does, e.g. what is in the variables first, second, and so on:\nfirst = pd.read_csv('data-python/gapminder_all.csv', index_col='country')\nsecond = first[first['continent'] == 'Americas']\nthird = second.drop('Puerto Rico')\nfourth = third.drop('continent', axis = 1)\nfourth.to_csv('result.csv')\n\n\n\n\n\n\n\n\n\nCautionExercise 13.9\n\n\n\n\n\nExplain in simple terms what idxmin() and idxmax() do in the short program below. When would you use these methods?\ndata = pd.read_csv('data-python/gapminder_gdp_europe.csv', index_col='country')\nprint(data.idxmin())\nprint(data.idxmax())\n\n\n\nHow do you create a dataframe from scratch? There are many ways; perhaps, the easiest is by defining columns:\ncol1 = [1,2,3]\ncol2 = [4,5,6]\npd.DataFrame({'a': col1, 'b': col2})       # dataframe from a dictionary\nWe can index (assign names to) the rows with this syntax:\npd.DataFrame({'a': col1, 'b': col2}, index=['a1','a2','a3'])\n\n\n\nLet’s try reading some public-domain data about Jeopardy questions with pandas (31MB file, so it might take a while):\nimport pandas as pd\ndata = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndata.shape      # 216930 rows, 7 columns\ndata.head(10)   # first 10 rows\ndata.tail()     # last 5 rows\ndata.iloc[2:5]  # rows 2-4\ndata.columns    # names of the columns\n\ndata['Category']\ndata['Category']=='HISTORY'\ndata.loc[data['Category']=='HISTORY'].shape   # 349 matches\ndata.loc[data['Category']=='HISTORY'].to_csv(\"history.csv\")   # write to a file\nLet’s check what time period is covered by these data:\ndata[\"Air Date\"]\ndata[\"Air Date\"][0][-2:]   # first row, last two digits is the year\nyear = data[\"Air Date\"].apply(lambda x: x[-2:])   # last two digits of the year from all rows\nyear.min(); year.max()     # '00' and '99' - not very informative, wraps at the turn of the century\n\nfor y in range(100):\n    twoDigits = str(y).zfill(2)\n    print(twoDigits, sum(year==twoDigits))\nThis shows that this table covers years from 1984 to 2012.\n\n\n\n\nIn this section we will see that there are different techniques to process a Pandas dataframe, and some of them are more efficient than others. However, first we need to learn how to time execution of a block of Python code:\n\nInside a Jupyter notebook, you can use %%timeit to time an entire cell.\nInside your code, you can call time.time() function:\n\nimport time\nstart = time.time()\n...\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\nNow, let’s go to our computational problem. “Fizz buzz” is a children’s game to practice divisions. Players take turn counting out loud while replacing: - any number divisible by 3 with the word “Fizz”, - any number divisible by 5 with the word “Buzz”, - any number divisible by both 3 and 5 with the word “FizzBuzz”.\nLet’s implement this in pandas! First, create a simple dataframe from scratch:\nimport pandas as pd\ndf = pd.DataFrame()\nsize = 10_000\ndf['number'] = np.arange(1, size+1)\nDefine for pretty printing:\ndef show(frame):\n    print(df.tail(15).to_string(index=False))   # print last 15 rows without the row index\n\nshow(df)\nLet’s built a new column response containing either “Fizz” or “Buzz” or “FizzBuzz” or the original number, based on the number value in that row. We start by processing a row:\ndef count(row):\n    if (row['number'] % 3 == 0) and (row['number'] % 5 == 0):\n        return 'FizzBuzz'\n    elif row['number'] % 3 == 0:\n        return 'Fizz'\n    elif row['number'] % 5 == 0:\n      return 'Buzz'\n    else:\n      return str(row['number'])\nHere is how you would use this function:\nprint(df.iloc[2])\nprint(count(df.iloc[2]))\n\nWe can apply this function to each row in a loop:\n\n%%timeit\nfor index, row in df.iterrows():\n    df.loc[index, 'response'] = count(row)\n413 ms ± 11.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nshow(df)\n\nWe can use df.apply() to apply this function to each row:\n\n%%timeit\ndf['response'] = df.apply(count, axis=1)\n69.1 ms ± 380 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nshow(df)\n\nOr we could use a mask to only assign correct responses to the corresponding rows:\n\n%%timeit\ndf['response'] = df['number'].astype(str)\ndf.loc[df['number'] % 3==0, 'response'] = 'Fizz'\ndf.loc[df['number'] % 5==0, 'response'] = 'Buzz'\ndf.loc[(df['number'] % 3==0) & (df['number'] % 5==0), 'response'] = 'FizzBuzz'\n718 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nshow(df)\n\n\n\nLet’s say we want to read several files in data-python/. We can use for to loop through their list:\nfor name in ['africa.csv', 'asia.csv']:\n    data = pd.read_csv('data-python/gapminder_gdp_'+name, index_col='country')\n    print(name+'\\n', data.min(), sep='')   # print min for each column\nIf we have many (10s or 100s) files, we want to specify them with a pattern:\nfrom glob import glob\nprint('all csv files in data-python:', glob('data-python/*.csv'))    # returns a list\nprint('all text files in data-python:', glob('data-python/*.txt'))   # empty list\nlist = glob('data-python/*.csv')\nlen(list)\nfor filename in glob('data-python/gapminder*.csv'):\n    data = pd.read_csv(filename)\n    print(filename, data.gdpPercap_1952.min())\n\n\n\n\n\n\nCautionExercise 13.10\n\n\n\n\n\nWhich of these files is not matched by the expression glob('data-python/*as*.csv')?\nA. data-python/gapminder_gdp_africa.csv\nB. data-python/gapminder_gdp_americas.csv\nC. data-python/gapminder_gdp_asia.csv\nD. 1 and 2 are not matched\n\n\n\n\n\n\n\n\n\nCautionExercise 13.11\n\n\n\n\n\nModify this program so that it prints the number of records in the file that has the fewest records.\nfewest = ____\nfor filename in glob('data-python/*.csv'):\n    fewest = ____\nprint('smallest file has', fewest, 'records')",
    "crumbs": [
      "MAIN PART",
      "Dataframes with Pandas and Polars"
    ]
  },
  {
    "objectID": "python2/python-13-pandas.html#reading-tabular-data",
    "href": "python2/python-13-pandas.html#reading-tabular-data",
    "title": "Dataframes",
    "section": "",
    "text": "In this section we will be reading datasets from data-python. If you have not downloaded it in the previous section, in the terminal run:\nwget http://bit.ly/pythfiles -O pfiles.zip\nunzip pfiles.zip && rm pfiles.zip        # this should unpack into the directory data-python/\n\n\n\n\n\nPandas is a widely-used Python library for working with tabular data. It borrows heavily from R’s dataframes and is built on top of NumPy. We will be reading the data we downloaded a minute ago into a pandas dataframe:\nimport pandas as pd\ndata = pd.read_csv('data-python/gapminder_gdp_oceania.csv')\nprint(data)\ndata   # this prints out the table nicely in Jupyter Notebook!\ndata.shape    # shape is a *member variable inside data*\ndata.info()   # info is a *member method inside data*\n\n\n\n\n\n\nCautionExercise 13.1\n\n\n\n\n\nTry reading a much bigger Jeopardy dataset from https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv. There are two ways to do this:\n1. you can first download it using wget and then read a local file, or\n2. you can try doing this without downloading the file.\nHow many lines and columns does it have? What are the column names?\n\n\n\n\nUse dir(data) to list all dataframe’s variables and methods. Then call one of them without (), and if it’s a method it’ll tell you, so you’ll need to use ().\n\nYou can think of rows as observations, and columns as the observed variables. You can add new observations at any time.\nCurrently the rows are indexed by number. Let’s index by country:\ndata = pd.read_csv('data-python/gapminder_gdp_oceania.csv', index_col='country')\ndata\ndata.shape        # now 12 columns\ndata.info()       # it's a dataframe! show row/column names, precision, memory usage\nprint(data.columns)   # list all the columns\nprint(data.T)     # this will transpose the dataframe; curously this is a variable\ndata.describe()   # will print some statistics of numerical columns (very useful for 1000s of rows!)\n\n\n\n\n\n\nCautionQuick question\n\n\n\n\n\nHow would you list all country names?\nHint: try data.T.columns\n\n\n\n\n\n\n\n\n\nCautionExercise 13.3\n\n\n\n\n\nRead the data in gapminder_gdp_americas.csv (which should be in the same directory as gapminder_gdp_oceania.csv) into a variable called americas and display its summary statistics.\n\n\n\n\n\n\n\n\n\nCautionExercise 13.4\n\n\n\n\n\nWrite a command to display the first three rows of the americas data frame. What about the last three columns of this data frame?\n\n\n\n\n\n\n\n\n\nCautionExercise 13.5\n\n\n\n\n\nThe data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis:\nyour_home_directory\n├── fieldData\n│   └── microbes.csv\n└── thesis\n    └── analysis.ipynb\nWhat value(s) should you pass to read_csv() to read microbes.csv in analysis.ipynb?\n\n\n\n\n\n\n\n\n\nCautionExercise 13.6\n\n\n\n\n\nAs well as the pd.read_csv() function for reading data from a file, Pandas provides a to_csv() function to write data frames to files. Applying what you’ve learned about reading from files, write one of your data frames to a file called processed.csv. You can use help to get information on how to use to_csv().",
    "crumbs": [
      "MAIN PART",
      "Dataframes with Pandas and Polars"
    ]
  },
  {
    "objectID": "python2/python-13-pandas.html#subsetting",
    "href": "python2/python-13-pandas.html#subsetting",
    "title": "Dataframes",
    "section": "",
    "text": "data = pd.read_csv('data-python/gapminder_gdp_europe.csv', index_col='country')\ndata.head()\nLet’s rename the first column:\ndata.rename(columns={'gdpPercap_1952': 'y1952'})   # this renames only one but does not change `data`\nNote: we could also name the column ‘1952’, but some Pandas operations don’t work with purely numerical column names.\nLet’s go through all columns and assign the new names:\nfor col in data.columns:\n    print(col, col[-4:])\n    data = data.rename(columns={col: 'y'+col[-4:]})\n\ndata\nPandas lets you subset elements using either their numerical indices or their row/column names. Long time ago Pandas used to have a single function to do both. Now there are two separate functions, iloc() and loc(). Let’s print one element:\ndata.iloc[0,0]                # the very first element by position\ndata.loc['Albania','y1952']   # exactly the same; the very first element by label\nPrinting a row:\ndata.loc['Albania',:]   # usual Python's slicing notation - show all columns in that row\ndata.loc['Albania']     # exactly the same\ndata.loc['Albania',]    # exactly the same\nPrinting a column:\ndata.loc[:,'y1952']   # show all rows in that column\ndata['y1952']         # exactly the same; single index refers to columns\ndata.y1952            # most compact notation; does not work with numerical-only names\nPrinting a range:\ndata.loc['Italy':'Poland','y1952':'y1967']   # select multiple rows/columns\ndata.iloc[0:2,0:3]\nResult of slicing can be used in further operations:\ndata.loc['Italy':'Poland','y1952':'y1967'].max()   # max for each column\ndata.loc['Italy':'Poland','y1952':'y1967'].min()   # min for each column\nUse comparisons to select data based on value:\nsubset = data.loc['Italy':'Poland', 'y1962':'y1972']\nprint(subset)\nprint(subset &gt; 1e4)\nUse a Boolean mask to print values (meeting the condition) or NaN (not meeting the condition):\nmask = (subset &gt; 1e4)\nprint(mask)\nprint(subset[mask])   # will print numerical values only if the corresponding elements in mask are True\nNaN’s are ignored by statistical operations which is handy:\nsubset[mask].describe()\nsubset[mask].max()\n\n\n\n\n\n\nCautionExercise 13.7\n\n\n\n\n\nAssume Pandas has been imported into your notebook and the Gapminder GDP data for Europe has been loaded:\ndf = pd.read_csv('data-python/gapminder_gdp_europe.csv', index_col='country')\nWrite an expression to find the per capita GDP of Serbia in 2007.\n\n\n\n\n\n\n\n\n\nCautionExercise 13.8\n\n\n\n\n\nExplain what each line in the following short program does, e.g. what is in the variables first, second, and so on:\nfirst = pd.read_csv('data-python/gapminder_all.csv', index_col='country')\nsecond = first[first['continent'] == 'Americas']\nthird = second.drop('Puerto Rico')\nfourth = third.drop('continent', axis = 1)\nfourth.to_csv('result.csv')\n\n\n\n\n\n\n\n\n\nCautionExercise 13.9\n\n\n\n\n\nExplain in simple terms what idxmin() and idxmax() do in the short program below. When would you use these methods?\ndata = pd.read_csv('data-python/gapminder_gdp_europe.csv', index_col='country')\nprint(data.idxmin())\nprint(data.idxmax())\n\n\n\nHow do you create a dataframe from scratch? There are many ways; perhaps, the easiest is by defining columns:\ncol1 = [1,2,3]\ncol2 = [4,5,6]\npd.DataFrame({'a': col1, 'b': col2})       # dataframe from a dictionary\nWe can index (assign names to) the rows with this syntax:\npd.DataFrame({'a': col1, 'b': col2}, index=['a1','a2','a3'])",
    "crumbs": [
      "MAIN PART",
      "Dataframes with Pandas and Polars"
    ]
  },
  {
    "objectID": "python2/python-13-pandas.html#example-with-a-larger-dataframe",
    "href": "python2/python-13-pandas.html#example-with-a-larger-dataframe",
    "title": "Dataframes",
    "section": "",
    "text": "Let’s try reading some public-domain data about Jeopardy questions with pandas (31MB file, so it might take a while):\nimport pandas as pd\ndata = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndata.shape      # 216930 rows, 7 columns\ndata.head(10)   # first 10 rows\ndata.tail()     # last 5 rows\ndata.iloc[2:5]  # rows 2-4\ndata.columns    # names of the columns\n\ndata['Category']\ndata['Category']=='HISTORY'\ndata.loc[data['Category']=='HISTORY'].shape   # 349 matches\ndata.loc[data['Category']=='HISTORY'].to_csv(\"history.csv\")   # write to a file\nLet’s check what time period is covered by these data:\ndata[\"Air Date\"]\ndata[\"Air Date\"][0][-2:]   # first row, last two digits is the year\nyear = data[\"Air Date\"].apply(lambda x: x[-2:])   # last two digits of the year from all rows\nyear.min(); year.max()     # '00' and '99' - not very informative, wraps at the turn of the century\n\nfor y in range(100):\n    twoDigits = str(y).zfill(2)\n    print(twoDigits, sum(year==twoDigits))\nThis shows that this table covers years from 1984 to 2012.",
    "crumbs": [
      "MAIN PART",
      "Dataframes with Pandas and Polars"
    ]
  },
  {
    "objectID": "python2/python-13-pandas.html#three-solutions-to-a-classification-problem",
    "href": "python2/python-13-pandas.html#three-solutions-to-a-classification-problem",
    "title": "Dataframes",
    "section": "",
    "text": "In this section we will see that there are different techniques to process a Pandas dataframe, and some of them are more efficient than others. However, first we need to learn how to time execution of a block of Python code:\n\nInside a Jupyter notebook, you can use %%timeit to time an entire cell.\nInside your code, you can call time.time() function:\n\nimport time\nstart = time.time()\n...\nend = time.time()\nprint(\"Time in seconds:\", round(end-start,3))\nNow, let’s go to our computational problem. “Fizz buzz” is a children’s game to practice divisions. Players take turn counting out loud while replacing: - any number divisible by 3 with the word “Fizz”, - any number divisible by 5 with the word “Buzz”, - any number divisible by both 3 and 5 with the word “FizzBuzz”.\nLet’s implement this in pandas! First, create a simple dataframe from scratch:\nimport pandas as pd\ndf = pd.DataFrame()\nsize = 10_000\ndf['number'] = np.arange(1, size+1)\nDefine for pretty printing:\ndef show(frame):\n    print(df.tail(15).to_string(index=False))   # print last 15 rows without the row index\n\nshow(df)\nLet’s built a new column response containing either “Fizz” or “Buzz” or “FizzBuzz” or the original number, based on the number value in that row. We start by processing a row:\ndef count(row):\n    if (row['number'] % 3 == 0) and (row['number'] % 5 == 0):\n        return 'FizzBuzz'\n    elif row['number'] % 3 == 0:\n        return 'Fizz'\n    elif row['number'] % 5 == 0:\n      return 'Buzz'\n    else:\n      return str(row['number'])\nHere is how you would use this function:\nprint(df.iloc[2])\nprint(count(df.iloc[2]))\n\nWe can apply this function to each row in a loop:\n\n%%timeit\nfor index, row in df.iterrows():\n    df.loc[index, 'response'] = count(row)\n413 ms ± 11.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nshow(df)\n\nWe can use df.apply() to apply this function to each row:\n\n%%timeit\ndf['response'] = df.apply(count, axis=1)\n69.1 ms ± 380 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nshow(df)\n\nOr we could use a mask to only assign correct responses to the corresponding rows:\n\n%%timeit\ndf['response'] = df['number'].astype(str)\ndf.loc[df['number'] % 3==0, 'response'] = 'Fizz'\ndf.loc[df['number'] % 5==0, 'response'] = 'Buzz'\ndf.loc[(df['number'] % 3==0) & (df['number'] % 5==0), 'response'] = 'FizzBuzz'\n718 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nshow(df)",
    "crumbs": [
      "MAIN PART",
      "Dataframes with Pandas and Polars"
    ]
  },
  {
    "objectID": "python2/python-13-pandas.html#looping-over-data-sets",
    "href": "python2/python-13-pandas.html#looping-over-data-sets",
    "title": "Dataframes",
    "section": "",
    "text": "Let’s say we want to read several files in data-python/. We can use for to loop through their list:\nfor name in ['africa.csv', 'asia.csv']:\n    data = pd.read_csv('data-python/gapminder_gdp_'+name, index_col='country')\n    print(name+'\\n', data.min(), sep='')   # print min for each column\nIf we have many (10s or 100s) files, we want to specify them with a pattern:\nfrom glob import glob\nprint('all csv files in data-python:', glob('data-python/*.csv'))    # returns a list\nprint('all text files in data-python:', glob('data-python/*.txt'))   # empty list\nlist = glob('data-python/*.csv')\nlen(list)\nfor filename in glob('data-python/gapminder*.csv'):\n    data = pd.read_csv(filename)\n    print(filename, data.gdpPercap_1952.min())\n\n\n\n\n\n\nCautionExercise 13.10\n\n\n\n\n\nWhich of these files is not matched by the expression glob('data-python/*as*.csv')?\nA. data-python/gapminder_gdp_africa.csv\nB. data-python/gapminder_gdp_americas.csv\nC. data-python/gapminder_gdp_asia.csv\nD. 1 and 2 are not matched\n\n\n\n\n\n\n\n\n\nCautionExercise 13.11\n\n\n\n\n\nModify this program so that it prints the number of records in the file that has the fewest records.\nfewest = ____\nfor filename in glob('data-python/*.csv'):\n    fewest = ____\nprint('smallest file has', fewest, 'records')",
    "crumbs": [
      "MAIN PART",
      "Dataframes with Pandas and Polars"
    ]
  },
  {
    "objectID": "python2/python-16-scripts.html",
    "href": "python2/python-16-scripts.html",
    "title": "Python scripts from the command line",
    "section": "",
    "text": "In this lesson we’ll work with bash command line, instead of the Jupyter notebook. We want to write a python script read.py that takes a set of gapminder_gdp_*.csv files (one/few/many) as an argument and prints out various statistic (min, max, mean) for each year for all countries in that file:\nIt would be best to open two shells for the following programming, and keep nano always open in one shell.\nPut the following into a file called read.py:\nand then run it from bash:\nLet’s modify our script:\nLet’s modify our script:\nLet’s actually read the data:\nLet’s modify our script changing print(data.shape) -&gt; print(data.mean()) to compute average per year for each file:\nLet’s add an action flag and calculate statistics based on the flag:\nLet’s add assertion for action (syntax is: assert n &gt; 0.0, ‘should be positive’) right after the definition of action:\nand try passing some other action.\nNow let’s package processing of a file (reading + computing + printing) as a function:",
    "crumbs": [
      "ADDITIONAL MATERIAL",
      "Running Python scripts from the command line"
    ]
  },
  {
    "objectID": "python2/python-16-scripts.html#adding-standard-input-support-to-python-scripts",
    "href": "python2/python-16-scripts.html#adding-standard-input-support-to-python-scripts",
    "title": "Python scripts from the command line",
    "section": "Adding standard input support to Python scripts",
    "text": "Adding standard input support to Python scripts\nPython scripts can process standard input. Consider the following script input.py:\n#!/usr/bin/env python\nimport sys\nfor line in sys.stdin:\n    print(line, end='')\nIn the terminal make it executable (chmod u+x input.py), and then use it to receive any standard input:\n./input.py                          # repeat each line you type until Ctrl-C\necho one two three | ./input.py     # print back the line\nls -l | ./input.py                  # list all files\ncat input.py | ./input.py           # print itself from standard input\ntail -1 input.py | ./input.py       # print its own last line\nLet’s add support for Unix standard input to read.py. Delete print('\\n', f[26:-4].capitalize()) and change the last two lines in read.py from:\nfor f in filenames:\n    process(f, action)\nto:\nif len(filenames) == 0:\n    process(sys.stdin,action)       # process standard input\nelse:\n    for f in filenames:             # same as before\n        print('\\n', f[26:-4].capitalize())\n        process(f,action)\nNow sys.stdin acts as if it was the name of a file containing whatever was passed as standard input.\n$ python read.py --mean data-python/gapminder_gdp_europe.csv\n$ python read.py --mean &lt; data-python/gapminder_gdp_europe.csv    # anyone knows why this could be useful?\nWhy would you want to support standard input? Answer: you can do preprocessing in bash before passing the data to your Python script, e.g.\n$ head -5 data-python/gapminder_gdp_europe.csv | python read.py --mean    # process only first five countries\ncp data-python/gapminder_gdp_asia.csv a1\ncat data-python/gapminder_gdp_europe.csv | sed '1d' &gt;&gt; a1   # add all European data without the header\ncat a1 | python read.py --mean          # merge Asian and European data and calculate joint statistics\nHere is our full script:\nimport sys\nimport pandas as pd\naction = sys.argv[1]\nassert action in ['--min', '--mean', '--max'], 'action must be one of: --min --mean --max'\nfilenames = sys.argv[2:]\ndef process(filename, action):\n    data = pd.read_csv(filename, index_col='country')\n    if 'continent' in data.columns.tolist():   # if 'continent' column exists\n        data = data.drop('continent',1)        #     delete it (1 stands for column, 0 for row)\n    if action == '--min':\n        values = data.min().tolist()\n    elif action == '--mean':\n        values = data.mean().tolist()\n    elif action == '--max':\n        values = data.max().tolist()\n    print(values,'\\n')\nif len(filenames) == 0:\n    process(sys.stdin,action)       # process standard input\nelse:\n    for f in filenames:             # same as before\n        print('\\n', f[26:-4].capitalize())\n        process(f,action)",
    "crumbs": [
      "ADDITIONAL MATERIAL",
      "Running Python scripts from the command line"
    ]
  },
  {
    "objectID": "python2/python-12-matplotlib.html",
    "href": "python2/python-12-matplotlib.html",
    "title": "Plotting with matplotlib",
    "section": "",
    "text": "There are hundreds of visualization packages in Python. Check out this diagram of the Python Visualization Landscape (circa 2017, by Nicolas Rougier) which focuses on 1D+2D packages at the time (and only barely mentions 3D sci-vis packages). For 3D examples, check our gallery in which most images were rendered with Python.\nOne of the most widely used Python plotting libraries is matplotlib. Matplotlib is open source and produces static images (and non-interactive animations).",
    "crumbs": [
      "MAIN PART",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python2/python-12-matplotlib.html#simple-linescatter-plots",
    "href": "python2/python-12-matplotlib.html#simple-linescatter-plots",
    "title": "Plotting with matplotlib",
    "section": "Simple line/scatter plots",
    "text": "Simple line/scatter plots\nIf working in a Jupyter notebook, you can create a simple line plot with:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\nplt.plot(x, y, 'bo-')\nplt.xlabel('x', fontsize=18)\nplt.ylabel('f(x)', fontsize=18)\nIf working inside a terminal on your own computer (where you can open windows), you can display the graph with:\nplt.show()       # not needed inside the Jupyter notebook\nBoth in a Jupyter notebook and in the terminal, you can save the plot with:\nplt.savefig('filename.png')\n# plt.savefig('filename.png', dpi=300)   # optionally specify the resolution\n\n\nOffscreen plotting - You can create the same plot with offscreen rendering directly to a file:\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.use('Agg')   # enable PNG backend\nplt.figure(figsize=(10,8))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\nplt.plot(x, y, 'bo-')\nplt.xlabel('x', fontsize=18)\nplt.ylabel('f(x)', fontsize=18)\nplt.savefig('filename.png')\n\nLet’s add the second line, the labels, and the legend. Note that matplotlib automatically adjusts the axis ranges to fit both plots:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\nplt.plot(x, y, 'bo-', label='one')\nplt.plot(x+0.3, 2*sin(10*x), 'r-', label='two')\nplt.legend(loc='lower right')\nplt.xlabel('x', fontsize=18)\nplt.ylabel('f(x)', fontsize=18)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s plot these two functions side-by-side:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(12,4))\nfrom numpy import linspace, sin\nx = linspace(0.01,1,300)\ny = sin(1/x)\n\nax = fig.add_subplot(121)   # on 1x2 layout create plot #1 (`axes` object with some data space)\nplt.plot(x, y, 'bo-', label='one')\nax.set_ylim(-1.5, 1.5)\nplt.xlabel('x')\nplt.ylabel('f1')\nplt.legend(loc='lower right')\n\nfig.add_subplot(122)   # on 1x2 layout create plot #2\nplt.plot(x+0.2, 2*sin(10*x), 'r-', label='two')\nplt.xlabel('x')\nplt.ylabel('f2')\nplt.legend(loc='lower right')\nThere is also an option to specify absolute coordinates of each plot with fig.add_axes():\n\nreplace the first ax = fig.add_subplot(121) with ax = fig.add_axes([0.1, 0.7, 0.8, 0.3])   # left, bottom, width, height\nreplace the second fig.add_subplot(122) with fig.add_axes([0.1, 0.2, 0.8, 0.4])   # left, bottom, width, height\n\nThe 3rd option for more fine-grained control is plt.axes() – it creates an axes object (a region of the figure with some data space). These two lines are equivalent - both create a new figure with one subplot:\nfig = plt.figure(figsize=(8,8)); ax = fig.add_subplot(111)\nfig = plt.figure(figsize=(8,8)); ax = plt.axes()\nShortly we will see that we can pass additional flags to fig.add_subplot() and plt.axes() for more coordinate system control.\n\n\n\n\n\n\nCautionQuestion 12.1\n\n\n\n\n\nBreak the plot into two subplots, the fist taking 1/3 of the space on the left, the second one 2/3 of the space on the right.\n\n\n\nLet’s plot a simple line in the x-y plane:\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111)\nx = np.linspace(0,1,100)\nplt.plot(2*np.pi*x, x, 'b-')\nplt.xlabel('x')\nplt.ylabel('f1')\nReplace ax = fig.add_subplot(111) with ax = fig.add_subplot(111, projection='polar'). Now we have a plot in the phi-r plane, i.e. in polar coordinates. Phi goes [0,2\\(\\pi\\)], whereas r goes [0,1].\n?fig.add_subplot    # look into `projection` parameter\nimport matplotlib.pyplot as plt\nimport numpy as np\nfig = plt.figure(figsize=(12,12))\nax = fig.add_subplot(111, projection='mollweide')\nlon = np.radians(np.linspace(30,90,10))\nlat = np.radians(np.linspace(15,18,10))\nplt.plot(lon, lat, 'go-')\nYou can use this projection parameter together with cartopy package to process 2D geospatial data to produce maps, while all plotting is still being done by Matplotlib. We teach cartopy in a separate workshop.\nLet’s try a scatter plot:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.figure(figsize=(10,8))\nx = np.random.random(size=1000)   # 1D array of 1000 random numbers in [0,1]\ny = np.random.random(size=1000)\nsize = 1 + 50*np.random.random(size=1000)\nplt.scatter(x, y, s=size, color='lightblue')\nFor other plot types, click on any example in the Matplotlib gallery.\nFor colours, see Choosing Colormaps in Matplotlib.",
    "crumbs": [
      "MAIN PART",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python2/python-12-matplotlib.html#heatmaps",
    "href": "python2/python-12-matplotlib.html#heatmaps",
    "title": "Plotting with matplotlib",
    "section": "Heatmaps",
    "text": "Heatmaps\nLet’s plot a heatmap of monthly temperatures at the South Pole:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nplt.figure(figsize=(15,10))\n\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Year']\nrecordHigh = [-14.4,-20.6,-26.7,-27.8,-25.1,-28.8,-33.9,-32.8,-29.3,-25.1,-18.9,-12.3,-12.3]\naverageHigh = [-26.0,-37.9,-49.6,-53.0,-53.6,-54.5,-55.2,-54.9,-54.4,-48.4,-36.2,-26.3,-45.8]\ndailyMean = [-28.4,-40.9,-53.7,-57.8,-58.0,-58.9,-59.8,-59.7,-59.1,-51.6,-38.2,-28.0,-49.5]\naverageLow = [-29.6,-43.1,-56.8,-60.9,-61.5,-62.8,-63.4,-63.2,-61.7,-54.3,-40.1,-29.1,-52.2]\nrecordLow = [-41.1,-58.9,-71.1,-75.0,-78.3,-82.8,-80.6,-79.3,-79.4,-72.0,-55.0,-41.1,-82.8]\n\nvlabels = ['record high', 'average high', 'daily mean', 'average low', 'record low']\n\nZ = np.stack((recordHigh,averageHigh,dailyMean,averageLow,recordLow))\nplt.imshow(Z, cmap=cm.winter)\nplt.colorbar(orientation='vertical', shrink=0.45, aspect=20)\nplt.xticks(range(13), months, fontsize=15)\nplt.yticks(range(5), vlabels, fontsize=12)\nplt.ylim(-0.5, 4.5)\n\nfor i in range(len(months)):\n    for j in range(len(vlabels)):\n        text = plt.text(i, j, Z[j,i],\n                       ha=\"center\", va=\"center\", color=\"w\", fontsize=14, weight='bold')\n\n\n\n\n\n\nCautionQuestion 12.2\n\n\n\n\n\nChange the text colour to black in the brightest (green) rows and columns. You can do this either by specifying rows/columns explicitly, or (better) by setting a threshold background colour.\n\n\n\n\n\n\n\n\n\nCautionQuestion 12.3\n\n\n\n\n\nThis is a take-home exercise. Modify the code to display only 4 seasons instead of the individual months.",
    "crumbs": [
      "MAIN PART",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python2/python-12-matplotlib.html#d-topographic-elevation",
    "href": "python2/python-12-matplotlib.html#d-topographic-elevation",
    "title": "Plotting with matplotlib",
    "section": "3D topographic elevation",
    "text": "3D topographic elevation\nFor this we need a data file – let’s download it. Open a terminal inside your Jupyter dashboard. Inside the terminal, type:\nwget http://bit.ly/pythfiles -O pfiles.zip\nunzip pfiles.zip && rm pfiles.zip        # this should unpack into the directory data-python/\nThis will download and unpack the ZIP file into your home directory. Now switch back to Python.\n%pwd       # run `pwd` bash command\n%ls        # make sure you see data-python/\nLet’s plot tabulated topographic elevation data:\nfrom matplotlib import cm\nfrom matplotlib.colors import LightSource\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ntable = pd.read_csv('data-python/mt_bruno_elevation.csv')\nz = np.array(table)\nnrows, ncols = z.shape\nx = np.linspace(0,1,ncols)\ny = np.linspace(0,1,nrows)\nx, y = np.meshgrid(x, y)\nrgb = LightSource(270, 45).shade(z, cmap=cm.gist_earth, vert_exag=0.1, blend_mode='soft')\n\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(10,10))    # figure with one subplot\nax.view_init(20, 30)      # (theta, phi) viewpoint\nsurf = ax.plot_surface(x, y, z, facecolors=rgb, linewidth=0, antialiased=False, shade=False)\n\n\n\n\n\n\n\n\nCautionQuestion 12.4\n\n\n\n\n\nReplace fig, ax = plt.subplots() with fig = plt.figure() followed by ax = fig.add_subplot(). Don’t forget about the 3d projection. This one is a little tricky – feel free to google the problem, or even better use our earlier examples.\n\n\n\n\n\n\n\nLet’s add the following to the previous code (running this takes ~10s on my laptop):\nfor angle in range(90):\n    print(angle)\n    ax.view_init(20, 30+angle)\n    plt.savefig('frame%04d'%(angle)+'.png')\nAnd then we can create a movie in bash:\nffmpeg -r 30 -i frame%04d.png -c:v libx264 -pix_fmt yuv420p -vf \"scale=trunc(iw/2)*2:trunc(ih/2)*2\" spin.mp4",
    "crumbs": [
      "MAIN PART",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python2/python-12-matplotlib.html#matplotlibs-built-in-animation",
    "href": "python2/python-12-matplotlib.html#matplotlibs-built-in-animation",
    "title": "Plotting with matplotlib",
    "section": "Matplotlib’s built-in animation",
    "text": "Matplotlib’s built-in animation\nMatplotlib can do live animation with one of its Animation classes:\n\nFuncAnimation class\nFuncAnimation creates an animation by repeatedly calling a function.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\nfig = plt.figure(figsize=(8,5))\nax = plt.subplot(111)\n\nax.set_xlim(( 0, 2))            \nax.set_ylim((-2, 2))\nax.set_xlabel('time')\nax.set_ylabel('magnitude')\n\n# create an empty title and 2 empty plots\ntitle = ax.set_title('')\nline1 = ax.plot([], [], 'b', lw=1)[0]   # `ax.plot` returns a list of 2D line objects\nline2 = ax.plot([], [], 'r', lw=2)[0]\n\nax.legend(['sin','cos'])\n\ndef drawframe(j):\n    x = np.linspace(0, 2, 100)\n    y1 = np.sin(2 * np.pi * (x-0.01*j))\n    y2 = np.cos(2 * np.pi * (x-0.01*j))\n    line1.set_data(x, y1)\n    line2.set_data(x, y2)\n    title.set_text('frame = {0:4d}'.format(j))\n    return (line1,line2,title)   # the animation function must return a sequence of Artist objects\n\n# blit=True re-draws only the parts that have changed, update every 20ms, calls drawframe() with j=0..99\nanim = animation.FuncAnimation(fig, drawframe, frames=100, interval=20, blit=True)\n\n# ---\n\n# Output option 1: Python shell, open a new window\nplt.show()\n\n# Output option 2: Jupyter notebook\nfrom IPython.display import HTML\nHTML(anim.to_html5_video())\n\n# Output option 3: save to a file\nanim.save(\"twoLines.mp4\")\n\n# Output option 4: save to a file, more granular control\nwriter = animation.FFMpegWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nanim.save(\"twoLines.mp4\", writer=writer)\n\n\nArtistAnimation class\nArtistAnimation creates an animation by using a fixed set of Artist objects.\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import animation\n\nfig, ax = plt.subplots()\n\ndef f(x, y):\n    return np.sin(x) + np.cos(y)\n\nx = np.linspace(0, 2 * np.pi, 120)\ny = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n\nims = []   # list of rows, each row is a list of artists (images) to draw in the current frame\nfor i in range(60):\n    x += np.pi / 15\n    y += np.pi / 30\n    im = ax.imshow(f(x, y), animated=True)\n    if i == 0:\n        ax.imshow(f(x, y))  # show an initial one first\n    ims.append([im])\n\n# blit=True re-draws only the parts that have changed, update every 50ms\nanim = animation.ArtistAnimation(fig, ims, interval=50, blit=True)\n\n# ---\n\n# Output option 1: Python shell, open a new window\nplt.show()\n\n# Output option 2: Jupyter notebook\nfrom IPython.display import HTML\nHTML(anim.to_html5_video())\n\n# Output option 3: save to a file\nanim.save(\"movingPlane.mp4\")\n\n# Output option 4: save to a file, more granular control\nwriter = animation.FFMpegWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\nanim.save(\"movingPlane.mp4\", writer=writer)",
    "crumbs": [
      "MAIN PART",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python2/python-12-matplotlib.html#d-parametric-plot",
    "href": "python2/python-12-matplotlib.html#d-parametric-plot",
    "title": "Plotting with matplotlib",
    "section": "3D parametric plot",
    "text": "3D parametric plot\nHere is something visually very different, still using ax.plot_surface():\nfrom matplotlib import cm\nfrom matplotlib.colors import LightSource\nimport matplotlib.pyplot as plt\nfrom numpy import pi, sin, cos, mgrid\n\ndphi, dtheta = pi/250, pi/250    # 0.72 degrees\n[phi, theta] = mgrid[0:pi+dphi*1.5:dphi, 0:2*pi+dtheta*1.5:dtheta]\n        # define two 2D grids: both phi and theta are (252,502) numpy arrays\nr = sin(4*phi)**3 + cos(2*phi)**3 + sin(6*theta)**2 + cos(6*theta)**4\nx = r*sin(phi)*cos(theta)   # x is also (252,502)\ny = r*cos(phi)              # y is also (252,502)\nz = r*sin(phi)*sin(theta)   # z is also (252,502)\n\nrgb = LightSource(270, 45).shade(z, cmap=cm.gist_earth, vert_exag=0.1, blend_mode='soft')\n\nfig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(10,10))\nax.view_init(20, 30)\nsurf = ax.plot_surface(x, y, z, facecolors=rgb, linewidth=0, antialiased=False, shade=False)\n\n\n\n\n\n\nCautionQuestion 12.5\n\n\n\n\n\nCreate an animation in which you change the light source position.",
    "crumbs": [
      "MAIN PART",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "python2/python-12-matplotlib.html#more-examples",
    "href": "python2/python-12-matplotlib.html#more-examples",
    "title": "Plotting with matplotlib",
    "section": "More examples",
    "text": "More examples\nFor more 3D examples in matplotlib, click on any example in the 3D gallery to see the code behind that plot. Try pasting it into your Jupyter notebook and running it, and try to modify the code.\n\nMatplotlib cheatsheets and handouts",
    "crumbs": [
      "MAIN PART",
      "Plotting with matplotlib"
    ]
  },
  {
    "objectID": "scientificPython-menu.html",
    "href": "scientificPython-menu.html",
    "title": "Scientific Python",
    "section": "",
    "text": "June 6th, 1:30pm–4:30pm Pacific Time\n\n\n\nBuilding on our Python level 1 course, this course will cover more advanced scientific computing in Python. We will talk about speeding up calculations and working with mathematical arrays with NumPy, working with dataframes in Pandas and Polars, basic plotting, working with scientific datasets with xarray, and a few other topics.\nInstructors: Alex Razoumov\nPrerequisites: This course assumes some basic knowledge of Python: for loops, if statements, writing functions, lists, dictionaries.\nSoftware: We will be using Python on our training cluster, so no need to install it on your computer. However, in the long run you would probably benefit from Python on your computer, so you might want to look into this. To access the training cluster, you will need a remote secure shell (SSH) client installed on your computer in order to participate in the course exercises. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there)."
  },
  {
    "objectID": "chapel2/chapel-01-intro.html",
    "href": "chapel2/chapel-01-intro.html",
    "title": "Introduction to Chapel",
    "section": "",
    "text": "Modern, open-source parallel programming language developed at Cray Inc. (acquired by Hewlett Packard Enterprise in 2019).\nOffers simplicity and readability of scripting languages such as Python or Matlab: “Python for parallel programming”.\nCompiled language \\(\\Rightarrow\\) provides the speed and performance of Fortran and C.\nSupports high-level abstractions for data distribution and data parallel processing, and for task parallelism.\nBased on the PGAS (Partitioned Global Address space) programming model: can access variables in global address space from each node, a lot of behind-the-scenes work to reduce/buffer remote memory access.\nProvides data-driven placement of computations. \nDesigned around a multi-resolution philosophy: users can incrementally add more detail to their original code, to bring it as close to the machine as required, at the same time they can achieve anything you can normally do with MPI and OpenMP.\n\n\nThe Chapel community is fairly small: relatively few people know/use Chapel  ⇄  too few libraries. However, you can use functions/libraries written in other languages:",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#chapel-a-language-for-parallel-computing-on-large-scale-systems",
    "href": "chapel2/chapel-01-intro.html#chapel-a-language-for-parallel-computing-on-large-scale-systems",
    "title": "Introduction to Chapel",
    "section": "",
    "text": "Modern, open-source parallel programming language developed at Cray Inc. (acquired by Hewlett Packard Enterprise in 2019).\nOffers simplicity and readability of scripting languages such as Python or Matlab: “Python for parallel programming”.\nCompiled language \\(\\Rightarrow\\) provides the speed and performance of Fortran and C.\nSupports high-level abstractions for data distribution and data parallel processing, and for task parallelism.\nBased on the PGAS (Partitioned Global Address space) programming model: can access variables in global address space from each node, a lot of behind-the-scenes work to reduce/buffer remote memory access.\nProvides data-driven placement of computations. \nDesigned around a multi-resolution philosophy: users can incrementally add more detail to their original code, to bring it as close to the machine as required, at the same time they can achieve anything you can normally do with MPI and OpenMP.\n\n\nThe Chapel community is fairly small: relatively few people know/use Chapel  ⇄  too few libraries. However, you can use functions/libraries written in other languages:",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-chapel-codes-on-cedar-graham-béluga-narval",
    "href": "chapel2/chapel-01-intro.html#running-chapel-codes-on-cedar-graham-béluga-narval",
    "title": "Introduction to Chapel",
    "section": "Running Chapel codes on Cedar / Graham / Béluga / Narval",
    "text": "Running Chapel codes on Cedar / Graham / Béluga / Narval\nOn Alliance’s production clusters Cedar / Graham / Béluga / Narval we have three versions of Chapel:\n\nsingle-locale (single-node) chapel-multicore\nmulti-locale chapel-ucx for InfiniBand clusters (all newer clusters)\nolder multi-locale chapel-ofi for OmniPath clusters (Cedar)\n\nYou can find the documentation on running Chapel in our wiki.\nIf you want to start single-locale Chapel, you will need to load chapel-multicore module, e.g.\n$ module spider chapel-multicore   # list all versions\n$ module load chapel-multicore/2.4.0",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-chapel-codes-inside-a-docker-container",
    "href": "chapel2/chapel-01-intro.html#running-chapel-codes-inside-a-docker-container",
    "title": "Introduction to Chapel",
    "section": "Running Chapel codes inside a Docker container",
    "text": "Running Chapel codes inside a Docker container\nIf you are familiar with Docker and have it installed, you can run multi-locale Chapel inside a Docker container (e.g., on your laptop, or inside an Ubuntu VM on Arbutus):\n$ docker pull chapel/chapel  # will emulate a cluster with 4 cores/node\n$ mkdir -p ~/tmp\n$ docker run -v /home/ubuntu/tmp:/mnt -it -h chapel chapel/chapel  # map host's ~/tmp to container's /mnt\n$ cd /mnt\n$ apt-get update\n$ apt-get install nano  # install nano inside the Docker container\n$ nano test.chpl        # file is /mnt/test.chpl inside the container and ~ubuntu/tmp/test.chpl on the host VM\n$ chpl test.chpl\n$ ./test -nl 8\nYou can find more information at https://hub.docker.com/r/chapel/chapel",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-single-locale-chapel-in-macos",
    "href": "chapel2/chapel-01-intro.html#running-single-locale-chapel-in-macos",
    "title": "Introduction to Chapel",
    "section": "Running single-locale Chapel in MacOS",
    "text": "Running single-locale Chapel in MacOS\nYou can compile and run Chapel codes in MacOS. Multi-locale codes (e.g. containing distributed arrays) will compile but will run only as single-locale.\nbrew install chapel",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-chapel-codes-on-the-training-cluster",
    "href": "chapel2/chapel-01-intro.html#running-chapel-codes-on-the-training-cluster",
    "title": "Introduction to Chapel",
    "section": "Running Chapel codes on the training cluster",
    "text": "Running Chapel codes on the training cluster\n\n\n\n\n\n\nCautionTraining cluster\n\n\n\nNow we will distribute the usernames and passwords. Once you have these, log in to the training cluster and do the following:\n1. load single-locale Chapel and compile a simple code,\n2. write a makefile for compiling Chapel codes, and\n3. submit a serial job script to run Chapel on a compute node.\n\n\n\n\nOn the training cluster, you can start single-locale Chapel with:\n\n\n\n$ module load chapel-multicore/2.4.0\n\n\n\n\nLet’s write a simple Chapel code, compile and run it:\n$ cd ~/tmp\n$ nano test.chpl\n$     writeln('If you can see this, everything works!');\n$ chpl test.chpl\n$ ./test\nYou can optionally pass the flag --fast to the compiler to optimize the binary to run as fast as possible for the given architecture.\n\n\n\n\n\n\n\nDepending on the code, it might utilize one / several / all cores on the current node. The command above implies that you are allowed to utilize all cores. This might not be the case on an HPC cluster, where a login node is shared by many people at the same time, and where it might not be a good idea to occupy all cores on a login node with CPU-intensive tasks. Therefore, we’ll be running test Chapel codes inside submitted jobs on compute nodes.\nLet’s write the job script serial.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\n./test\nand then submit it:\n$ chpl test.chpl\n$ sbatch serial.sh\n$ sq                         # same as `squeue -u $USER`\n$ cat slurm-jobID.out\nAlternatively, we could work inside a serial interactive job:\n$ salloc --time=2:0:0 --mem-per-cpu=3600",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#makefiles",
    "href": "chapel2/chapel-01-intro.html#makefiles",
    "title": "Introduction to Chapel",
    "section": "Makefiles",
    "text": "Makefiles\nIn the rest of this workshop, we’ll be compiling codes test.chpl, baseSolver.chpl, begin.chpl, cobegin.chpl and many others. To simplify compilation, we suggest writing a file called Makefile in your working directory:\n%: %.chpl\n    chpl $^ -o $@\nclean:\n    @find . -maxdepth 1 -type f -executable -exec rm {} +\nNote that the 2nd and the 4th lines start with TAB and not with multiple spaces – this is very important!\nWith this makefile, to compile any Chapel code, e.g. test.chpl, you would type:\n$ make test\nAdd --fast flag to the makefile to optimize your code. And you can type make clean to delete all executables in the current directory.",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html",
    "title": "Heat transfer solver on distributed domains",
    "section": "",
    "text": "For a tightly coupled parallel calculation, consider a heat diffusion problem:\n\nwe have a square metallic plate initially at 25 degrees (initial condition)\nwe want to simulate the evolution of the temperature across the plate; governed by the 2D heat (diffusion) equation: \ndiscretize the solution \\(T(x,y,t)\\approx T^{(n)}_{i,j}\\) with \\(i=1,...,{\\rm rows}\\) and \\(j=1,...,{\\rm cols}\\)\n\nthe upper left corner is (1,1) and the lower right corner is (rows,cols)\n\nthe plate’s border is in contact with a different temperature distribution (boundary condition):\n\nupper side \\(T^{(n)}_{0,1..{\\rm cols}}\\equiv 0\\)\nleft side \\(T^{(n)}_{1..{\\rm rows},0}\\equiv 0\\)\nbottom side \\(T^{(n)}_{{\\rm rows}+1,1..{\\rm cols}} = {80\\cdot j/\\rm cols}\\) (linearly increasing from 0 to 80 degrees)\nright side \\(T^{(n)}_{1..{\\rm rows},{\\rm cols}+1} = 80\\cdot i/{\\rm rows}\\) (linearly increasing from 0 to 80 degrees)\n\n\nWe discretize the equation with forward Euler time stepping:\n\nIf for simplicity we assume \\(\\Delta x=\\Delta y=1\\) and \\(\\Delta t=1/4\\), our finite difference equation becomes:\n\nAt each time iteration, at each point we’ll be computing the new temperature Tnew according to:\nTnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1])\n\nTnew = new temperature computed at the current iteration\nT = temperature calculated at the past iteration (or the initial conditions at the first iteration)\nthe indices (i,j) indicate the grid point located at the i-th row and the j-th column\n\n\n\n\n\n\n\n\n\n\nHere is a serial implementation of this solver baseSolver.chpl:\nuse Time;\nconfig const rows, cols = 100;   // number of rows and columns in our matrix\nconfig const niter = 500;        // max number of iterations\nconfig const tolerance = 1e-4: real;   // temperature difference tolerance\nvar count = 0: int;                    // the iteration counter\nvar delta: real;   // the greatest temperature difference between Tnew and T\nvar tmp: real;     // for temporary results\n\nvar T: [0..rows+1,0..cols+1] real;\nvar Tnew: [0..rows+1,0..cols+1] real;\nT[1..rows,1..cols] = 25;\n\ndelta = tolerance*10;    // some safe initial large value\nvar watch: stopwatch;\nwatch.start();\nwhile (count &lt; niter && delta &gt;= tolerance) {\n  for i in 1..rows do T[i,cols+1] = 80.0*i/rows;   // right side boundary condition\n  for j in 1..cols do T[rows+1,j] = 80.0*j/cols;   // bottom side boundary condition\n  count += 1;    // update the iteration counter\n  for i in 1..rows {\n    for j in 1..cols {\n      Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n    }\n  }\n  delta = 0;\n  for i in 1..rows {\n    for j in 1..cols {\n      tmp = abs(Tnew[i,j]-T[i,j]);\n      if tmp &gt; delta then delta = tmp;\n    }\n  }\n  if count%100 == 0 then writeln(\"delta = \", delta);\n  T = Tnew;\n}\nwatch.stop();\n\nwriteln('Largest temperature difference was ', delta);\nwriteln('Converged after ', count, ' iterations');\nwriteln('Simulation took ', watch.elapsed(), ' seconds');\nchpl --fast baseSolver.chpl\n./baseSolver --rows=650 --cols=650 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.00199985\nSimulation took 17.3374 seconds",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#case-study-2-solving-the-heat-transfer-problem",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#case-study-2-solving-the-heat-transfer-problem",
    "title": "Heat transfer solver on distributed domains",
    "section": "",
    "text": "For a tightly coupled parallel calculation, consider a heat diffusion problem:\n\nwe have a square metallic plate initially at 25 degrees (initial condition)\nwe want to simulate the evolution of the temperature across the plate; governed by the 2D heat (diffusion) equation: \ndiscretize the solution \\(T(x,y,t)\\approx T^{(n)}_{i,j}\\) with \\(i=1,...,{\\rm rows}\\) and \\(j=1,...,{\\rm cols}\\)\n\nthe upper left corner is (1,1) and the lower right corner is (rows,cols)\n\nthe plate’s border is in contact with a different temperature distribution (boundary condition):\n\nupper side \\(T^{(n)}_{0,1..{\\rm cols}}\\equiv 0\\)\nleft side \\(T^{(n)}_{1..{\\rm rows},0}\\equiv 0\\)\nbottom side \\(T^{(n)}_{{\\rm rows}+1,1..{\\rm cols}} = {80\\cdot j/\\rm cols}\\) (linearly increasing from 0 to 80 degrees)\nright side \\(T^{(n)}_{1..{\\rm rows},{\\rm cols}+1} = 80\\cdot i/{\\rm rows}\\) (linearly increasing from 0 to 80 degrees)\n\n\nWe discretize the equation with forward Euler time stepping:\n\nIf for simplicity we assume \\(\\Delta x=\\Delta y=1\\) and \\(\\Delta t=1/4\\), our finite difference equation becomes:\n\nAt each time iteration, at each point we’ll be computing the new temperature Tnew according to:\nTnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1])\n\nTnew = new temperature computed at the current iteration\nT = temperature calculated at the past iteration (or the initial conditions at the first iteration)\nthe indices (i,j) indicate the grid point located at the i-th row and the j-th column\n\n\n\n\n\n\n\n\n\n\nHere is a serial implementation of this solver baseSolver.chpl:\nuse Time;\nconfig const rows, cols = 100;   // number of rows and columns in our matrix\nconfig const niter = 500;        // max number of iterations\nconfig const tolerance = 1e-4: real;   // temperature difference tolerance\nvar count = 0: int;                    // the iteration counter\nvar delta: real;   // the greatest temperature difference between Tnew and T\nvar tmp: real;     // for temporary results\n\nvar T: [0..rows+1,0..cols+1] real;\nvar Tnew: [0..rows+1,0..cols+1] real;\nT[1..rows,1..cols] = 25;\n\ndelta = tolerance*10;    // some safe initial large value\nvar watch: stopwatch;\nwatch.start();\nwhile (count &lt; niter && delta &gt;= tolerance) {\n  for i in 1..rows do T[i,cols+1] = 80.0*i/rows;   // right side boundary condition\n  for j in 1..cols do T[rows+1,j] = 80.0*j/cols;   // bottom side boundary condition\n  count += 1;    // update the iteration counter\n  for i in 1..rows {\n    for j in 1..cols {\n      Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n    }\n  }\n  delta = 0;\n  for i in 1..rows {\n    for j in 1..cols {\n      tmp = abs(Tnew[i,j]-T[i,j]);\n      if tmp &gt; delta then delta = tmp;\n    }\n  }\n  if count%100 == 0 then writeln(\"delta = \", delta);\n  T = Tnew;\n}\nwatch.stop();\n\nwriteln('Largest temperature difference was ', delta);\nwriteln('Converged after ', count, ' iterations');\nwriteln('Simulation took ', watch.elapsed(), ' seconds');\nchpl --fast baseSolver.chpl\n./baseSolver --rows=650 --cols=650 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.00199985\nSimulation took 17.3374 seconds",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#distributed-version",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#distributed-version",
    "title": "Heat transfer solver on distributed domains",
    "section": "Distributed version",
    "text": "Distributed version\nNow let us use distributed domains to write a parallel version of our original heat transfer solver code. We’ll start by copying baseSolver.chpl into parallelSolver.chpl and making the following modifications to the latter:\n\nAdd\n\nuse BlockDist;\nconst mesh: domain(2) = {1..rows, 1..cols};   // local 2D domain\n\nAdd a larger \\((rows+2)\\times (cols+2)\\) block-distributed domain largerMesh with a layer of ghost points on perimeter locales, and define a temperature array T on top of it, by adding the following to our code:\n\nconst largerMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = {0..rows+1, 0..cols+1};\n\nChange the definitions of T and Tnew (delete those two lines) to\n\nvar T, Tnew: [largerMesh] real;   // block-distributed arrays of temperatures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMove the linearly increasing boundary conditions (right/bottom sides) before the while loop.\nReplace the loop for computing inner Tnew:\n\nfor i in 1..rows {  // do smth for row i\n  for j in 1..cols {   // do smth for row i and column j\n    Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n  }\n}\nwith a parallel forall loop (contains a mistake on purpose!):\nforall (i,j) in mesh do\n  Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n\n\n\n\n\n\nCautionQuestion Data.4\n\n\n\n\n\nCan anyone spot a mistake in this loop?\n\n\n\n\nReplace\n\ndelta = 0;\nfor i in 1..rows {\n  for j in 1..cols {\n    tmp = abs(Tnew[i,j]-T[i,j]);\n    if tmp &gt; delta then delta = tmp;\n  }\n}\nwith\ndelta = max reduce abs(Tnew[1..rows,1..cols]-T[1..rows,1..cols]);\n\nReplace\n\nT = Tnew;\nwith the inner-only update\nT[1..rows,1..cols] = Tnew[1..rows,1..cols];   // uses parallel `forall` underneath",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#benchmarking",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#benchmarking",
    "title": "Heat transfer solver on distributed domains",
    "section": "Benchmarking",
    "text": "Benchmarking\nLet’s compile both serial and data-parallel versions using the same multi-locale compiler (and we will need -nl flag when running both):\n$ source /project/def-sponsor00/shared/syncHPC/startMultiLocale.sh\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ chpl --fast parallelSolver.chpl -o parallelSolver\nFirst, let’s try this on a smaller problem. Let’s write a job submission script distributed.sh:\n#!/bin/bash\n# this is distributed.sh\n#SBATCH --time=0:15:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\necho Running on $SLURM_NNODES nodes\n./baseSolver --rows=30 --cols=30 --niter=2000 -nl $SLURM_NNODES\n# ./parallelSolver --rows=30 --cols=30 --niter=2000 -nl $SLURM_NNODES\n\n\nLet’s run both codes, (un)commenting the relevant lines in distributed.sh:\n$ sbatch distributed.sh\nLargest temperature difference was 9.9534e-05\nConverged after 1148 iterations\nSimulation took ... seconds\nWait for the jobs to finish and then check the results:\n\n\n\n--nodes\n1\n4\n\n\n--cpus-per-task\n1\n2\n\n\nbaseSolver (sec)\n0.00725\n\n\n\nparallelSolver (sec)\n\n67.5\n\n\n\nAs you can see, on the training cluster the parallel code on 4 nodes (with 2 cores each) ran ~9,300 times slower than a serial code on a single node … What is going on here!? Shouldn’t the parallel code run ~8X faster, since we have 8X as many processors?\nThis is a fine-grained parallel code that needs a lot of communication between tasks, and relatively little computing. So, we are seeing the communication overhead. The training cluster has a very slow interconnect, so the problem is even worse there than on a production cluster!\nIf we increase our 2D problem size, there will be more computation (scaling as \\(O(n^2)\\)) in between communications (scaling as \\(O(n)\\)), and at some point the parallel code should catch up to the serial code and eventually run faster. Let’s try these problem sizes:\n--rows=650 --cols=650 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 7766 iterations\n\n--rows=2000 --cols=2000 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 9158 iterations\n\n--rows=8000 --cols=8000 --niter=9800 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 9725 iterations\n\n--rows=16000 --cols=16000 --niter=9900 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 9847 iterations\n\nOn the training cluster (slower interconnect)\nI switched both codes to single precision (change real to real(32) and use (80.0*i/rows):real(32) when assigning to real(32) variables), to be able to accommodate larger arrays. The table below shows the slowdown factor when going from serial to parallel:\n\n\n\n\n\n\n\n\n\n\n\n\n30^2\n650^2\n2,000^2\n8,000^2\n16,000^2\n\n\n\n\n--nodes=4 --cpus-per-task=8\n5104\n14.78\n2.29\n1/1.95\n1/3.31",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#final-parallel-code",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#final-parallel-code",
    "title": "Heat transfer solver on distributed domains",
    "section": "Final parallel code",
    "text": "Final parallel code\nHere is the final single-precision parallel version of the code, minus the comments:\nuse Time, BlockDist;\nconfig const rows, cols = 100;\nconfig const niter = 500;\nconfig const tolerance = 1e-4: real(32);\nvar count = 0: int;\nvar delta: real(32);\nvar tmp: real(32);\n\nconst mesh: domain(2) = {1..rows, 1..cols};\nconst largerMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = {0..rows+1, 0..cols+1};\nvar T, Tnew: [largerMesh] real(32);\nT[1..rows,1..cols] = 25;\n\ndelta = tolerance*10;\nvar watch: stopwatch;\nwatch.start();\nfor i in 1..rows do T[i,cols+1] = (80.0*i/rows):real(32);   // right side boundary condition\nfor j in 1..cols do T[rows+1,j] = (80.0*j/cols):real(32);   // bottom side boundary condition\nwhile (count &lt; niter && delta &gt;= tolerance) {\n  count += 1;\n  forall (i,j) in largerMesh[1..rows,1..cols] do\n    Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n  delta = max reduce abs(Tnew[1..rows,1..cols]-T[1..rows,1..cols]);\n  if count%100 == 0 then writeln(\"delta = \", delta);\n  T[1..rows,1..cols] = Tnew[1..rows,1..cols];   // uses parallel `forall` underneath\n}\nwatch.stop();\n\nwriteln('Largest temperature difference was ', delta);\nwriteln('Converged after ', count, ' iterations');\nwriteln('Simulation took ', watch.elapsed(), ' seconds');\nThis is the entire multi-locale, data-parallel, hybrid shared-/distributed-memory solver!",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-08-output.html",
    "href": "chapel2/chapel-08-output.html",
    "title": "Writing output",
    "section": "",
    "text": "From Chapel you can save data as binary (not recommended, for portability reasons), as HDF5 (serial or parallel), or NetCDF. Here is a modified serial code juliaSetSerial.chpl with NetCDF output:\nuse Time;\nuse NetCDF.C_NetCDF;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i:c_int;\n  }\n  return 255:c_int;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] c_int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nThe reason we are using C types (c_int) here – and not Chapel’s own int(32) or int(64) – is that we can save the resulting array stability into a compressed netCDF file. To the best of my knowledge, this can only be done using NetCDF.C_NetCDF library that relies on C types. You can add this to your code:\nwriteln(\"Writing NetCDF ...\");\nuse NetCDF.C_NetCDF;\nproc cdfError(e) {\n  if e != NC_NOERR {\n    writeln(\"Error: \", nc_strerror(e): string);\n    exit(2);\n  }\n}\nvar ncid, xDimID, yDimID, varID: c_int;\nvar dimIDs: [0..1] c_int;   // two elements\ncdfError(nc_create(\"test.nc\", NC_NETCDF4, ncid));       // const NC_NETCDF4 =&gt; file in netCDF-4 standard\ncdfError(nc_def_dim(ncid, \"x\", n, xDimID)); // define the dimensions\ncdfError(nc_def_dim(ncid, \"y\", n, yDimID));\ndimIDs = [xDimID, yDimID];                              // set up dimension IDs array\ncdfError(nc_def_var(ncid, \"stability\", NC_INT, 2, dimIDs[0], varID));   // define the 2D data variable\ncdfError(nc_def_var_deflate(ncid, varID, NC_SHUFFLE, deflate=1, deflate_level=9)); // compress 0=no 9=max\ncdfError(nc_enddef(ncid));                              // done defining metadata\ncdfError(nc_put_var_int(ncid, varID, stability[1,1]));  // write data to file\ncdfError(nc_close(ncid));\nTesting on my laptop, it took the code 0.471 seconds to compute a \\(2000^2\\) fractal.\nTry running it yourself! It will produce a file test.nc that you can download to your computer and render with ParaView or other visualization tool. Does the size of test.nc make sense?",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Writing output"
    ]
  },
  {
    "objectID": "chapel2/chapel-08-output.html#write-to-a-netcdf-file",
    "href": "chapel2/chapel-08-output.html#write-to-a-netcdf-file",
    "title": "Writing output",
    "section": "",
    "text": "From Chapel you can save data as binary (not recommended, for portability reasons), as HDF5 (serial or parallel), or NetCDF. Here is a modified serial code juliaSetSerial.chpl with NetCDF output:\nuse Time;\nuse NetCDF.C_NetCDF;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i:c_int;\n  }\n  return 255:c_int;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] c_int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nThe reason we are using C types (c_int) here – and not Chapel’s own int(32) or int(64) – is that we can save the resulting array stability into a compressed netCDF file. To the best of my knowledge, this can only be done using NetCDF.C_NetCDF library that relies on C types. You can add this to your code:\nwriteln(\"Writing NetCDF ...\");\nuse NetCDF.C_NetCDF;\nproc cdfError(e) {\n  if e != NC_NOERR {\n    writeln(\"Error: \", nc_strerror(e): string);\n    exit(2);\n  }\n}\nvar ncid, xDimID, yDimID, varID: c_int;\nvar dimIDs: [0..1] c_int;   // two elements\ncdfError(nc_create(\"test.nc\", NC_NETCDF4, ncid));       // const NC_NETCDF4 =&gt; file in netCDF-4 standard\ncdfError(nc_def_dim(ncid, \"x\", n, xDimID)); // define the dimensions\ncdfError(nc_def_dim(ncid, \"y\", n, yDimID));\ndimIDs = [xDimID, yDimID];                              // set up dimension IDs array\ncdfError(nc_def_var(ncid, \"stability\", NC_INT, 2, dimIDs[0], varID));   // define the 2D data variable\ncdfError(nc_def_var_deflate(ncid, varID, NC_SHUFFLE, deflate=1, deflate_level=9)); // compress 0=no 9=max\ncdfError(nc_enddef(ncid));                              // done defining metadata\ncdfError(nc_put_var_int(ncid, varID, stability[1,1]));  // write data to file\ncdfError(nc_close(ncid));\nTesting on my laptop, it took the code 0.471 seconds to compute a \\(2000^2\\) fractal.\nTry running it yourself! It will produce a file test.nc that you can download to your computer and render with ParaView or other visualization tool. Does the size of test.nc make sense?",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Writing output"
    ]
  },
  {
    "objectID": "chapel2/chapel-08-output.html#write-to-a-png-image",
    "href": "chapel2/chapel-08-output.html#write-to-a-png-image",
    "title": "Writing output",
    "section": "Write to a PNG image",
    "text": "Write to a PNG image\nChapel’s Image library lets you write arrays of pixels to a PNG file. The following code – when added to the non-GPU code juliaSetSerial.chpl – writes the array stability to a file 2000.png assuming that in the current directory you\n\nhave downloaded the colour map in CSV and\nhave added the file sciplot.chpl (pasted below)\n\nwriteln(\"Plotting ...\");\nuse Image, Math, sciplot;\nwatch.clear();\nwatch.start();\nconst smin = min reduce(stability);\nconst smax = max reduce(stability);\nvar colour: [1..n, 1..n] 3*int;\nvar cmap = readColourmap('nipy_spectral.csv');   // cmap.domain is {1..256, 1..3}\nfor i in 1..n {\n  for j in 1..n {\n    var idx = ((stability[i,j]:real-smin)/(smax-smin)*255):int + 1; //scale to 1..256\n    colour[i,j] = ((cmap[idx,1]*255):int, (cmap[idx,2]*255):int, (cmap[idx,3]*255):int);\n  }\n}\nvar pixels = colorToPixel(colour);               // array of pixels\nwriteImage(n:string+\".png\", imageType.png, pixels);\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n// save this as sciplot.chpl\nuse IO;\nuse List;\n\nproc readColourmap(filename: string) {\n  var reader = open(filename, ioMode.r).reader();\n  var line: string;\n  if (!reader.readLine(line)) then   // skip the header\n    halt(\"ERROR: file appears to be empty\");\n  var dataRows : list(string); // a list of lines from the file\n  while (reader.readLine(line)) do   // read all lines into the list\n    dataRows.pushBack(line);\n  var cmap: [1..dataRows.size, 1..3] real;\n  for (i, row) in zip(1..dataRows.size, dataRows) {\n    var c1 = row.find(','):int;    // position of the 1st comma in the line\n    var c2 = row.rfind(','):int;   // position of the 2nd comma in the line\n    cmap[i,1] = row[0..c1-1]:real;\n    cmap[i,2] = row[c1+1..c2-1]:real;\n    cmap[i,3] = row[c2+1..]:real;\n  }\n  reader.close();\n  return cmap;\n}\nHere are the typical timings on a CPU:\nComputing 2000x2000 Julia set ...\nIt took 0.382409 seconds\nPlotting ...\nIt took 0.192508 seconds",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Writing output"
    ]
  },
  {
    "objectID": "chapel2/chapel-22-task-parallel-heat-transfer.html",
    "href": "chapel2/chapel-22-task-parallel-heat-transfer.html",
    "title": "Task-parallelizing the heat transfer solver",
    "section": "",
    "text": "Important: In a shorter Chapel course, we suggest skipping this section and focus mostly on data parallelism, coming back here only when you have time.\nHere is our plan to task-parallelize the heat transfer equation:\n\ndivide the entire grid of points into blocks and assign blocks to individual tasks,\neach task should compute the new temperature of its assigned points,\nperform a reduction over the whole grid, to update the greatest temperature difference between Tnew and T.\n\nFor the reduction of the grid we can simply use the max reduce statement, which is already parallelized. Now, let’s divide the grid into rowtasks * coltasks subgrids, and assign each subgrid to a task using the coforall loop (we will have rowtasks * coltasks tasks in total).\nRecall out code gmax.chpl in which we broke the 1D array with 1e8 elements into numtasks=2 blocks, and each task was processing elements start..finish. Now we’ll do exactly the same in 2D. First, let’s write a quick serial code test.chpl to test the indices:\nconfig const rows, cols = 100;               // number of rows and columns in our matrix\n\nconfig const rowtasks = 3, coltasks = 4; // number of blocks in x- and y-dimensions\n// each block processed by a separate task\n// let's pretend we have 12 cores\n\nconst nr = rows / rowtasks;   // number of rows per task\nconst rr = rows % rowtasks;   // remainder rows (did not fit into the last row of tasks)\nconst nc = cols / coltasks;   // number of columns per task\nconst rc = cols % coltasks;   // remainder columns (did not fit into the last column of tasks)\n\ncoforall taskid in 0..coltasks*rowtasks-1 {\n  var row1, row2, col1, col2: int;\n  row1 = taskid/coltasks*nr + 1;\n  row2 = taskid/coltasks*nr + nr;\n  if row2 == rowtasks*nr then row2 += rr; // add rr rows to the last row of tasks\n  col1 = taskid%coltasks*nc + 1;\n  col2 = taskid%coltasks*nc + nc;\n  if col2 == coltasks*nc then col2 += rc; // add rc columns to the last column of tasks\n  writeln('task ', taskid, ': rows ', row1, '-', row2, ' and columns ', col1, '-', col2);\n}\n$ chpl test.chpl -o test\n$ sed -i -e 's|atomic|test|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\ntask 0: rows 1-33 and columns 1-25\ntask 1: rows 1-33 and columns 26-50\ntask 2: rows 1-33 and columns 51-75\ntask 3: rows 1-33 and columns 76-100\ntask 4: rows 34-66 and columns 1-25\ntask 5: rows 34-66 and columns 26-50\ntask 6: rows 34-66 and columns 51-75\ntask 7: rows 34-66 and columns 76-100\ntask 8: rows 67-100 and columns 1-25\ntask 9: rows 67-100 and columns 26-50\ntask 10: rows 67-100 and columns 51-75\ntask 11: rows 67-100 and columns 76-100\nAs you can see, dividing Tnew computation between concurrent tasks could be cumbersome. Chapel provides high-level abstractions for data parallelism that take care of all the data distribution for us. We will study data parallelism in the following lessons, but for now let’s compare the benchmark solution (baseSolver.chpl) with our coforall parallelization to see how the performance improved.\nNow we’ll parallelize our heat transfer solver. Let’s copy baseSolver.chpl into parallel1.chpl and then start editing the latter. We’ll make the following changes in parallel1.chpl:\ndiff baseSolver.chpl parallel1.chpl\n18a19,24\n&gt; config const rowtasks = 3, coltasks = 4;   // let's pretend we have 12 cores\n&gt; const nr = rows / rowtasks;    // number of rows per task\n&gt; const rr = rows - nr*rowtasks; // remainder rows (did not fit into the last task)\n&gt; const nc = cols / coltasks;    // number of columns per task\n&gt; const rc = cols - nc*coltasks; // remainder columns (did not fit into the last task)\n&gt;\n31,32c37,46\n&lt;   for i in 1..rows {    // do smth for row i\n&lt;     for j in 1..cols {  // do smth for row i and column j\n&lt;       Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n&lt;     }\n&lt;   }\n---\n&gt;   coforall taskid in 0..coltasks*rowtasks-1 { // each iteration processed by a separate task\n&gt;     var row1, row2, col1, col2: int;\n&gt;     row1 = taskid/coltasks*nr + 1;\n&gt;     row2 = taskid/coltasks*nr + nr;\n&gt;     if row2 == rowtasks*nr then row2 += rr; // add rr rows to the last row of tasks\n&gt;     col1 = taskid%coltasks*nc + 1;\n&gt;     col2 = taskid%coltasks*nc + nc;\n&gt;     if col2 == coltasks*nc then col2 += rc; // add rc columns to the last column of tasks\n&gt;     for i in row1..row2 {\n&gt;       for j in col1..col2 {\n&gt;         Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n&gt;       }\n&gt;     }\n&gt;   }\n&gt;\n36,42d49\n&lt;   delta = 0;\n&lt;   for i in 1..rows {\n&lt;     for j in 1..cols {\n&lt;       tmp = abs(Tnew[i,j]-T[i,j]);\n&lt;       if tmp &gt; delta then delta = tmp;\n&lt;     }\n&lt;   }\n43a51,52\n&gt;   delta = max reduce abs(Tnew[1..rows,1..cols]-T[1..rows,1..cols]);\nLet’s compile and run both codes on the same large problem:\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ sed -i -e 's|test|baseSolver --rows=650 --iout=200 --niter=10_000 --tolerance=0.002 --nout=1000|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] after 7750 iterations is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 8.96548 seconds\n\n$ chpl --fast parallel1.chpl -o parallel1\n$ sed -i -e 's|baseSolver|parallel1|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] after 7750 iterations is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 25.106 seconds\nBoth ran to 7750 iterations, with the same numerical results, but the parallel code is nearly 3X slower – that’s terrible!\n\nDiscussion\nWhat happened!?…\n\nTo understand the reason, let’s analyze the code. When the program starts, the main task does all the declarations and initializations, and then, it enters the main loop of the simulation (the while loop). Inside this loop, the parallel tasks are launched for the first time. When these tasks finish their computations, the main task resumes its execution, it updates delta and T, and everything is repeated again. So, in essence, parallel tasks are launched and terminated 7750 times, which introduces a significant amount of overhead (the time the system needs to effectively start and destroy tasks in the specific hardware, at each iteration of the while loop).\nClearly, a better approach would be to launch the parallel tasks just once, and have them execute all the time steps, before resuming the main task to print the final results.\nLet’s copy parallel1.chpl into parallel2.chpl and then start editing the latter. We’ll make the following changes:\n\nMove the rows\n\n  coforall taskid in 0..coltasks*rowtasks-1 { // each iteration processed by a separate task\n    var row1, row2, col1, col2: int;\n    row1 = taskid/coltasks*nr + 1;\n    row2 = taskid/coltasks*nr + nr;\n    if row2 == rowtasks*nr then row2 += rr; // add rr rows to the last row of tasks\n    col1 = taskid%coltasks*nc + 1;\n    col2 = taskid%coltasks*nc + nc;\n    if col2 == coltasks*nc then col2 += rc; // add rc columns to the last column of tasks\nand the corresponding closing bracket } of this coforall loop outside the while loop, so that while is now nested inside coforall.\n\nSince now copying Tnew into T is a local operation for each task, i.e. we should replace T = Tnew; with\n\nT[row1..row2,col1..col2] = Tnew[row1..row2,col1..col2];\nBut this is not sufficient! We need to make sure we finish computing all elements of Tnew in all tasks before computing the greatest temperature difference delta. For that we need to synchronize all tasks, right after computing Tnew. We’ll also need to synchronize tasks after computing delta and T from Tnew, as none of the tasks should jump into the new iteration without having delta and T! So, we need two synchronization points inside the coforall loop.\n\n\n\n\nDefine two atomic variables that we’ll use for synchronization\n\nvar lock1, lock2: atomic int;\nand add after the (i,j)-loops to compute Tnew the following:\nlock1.add(1);   // each task atomically adds 1 to lock\nlock1.waitFor(coltasks*rowtasks*count);   // then it waits for lock to be equal coltasks*rowtasks\nand after T[row1..row2,col1..col2] = Tnew[row1..row2,col1..col2]; the following:\nlock2.add(1);   // each task atomically subtracts 1 from lock\nlock2.waitFor(coltasks*rowtasks*count);   // then it waits for lock to be equal 0\nNotice that we have a product coltasks*rowtasks*count, since lock1/lock2 will be incremented by all tasks at all iterations.\n\nMove var count = 0: int; into coforall so that it becomes a local variable for each task. Also, remove count instance (in writeln()) after coforall ends.\nMake delta atomic:\n\nvar delta: atomic real;    // the greatest temperature difference between Tnew and T\n...\ndelta.write(tolerance*10); // some safe initial large value\n...\nwhile (count &lt; niter && delta.read() &gt;= tolerance) {\n\nDefine an array of local delta’s for each task and use it to compute delta:\n\nvar arrayDelta: [0..coltasks*rowtasks-1] real;\n...\nvar tmp: real;  // inside coforall\n...\ntmp = 0;        // inside while\n...\ntmp = max(abs(Tnew[i,j]-T[i,j]),tmp);    // next line after Tnew[i,j] = ...\n...\narrayDelta[taskid] = tmp;   // right after (i,j)-loop to compute Tnew[i,j]\n...\nif taskid == 0 then {       // compute delta right after lock1.waitFor()\n    delta.write(max reduce arrayDelta);\n    if count%nout == 0 then writeln('Temperature at iteration ', count, ': ', Tnew[iout,jout]);\n}\n\nRemove the original T[iout,jout] output line.\nFinally, move the boundary conditions (right+bottom edges) before the while loop. Why can we do it now?\n\nNow let’s compare the performance of parallel2.chpl to the benchmark serial solution baseSolver.chpl:\n$ sed -i -e 's|parallel1|baseSolver|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] after 7750 iterations is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 9.40637 seconds\n\n$ chpl --fast parallel2.chpl -o parallel2\n$ sed -i -e 's|baseSolver|parallel2|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 4.74536 seconds\nWe get a speedup of 2X on two cores, as we should.\nFinally, here is a parallel scaling test on Cedar inside a 32-core interactive job:\n$ ./parallel2 ... --rowtasks=1 --coltasks=1\nThe simulation took 32.2201 seconds\n\n$ ./parallel2 ... --rowtasks=2 --coltasks=2\nThe simulation took 10.197 seconds\n\n$ ./parallel2 ... --rowtasks=4 --coltasks=4\nThe simulation took 3.79577 seconds\n\n$ ./parallel2 ... --rowtasks=4 --coltasks=8\nThe simulation took 2.4874 seconds",
    "crumbs": [
      "PART 3: TASK PARALLELISM (BRIEFLY)",
      "Task-parallelizing the heat transfer solver"
    ]
  },
  {
    "objectID": "chapel2/chapel-11-single-locale-data-parallel.html",
    "href": "chapel2/chapel-11-single-locale-data-parallel.html",
    "title": "Single-locale data parallelism",
    "section": "",
    "text": "As we mentioned in the previous section, Data Parallelism is a style of parallel programming in which parallelism is driven by computations over collections of data elements or their indices. The main tool for this in Chapel is a forall loop – it’ll create an appropriate number of threads to execute a loop, dividing the loop’s iterations between them.\nWhat is the appropriate number of tasks/threads?\nConsider a simple code test.chpl:\nIn this code we update all elements of the array A. The code will run on a single node, lauching as many threads as the number of available cores. It is thread-safe, meaning that no two threads are writing into the same variable at the same time.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Single-locale data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-11-single-locale-data-parallel.html#reduction",
    "href": "chapel2/chapel-11-single-locale-data-parallel.html#reduction",
    "title": "Single-locale data parallelism",
    "section": "Reduction",
    "text": "Reduction\nConsider a simple code forall.chpl that we’ll run inside a 4-core interactive job. We have a range of indices 1..1000, and they get broken into 4 groups that are processed by individual threads:\nvar count = 0;\nforall i in 1..1000 with (+ reduce count) {   // parallel loop\n  count += i;\n}\nwriteln('count = ', count);\nIf we have not done so, let’s write a script shared.sh for submitting single-locale, two-core Chapel jobs:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./forall\n\n\n$ chpl forall.chpl -o forall\n$ sbatch shared.sh\n$ cat solution.out\ncount = 500500\n\nNumber of cores at runtime\nWe computed the sum of integers from 1 to 1000 in parallel. How many cores did the code run on? Looking at the code or its output, we don’t know. Most likely, on all 4 cores available to us inside the job. But we can actually check that! Do this:\n\nreplace count += i; with count = 1;\nchange the last line to writeln('actual number of threads = ', count);\n\n$ chpl forall.chpl -o forall\n$ sbatch shared.sh\n$ cat solution.out\nactual number of threads = 4\nIf you see one thread, try running this code as a batch multi-core job.\n\n\nAlternative syntax\nWe can also do parallel reduction over a loop in this way:\nvar count = (+ reduce forall i in 1..1000 do i**2);\nwriteln('count = ', count);\nWe can also initialize in array and do parallel reduction over all array elements:\nvar A = (for i in 1..1000 do i);\nvar count = (+ reduce A);   // multiple threads\nwriteln('count = ', count);\nOr we could do it this way if we want to do some processing on individual elements:\nvar A = (for i in 1..1000 do i);\nvar count = (+ reduce forall a in A do a**2);\nwriteln('count = ', count);\n \n\n\n\n\n\n\nCautionQuestion Parallel \\(\\pi\\)\n\n\n\n\n\nUsing the first version of forall.chpl (where we computed the sum of integers 1..1000) as a template, write a Chapel code to compute \\(\\pi\\) by calculating the integral numerically via summation using forall parallelism. Implement the number of intervals as config variable.\nTo get you started, here is a serial version of this code pi.chpl:\nconfig const n = 1000;\nvar h, total: real;\nh = 1.0 / n;                          // interval width\nfor i in 1..n {\n  var x = h * ( i - 0.5 );\n  total += 4.0 / ( 1.0 + x**2);\n}\nwritef('pi is %3.10r\\n', total*h);    // C-style formatted write, r stands for real",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Single-locale data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-13-multi-locale-chapel.html",
    "href": "chapel2/chapel-13-multi-locale-chapel.html",
    "title": "Multi-locale Chapel",
    "section": "",
    "text": "So far we have been working with single-locale Chapel codes that may run on one or many cores on a single compute node, making use of the shared memory space and accelerating computations by launching parallel threads on individual cores. Chapel codes can also run on multiple nodes on a compute cluster. In Chapel this is referred to as multi-locale execution.\n\n\n\n\n\n\nNoteDocker side note\n\n\n\nIf you work inside a Chapel Docker container, e.g., chapel/chapel, the container environment simulates a multi-locale cluster, so you would compile and launch multi-locale Chapel codes directly by specifying the number of locales with -nl flag:\n$ chpl --fast mycode.chpl -o mybinary\n$ ./mybinary -nl 3\nInside the Docker container on multiple locales your code will not run any faster than on a single locale, since you are emulating a virtual cluster, and all tasks run on the same physical node. To achieve actual speedup, you need to run your parallel multi-locale Chapel code on a real HPC cluster.\n\n\nOn an HPC cluster you would need to submit either an interactive or a batch job asking for several nodes and then run a multi-locale Chapel code inside that job. In practice, the exact commands to run multi-locale Chapel codes depend on how Chapel was configured on the cluster.\nWhen you compile a Chapel code with the multi-locale Chapel compiler, two binaries will be produced. One is called mybinary and is a launcher binary used to submit the real executable mybinary_real. If the Chapel environment is configured properly with the launcher for the cluster’s physical interconnect, then you would simply compile the code and use the launcher binary mybinary to run a multi-locale code.\nFor the rest of this class we assume that you have a working multi-locale Chapel environment, whether provided by a Docker container or by multi-locale Chapel on a physical HPC cluster. We will run all examples on 3 nodes with 2 cores per node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s write a job submission script distributed.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=1000   # in MB\n#SBATCH --nodes=3\n#SBATCH --cpus-per-task=2\n#SBATCH --output=solution.out\n./test -nl 3   # in this case the 'srun' launcher is already configured for our interconnect",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Multi-locale Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-13-multi-locale-chapel.html#setup",
    "href": "chapel2/chapel-13-multi-locale-chapel.html#setup",
    "title": "Multi-locale Chapel",
    "section": "",
    "text": "So far we have been working with single-locale Chapel codes that may run on one or many cores on a single compute node, making use of the shared memory space and accelerating computations by launching parallel threads on individual cores. Chapel codes can also run on multiple nodes on a compute cluster. In Chapel this is referred to as multi-locale execution.\n\n\n\n\n\n\nNoteDocker side note\n\n\n\nIf you work inside a Chapel Docker container, e.g., chapel/chapel, the container environment simulates a multi-locale cluster, so you would compile and launch multi-locale Chapel codes directly by specifying the number of locales with -nl flag:\n$ chpl --fast mycode.chpl -o mybinary\n$ ./mybinary -nl 3\nInside the Docker container on multiple locales your code will not run any faster than on a single locale, since you are emulating a virtual cluster, and all tasks run on the same physical node. To achieve actual speedup, you need to run your parallel multi-locale Chapel code on a real HPC cluster.\n\n\nOn an HPC cluster you would need to submit either an interactive or a batch job asking for several nodes and then run a multi-locale Chapel code inside that job. In practice, the exact commands to run multi-locale Chapel codes depend on how Chapel was configured on the cluster.\nWhen you compile a Chapel code with the multi-locale Chapel compiler, two binaries will be produced. One is called mybinary and is a launcher binary used to submit the real executable mybinary_real. If the Chapel environment is configured properly with the launcher for the cluster’s physical interconnect, then you would simply compile the code and use the launcher binary mybinary to run a multi-locale code.\nFor the rest of this class we assume that you have a working multi-locale Chapel environment, whether provided by a Docker container or by multi-locale Chapel on a physical HPC cluster. We will run all examples on 3 nodes with 2 cores per node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s write a job submission script distributed.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=1000   # in MB\n#SBATCH --nodes=3\n#SBATCH --cpus-per-task=2\n#SBATCH --output=solution.out\n./test -nl 3   # in this case the 'srun' launcher is already configured for our interconnect",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Multi-locale Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-13-multi-locale-chapel.html#simple-multi-locale-codes",
    "href": "chapel2/chapel-13-multi-locale-chapel.html#simple-multi-locale-codes",
    "title": "Multi-locale Chapel",
    "section": "Simple multi-locale codes",
    "text": "Simple multi-locale codes\nLet us test our multi-locale Chapel environment by launching the following code:\n\n\nwriteln(Locales);\n$ source /project/def-sponsor00/shared/syncHPC/startMultiLocale.sh\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\nThis code will print the built-in global array Locales. Running it on four locales will produce\nLOCALE0 LOCALE1 LOCALE2\nWe want to run some code on each locale (node). For that, we can cycle through locales:\nfor loc in Locales do   // this is still a serial program\n  on loc do             // run the next line on locale `loc`\n    writeln(\"this locale is named \", here.name[0..4]);   // `here` is the locale on which the code is running\nThis will produce\nthis locale is named node1\nthis locale is named node2\nthis locale is named node3\nHere the built-in variable class here refers to the locale on which the code is running, and here.name is its hostname. We started a serial for loop cycling through all locales, and on each locale we printed its name, i.e., the hostname of each node. This program ran in serial starting a task on each locale only after completing the same task on the previous locale. Note the order in which locales were listed.\nTo run this code in parallel, starting four simultaneous tasks, one per locale, we simply need to replace for with forall:\nforall loc in Locales do   // now this is a parallel loop\n  on loc do\n    writeln(\"this locale is named \", here.name[0..4]);\nThis starts four tasks in parallel, and the order in which the print statement is executed depends on the runtime conditions and can change from run to run:\nthis locale is named node1\nthis locale is named node3\nthis locale is named node2\nWe can print few other attributes of each locale. Here it is actually useful to revert to the serial loop for so that the print statements appear in order:\nuse MemDiagnostics;\nfor loc in Locales do\n  on loc {\n    writeln(\"locale #\", here.id, \"...\");\n    writeln(\"  ...is named: \", here.name);\n    writeln(\"  ...has \", here.numPUs(), \" processor cores\");\n    writeln(\"  ...has \", here.physicalMemory(unit=MemUnits.GB, retType=real), \" GB of memory\");\n    writeln(\"  ...has \", here.maxTaskPar, \" maximum parallelism to expect\");\n  }\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\nlocale #0...\n  ...is named: node1.int.school.vastcloud.org\n  ...has 2 processor cores\n  ...has 29.124 GB of memory\n  ...has 2 maximum parallelism to expect\nlocale #1...\n  ...is named: node2.int.school.vastcloud.org\n  ...has 2 processor cores\n  ...has 29.124 GB of memory\n  ...has 2 maximum parallelism to expect\nlocale #2...\n  ...is named: node3.int.school.vastcloud.org\n  ...has 2 processor cores\n  ...has 29.124 GB of memory\n  ...has 2 maximum parallelism to expect\nNote that while Chapel correctly determines the number of cores available inside our job on each node (maximum parallelism) but lists the total physical memory on each node available to all running jobs which is not the same as the total memory per node allocated to our job.",
    "crumbs": [
      "PART 2: DATA PARALLELISM",
      "Multi-locale Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-06-command-line-arguments.html",
    "href": "chapel2/chapel-06-command-line-arguments.html",
    "title": "Using command-line arguments",
    "section": "",
    "text": "If we want to resize our image, we would need to edit the line const n = 2_000; in the code and then recompile it. Wouldn’t it be great if we could pass this number to the binary when it is called at the command line, without having to recompile it?\nThe Chapel mechanism for this is config variables. When a variable is declared with the config keyword, in addition to var or const, like this:\nconfig const n = 2_000;   // vertical and horizontal size of our image\nyou can its new value from the command line:\n\n$ chpl juliaSetSerial.chpl   # using the default value 2_000\n$ ./juliaSetSerial --n=500   # passing another value from the command line\n\n\n\n\n\n\nCautionQuestion Basic.4\n\n\n\n\n\nMake c a config variable, and test passing different values.",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Using command-line arguments"
    ]
  },
  {
    "objectID": "chapel2/chapel-02-variables.html",
    "href": "chapel2/chapel-02-variables.html",
    "title": "Basic syntax and variables",
    "section": "",
    "text": "The basic concept of parallel computing is simple to understand: we divide our job into tasks that can be executed at the same time, so that we finish the job in a fraction of the time that it would have taken if the tasks are executed one by one.\n\n\n\n\n\n\nNote\n\n\n\nTask is a unit of computation that can run in parallel with other tasks. In this course, we’ll be using a more general term “task” that – depending on the context – could mean either a Unix process (MPI task or rank) or a Unix thread. Consequently, parallel execution in Chapel could mean either multiprocessing and multithreading, or both (hybrid parallelism). In Chapel in many cases this distinction is hidden from the programmer.\n\n\nImplementing parallel computations is not always easy. How easy it is to parallelize a code really depends on the underlying problem you are trying to solve. This can result in:\n\na fine-grained, or tightly coupled parallel code that needs a lot of communication / synchronization between tasks, or\na coarse-grained code that requires little communication between tasks.\n\nIn this sense grain size refers to the amount of independent computing in between communication events. An extreme case of a coarse-grained problem would be an embarrassing parallel problem where all tasks can be executed completely independent from each other (no communications required).\nIn the non-GPU part of this course we’ll be solving two numerical problems:\n\nJulia set is an embarrassingly parallel problem (no communication between tasks), and\nheat diffusion is a tightly coupled problem that requires communication between tasks at each step of the iteration.\n\nWe’ll start with a serial of the Julia set and will use it to learn the basics of Chapel.",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "chapel2/chapel-02-variables.html#types-of-parallel-problems",
    "href": "chapel2/chapel-02-variables.html#types-of-parallel-problems",
    "title": "Basic syntax and variables",
    "section": "",
    "text": "The basic concept of parallel computing is simple to understand: we divide our job into tasks that can be executed at the same time, so that we finish the job in a fraction of the time that it would have taken if the tasks are executed one by one.\n\n\n\n\n\n\nNote\n\n\n\nTask is a unit of computation that can run in parallel with other tasks. In this course, we’ll be using a more general term “task” that – depending on the context – could mean either a Unix process (MPI task or rank) or a Unix thread. Consequently, parallel execution in Chapel could mean either multiprocessing and multithreading, or both (hybrid parallelism). In Chapel in many cases this distinction is hidden from the programmer.\n\n\nImplementing parallel computations is not always easy. How easy it is to parallelize a code really depends on the underlying problem you are trying to solve. This can result in:\n\na fine-grained, or tightly coupled parallel code that needs a lot of communication / synchronization between tasks, or\na coarse-grained code that requires little communication between tasks.\n\nIn this sense grain size refers to the amount of independent computing in between communication events. An extreme case of a coarse-grained problem would be an embarrassing parallel problem where all tasks can be executed completely independent from each other (no communications required).\nIn the non-GPU part of this course we’ll be solving two numerical problems:\n\nJulia set is an embarrassingly parallel problem (no communication between tasks), and\nheat diffusion is a tightly coupled problem that requires communication between tasks at each step of the iteration.\n\nWe’ll start with a serial of the Julia set and will use it to learn the basics of Chapel.",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "chapel2/chapel-02-variables.html#case-study-1-computing-the-julia-set",
    "href": "chapel2/chapel-02-variables.html#case-study-1-computing-the-julia-set",
    "title": "Basic syntax and variables",
    "section": "Case study 1: computing the Julia set",
    "text": "Case study 1: computing the Julia set\nThis project is a mathematical problem to compute a Julia set, defined as a set of points on the complex plane that remain bound under infinite recursive transformation \\(f(z)\\). We will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\n\npick a point \\(z_0\\in\\mathbb{C}\\)\ncompute iterations \\(z_{i+1}=z_i^2+c\\) until \\(|z_i|&gt;4\\) (arbitrary fixed radius; here \\(c\\) is a complex constant)\nstore the iteration number \\(\\xi(z_0)\\) at which \\(z_i\\) reaches the circle \\(|z|=4\\)\nlimit max iterations at 255\n4.1 if \\(\\xi(z_0)=255\\), then \\(z_0\\) is a stable point\n4.2 the quicker a point diverges, the lower its \\(\\xi(z_0)\\) is\nplot \\(\\xi(z_0)\\) for all \\(z_0\\) in a rectangular region \\(-1&lt;=\\mathfrak{Re}(z_0)&lt;=1\\), \\(-1&lt;=\\mathfrak{Im}(z_0)&lt;=1\\)\n\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\n\n\n\n\n\n\n\nNote\n\n\n\nYou might want to try these values too:\n\\(c = 1.2e^{1.1πi}\\) \\(~\\Rightarrow~\\) original textbook example\n\\(c = -0.4-0.59i\\) and 1.5X zoom-out \\(~\\Rightarrow~\\) denser spirals\n\\(c = 1.34-0.45i\\) and 1.8X zoom-out \\(~\\Rightarrow~\\) beans\n\\(c = 0.34-0.05i\\) and 1.2X zoom-out \\(~\\Rightarrow~\\) connected spiral boots",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "chapel2/chapel-02-variables.html#variables",
    "href": "chapel2/chapel-02-variables.html#variables",
    "title": "Basic syntax and variables",
    "section": "Variables",
    "text": "Variables\nChapel is a statically typed language, i.e. the type of every variable must be known at compile time.\n\n\n\n\nVariables in Chapel are declared with the var or const keywords. When a variable declared as const is initialized, its value cannot be modified anymore during the execution of the program.\nTo declare a variable, we must either (1) specify its type or (2) initialize it in place with some value from which the compiler will infer its type. The common variable types in Chapel are:\n\ninteger int – defaults to int(64), or you can explicitly specify int(32),\nfloating point number real – defaults to real(64), or you can explicitly specify real(32),\nboolean bool,\nstring string\n\nIf a variable is declared without a type, Chapel will infer it from the given initial value, for example (let’s store this in juliaSetSerial.chpl):\n\nconst n = 2_000;   // vertical and horizontal size of our image\nAll these constant variables will be created as integers, and no other values can be assigned to these variables during the execution of the program.\nOn the other hand, if a variable is declared without an initial value, Chapel will initialize it with a default value depending on the declared type:\nvar y: real;        // vertical coordinate in our plot; real(64) variable set to 0.0\nvar point: complex; // current point in our image; complex(64) variable set to 0.0+0.0*1i\nOf course, we can use both, the initial value and the type, when declaring a varible as follows:\nconst c: complex = 0.355 + 0.355i;   // Julia set constant\n\n\n\n\n\n\nNote\n\n\n\nThe following two notations are different, but produce the same result in the end:\nvar a: real = 10;   // we specify both the type and the value\nvar a = 10: real;   // we specify only the value (integer 10 converted to real)\n\n\nLet’s print our configuration after we set all parameters:\nwriteln('Computing ', n, 'x', n, ' Julia set ...');\nAlternatively, you can use formatted output:\nwritef(\"Computing %ix%i Julia set\\n\", n, n);\nFor other format specifiers, check the table at https://chapel-lang.org/docs/modules/standard/IO/FormattedIO.html\n\nPrinting variable’s type\nTo check a variable’s type, use .type query:\nvar x = 1e8:int;\ntype t = x.type;\nwriteln(t:string);\nor in a single line:\nwriteln((1e8:int).type:string);\nwriteln((0.355 + 0.355i).type: string);",
    "crumbs": [
      "PART 1: BASIC LANGUAGE FEATURES",
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "pytables-20230427.html",
    "href": "pytables-20230427.html",
    "title": "Managing large hierarchical datasets with PyTables",
    "section": "",
    "text": "May 23rd, 2023\nYou can find this page at   https://folio.vastcloud.org/pytables\nAbstract: PyTables is a free and open-source Python library for managing large hierarchical datasets. It is built on top of numpy and the HDF5 scientific dataset library, and it focuses both on performance and interactive analysis of very large datasets. For large data streams (think multi-dimensional arrays or billions of records) it outperforms databases in terms of speed, memory usage and I/O bandwidth, although it is not a replacement to traditional relational databases as PyTables does not support broad relationships between dataset variables. PyTables can be even used to organize a workflow with many (thousands to millions) of small files, as you can create a PyTables database of nodes that can be used like regular opened files in Python. This lets you store a large number of arbitrary files in a PyTables database with on-the-fly compression, making it very efficient for handling huge amounts of data."
  },
  {
    "objectID": "pytables-20230427.html#installation",
    "href": "pytables-20230427.html#installation",
    "title": "Managing large hierarchical datasets with PyTables",
    "section": "Installation",
    "text": "Installation\n\nPyTables vs. h5py (official HDF5 for Python): comparison 1 and comparison 2\n\nTo run PyTables in Python, you will need:\n\na recent version of Python\nthe HDF5 (C flavour) library from http://www.hdfgroup.org\nthe NumPy package\n\nDepending on your operating system, your processor architecture, and your preferred way to install Python and its packages, potentially there is a large number of installation paths. Here is how I proceeded on my Macbook, with Homebrew installed:\nbrew install python   # installs into /opt/homebrew/bin/python3\nexport PATH=\"$(brew --prefix)/opt/python@3/libexec/bin:$PATH\"   # add this to your ~/.bashrc\n                                                                # so that it finds pip, python\nbrew install hdf5         # C-flavoured HDF5 library\nbrew install virtualenv\npip install --upgrade pip\npip install pip_search\npip_search tables   # first entry, version 3.8.0, \"Hierarchical datasets for Python\"\n\nvirtualenv ~/pytables-env\nsource ~/pytables-env/bin/activate\nenv HDF5_DIR=/opt/homebrew/opt/hdf5 pip install tables   # install PyTables\npip install netCDF4       # if you want to work with NetCDF data\npip install Pillow        # if you want to work with images\npip install wonderwords   # if you want to name stars\n...\ndeactivate\nOn an Alliance cluster you don’t have to install anything (unless you are planning to use Pillow, wonderwords which you can install in a virtual environment) – simply load the modules:\nmodule load netcdf    # only if you want to play with the NetCDF example below\nmodule load hdf5/1.12.1 python/3.10.2 scipy-stack\npython\n&gt;&gt;&gt; import tables"
  },
  {
    "objectID": "pytables-20230427.html#classical-intro-groups-tables-arrays",
    "href": "pytables-20230427.html#classical-intro-groups-tables-arrays",
    "title": "Managing large hierarchical datasets with PyTables",
    "section": "Classical intro: groups, tables, arrays",
    "text": "Classical intro: groups, tables, arrays\nGoal: easily manipulate data tables and array objects in a hierarchical structure (HDF5 file).\nConceptually, storage inside an HDF5 file is similar to a Unix filesystem: - any hierarchy of groups and nodes, all stored under root / - groups playing the role of directories - nodes playing the role of files; could be tables, arrays, file nodes, etc.\nA single HDF5 file may contain all these objects and be portable across different computer architectures and HDF5 library implementations, in addition supporting compression, parallel I/O and other nice features.\ncd ~/tmp/pytables\n/bin/rm -f *.h5 compressed.xmf\nsource ~/pytables-env/bin/activate\npython\nImagine we are compiling a catalogue of stars in which we record each star’s properties:\nimport tables as tt\nimport numpy as np\n\nhelp(tt.IsDescription)          # class IsDescription inside PyTables\n\nclass Star(tt.IsDescription):   # create a child class \"Star\" (class inheritance)\n    name     = tt.StringCol(16) # each name contains 16 8-bit numbers, could be used to store ASCII characters\n    ra       = tt.Float64Col()  # Right Ascension as double-precision float\n    dec      = tt.Float64Col()  # Declination as double-precision float\n    white    = tt.Float32Col()  # white magnitude as single-precision float\n    red      = tt.Float32Col()  # red magnitude as single-precision float\n    blue     = tt.Float32Col()  # blue magnitude as single-precision float\n    parallax = tt.Float32Col()  # parallax as single-precision float\n    proper   = tt.Float32Col()  # proper motion as single-precision float\n\nh5file = tt.open_file(\"catalogue.h5\", mode=\"w\", title=\"Stellar catalogue\")\ng1 = h5file.create_group(\"/\", 'data', 'Observational data')    # new group under root node; title and description\ntable = h5file.create_table(g1, 'stars', Star, \"All stars\") # table stored as a node named `stars`\n                                                               # while the `Star` class defines its columns\nprint(h5file)   # show the object tree\nh5file          # show more info, including the table columns\n\nimport random\nfrom math import asin, degrees\nfrom wonderwords import RandomWord\nr = RandomWord()\nstar = table.row   # get a pointer to this table's row instance\nfor i in range(100):\n    # star['name'] = 'star' + str(random.randint(0,999))\n    star['name'] = r.word(include_parts_of_speech=[\"nouns\"], word_min_length=4, word_max_length=8)\n    star['ra'] = 360 * random.random()\n    star['dec'] = degrees(asin(2*random.random()-1))\n    star['white'] = random.gauss(mu=0, sigma=1)\n    star['red'] = random.gauss(mu=0, sigma=1)\n    star['blue'] = random.gauss(mu=0, sigma=1)\n    star['parallax'] = 1.e-4 * random.random()\n    star['proper'] = random.random()\n    star.append()   # insert this new star into the catalogue\n\ntable.shape     # nothing's been written yet\ntable.flush()   # flush the table's I/O buffer: write to disk, free memory resources\ntable.shape     # now we have data in the table\nh5file.close()\nimport tables as tt\nimport numpy as np\nh5file = tt.open_file(\"catalogue.h5\", mode=\"a\")   # append mode\n\ntable = h5file.root.data.stars\ntable.cols               # 8 columns from Star class\ntable.shape              # 10 rows\ntable.col('name')        # data in the \"name\" column\ntable.col('parallax')    # data in the \"parallax\" column\ntable.cols.parallax[:]   # same\nfor row in table.iterrows():\n    print(row[:])   # print each row\n\nmin(table.col('parallax')), max(table.col('parallax'))\nnames = [row['name'] for row in table.iterrows() if 3e-5 &lt;= row['parallax'] &lt; 5e-5]\nnames    # star names stored as 16-element binary objects (not strings)\nb'A' == b'\\x41'   # true; each 8-bit number (0-255) can store a character\n\ncondition = \"dec &gt; 45\"\nfor row in table.where(condition):\n    print(row['name'], row['dec'])\n\ncondition = \"(dec &gt; 45) & (blue &gt; 0.5)\"\nfor row in table.where(condition):\n    print(row['name'], row['dec'], row['blue'])\n\nnames = [row['name'] for row in table.where(\"(dec &gt; 45) & (blue &gt; 0.5)\")]\nnames\n\nsel = h5file.create_group(h5file.root, \"selection\", \"Northern blue stars\")         # add a group: name and description\nh5file.create_array(sel, 'name', np.array(names), \"Northern blue stars selection\") # add an array of names\n\nh5file.close()\nh5dump catalogue.h5     # full content of the file as text\nh5ls -rd catalogue.h5   # shorter format\nimport tables as tt\nimport numpy as np\nh5file = tt.open_file(\"catalogue.h5\", mode=\"a\")   # append mode\n\nfor node in h5file: # root, 2 groups /data and /selection, 1 table /data/stars, 1 array /selection/name\n    print(node)\n\nfor group in h5file.walk_groups():   # root and 2 groups\n    print(group)\n\nfor group in h5file.walk_groups(\"/data\"):   # 1 group under /data\n    print(group)\n\nfor node in h5file.list_nodes(\"/data\", classname='Table'):   # 1 table under /data\n    print(node)\n    \nh5file.list_nodes(\"/data\", classname='Table')   # more info on this table\n\ntable = h5file.root.data.stars\ntable.shape\n\ntable.attrs.date = \"Thu, 2023-May-27 14:33\"\ntable.attrs.temperature = 18.4\ntable.attrs.temp_unit = \"Celsius\"\ntable.attrs                   # list all attributes\ndel table.attrs.temperature   # delete an attribute\ndel table.attrs.temp_unit\n\nimport random\nfrom math import asin, degrees\nfrom wonderwords import RandomWord\nr = RandomWord()\ntable = h5file.root.data.stars\nstar = table.row\nfor i in range(50):   # add 50 more stars to the existing table\n    star['name'] = r.word(include_parts_of_speech=[\"nouns\"], word_min_length=4, word_max_length=8)\n    star['ra'] = 360 * random.random()\n    star['dec'] = degrees(asin(2*random.random()-1))\n    star['white'] = random.gauss(mu=0, sigma=1)\n    star['red'] = random.gauss(mu=0, sigma=1)\n    star['blue'] = random.gauss(mu=0, sigma=1)\n    star['parallax'] = 1.e-4 * random.random()\n    star['proper'] = random.random()\n    star.append()   # insert this new star into the catalogue\n\ntable.shape   # 100 original stars\ntable.flush()\ntable.shape   # now 150 stars\n\nbodies = table.cols.name\nbodies[:10]   # the first 10 star names (out of 150)\n\nbodies[:2] = [b'one', b'two']   # overwrite the first two values\nbodies[:]\n\ntable.flush()   # flush the table's I/O buffer: write to disk, free memory resources\nh5file.close()\nYou can find more examples in this tutorial."
  },
  {
    "objectID": "pytables-20230427.html#working-with-large-multidimensional-arrays",
    "href": "pytables-20230427.html#working-with-large-multidimensional-arrays",
    "title": "Managing large hierarchical datasets with PyTables",
    "section": "Working with large multidimensional arrays",
    "text": "Working with large multidimensional arrays\nWe could start with a randomly-generated numpy array but it is much more interesting to look at some real data. Let’s read a cool dataset in NetCDF (we’ll visualize it towards the end of this section) and then learn how to write/read it in HDF5 format.\nOur workflow in this section: 1. read the NetCDF data into a Python numpy-like array 1. save it into an uncompressed HDF5 file 1. save it into a compressed HDF5 file 1. append a 2nd array to the same file 1. read this file in Python 1. remove the 2nd array from the file 1. create a soft link inside the file 1. create an XMF descriptor file 1. load this XMF file into ParaView to visualize our large array\nLet’s download our large-array NetCDF file:\nwget https://bit.ly/paraviewzipp -O paraviewWorkshop.zip\nunzip paraviewWorkshop.zip data/mandelbulb300.nc && /bin/rm paraviewWorkshop.zip\nls -l data/mandelbulb300.nc   # 1MB file\nActivate your PyTables environment:\ncd ~/tmp/pytables\nsource ~/pytables-env/bin/activate\npython\nimport tables as tt\nimport numpy as np\n\nimport netCDF4 as nc\nds = nc.Dataset(\"data/mandelbulb300.nc\")\nprint(ds)\nprint(ds[\"stability\"].shape)\n\nh5file = tt.open_file(\"uncompressed.h5\", mode=\"w\", title=\"Mandelbulb at 300^3 resolution\")\ng1 = h5file.create_group(\"/\", 'g1', 'Fractal data')   # new group under root node; title and description\nh5file.create_array(g1, 'stability', ds[\"stability\"][:,:,:], \"stability variable\")\nh5file.close()\nThe resulting file is much bigger (103M = 300^3 in single precision) than the original (1.0M). Let’s enable compression:\ncompress = tt.Filters(complib='zlib', complevel=5)   # create the compression filter\nh5file = tt.open_file(\"compressed.h5\", mode=\"w\", title=\"Mandelbulb at 300^3 resolution\", filters=compress)\ng1 = h5file.create_group(\"/\", 'g1', 'Fractal data')   # new group under root node; title and description\nh5file.create_carray('/g1', 'stability', obj=ds[\"stability\"][:,:,:])   # use chunked arrays, write into g1\nh5file.close()\nThe compressed file is 1.4M.\nLet’s append a second array to the same group:\nimport tables as tt\nimport numpy as np\ncompress = tt.Filters(complib='zlib', complevel=5)   # create the compression filter\nh5file = tt.open_file(\"compressed.h5\", mode=\"a\", filters=compress)   # append mode\na = np.random.randn(100,100)   # 100^2 array from a normal (Gaussian with x0=0, sigma=1) distribution\nh5file.create_carray('/g1', 'a', obj=a)   # use chunked arrays, write into g1\nh5file.close()\nLet’s read this file:\nimport tables as tt\nf1 = tt.open_file(\"compressed.h5\", mode=\"r\")   # read mode\nfor node in f1:   # root, 1 group g1, 2 arrays inside g1, both compressed\n    print(node)\n\nfor array in f1.walk_nodes(\"/\", \"Array\"):\n    print('---')\n    print(array)\n    print(array.shape)\n    # print(array[:])\n\nf1.root.g1.a[:]                  # entire array\n\nf1.root.g1.stability[:]          # entire array\nf1.root.g1.stability[:3,:3,:3]   # 3x3x3 starting corner block\nf1.root.g1.stability.shape\nf1.root.g1.stability.filters\nf1.root.g1.stability.title\nf1.root.g1.stability.dtype\nf1.root.g1.stability.size_in_memory   # uncompressed in memory\nf1.close()\nLet’s remove the array a from the file:\nimport tables as tt\nf1 = tt.open_file(\"compressed.h5\", mode=\"a\")   # append mode\nf1.root.g1.a.remove()\nf1.close()\nLet’s create a soft link inside the file:\nimport tables as tt\nf1 = tt.open_file(\"compressed.h5\", mode=\"a\")   # append mode\nf1.create_soft_link(\"/\", \"stab\", \"/g1/stability\")   # link in root called \"stab\" pointing to \"/g1/stability\"\nf1.root.stab.shape\nf1.root.stab[:]\nf1.close()\nwhich ptdump           # provided by PyTables\nptdump compressed.h5   # very useful command: dumps all objects from the file\nLet’s view this array in ParaView 5.11. Create a file compressed.xmf with the following:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;!DOCTYPE Xdmf SYSTEM \"Xdmf.dtd\" []&gt;\n&lt;Xdmf Version=\"2.0\"&gt;\n &lt;Domain&gt;\n   &lt;Grid Name=\"mesh\" GridType=\"Uniform\"&gt;\n     &lt;Topology TopologyType=\"3DCoRectMesh\" NumberOfElements=\"300 300 300\"/&gt;\n     &lt;Geometry GeometryType=\"Origin_DxDyDz\"&gt;\n       &lt;DataItem Dimensions=\"3\" NumberType=\"Float\" Precision=\"4\" Format=\"XML\"&gt;\n          1 2 3\n       &lt;/DataItem&gt;\n       &lt;DataItem Dimensions=\"3\" NumberType=\"Float\" Precision=\"4\" Format=\"XML\"&gt;\n        0.1 0.1 0.1\n       &lt;/DataItem&gt;\n     &lt;/Geometry&gt;\n     &lt;Attribute Name=\"stability\" AttributeType=\"Scalar\" Center=\"Node\"&gt;\n       &lt;DataItem Dimensions=\"300 300 300\" NumberType=\"Int\" Precision=\"4\" Format=\"HDF\"&gt;\n        compressed.h5:/g1/stability\n       &lt;/DataItem&gt;\n     &lt;/Attribute&gt;\n   &lt;/Grid&gt;\n &lt;/Domain&gt;\n&lt;/Xdmf&gt;\nOpen this file in ParaView using the XDMF Reader and visualize it as usual."
  },
  {
    "objectID": "pytables-20230427.html#simulating-a-filesystem-with-pytables",
    "href": "pytables-20230427.html#simulating-a-filesystem-with-pytables",
    "title": "Managing large hierarchical datasets with PyTables",
    "section": "Simulating a filesystem with PyTables",
    "text": "Simulating a filesystem with PyTables\n\nThe module filenode creates a PyTables database of nodes which can be used like regular opened files in Python.\nDetails at https://www.pytables.org/usersguide/filenode.html\n\nYou can use file nodes and PyTables groups to mimic a filesystem with files and directories, all inside a single HDF5 file. This lets you use HDF5 as a portable compressed file container/archive, across different computer platforms and architectures.\nimport tables as tt\nfrom tables.nodes import filenode\n\nf1 = tt.open_file('storage.h5', 'w')                         # create a new HDF5 file\nfnode = filenode.new_node(f1, where='/', name='text_file')   # create a new node file inside this file\nprint(f1.get_node_attr('/text_file', 'NODE_TYPE'))           # it looks like a file inside our HDF5 file\n\nfnode.write(\"This is a sample line.\\n\".encode('utf8'))\nfnode.write(\"Write a second line.\\n\".encode('ascii'))\nfnode.write(b\"Write a third line.\\n\")\n\nfnode.seek(0)   # rewind to the beginning of the file\nfor line in fnode:\n    print(line)\n\nfnode.close()\nprint(fnode.closed)   # check that it's closed\nf1.close()\nls -l storage.h5    # 68K\nptdump storage.h5   # root /, file /text_file\nLet’s open it again to read data:\nimport tables as tt\nfrom tables.nodes import filenode\nf1 = tt.open_file('storage.h5', 'r')\nfnode = filenode.open_node(f1.root.text_file, 'r')\nfor line in fnode:\n    print(line)\n\nfnode.close()\nf1.close()\nLet’s add some data:\nimport tables as tt\nfrom tables.nodes import filenode\nf1 = tt.open_file('storage.h5', 'a')                  # append mode\nfnode = filenode.open_node(f1.root.text_file, 'a+')   # read and append mode\nfnode.write(b\"Write a fourth line.\\n\")\nfnode.seek(0)   # rewind to the beginning of the file\nfor line in fnode:\n    print(line)\n\nfnode.close()\nf1.close()\nls -l tuscany.avif               # 2874x2154 file\nwidth=2874\nheight=2154\nfor num in $(seq -w 00 19); do   # crop it into twenty 300x300 random images\n    echo $num\n    x=$(echo \"scale=8; $RANDOM / 32767 * ($width-300)\" | bc)   # $RANDOM goes from 0 to 32767\n    x=$(echo $x | awk '{print int($1+0.5)}')\n    y=$(echo \"scale=8; $RANDOM / 32767 * ($height-300)\" | bc)\n    y=$(echo $y | awk '{print int($1+0.5)}')\n    convert tuscany.avif -crop 300x300+$x+$y small$num.png\ndone\nNormally, in Pillow you read, display, write individual images with:\nfrom PIL import Image\nimage = Image.open('small00.png')\nimage.show()\n# image.save('copy00.png')\nLet’s try to copy all our images into our HDF5 file storage.h5:\nimport tables as tt\nfrom tables.nodes import filenode\nfrom PIL import Image\nfrom glob import glob\n\nf1 = tt.open_file('storage.h5', 'a')\ntuscany = f1.create_group(\"/\", 'tuscany')\n\nfor input in glob(\"small*.png\"):\n    print('copying', input)\n    image = Image.open(input)\n    fnode = filenode.new_node(f1, where='/tuscany', name=input.replace('.png', ''))\n    image.save(fp=fnode, format='png')\n    fnode.close()\n\nf1.close()\n/bin/rm -f small*png\nls -l storage.h5    # 2.2M - all our images are now stored inside this HDF5 file, along with the earlier text_file\nptdump storage.h5   # very useful command: dumps all objects from the file\nLet’s read and display these images:\nimport tables as tt\nfrom tables.nodes import filenode\nfrom PIL import Image\n\nf1 = tt.open_file('storage.h5', 'r')\n\nimage = Image.open(fp=f1.root.tuscany.small08)\nimage.show()\n\nfor i in f1.root.tuscany:   # cycle through all its children\n    print(i)\n \nf1.root.tuscany.small00      # one possible syntax\nf1.root.tuscany['small00']   # another syntax\n\nimage = Image.open(fp=f1.root.tuscany['small08'])\nimage.show()\nFinally, let’s see how we can rename and remove nodes:\nimport tables as tt\nfrom tables.nodes import filenode\nfrom PIL import Image\n\nf1 = tt.open_file('storage.h5', 'a')\nf1.root.tuscany.small19.remove()               # remove the node\nf1.root.tuscany.small18.rename(\"last_image\")   # rename the node\n\nf1.root.tuscany             # all in one array\n\nfor i in f1.root.tuscany:   # cycle through all its children\n    print(i)\nCurrent limitations:\n\nnot a generic filesystem, but a filesystem accessible only though Python I/O: writing to a file is replaced by writing to an HDF5 node\nnode files are restricted in their naming: only valid Python identifiers are valid  ⇨  use metadata to provide more description\nonly binary I/O is supported\nno universal newline support yet, besides \\n"
  },
  {
    "objectID": "bash1/bash-01-intro.html",
    "href": "bash1/bash-01-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "We use HPC systems (clusters) to do computing beyond the scale of a desktop. For example, we can:\nThe use of HPC in modeling complex physical phenomena such as astrophysical processes, weather, fluid dynamics, molecular interactions, and engineering design is well known to researchers in those fields. More recently, HPC is being used by researchers in other fields from genomics and medical imaging to social sciences and humanities.",
    "crumbs": [
      "PART 1",
      "Introduction"
    ]
  },
  {
    "objectID": "bash1/bash-01-intro.html#why-the-shell",
    "href": "bash1/bash-01-intro.html#why-the-shell",
    "title": "Introduction",
    "section": "Why the shell?",
    "text": "Why the shell?\n\nUsing HPC systems usually involves the use of a shell. Unlike a GUI, the shell is a text-based interface to the operating system that excels at two things:\n\nlaunching other tools and scripts, and\nconnecting standard input/output of these tools through pipes to form more complex commands.\n\nThe shell design follows the classic UNIX philosophy of breaking complex projects into simpler subtasks and chaining together components and utilities. The name “shell” comes from the coconut anallogy, as it is a shell around the operating system kernel, surrounded itself by utilities and applications. Shell commands are often very cryptic, and this is by design to avoid too much typing.\nWe use the Unix shell because it is very powerful, great for automation and for creating reproducible workflows, and is necessary to work on larger Unix systems. Bash is one of many Unix shell implementations out there. It is a default on the Alliance systems, but you can easily switch to a different shell such as tcsh, zsh, etc. The main difference between these is a slight change in the command syntax.\nFor the hands-on work, we have set up a small training cluster sfuschool.c3.ca that features the same software setup as our real production clusters. In our “Introduction to HPC” course we teach the specifics of working on a cluster: its software environment, scheduler, compilers, parallel programming models, and so on. In this course we start with the basics and learn how to work with a remote Linux machine and its filesystem, the basic Linux commands, how to transfer files to/from/between remote systems, how to automate things, and similar introductory topics.",
    "crumbs": [
      "PART 1",
      "Introduction"
    ]
  },
  {
    "objectID": "bash1/bash-01-intro.html#logging-in-to-a-remote-system",
    "href": "bash1/bash-01-intro.html#logging-in-to-a-remote-system",
    "title": "Introduction",
    "section": "Logging in to a remote system",
    "text": "Logging in to a remote system\nYou can connect to a remote HPC system via SSH (secure shell) using a terminal on your laptop. Linux and Mac laptops have built-in terminals, whereas on Windows we suggest using a free version of MobaXterm that comes with its own terminal emulator and a simple interface for remote SSH sessions.\nLet’s log in to sfuschool.c3.ca using a username userXX (where XX is two digits):\n[local]$ ssh userXX@sfuschool.c3.ca   # password supplied by the instructor\n\nthose on Windows please use MobaXterm: click on Session | SSH, and then fill in the Remote host name and your username.\n\nPlease enter the password when prompted. Note that no characters will be shown when typing the password. If you make a mistake, you will have to start your connection from scratch.\nOnce connected, compare the prompt on the local and remote systems – do you notice any difference?\n\nSSH connection problems\nIf you are trying to ssh into the training cluster and you get one of these errors\n\n“Permission denied, please try again”\n“Network error: Connection timed out”\n“Connection refused”\n\nand you are 100% certain that you type the password correctly, we might need to check if your IP address is blocked after too many attempts to log in. Please go to https://whatismyipaddress.com and tell us your IPv4 address so that we could whitelist it.",
    "crumbs": [
      "PART 1",
      "Introduction"
    ]
  },
  {
    "objectID": "bash1/bash-11-other.html",
    "href": "bash1/bash-11-other.html",
    "title": "Other topics and exercises",
    "section": "",
    "text": "This page is a placeholder to cover tools that we do not normally teach in our Linux command line introduction but we would like to mention as we find them useful. Also, over the years we taught a number of webinars on advanced bash that you can watch at any time.",
    "crumbs": [
      "PART 2",
      "Other topics and exercises"
    ]
  },
  {
    "objectID": "bash1/bash-11-other.html#fuzzy-finder",
    "href": "bash1/bash-11-other.html#fuzzy-finder",
    "title": "Other topics and exercises",
    "section": "Fuzzy finder",
    "text": "Fuzzy finder\nFuzzy finder fzf is a third-party tool, not installed by default. With basic usage, it does interactive processing of standard input. At a more advanced level (not covered in the video below), it provides key bindings and fuzzy completion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.\n\n\n\n\n\n\n\n\nCautionQuestion 11.1\n\n\n\n\n\nLet’s study together these commands:\n$ source ~/projects/def-sponsor00/shared/fzf/.fzf.bash\n$ kill -9 `/bin/ps aux | fzf | awk '{print $2}'`",
    "crumbs": [
      "PART 2",
      "Other topics and exercises"
    ]
  },
  {
    "objectID": "bash1/bash-11-other.html#additional-topics",
    "href": "bash1/bash-11-other.html#additional-topics",
    "title": "Other topics and exercises",
    "section": "Additional topics",
    "text": "Additional topics\nIf there is interest, we could explore some other topics in the future:\n\nBash arithmetics\nPermissions\nHow to control processes\nGNU Parallel",
    "crumbs": [
      "PART 2",
      "Other topics and exercises"
    ]
  },
  {
    "objectID": "bash1/bash-11-other.html#bash-webinars",
    "href": "bash1/bash-11-other.html#bash-webinars",
    "title": "Other topics and exercises",
    "section": "Bash webinars",
    "text": "Bash webinars\nOur 2022 webinar Lesser known but very useful Bash features covers running commands in a subshell, subsetting string variables, Bash arrays, modifying separators with IFS, running Python code from inside self-contained Bash functions, editing your command history, running unaliased versions of commands, handy use of bracket expansion, and a few other topics.",
    "crumbs": [
      "PART 2",
      "Other topics and exercises"
    ]
  },
  {
    "objectID": "bash1/bash-11-other.html#more-exercises",
    "href": "bash1/bash-11-other.html#more-exercises",
    "title": "Other topics and exercises",
    "section": "More exercises",
    "text": "More exercises\n\nFind all files in a directory hierarchy that have a matching substring inside, and run ls -l on these files.\nSame, but replace this string with another substring in all these matching files.\nList all files in a directory (passed as an argument), printing only their name, size, and the MD5 checksum.\nList ten largest files in a directory (passed as an argument).",
    "crumbs": [
      "PART 2",
      "Other topics and exercises"
    ]
  },
  {
    "objectID": "bash1/bash-05-file-transfer.html",
    "href": "bash1/bash-05-file-transfer.html",
    "title": "File transfer",
    "section": "",
    "text": "To copy a single file to/from the cluster, we can use scp:\n[local]$ scp /path/to/local/file.txt userXX@sfuschool.c3.ca:/path/on/remote/computer\n[local]$ scp local-file.txt userXX@sfuschool.c3.ca:   # will put into your remote home\n[local]$ scp userXX@sfuschool.c3.ca:/path/on/remote/computer/file.txt /path/to/local/\nTo recursively copy a directory, we just add the -r (recursive) flag:\n[local]$ scp -r some-local-folder/ userXX@sfuschool.c3.ca:target-directory/\nYou can also use wildcards to transfer multiple files:\n[local]$ scp userXX@sfuschool.c3.ca:start*.sh .\nWith MobaXterm in Windows, you can actually copy files by dragging them between your desktop and the left pane when you are logged into the cluster (no need to type any commands), or you can click the download/upload buttons.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Transferring files to/from remote computers"
    ]
  },
  {
    "objectID": "bash1/bash-05-file-transfer.html#transferring-files-and-folders-with-scp",
    "href": "bash1/bash-05-file-transfer.html#transferring-files-and-folders-with-scp",
    "title": "File transfer",
    "section": "",
    "text": "To copy a single file to/from the cluster, we can use scp:\n[local]$ scp /path/to/local/file.txt userXX@sfuschool.c3.ca:/path/on/remote/computer\n[local]$ scp local-file.txt userXX@sfuschool.c3.ca:   # will put into your remote home\n[local]$ scp userXX@sfuschool.c3.ca:/path/on/remote/computer/file.txt /path/to/local/\nTo recursively copy a directory, we just add the -r (recursive) flag:\n[local]$ scp -r some-local-folder/ userXX@sfuschool.c3.ca:target-directory/\nYou can also use wildcards to transfer multiple files:\n[local]$ scp userXX@sfuschool.c3.ca:start*.sh .\nWith MobaXterm in Windows, you can actually copy files by dragging them between your desktop and the left pane when you are logged into the cluster (no need to type any commands), or you can click the download/upload buttons.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 1",
      "Transferring files to/from remote computers"
    ]
  },
  {
    "objectID": "bash1/bash-05-file-transfer.html#transferring-files-interactively-with-sftp",
    "href": "bash1/bash-05-file-transfer.html#transferring-files-interactively-with-sftp",
    "title": "File transfer",
    "section": "Transferring files interactively with sftp",
    "text": "Transferring files interactively with sftp\nscp is useful, but what if we don’t know the exact location of what we want to transfer? Or perhaps we’re simply not sure which files we want to transfer yet. sftp is an interactive way of downloading and uploading files. Let’s connect to a cluster with sftp:\n[local]$ sftp userXX@sfuschool.c3.ca\nThis will start what appears to be a shell with the prompt sftp&gt;. However, we only have access to a limited number of commands. We can see which commands are available with help:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp grp path                     Change group of file 'path' to 'grp'\nchmod mode path                    Change permissions of file 'path' to 'mode'\nchown own path                     Change owner of file 'path' to 'own'\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afPpRr] remote [local]       Download file\nreget [-fPpRr] remote [local]      Resume download file\nreput [-fPpRr] [local] remote      Resume upload file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\n...\nNotice the presence of multiple commands that make mention of local and remote. We are actually browsing two filesystems at once, with two working directories!\nsftp&gt; pwd    # show our remote working directory\nsftp&gt; lpwd   # show our local working directory\nsftp&gt; ls     # show the contents of our remote directory\nsftp&gt; lls    # show the contents of our local directory\nsftp&gt; cd     # change the remote directory\nsftp&gt; lcd    # change the local directory\nsftp&gt; put localFile    # upload a file\nsftp&gt; get remoteFile   # download a file\nAnd we can recursively put/get files by just adding -r. Note that the directory needs to be present beforehand:\nsftp&gt; mkdir content\nsftp&gt; put -r content/\nTo quit, type exit or bye.\n\n\n\n\n\n\nCautionExercise\n\n\n\nUsing one of the above methods, try transferring files to and from the cluster. For example, you can download bfiles.tar.gz to your laptop. Which method do you like best?\n\n\n\n\n\n\n\n\nNoteNote on Windows line endings\n\n\n\n\nWhen you transfer files to from a Windows system to a Unix system (Mac, Linux, BSD, Solaris, etc.) this can cause problems. Windows encodes its files slightly different than Unix, and adds an extra character to every line. On a Unix system, every line in a file ends with a \\n (newline), whereas on Windows every line in a file ends with a \\r\\n (carriage return + newline). This causes problems sometimes.\nIn some bash implementations, you can identify if a file with Windows line endings with cat -A   filename. A file with Windows line endings will have ^M$ at the end of every line. A file with Unix line endings will have $ at the end of a line.\nThough most modern programming languages and software handle this correctly, in some rare instances you may run into an issue. The solution is to convert a file from Windows to Unix encoding with the command dos2unix filename. Conversely, to convert back to Windows format, you can run unix2dos filename.\n\n\n\n\n\n\n\n\n\nNoteNote on syncing\n\n\n\nThere also a command rsync for synching two directories. It is super useful, especially for work in progress. For example, you can use it the download all the latest PNG images from your working directory on the cluster.\n\n\n\n\n\n\n\n\nCautionQuestion scp and sftp\n\n\n\n\n\nCopy a file to/from the training cluster using either scp or sftp.\n\n\n\n\n\n\n\n\n\nCautionQuestion rsync\n\n\n\n\n\nBring up the manual page on rsync, then use it to synchronize a directory from the training cluster.\n\n\n\n\n\n\n\n\n\nCautionQuestion Multiple files\n\n\n\n\n\nWrite a command to scp multiple files with a single command so that scp asks you for password (and possible MFA) only once for all files, not once per file. Feel free to google this problem. If you have an Alliance account, try copying a set of files from the training cluster to Cedar (you will run into a problem):\nfor i in {1..5}; do\n    echo ${RANDOM}${RANDOM}${RANDOM} &gt; a$i.txt\ndone\nscp a*.txt username@cedar.alliancecan.ca:scratch\nHow many times does it ask for a password?\n\n\n\n\n\n\n\n\n\nCautionQuestion ssh config file\n\n\n\n\n\nLet’s discuss remote system profiles in ~/.ssh/config. Consider this:\nHost &lt;name&gt;\n    HostName sfuschool.c3.ca\n    User userXX\n\nNote that in Windows you can create .ssh/config in C:\\Users\\username. If referencing this folder from Windows Subsystem for Linux, make sure to run chmod 600 ~\\.ssh\\* after creating the config file.\nAnother useful setting is IdentityFile private_ssh_key coupled with IdentitiesOnly yes (i.e. use only the provided key).",
    "crumbs": [
      "PART 1",
      "Transferring files to/from remote computers"
    ]
  },
  {
    "objectID": "bash1/bash-08-scripts-functions.html",
    "href": "bash1/bash-08-scripts-functions.html",
    "title": "Scripts, functions, and variables",
    "section": "",
    "text": "We now know a lot of UNIX commands! Wouldn’t it be great if we could save certain commands so that we could run them later or not have to type them out again? As it turns out, this is extremely easy to do. Saving a list of commands to a file is called a “shell script”. These shell scripts can be run whenever we want, and are a great way to automate our work.\n$ cd /path/to/data-shell/molecules\n$ nano process.sh\n    #!/bin/bash         # this is called sha-bang; can be omitted for generic (bash/csh/tcsh) commands\n    echo Looking into file octane.pdb\n    head -15 octane.pdb | tail -5       # what does it do?\n$ bash process.sh   # the script ran!\nAlternatively, you can change file permissions:\n$ chmod u+x process.sh\n$ ./process.sh\nLet’s pass an arbitrary file to it:\n$ nano process.sh\n    #!/bin/bash\n    echo Looking into file $1       # $1 means the first argument to the script\n    head -15 $1 | tail -5\n$ ./process cubane.pdb\n$ ./process propane.pdb\n\nhead -15 “$1” | tail -5 # placing in double-quotes lets us pass filenames with spaces\nhead $2 $1 | tail $3 # what will this do?\n$# holds the number of command-line arguments\n$@ means all command-lines arguments to the script (words in a string)\n\n\n\n\n\n\n\n\n\n\n\nCautionTopic File permissions\n\n\n\n\n\nLet’s talk more about file permissions.\n\n\n\n\n\n\n\n\n\nCautionQuestion 8.2\n\n\n\n\n\nIn the molecules directory (download link mentioned here), create a shell script called scan.sh containing the following:\n#!/bin/bash\nhead -n $2 $1\ntail -n $3 $1\nWhile you are in that current directory, you type the following command (with space between two 1s):\n./scan.sh  '*.pdb'  1  1\nWhat output would you expect to see? 1. All of the lines between the first and the last lines of each file ending in .pdb in the current directory 2. The first and the last line of each file ending in .pdb in the current directory 3. The first and the last line of each file in the current directory 4. An error because of the quotes around *.pdb\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Bash scripts and functions, and variables"
    ]
  },
  {
    "objectID": "bash1/bash-08-scripts-functions.html#shell-scripts",
    "href": "bash1/bash-08-scripts-functions.html#shell-scripts",
    "title": "Scripts, functions, and variables",
    "section": "",
    "text": "We now know a lot of UNIX commands! Wouldn’t it be great if we could save certain commands so that we could run them later or not have to type them out again? As it turns out, this is extremely easy to do. Saving a list of commands to a file is called a “shell script”. These shell scripts can be run whenever we want, and are a great way to automate our work.\n$ cd /path/to/data-shell/molecules\n$ nano process.sh\n    #!/bin/bash         # this is called sha-bang; can be omitted for generic (bash/csh/tcsh) commands\n    echo Looking into file octane.pdb\n    head -15 octane.pdb | tail -5       # what does it do?\n$ bash process.sh   # the script ran!\nAlternatively, you can change file permissions:\n$ chmod u+x process.sh\n$ ./process.sh\nLet’s pass an arbitrary file to it:\n$ nano process.sh\n    #!/bin/bash\n    echo Looking into file $1       # $1 means the first argument to the script\n    head -15 $1 | tail -5\n$ ./process cubane.pdb\n$ ./process propane.pdb\n\nhead -15 “$1” | tail -5 # placing in double-quotes lets us pass filenames with spaces\nhead $2 $1 | tail $3 # what will this do?\n$# holds the number of command-line arguments\n$@ means all command-lines arguments to the script (words in a string)\n\n\n\n\n\n\n\n\n\n\n\nCautionTopic File permissions\n\n\n\n\n\nLet’s talk more about file permissions.\n\n\n\n\n\n\n\n\n\nCautionQuestion 8.2\n\n\n\n\n\nIn the molecules directory (download link mentioned here), create a shell script called scan.sh containing the following:\n#!/bin/bash\nhead -n $2 $1\ntail -n $3 $1\nWhile you are in that current directory, you type the following command (with space between two 1s):\n./scan.sh  '*.pdb'  1  1\nWhat output would you expect to see? 1. All of the lines between the first and the last lines of each file ending in .pdb in the current directory 2. The first and the last line of each file ending in .pdb in the current directory 3. The first and the last line of each file in the current directory 4. An error because of the quotes around *.pdb\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Bash scripts and functions, and variables"
    ]
  },
  {
    "objectID": "bash1/bash-08-scripts-functions.html#if-statements",
    "href": "bash1/bash-08-scripts-functions.html#if-statements",
    "title": "Scripts, functions, and variables",
    "section": "If statements",
    "text": "If statements\nLet’s write and run the following script:\n$ nano check.sh\n    for f in $@\n    do\n      if [ -e $f ]      # make sure to have spaces around each bracket!\n      then\n        echo $f exists\n      else\n        echo $f does not exist\n      fi\n    done\n$ chmod u+x check.sh\n$ ./check.sh a b c check.sh\n\nFull syntax is:\n\nif [ condition1 ]\nthen\n  command 1\n  command 2\n  command 3\nelif [ condition2 ]\nthen\n  command 4\n  command 5\nelse\n  default command\nfi\nSome examples of conditions (make sure to have spaces around each bracket!):\n\n[ $myvar == 'text' ] checks if variable is equal to ‘text’\n[ $myvar == number ] checks if variable is equal to number\n[ -e fileOrDirName ] checks if fileOrDirName exists\n[ -d name ] checks if name is a directory\n[ -f name ] checks if name is a file\n[ -s name ] checks if file name has length greater than 0\n\n\n\n\n\n\n\nCautionQuestion 8.3\n\n\n\n\n\nWrite a script that complains when it does not receive any arguments.",
    "crumbs": [
      "PART 2",
      "Bash scripts and functions, and variables"
    ]
  },
  {
    "objectID": "bash1/bash-08-scripts-functions.html#variables",
    "href": "bash1/bash-08-scripts-functions.html#variables",
    "title": "Scripts, functions, and variables",
    "section": "Variables",
    "text": "Variables\nWe already saw variables that were specific to scripts ($1, \\(@, ...) and to loops (\\)file). Variables can be used outside of scripts:\n$ myvar=3        # no spaces permitted around the equality sign!\n$ echo myvar     # will print the string 'myvar'\n$ echo $myvar    # will print the value of myvar\nSometimes you can see the notation:\n$ export myvar=3\nUsing ‘export’ will make sure that all inherited processes of this shell will have access to this variable. Try defining the variable newvar without/with ‘export’ and then running the script:\n$ nano process.sh\n    #!/bin/bash\n    echo $newvar\nYou can assign a command’s output to a variable to use in another command (this is called command substitution) – we’ll see this later when we play with ‘find’ command.\n$ printenv    # print all declared variables\n$ env         # same\n$ unset myvar   # unset a variable\n\n\n\n\n\n\nCautionQuestion Using a variable inside a string\n\n\n\n\n\nvar=\"sun\"\necho $varshine\necho ${var}shine\necho \"$var\"shine\n\n\n\n\n\n\n\n\n\nCautionQuestion Variable manipulation\n\n\n\n\n\nmyvar=\"hello\"\necho $myvar\necho ${myvar:offset}\necho ${myvar:offset:length}\necho ${myvar:2:3}    # 3 characters starting from character 2\necho ${myvar/l/L}    # replace the first match of a pattern\necho ${myvar//l/L}   # replace all matches of a pattern\n\n\n\nEnvironment variables are those that affect the behaviour of the shell and user interface:\n$ echo $HOME\n$ echo $PATH\n$ echo $PWD\n$ echo $PS1\nIt is best to define custom environment variables inside your ~/.bashrc file. It is loaded every time you start a new shell.\n\n\n\n\n\n\nCautionQuestion 8.6\n\n\n\n\n\nPlay with variables and their values. Change the prompt, e.g. PS1=\"\\u@\\h \\w&gt; \".\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Bash scripts and functions, and variables"
    ]
  },
  {
    "objectID": "bash1/bash-08-scripts-functions.html#functions",
    "href": "bash1/bash-08-scripts-functions.html#functions",
    "title": "Scripts, functions, and variables",
    "section": "Functions",
    "text": "Functions\nFunctions are similar to scripts, but there are some differences. A bash script is an executable file sitting at a given path. A bash function is defined in your environment. Therefore, when running a script, you need to prepend its path to its name, whereas a function – once defined in your environment – can be called by its name without a need for a path. Both scripts and functions can take command-line arguments.\nA convenient place to put all your function definitions is ~/.bashrc file which is run every time you start a new shell (local or remote).\nLike in any programming language, in bash a function is a block of code that you can access by its name. The syntax is:\nfunctionName() {\n  command 1\n  command 2\n  ...\n}\nInside functions you can access its arguments with variables $1 $2 … $# $@ – exactly the same as in scripts. Functions are very convenient because you can define them inside your ~/.bashrc file. Alternatively, you can place them into a file and then source them whenever needed:\n$ source allMyFunctions.sh\nHere is our first function:\ngreetings() {\n  echo hello\n}\nLet’s write a function ‘combine()’ that takes all the files we pass to it, copies them into a randomly-named directory and prints that directory to the screen:\ncombine() {\n  if [ $# -eq 0 ]; then\n    echo \"No arguments specified. Usage: combine file1 [file2 ...]\"\n    return 1        # return a non-zero error code\n  fi\n  dir=$RANDOM$RANDOM\n  mkdir $dir\n  cp $@ $dir\n  echo look in the directory $dir\n}\n\n\n\n\n\n\nCautionQuestion Swap file names\n\n\n\n\n\nWrite a function to swap two file names. Add a check that both files exist, before renaming them.\n\n\n\n\n\n\n\n\n\nCautionQuestion archive()\n\n\n\n\n\nWrite a function archive() to replace directories with their gzipped archives.\n$ ls -F\nchapter1/  chapter2/  notes/\n$ archive chapter* notes/\n$ ls\nchapter1.tar.gz  chapter2.tar.gz  notes.tar.gz\n\n\n\n\n\n\n\n\n\n\nCautionQuestion countfiles()\n\n\n\n\n\nWrite a function countfiles() to count files in all directories passed to it as arguments (need to loop through all arguments). At the beginning add the check:\n    if [ $# -eq 0 ]; then\n        echo \"No arguments given. Usage: countfiles dir1 dir2 ...\"\n        return 1\n    fi\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Bash scripts and functions, and variables"
    ]
  },
  {
    "objectID": "bash1/bash-08-scripts-functions.html#scripts-in-other-languages",
    "href": "bash1/bash-08-scripts-functions.html#scripts-in-other-languages",
    "title": "Scripts, functions, and variables",
    "section": "Scripts in other languages",
    "text": "Scripts in other languages\nAs a side note, it possible to incorporate scripts in other languages into your bash code, e.g. consider this:\nfunction test() {\n    randomFile=${RANDOM}${RANDOM}.py\n    cat &lt;&lt; EOF &gt; $randomFile\n#!/usr/bin/python3\nprint(\"do something in Python\")\nEOF\n    chmod u+x $randomFile\n    ./$randomFile\n    /bin/rm $randomFile\n}\nHere EOF is a random delimiter string, and &lt;&lt; tells bash to wait for the delimiter to end input. For example, try the following:\ncat &lt;&lt; the_end\nThis text will be\nprinted in the terminal.\nthe_end",
    "crumbs": [
      "PART 2",
      "Bash scripts and functions, and variables"
    ]
  },
  {
    "objectID": "bash1/bash-10-text-manipulation.html",
    "href": "bash1/bash-10-text-manipulation.html",
    "title": "Text manipulation",
    "section": "",
    "text": "(This example was kindly provided by John Simpson.)\nIn this section we’ll use two tools for text manipulation: sed and tr. Our goal is to calculate the frequency of all dictionary words in the novel “The Invisible Man” by Herbert Wells (public domain). First, let’s apply our knowledge of grep to this text:\n$ cd /path/to/data-shell\n$ ls   # shows wellsInvisibleMan.txt\n$ wc wellsInvisibleMan.txt                          # number of lines, words, characters\n$ grep invisible wellsInvisibleMan.txt              # see the invisible man\n$ grep invisible wellsInvisibleMan.txt | wc -l      # returns 60; adding -w gives the same count\n$ grep -i invisible wellsInvisibleMan.txt | wc -l   # returns 176 (includes: invisible Invisible INVISIBLE)\nLet’s sidetrack for a second and see how we can use the “stream editor” sed:\n$ sed 's/[iI]nvisible/visible/g' wellsInvisibleMan.txt &gt; visible.txt   # make him visible\n$ cat wellsInvisibleMan.txt | sed 's/[iI]nvisible/visible/g' &gt; visible.txt   # this also works (standard input)\n$ grep -i invisible visible.txt   # see what was not converted\n$ man sed\nNow let’s remove punctuation from the original file using “tr” (translate) command:\n$ cat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; nopunct.txt    # tr only takes standard input\n$ tail wellsInvisibleMan.txt\n$ tail nopunct.txt\nNext, convert all upper case to lower case:\n$ cat nopunct.txt | tr '[:upper:]' '[:lower:]' &gt; lower.txt\n$ tail lower.txt\nNext, replace spaces with new lines:\n$ cat lower.txt | sed 's/ /\\'$'\\n/g' &gt; words.txt   # \\'$'\\n is a shortcut for a new line\n$ more words.txt\nNext, remove empty lines:\n$ sed '/^$/d' words.txt  &gt; compact.txt\nNext, sort the list alphabetically, count each word’s occurrence, and remove duplicate words:\n$ cat compact.txt | sort | uniq -c &gt; dictionary.txt\n$ more dictionary.txt\nNext, sort the list into most frequent words:\n$ cat dictionary.txt | sort -gr &gt; frequency.txt   # use 'man sort'\n$ more frequency.txt\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.\n\n\nQuick reference:\nsed 's/pattern1/pattern2/' filename    # replace pattern1 with pattern2, one per line\nsed 's/pattern1/pattern2/g' filename   # same but multiple per line\nsed 's|pattern1|pattern2|g' filename   # same\n\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; nopunct.txt # remove punctuation; tr only takes standard input\ncat nopunct.txt | tr '[:upper:]' '[:lower:]' &gt; lower.txt    # convert all upper case to lower case\ncat lower.txt | sed 's/ /\\'$'\\n/g' &gt; words.txt              # replace spaces with new lines\nsed '/^$/d' words.txt  &gt; compact.txt                # remove empty lines\ncat compact.txt | sort | uniq -c &gt; dictionary.txt   # sort the list alphabetically, count each word's occurrence\ncat dictionary.txt | sort -gr &gt; frequency.txt       # sort the list into most frequent words\n\n\n\n\n\n\nCautionQuestion 10.1\n\n\n\n\n\nCan you shorten our novel-manipulation workflow putting it into a single line using pipes?\n\n\n\n\n\n\n\n\n\nCautionQuestion 10.2\n\n\n\n\n\nWrite a script that takes an English-language text file and prints the list of its 100 most common words, along with the word count in its dictionary. Hint: use the workflow we just studied. Next, convert this script into a bash function.",
    "crumbs": [
      "PART 2",
      "Text manipulation"
    ]
  },
  {
    "objectID": "bash1/bash-10-text-manipulation.html#text-manipulation",
    "href": "bash1/bash-10-text-manipulation.html#text-manipulation",
    "title": "Text manipulation",
    "section": "",
    "text": "(This example was kindly provided by John Simpson.)\nIn this section we’ll use two tools for text manipulation: sed and tr. Our goal is to calculate the frequency of all dictionary words in the novel “The Invisible Man” by Herbert Wells (public domain). First, let’s apply our knowledge of grep to this text:\n$ cd /path/to/data-shell\n$ ls   # shows wellsInvisibleMan.txt\n$ wc wellsInvisibleMan.txt                          # number of lines, words, characters\n$ grep invisible wellsInvisibleMan.txt              # see the invisible man\n$ grep invisible wellsInvisibleMan.txt | wc -l      # returns 60; adding -w gives the same count\n$ grep -i invisible wellsInvisibleMan.txt | wc -l   # returns 176 (includes: invisible Invisible INVISIBLE)\nLet’s sidetrack for a second and see how we can use the “stream editor” sed:\n$ sed 's/[iI]nvisible/visible/g' wellsInvisibleMan.txt &gt; visible.txt   # make him visible\n$ cat wellsInvisibleMan.txt | sed 's/[iI]nvisible/visible/g' &gt; visible.txt   # this also works (standard input)\n$ grep -i invisible visible.txt   # see what was not converted\n$ man sed\nNow let’s remove punctuation from the original file using “tr” (translate) command:\n$ cat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; nopunct.txt    # tr only takes standard input\n$ tail wellsInvisibleMan.txt\n$ tail nopunct.txt\nNext, convert all upper case to lower case:\n$ cat nopunct.txt | tr '[:upper:]' '[:lower:]' &gt; lower.txt\n$ tail lower.txt\nNext, replace spaces with new lines:\n$ cat lower.txt | sed 's/ /\\'$'\\n/g' &gt; words.txt   # \\'$'\\n is a shortcut for a new line\n$ more words.txt\nNext, remove empty lines:\n$ sed '/^$/d' words.txt  &gt; compact.txt\nNext, sort the list alphabetically, count each word’s occurrence, and remove duplicate words:\n$ cat compact.txt | sort | uniq -c &gt; dictionary.txt\n$ more dictionary.txt\nNext, sort the list into most frequent words:\n$ cat dictionary.txt | sort -gr &gt; frequency.txt   # use 'man sort'\n$ more frequency.txt\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.\n\n\nQuick reference:\nsed 's/pattern1/pattern2/' filename    # replace pattern1 with pattern2, one per line\nsed 's/pattern1/pattern2/g' filename   # same but multiple per line\nsed 's|pattern1|pattern2|g' filename   # same\n\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; nopunct.txt # remove punctuation; tr only takes standard input\ncat nopunct.txt | tr '[:upper:]' '[:lower:]' &gt; lower.txt    # convert all upper case to lower case\ncat lower.txt | sed 's/ /\\'$'\\n/g' &gt; words.txt              # replace spaces with new lines\nsed '/^$/d' words.txt  &gt; compact.txt                # remove empty lines\ncat compact.txt | sort | uniq -c &gt; dictionary.txt   # sort the list alphabetically, count each word's occurrence\ncat dictionary.txt | sort -gr &gt; frequency.txt       # sort the list into most frequent words\n\n\n\n\n\n\nCautionQuestion 10.1\n\n\n\n\n\nCan you shorten our novel-manipulation workflow putting it into a single line using pipes?\n\n\n\n\n\n\n\n\n\nCautionQuestion 10.2\n\n\n\n\n\nWrite a script that takes an English-language text file and prints the list of its 100 most common words, along with the word count in its dictionary. Hint: use the workflow we just studied. Next, convert this script into a bash function.",
    "crumbs": [
      "PART 2",
      "Text manipulation"
    ]
  },
  {
    "objectID": "bash1/bash-10-text-manipulation.html#column-based-text-processing-with-awk-scripting-language",
    "href": "bash1/bash-10-text-manipulation.html#column-based-text-processing-with-awk-scripting-language",
    "title": "Text manipulation",
    "section": "Column-based text processing with awk scripting language",
    "text": "Column-based text processing with awk scripting language\ncd /path/to/data-shell/writing\ncat haiku.txt   # 11 lines\nYou can define inline awk scripts with braces surrounded by single quotation:\nawk '{print $1}' haiku.txt        # $1 is the first field (word) in each line =&gt; processing columns\nawk '{print $0}' haiku.txt        # $0 is the whole line\nawk '{print}' haiku.txt           # the whole line is the default action\nawk -Fa '{print $1}' haiku.txt    # can specify another separator with -F (\"a\" in this case)\nawk -F, '{print $1}' cities.csv   # for a CSV file\nYou can use multiple commands inside your awk script:\necho Hello Tom &gt; hello.txt\necho Hello John &gt;&gt; hello.txt\nawk '{$2=\"Adam\"; print $0}' hello.txt   # we replaced the second word in each line with \"Adam\"\nMost common awk usage is to postprocess output of other commands:\n/bin/ps aux    # display all running processes as multi-column output\n/bin/ps aux | awk '{print $2 \" \" $11}'   # print only the process number and the command\nAwk also takes patterns in addition to scripts:\nawk '/Yesterday/' haiku.txt         # print the lines that contain \"Yesterday\"\nawk '/Yesterday|Today/' haiku.txt   # print the lines that contain \"Yesterday\" or \"Today\"\nAnd then you act on these patterns: if the pattern evaluates to True, then run the script:\nawk '/Yesterday|Today/{print $3}' haiku.txt\nawk '/Yesterday|Today/' haiku.txt | awk '{print $3}'   # same as previous line\n\neverything inside '' is processed by awk language; then nested further inside,\n\neverything inside // is a search pattern\neverything inside {} is an action to run\n\n\nAwk has a number of built-in variables; the most commonly used is NR:\nawk 'NR&gt;1' haiku.txt    # if NumberRecord &gt;1 then print it (default action), i.e. skip the first line\nawk 'NR&gt;1{print $0}' haiku.txt     # last command expanded\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt     # print lines 2-4\nawk 'NR&gt;=2 && NR &lt;= 4' haiku.txt   # the same\n\n\n\n\n\n\nCautionQuestion 10.3\n\n\n\n\n\nWrite an awk script to process cities.csv to print only town/city names and their population and store it in a separate file populations.csv. Try to do everything in a single-line command.\n\n\n\n\n\n\n\n\n\nCautionQuestion 10.4\n\n\n\n\n\nWrite an awk script that prints every 10th line from cities.csv starting from line 2, i.e. the first line after the header. Hint: use NR variable.\n\n\n\n\n\n\n\n\n\nCautionQuestion Copy every 10th file\n\n\n\n\n\nImagine that the directory /project/def-sponsor00/shared/toyModel contains results from a numerical simulation. Write a command to copy every 10th file (starting from yB31_oneblock_00000.vti) from this directory to one of your own directories. Hint: create an alphabetically sorted list of files in that directory and then use awk’s NR variable.\n\n\n\n\n\n\n\n\n\nCautionQuestion Archive every 20th file\n\n\n\n\n\nSimilarly to the previous exercise, write a command to create a tar archive that includes every 20th file from the simulation directory /project/def-sponsor00/shared/toyModel. Is it possible to do this in one command? Why does it remove leading ‘/’ from file paths?\n\n\n\n\n\n\n\n\n\nImportantQuick reference\n\n\n\nls -l | awk 'NR&gt;3 {print $5 \"  \" $9}'     # print 5th and 9th columns starting with line 4\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt            # print lines 2-4\nawk 'NR&gt;1 && NR &lt; 5 {print $1}' haiku.txt # print lines 2-4, column 1\nawk '/Yesterday|Today/' haiku.txt         # print lines that contain Yesterday or Today\n\n\n\n\n\n\n\n\nCautionQuestion 10.7\n\n\n\n\n\nWrite a one-line command that finds 5 largest files in the current directory (and all of its subdirectories) and prints only their names and file sizes in the human-readable format (indicating bytes, kB, MB, GB, …) in the decreasing file-size order. Hint: use find, xargs, and awk.\n\n\n\n\n\n\n\n\n\nCautionQuestion ps\n\n\n\n\n\nUse ps command to see how many processes you are running on the training cluster. Explore its flags. Write commands to reduce ps output to a few essential columns.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can watch a video for this topic after the workshop.",
    "crumbs": [
      "PART 2",
      "Text manipulation"
    ]
  },
  {
    "objectID": "cloud2hpc.html",
    "href": "cloud2hpc.html",
    "title": "From cloud to HPC",
    "section": "",
    "text": "This webinar (90% ready) was scheduled for 2025-Mar-11, but then cancelled as I was sick."
  },
  {
    "objectID": "cloud2hpc.html#building-a-gpu-enabled-container-on-arbutus",
    "href": "cloud2hpc.html#building-a-gpu-enabled-container-on-arbutus",
    "title": "From cloud to HPC",
    "section": "",
    "text": "This webinar (90% ready) was scheduled for 2025-Mar-11, but then cancelled as I was sick."
  },
  {
    "objectID": "cloud2hpc.html#abstract",
    "href": "cloud2hpc.html#abstract",
    "title": "From cloud to HPC",
    "section": "Abstract",
    "text": "Abstract\nIn this beginner-friendly webinar I will walk through the steps of creating a GPU container on a virtual machine (VM) on Arbutus that can be deployed on HPC clusters with NVIDIA GPUs. This webinar will teach you how to:\n\ncreate a VM on Arbutus cloud,\ncreate an Apptainer sandbox from scratch in Linux,\ninstall NVIDIA drivers and CUDA into the container,\ncompile GPU-enabled, single-locale Chapel in the container,\nconvert the sandbox into a SIF container,\ncompile and run Chapel codes inside the container, both on the VM and on production HPC clusters."
  },
  {
    "objectID": "cloud2hpc.html#spinning-up-a-vm-with-a-gpu",
    "href": "cloud2hpc.html#spinning-up-a-vm-with-a-gpu",
    "title": "From cloud to HPC",
    "section": "Spinning up a VM with a GPU",
    "text": "Spinning up a VM with a GPU\n\nhttps://arbutus.cloud.alliancecan.ca/project/instances # log in with CCDB credentials\nto CCInternal-magic-castle-training project\nCompute | Instances | Launch Instance\nInstance Name = alex-gpu-webinar\nSource = AlmaLinux-9.4-x64-2024-05\nFlavor = g1-8gb-c4-22gb\nNetworks = CCInternal-magic-castle-training-network\nSecurity Groups = default\nKey Pairs = alex20240821\nclick Launch (may take a few mins)\nInstances -&gt; in the drop-down menu select Associate Floating IP\nNetwork | Security Groups | on the ``default’’ row click Manage Rules\nAdd Rule | pick SSH from the first drop-down menu | click Add\n\nchmod 600 ~/.ssh/alex20240821.pem\nssh -i ~/.ssh/alex20240821.pem almalinux@206.12.91.229\nsudo dnf check-update              # check which packages have pending updates\nsudo dnf update -y                 # update these\nsudo dnf install -y epel-release   # enable Extra Packages for Enterprise Linux (EPEL)\nsudo dnf install -y git apptainer cmake bat\nsudo dnf install -y htop nano wget tmux emacs-nox netcdf netcdf-devel\nsudo reboot\n\ngit clone git@bitbucket.org:razoumov/synchpc.git syncHPC\n/bin/rm -f ~/.bashrc && ln -s syncHPC/bashrc ~/.bashrc && source ~/.bashrc\n/bin/rm -f ~/.emacs && ln -s syncHPC/emacs ~/.emacs\nln -s syncHPC/startSingleLocaleGPU.sh startSingleLocaleGPU.sh\nVolumes | Volumes | Create a volume, name=razoumovVol, no source, empty volume, type default, 300 GB, click Create from your instance Attach Volume: pick “razoumovVol” Compute | Instances | Attach volume, select razoumovVol, click Attach Compute | Instances | Volumes Attached, check which device it is attached to, e.g. /dev/vdb\nssh alma\nsudo fdisk -l               # find your volume/device\nsudo fdisk /dev/vdb         # type \"g\" to create a partition, then \"w\" to write and exit\nsudo mkfs.ext4 /dev/vdb     # format the partition\nsudo mkdir /data\nsudo mount /dev/vdb /data   # mount the parition\ndf -hT /data                # check it\nsudo mkdir /data/work\nsudo chown almalinux.almalinux -R /data/work\n“The Nouveau GPU driver is an open-source graphics driver for NVIDIA GPUs, developed as part of the Linux kernel. It provides support for NVIDIA graphics cards without requiring NVIDIA’s proprietary driver. However, Nouveau is often slower and lacks full support for newer GPU features compared to the official NVIDIA driver.”\nThe GPU driver details below are from https://docs.alliancecan.ca/wiki/Using_cloud_vGPUs\n# prevent loading of the buggy Nouveau GPU driver when the system boots\nsudo sh -c \"echo 'blacklist nouveau' &gt; /etc/modprobe.d/blacklist-nouveau.conf\"\nsudo sh -c \"echo 'options nouveau modeset=0' &gt;&gt; /etc/modprobe.d/blacklist-nouveau.conf\"\nsudo dracut -fv --omit-drivers nouveau\nsudo dnf -y update\n# sudo dnf -y install epel-release   # already done\nsudo reboot\n\n# install the vGPU driver\n# sudo dnf remove libglvnd-gles-1:1.3.4-1.el9.x86_64\n# sudo dnf remove libglvnd-glx-1:1.3.4-1.el9.x86_64\nsudo dnf -y install http://repo.arbutus.cloud.alliancecan.ca/pulp/repos/alma9/Packages/a/arbutus-cloud-vgpu-repo-1.0-1.el9.noarch.rpm   # install the Arbutus vGPU Cloud repository\nsudo dnf -y install nvidia-vgpu-gridd.x86_64 nvidia-vgpu-tools.x86_64 nvidia-vgpu-kmod.x86_64\nnvidia-smi\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  GRID V100D-8C                  On  |   00000000:00:05.0 Off |                    0 |\n| N/A   N/A    P0             N/A /  N/A  |       0MiB /   8192MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n# install CUDA\nSRC=https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers\nwget $SRC/cuda-repo-rhel9-12-4-local-12.4.0_550.54.14-1.x86_64.rpm   # 4.6GB download\nsudo dnf -y install cuda-repo-rhel9-12-4-local-12.4.0_550.54.14-1.x86_64.rpm\nsudo dnf clean all\nsudo dnf -y install cuda-toolkit-12-4\n&gt;&gt;&gt; do not delete cuda-repo-rhel9-12-4-local-12.4.0_550.54.14-1.x86_64.rpm (will be needed later)\nnow there is /usr/local/cuda-12.4/bin/nvcc"
  },
  {
    "objectID": "cloud2hpc.html#installing-chapel-with-gpu-support-natively",
    "href": "cloud2hpc.html#installing-chapel-with-gpu-support-natively",
    "title": "From cloud to HPC",
    "section": "Installing Chapel with GPU support natively",
    "text": "Installing Chapel with GPU support natively\nThis step is optional, just to test GPU Chapel without installing it into the container.\nalmalinux@alex-gpu-testing\nwget https://github.com/chapel-lang/chapel/releases/download/2.3.0/chapel-2.3.0.tar.gz\ntar xvfz chapel-2.3.0.tar.gz\ncd chapel-2.3.0\nsource util/setchplenv.bash\nexport CHPL_LLVM=bundled\nexport CHPL_COMM=none\nexport CHPL_TARGET_CPU=none   # full list https://chapel-lang.org/docs/usingchapel/chplenv.html#chpl-target-cpu\nexport CHPL_LOCALE_MODEL=gpu\nexport CHPL_GPU=nvidia\nexport CHPL_CUDA_PATH=/usr/local/cuda-12.4\nmkdir -p ~/c1/chapel-2.3.0 && /bin/rm -rf ~/c1/chapel-2.3.0/*\n./configure --chpl-home=$HOME/c1/chapel-2.3.0   # inspect the settings\nmake -j4\nmake install\nsource ~/startSingleLocaleGPU.sh\ngit clone git@bitbucket.org:razoumov/chapel.git ~/chapelCourse\ncd ~/chapelCourse/gpu\nchpl --fast probeGPU.chpl\n./probeGPU\ncd ../juliaSet\nchpl --fast juliaSetSerial.chpl\nchpl --fast juliaSetGPU.chpl\n./juliaSetSerial --n=8000   # 9.28693s\n./juliaSetGPU --n=8000      # 0.075753s\ncd ../primeFactorization\nchpl --fast primesGPU.chpl\n./primesGPU --n=10_000_000   # 0.04065s; A = 4561 1428578 5000001 4894 49"
  },
  {
    "objectID": "cloud2hpc.html#building-a-chapel-gpu-container",
    "href": "cloud2hpc.html#building-a-chapel-gpu-container",
    "title": "From cloud to HPC",
    "section": "Building a Chapel GPU container",
    "text": "Building a Chapel GPU container\n--nv does not work with --writable, so you can’t create a writable sandbox that mounts the host’s GPU drivers and libraries, and into which you would install GPU Chapel. There are some solutions around this:\n\nYou could do this via a writable overlay image, creating and then starting an immutable SIF container with --nv and then installing GPU Chapel into the overlay. It works, but in my experience this is not the best option from the performance standpoint.\nYou can install the NVIDIA Container Toolkit:\n\nURL=https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo\ncurl -s -L $URL | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\nsudo dnf install -y nvidia-container-toolkit\nand then combine --nv and --writable:\napptainer build --sandbox almalinux.dir docker://almalinux\nexport NVIDIA_DRIVER_CAPABILITIES=\"compute,utility\"\nsudo apptainer shell --writable --nv --nvccli almalinux.dir\nbut I don’t see nvcc inside the container (maybe I am missing something obvious?).\n\nYou can bootstrap from an NVIDIA development image, and compile GPU Chapel in there.\nInstall CUDA into the container and use it to compile GPU Chapel:\n\ncd /data/work\nmkdir tmp\nexport APPTAINER_TMPDIR=/data/work/tmp\napptainer build --sandbox almalinux.dir docker://almalinux   # sudo not yet required\nmkdir almalinux.dir/source\nmv ~/cuda-repo-rhel9-12-4-local-12.4.0_550.54.14-1.x86_64.rpm almalinux.dir/source/\nsudo apptainer shell --writable almalinux.dir\nApptainer&gt; dnf check-update\n           dnf update -y\n           dnf install -y cmake gcc g++ python3 wget\n           # install CUDA inside the container\n           dnf -y install /source/cuda-repo-rhel9-12-4-local-12.4.0_550.54.14-1.x86_64.rpm\n           dnf clean all   # remove cached package data\n           dnf -y install cuda-toolkit-12-4\n           /bin/rm /source/cuda-repo-rhel9-12-4-local-12.4.0_550.54.14-1.x86_64.rpm\n\nmkdir almalinux.dir/c1/\ncp ~/chapel-2.3.0.tar.gz almalinux.dir/source\nsudo apptainer shell --writable almalinux.dir\nApptainer&gt; cd /source\n           tar xvfz chapel-2.3.0.tar.gz\n           cd chapel-2.3.0\n           source util/setchplenv.bash\n           export CHPL_LLVM=bundled\n           export CHPL_COMM=none\n           export CHPL_TARGET_CPU=none\n           export CHPL_LOCALE_MODEL=gpu\n           export CHPL_GPU=nvidia\n           export CHPL_CUDA_PATH=/usr/local/cuda-12.4\n           mkdir -p /c1/chapel-2.3.0 && /bin/rm -rf /c1/chapel-2.3.0/*\n           ./configure --chpl-home=/c1/chapel-2.3.0\n           make -j4\n           make install\n           /bin/rm -r /source\n\nsudo apptainer build almalinux.sif almalinux.dir\nsudo apptainer shell --nv almalinux.sif\nApptainer&gt; nvidia-smi   # should show the same info as above"
  },
  {
    "objectID": "cloud2hpc.html#testing-on-the-vm",
    "href": "cloud2hpc.html#testing-on-the-vm",
    "title": "From cloud to HPC",
    "section": "Testing on the VM",
    "text": "Testing on the VM\ncd\nsudo apptainer shell --nv /data/work/almalinux.sif\ncd chapelCourse/gpu\nsource /c1/chapel-2.3.0/util/setchplenv.bash\nexport CHPL_GPU=nvidia\nexport CHPL_CUDA_PATH=/usr/local/cuda-12.4\nexport PATH=$CHPL_CUDA_PATH/bin:$PATH\nmake clean\nchpl --fast probeGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./probeGPU\ncd ../juliaSet\nmake clean\nchpl --fast juliaSetSerial.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\nchpl --fast juliaSetGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./juliaSetSerial --n=8000   # 9.36665s\n./juliaSetGPU --n=8000      # 0.06692s\ncd ../primeFactorization\nchpl --fast primesGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./primesGPU --n=10_000_000   # 0.039443s; A = 4561 1428578 5000001 4894 49\ncd /data/work\nscp almalinux.sif razoumov@fir.alliancecan.ca:/project/6003910/razoumov/apptainerImages/chapelGPU202503\nscp almalinux.sif razoumov@rorqual.alliancecan.ca:scratch"
  },
  {
    "objectID": "cloud2hpc.html#testing-on-fir",
    "href": "cloud2hpc.html#testing-on-fir",
    "title": "From cloud to HPC",
    "section": "Testing on Fir",
    "text": "Testing on Fir\nfir\ncd /project/6003910/razoumov/apptainerImages/chapelGPU202503\n# salloc --time=0:30:0 --mem-per-cpu=3600 --gpus-per-node=1 --account=def-razoumov-ac\nsalloc --time=0:30:0 --mem-per-cpu=3600 --gpus-per-node=v100l:1 \\\n       --account=cc-debug --reservation=asasfu_756\nnvidia-smi\nmodule load apptainer\napptainer shell --nv -B $SLURM_TMPDIR almalinux.sif\nsource /c1/chapel-2.3.0/util/setchplenv.bash\nexport CHPL_GPU=nvidia\nexport CHPL_CUDA_PATH=/usr/local/cuda-12.4\nexport PATH=$PATH:/usr/local/cuda-12.4/bin\ncp -r ~/chapelCourse $SLURM_TMPDIR\ncd $SLURM_TMPDIR/chapelCourse/gpu\nmake clean\nchpl --fast probeGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./probeGPU\ncd ../juliaSet\nchpl --fast juliaSetSerial.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\nchpl --fast juliaSetGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./juliaSetSerial --n=8000   # 12.4182s\n./juliaSetGPU --n=8000      # 0.083108s\ncd ../primeFactorization\nchpl --fast primesGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./primesGPU --n=10_000_000   # 0.166764s; A = 4561 1428578 5000001 4894 49"
  },
  {
    "objectID": "cloud2hpc.html#testing-on-rorqual",
    "href": "cloud2hpc.html#testing-on-rorqual",
    "title": "From cloud to HPC",
    "section": "Testing on Rorqual",
    "text": "Testing on Rorqual\nrorqual\ncd ~/chapelCourse/codes\nsalloc --time=0:30:0 --nodes=1 --cpus-per-task=1 --mem-per-cpu=3600 --gpus-per-node=1 --account=cc-debug\nnvidia-smi\nmodule load apptainer\nexport CDIR=~/scratch/chapelGPU20240826\napptainer shell --nv --overlay ${CDIR}/extra.img:ro ${CDIR}/almalinux.sif\nsource /extra/c1/chapel-2.3.0/util/setchplenv.bash\nexport CHPL_GPU=nvidia\nexport CHPL_CUDA_PATH=/usr/local/cuda-12.4\nexport PATH=$PATH:/usr/local/cuda-12.4/bin\ncd ~/tmp/2024/\nchpl --fast probeGPU.chpl -L/usr/local/cuda-12.4/targets/x86_64-linux/lib/stubs\n./probeGPU\nchpl --fast juliaSetGPU.chpl -L/usr/local/cuda-12.4/targets/x86_64-linux/lib/stubs\n./juliaSetGPU\n./juliaSetGPU --height=8000"
  },
  {
    "objectID": "cloud2hpc.html#using-grigorys-2-step-build-now-as-a-1-step-build",
    "href": "cloud2hpc.html#using-grigorys-2-step-build-now-as-a-1-step-build",
    "title": "From cloud to HPC",
    "section": "Using Grigory’s 2-step build, now as a 1-step build",
    "text": "Using Grigory’s 2-step build, now as a 1-step build\n\nmulti-stage builds https://docs.sylabs.io/guides/latest/user-guide/definition_files.html#multi-stage-builds\navailable containers https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc/tags\nnew one is nvcr.io/nvidia/nvhpc:25.1-devel-cuda_multi-ubuntu24.04\nolder one nvcr.io/nvidia/nvhpc:23.11-devel-cuda_multi-ubuntu22.04\n\ncd /data/work\nexport APPTAINER_TMPDIR=/data/work/tmp\napptainer build --nv test.sif docker://nvcr.io/nvidia/nvhpc:24.3-devel-cuda_multi-ubuntu22.04\nsudo apptainer shell --nv test.sif\nfind / -name nvcc   # check if their CUDA installation includes `nvcc` (needed for GPU Chapel runtime)\ncd /data/work\n&gt;&gt;&gt; create single.def\nBootStrap: docker   # this is `single.def`\nFrom: nvcr.io/nvidia/nvhpc:24.3-devel-cuda_multi-ubuntu22.04\nStage: build\n%post\n    . /.singularity.d/env/10-docker*.sh\n    apt update -y\n    apt install -y python3\n    mkdir /source && cd /source\n    wget https://github.com/chapel-lang/chapel/releases/download/2.3.0/chapel-2.3.0.tar.gz\n    tar xvfz chapel-2.3.0.tar.gz\n    cd chapel-2.3.0\n    . util/setchplenv.sh\n    export CHPL_LLVM=bundled\n    export CHPL_COMM=none\n    export CHPL_TARGET_CPU=none\n    export CHPL_LOCALE_MODEL=gpu\n    export CHPL_GPU=nvidia\n    export CHPL_CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/24.3/cuda/12.3\n    mkdir -p /c1/chapel-2.3.0\n    ./configure --chpl-home=/c1/chapel-2.3.0\n    make -j4\n    make install\n    /bin/rm -r /source\nexport APPTAINER_TMPDIR=/data/work/tmp\napptainer build --nv ubuntu.sif single.def\ncd\nsudo apptainer shell --nv /data/work/ubuntu.sif\ncd chapelCourse/gpu\nsource /c1/chapel-2.3.0/util/setchplenv.bash\nexport CHPL_GPU=nvidia\nexport CHPL_CUDA_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/24.3/cuda/12.3\nexport PATH=$CHPL_CUDA_PATH/bin/:$PATH\nmake clean\nchpl --fast probeGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./probeGPU\ncd ../juliaSet\nmake clean\nchpl --fast juliaSetSerial.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\nchpl --fast juliaSetGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./juliaSetSerial --n=8000   # 9.36665s\n./juliaSetGPU --n=8000      # 0.06692s\ncd ../primeFactorization\nchpl --fast primesGPU.chpl -L${CHPL_CUDA_PATH}/targets/x86_64-linux/lib/stubs\n./primesGPU --n=10_000_000   # 0.039443s; A = 4561 1428578 5000001 4894 49\ncd /data/work\nscp ubuntu.sif razoumov@fir.alliancecan.ca:/project/6003910/razoumov/apptainerImages/chapelGPU202503\nscp ubuntu.sif razoumov@rorqual.alliancecan.ca:scratch"
  },
  {
    "objectID": "datalad.html",
    "href": "datalad.html",
    "title": "Distributed datasets with DataLad",
    "section": "",
    "text": "You can find this page at   https://folio.vastcloud.org/datalad"
  },
  {
    "objectID": "datalad.html#git",
    "href": "datalad.html#git",
    "title": "Distributed datasets with DataLad",
    "section": "Git",
    "text": "Git\nGit a version control system designed to keep track of software projects and their history, to merge edits from multiple authors, to work with branches (distinct project copies) and merge them into the main projects. Since Git was designed for version control of text files, it can also be applied to writing projects, such as manuscripts, theses, website repositories, etc.\n\nI assume that most attendees are familiar with Git, but we can certainly do a quick command-line Git demo.\n\nGit can also keep track of binary (non-text) files and/or of large data files, but putting binary and/or large files under version control and especially modifying them will inflate the size of the repositories."
  },
  {
    "objectID": "datalad.html#git-annex",
    "href": "datalad.html#git-annex",
    "title": "Distributed datasets with DataLad",
    "section": "git-annex",
    "text": "git-annex\n\n\nGit-annex was built on top of Git and was designed to share and synchronize large file in a distributed fashion. The file content is managed separately from the dataset’s structure / metadata – the latter is kept under Git version control, while files are stored in separate directories. If you look inside a git-annex repository, you will see that files are replaced with symbolic links, and in fact you don’t have to have the actual data stored locally, e.g. if you want to reduce the disk space usage."
  },
  {
    "objectID": "datalad.html#datalad",
    "href": "datalad.html#datalad",
    "title": "Distributed datasets with DataLad",
    "section": "DataLad",
    "text": "DataLad\nDalaLad builds on top of Git and git-annex, retaining all their features, but adds few other functions:\n\nDatasets can be nested, and most DalaLad commands have a --recursive option that will traverse subdatasets and do “the right thing”.\nDalaLad can run commands on data, and if a dataset is not present locally, DalaLad will automatically get the required input files from a remote repository.\nDataLad can keep track of data provenance, e.g. datalad download-url will download files, add them to the repository, and keep a record of data origin.\nFew other features.\n\nAs you will see in this workshop, most DataLad workflows involve running all three – git, git annex, and datalad – commands, so we’ll be using the functionality of all three layers."
  },
  {
    "objectID": "datalad.html#create-a-new-dataset",
    "href": "datalad.html#create-a-new-dataset",
    "title": "Distributed datasets with DataLad",
    "section": "Create a new dataset",
    "text": "Create a new dataset\n\n\n\n\n\n\nNote\n\n\n\nSome files in your dataset will be stored as plain files, some files will be put in the annex, i.e. they will be replaced with their symbolic links and might not be even stored locally. Annexed files cannot be modified directly (more on this later). The command datalad run-procedure --discover shows you a list of available configurations. On my computer they are:\n- text2git: do not put anything that is a text file in the annex, i.e. process them with regular Git\n- yoda: configure a dataset according to the yoda principles\n- noannex: put everything under regular Git control\n\n\ncd ~/tmp\ndatalad create --description \"our first dataset\" -c text2git test   # use `text2git` configuration\ncd test\nls\ngit log"
  },
  {
    "objectID": "datalad.html#add-some-data",
    "href": "datalad.html#add-some-data",
    "title": "Distributed datasets with DataLad",
    "section": "Add some data",
    "text": "Add some data\nLet’s use some file examples from the official DalaLad handbook:\nmkdir books\nwget -q https://sourceforge.net/projects/linuxcommand/files/TLCL/19.01/TLCL-19.01.pdf -O books/theLinuxCommandLine.pdf\nwget -q https://homepages.uc.edu/~becktl/byte_of_python.pdf -O books/aByteOfPython.pdf\nls books\ndatalad status\ndatalad save -m \"added a couple of books on Linux and Python\"\nls books\ngit log -n 1        # check last commit\ngit log -n 1 -p     # check last commit in details\ngit config --global alias.one \"log --graph --date-order --date=short --pretty=format:'%C(cyan)%h %C(yellow)%ar %C(auto)%s%+b %C(green)%ae'\"\ngit one             # custom alias\ngit log --oneline   # a short alternative\nLet’s add another couple of books using a built-in downloading command:\ndatalad download-url https://github.com/progit/progit2/releases/download/2.1.154/progit.pdf \\\n    --dataset . -m \"added a reference book about git\" -O books/proGit.pdf\ndatalad download-url http://www.tldp.org/LDP/Bash-Beginners-Guide/Bash-Beginners-Guide.pdf \\\n    --dataset . -m \"added bash guide for beginners\" -O books/bashGuideForBeginners.pdf\nls books\ntree\ndatalad status   # nothing to be saved\ngit log          # `datalad download-url` took care of that\ngit annex whereis books/proGit.pdf   # show the available copies (including the URL source)\ngit annex whereis books              # show the same for all books\nCreate and commit a short text file:\ncat &lt;&lt; EOT &gt; notes.txt\nWe have downloaded 4 books.\nEOT\ndatalad save -m \"added notes.txt\"\ngit log -n 1      # see the last commit\ngit log -n 1 -p   # and its file changes\nNotice that the text file was not annexed: there is no symbolic link. This means that we can modify it easily:\necho Text files are not in the annex.&gt;&gt; notes.txt\ndatalad save -m \"edited notes.txt\""
  },
  {
    "objectID": "datalad.html#subdatasets",
    "href": "datalad.html#subdatasets",
    "title": "Distributed datasets with DataLad",
    "section": "Subdatasets",
    "text": "Subdatasets\nLet’s clone a remote dataset and store it locally as a subdataset:\ndatalad clone --dataset . https://github.com/datalad-datasets/machinelearning-books   # get its structure\ntree\ndu -s machinelearning-books             # not much data there (large files were not downloaded)\ncd machinelearning-books\ndatalad status --annex   # if all files were present: 9 annex'd files (74.4 MB recorded total size)\ndatalad status --annex all   # check how much data we have locally: 0.0 B/74.4 MB present/total size\ndatalad status --annex all A.Shashua-Introduction_to_Machine_Learning.pdf   # 683.7 KB\nOk, this file is not too large, so we can download it easily:\ndatalad get A.Shashua-Introduction_to_Machine_Learning.pdf\ndatalad status --annex all   # now we have 683.7 KB/74.4 MB present/total size\nopen A.Shashua-Introduction_to_Machine_Learning.pdf   # it should open\ndatalad drop A.Shashua-Introduction_to_Machine_Learning.pdf   # delete the local copy\ngit log    # this particular dataset's history (none of our commands show here: we did not modify it)\ncd .."
  },
  {
    "objectID": "datalad.html#running-scripts",
    "href": "datalad.html#running-scripts",
    "title": "Distributed datasets with DataLad",
    "section": "Running scripts",
    "text": "Running scripts\ncd ../machinelearning-books\ngit annex find --not --in=here     # show remote files\nmkdir code\ncat &lt;&lt; EOT &gt; code/titles.sh\nfor file in \\$(git annex find --not --in=here); do\n    echo \\$file | sed 's/^.*-//' | sed 's/.pdf//' | sed 's/_/ /g'\ndone\nEOT\ncat code/titles.sh\ndatalad save -m \"added a short script to write a list of book titles\"\ndatalad run -m \"create a list of books\" \"bash code/titles.sh &gt; list.txt\"\ncat list.txt\ngit log   # the command run record went into the log\nNow we will modify and rerun this script:\ndatalad unlock code/titles.sh   # move the script out of the annex to allow edits\ncat &lt;&lt; EOT &gt; code/titles.sh\nfor file in \\$(git annex find --not --in=here); do\n    title=\\$(echo \\$file | sed 's/^.*-//' | sed 's/.pdf//' | sed 's/_/ /g')\n    echo \\\"\\$title\\\"\ndone\nEOT\ndatalad save -m \"correction: enclose titles into quotes\" code/titles.sh\ngit log -n 5   # note the hash of the last commit\ndatalad rerun ba90706\nmore list.txt\ndatalad diff --from ba90706 --to f88e2ce   # show the filenames only\nFinally, let’s extract the title page from one of the books, A.Shashua-Introduction_to_Machine_Learning.pdf. First, let’s open the book itself:\nopen A.Shashua-Introduction_to_Machine_Learning.pdf   # this book is not here!\nThe book is not here … That’s not a problem for DalaLad, as it can process a file that is stored remotely (as long as it is part of the dataset)  🡲  it will automatically get the required input file.\ndatalad run -m \"extract the title page\" \\\n  --input \"A.Shashua-Introduction_to_Machine_Learning.pdf\" \\\n  --output \"title.pdf\" \\\n  \"convert -density 300 {inputs}[0] -quality 90 {outputs}\"\ngit log\ngit annex find --in=here     # show local files: it downloaded the book, extracted the first page\nopen title.pdf"
  },
  {
    "objectID": "datalad.html#two-users-on-a-shared-cluster-filesystem-working-with-the-same-dataset",
    "href": "datalad.html#two-users-on-a-shared-cluster-filesystem-working-with-the-same-dataset",
    "title": "Distributed datasets with DataLad",
    "section": "(1) two users on a shared cluster filesystem working with the same dataset",
    "text": "(1) two users on a shared cluster filesystem working with the same dataset\nFor simplicity, let’s assume both users share the same GID (group ID), i.e. they are from the same research group. Extending this workflow to multiple users with different GISs can be done via Access control lists (ACLs).\n\nStart with Git\nFirst, let’s consider a shared Git-only (no git-annex, no DalaLad) repository in /project, and how the two users can both push to it.\n\nuser001\ngit config --global user.name \"First User\"\ngit config --global user.email \"user001@westdri.ca\"\ngit config --global init.defaultBranch main\ncd /project/def-sponsor00\ngit init --bare --shared collab\nls -l | grep collab   # note the group permissions and the SGID (recursive)\n\ncd\ngit clone /project/def-sponsor00/collab\ncd collab\ndd if=/dev/urandom of=test1 bs=1024 count=$(( RANDOM + 1024 ))\ndd if=/dev/urandom of=test2 bs=1024 count=$(( RANDOM + 1024 ))\ndd if=/dev/urandom of=test3 bs=1024 count=$(( RANDOM + 1024 ))\ngit add test*\ngit commit -m \"added test{1..3}\"\ngit push\n\nuser002\ngit config --global user.name \"Second User\"\ngit config --global user.email \"user002@westdri.ca\"\ngit clone /project/def-sponsor00/collab\ncd collab\necho \"making some changes\" &gt; readme.txt\ngit add readme.txt\ngit commit -m \"added readme.txt\"\ngit push\n\n\n\n\nAdd DataLad datasets\n\nuser001\nmodule load git-annex   # need this each time you use DalaLad\nalias datalad=/project/def-sponsor00/shared/datalad-env/bin/datalad   # best to add this line to your ~/.bashrc file\n\nchmod -R u+wX ~/collab\n/bin/rm -rf /project/def-sponsor00/collab ~/collab\n\ncd /project/def-sponsor00\ngit init --bare --shared collab\nls -l | grep collab   # note the group permissions and the SGID (recursive)\n\ncd\ndatalad create --description \"my collab\" -c text2git collab   # create a dataset using `text2git` template\ncd collab\ndd if=/dev/urandom of=test1 bs=1024 count=$(( RANDOM + 1024 ))\ndd if=/dev/urandom of=test2 bs=1024 count=$(( RANDOM + 1024 ))\ndd if=/dev/urandom of=test3 bs=1024 count=$(( RANDOM + 1024 ))\ndatalad save -m \"added test1,2,3\"\ngit remote add origin /project/def-sponsor00/collab\n# git push --set-upstream origin main      # if we were using Git only\ndatalad push --to origin --data anything   # transfer all annexed content\ngit annex whereis test1                    # 1 copy\ndatalad push --to origin --data anything   # I find that I need to run it twice to actually transfer annexed data\ngit annex whereis test1                    # 2 copies (here and origin)\ndu -s /project/def-sponsor00/collab\nAfter making sure there is a remote copy, you can drop a local copy:\ndatalad drop test1\ngit annex whereis test1                    # 1 copy\nTo get this file at any time in the future, you would run:\ndatalad get test1\nLet’s actually drop all files for which there is a remote copy, to save on local disk space:\nfor file in $(git annex find --in=origin); do\n    datalad drop $file\ndone\ngit annex whereis test*      # only remote copies left\ndatalad status --annex all   # check local data usage\ndu -s .\n\n\n\n\n\n\n\n\n\n\n\n\nTo allow other users to write to the DalaLad repo, setting git init --shared ... on /project/def-sponsor00/collab is not sufficient, as it does not set proper permissions for /project/def-sponsor00/collab/annex. We have to do it manually:\ncd /project/def-sponsor00/collab\nchmod -R g+ws annex   # so that user002 could push with datalad\nuser002\nmodule load git-annex   # need this each time you use DalaLad\nalias datalad=/project/def-sponsor00/shared/datalad-env/bin/datalad   # best to add this line to your ~/.bashrc file\n\ngit config --global --add safe.directory /project/60105/collab           # allow git to work with files from other users\ngit config --global --add safe.directory /project/def-sponsor00/collab   # do the same for the linked version\n\n/bin/rm -rf collab\ndatalad clone --description \"user002's copy\" /project/def-sponsor00/collab collab\ncd collab\ndu -s .\ngit annex find --in=here   # show local files (none at the moment)\n\ndatalad get test1     # download the file\ndd if=/dev/urandom of=test4 bs=1024 count=$(( RANDOM + 1024 ))\ndatalad save -m \"added test4\"\ngit log\ngit remote -v    # our remote is origin\ndatalad push --to origin --data anything\ngit annex find --in=origin     # test4 now in both places\n\necho \"I started working with this dataset as well\" &gt; notes.txt\ngit add notes.txt\ngit commit -m \"added notes.txt\"\ndatalad push\nNow user001 can see the two new files:\nuser001\nmodule load git-annex   # need this each time you use DalaLad\nalias datalad=/project/def-sponsor00/shared/datalad-env/bin/datalad   # best to add this line to your ~/.bashrc file\n\ncd ~/collab\ndatalad update --how merge   # download most recent data from origin\ncat notes.txt                # it is here\ngit annex find --in=here     # none of the annexed content (e.g. test4) is here\n\ndatalad get test4            # but we can get it easily\ngit annex find --in=here\n\ndatalad status --annex all         # check how much data we have locally: present/total space\ngit annex find --lackingcopies 0   # show files that are stored only in one place\ngit annex whereis test*            # show location for all files\nYou also see information about what is stored in “user002’s copy”. However, you should take it with a grain of salt. For example, if user002 drops some files locally and does not run datalad push, origin (and hence user001) will have no knowledge of that fact."
  },
  {
    "objectID": "datalad.html#one-user-one-dataset-spread-over-multiple-drives-with-data-redundancy",
    "href": "datalad.html#one-user-one-dataset-spread-over-multiple-drives-with-data-redundancy",
    "title": "Distributed datasets with DataLad",
    "section": "(2) one user, one dataset spread over multiple drives, with data redundancy",
    "text": "(2) one user, one dataset spread over multiple drives, with data redundancy\nInitially I created this scenario with two external USB drives. In the interest of time, I simplified it to a single external drive, but it can easily be extended to any number of drives.\nFirst, let’s create an always-present dataset on the computer that will also keep track of all data stored in its clone on a removable USB drive:\ncd ~/tmp\ndatalad create --description \"Central location\" -c text2git distributed\ncd distributed\ngit config receive.denyCurrentBranch updateInstead   # allow clones to update this dataset\nmkdir books\nwget -q https://sourceforge.net/projects/linuxcommand/files/TLCL/19.01/TLCL-19.01.pdf -O books/theLinuxCommandLine.pdf\nwget -q https://homepages.uc.edu/~becktl/byte_of_python.pdf -O books/aByteOfPython.pdf\ndatalad save -m \"added a couple of books\"\nls books\ndu -s .   # 4.9M stored here\n\n\n\n\n\n\n\n\nCreate a clone on a portable USB drive:\ncd /Volumes/t7\ndatalad clone --description \"t7\" ~/tmp/distributed distributed\ncd distributed\ndu -s .   # no actual data was copied, just the links\ngit remote rename origin central\ncd books\nwget -q https://github.com/progit/progit2/releases/download/2.1.154/progit.pdf -O proGit.pdf\nwget -q http://www.tldp.org/LDP/Bash-Beginners-Guide/Bash-Beginners-Guide.pdf -O bashGuideForBeginners.pdf\ndatalad save -m \"added two more books\"\ngit log                          # we have history from both drives (all 4 books)\ngit annex find --in=here         # but only 2 books are stored here\ngit annex find --not --in=here   # and 2 books are stored not here\nfor book in $(git annex find --not --in=here); do\n    git annex whereis $book      # show their location: they are in central\ndone\ndatalad push --to central --data nothing   # push metadata to central\n\nOperations from the central dataset:\ncd ~/tmp/distributed\ngit annex find --in=here           # show local files: 2 books\ngit annex find --not --in=here     # show remote files: 2 books\ndatalad status --annex all         # check local data usage: 4.6 MB/17.6 MB present/total size\ngit annex find --lackingcopies 0   # show files that are stored only in one place\ngit annex whereis books/*          # show location\nLet’s mount t7 and get one of its books:\ndatalad get books/bashGuideForBeginners.pdf   # try getting this book from a remote =&gt; error\n... get(error): books/bashGuideForBeginners.pdf (file) [not available]\n\ngit remote    # nothing: central does not know where the remotes are stored\ndatalad siblings add -d . --name t7 --url /Volumes/t7/distributed\ngit remote    # now it knows where to find the remotes\ndatalad get books/bashGuideForBeginners.pdf   # successful!\nNow unmount t7.\ngit annex whereis books/bashGuideForBeginners.pdf   # 2 copies (here and t7)\nopen books/bashGuideForBeginners.pdf\nLet’s remove this the local copy of this book:\ndatalad drop books/bashGuideForBeginners.pdf   # error: tried to t7 for remaining physical copies\ndatalad drop --reckless availability books/bashGuideForBeginners.pdf   # do not check remotes (potentially dangerous)\ngit annex whereis books/bashGuideForBeginners.pdf   # only 1 copy left on t7\n\n\nLetting remotes know about central changes\nLet’s add a DalaLad book to central:\ncd ~/tmp/distributed\ndatalad download-url http://handbook.datalad.org/_/downloads/en/stable/pdf/ \\\n    --dataset . -m \"added the DataLad Handbook\" -O books/datalad.pdf\nThe remote knows nothing about this new book. Let’s push this update out! Make sure to mount t7 and then run the following:\ncd /Volumes/t7/distributed\ngit config receive.denyCurrentBranch updateInstead   # allow clones to update this dataset\ncd ~/tmp/distributed\ndatalad push --to t7 --data nothing                  # push metadata, but not the data\nAlternatively, we could update from the USB drive:\ncd /Volumes/t7/distributed\ndatalad update -s central --how=merge\nNow let’s check things from t7’s perspective:\ncd /Volumes/t7/distributed\nls books/                             # datalad.pdf is there\ngit annex whereis books/datalad.pdf   # it is in central only (plus on the web)\n\n\nData redundancy\nNow imagine that we want to backup all files that are stored in a single location, and always have a second copy on the other drive.\n\n\n\ncd /Volumes/t7/distributed\nfor file in $(git annex find --lackingcopies 0); do\n    datalad get $file\ndone\ndatalad push --to central --data nothing   # update the central\ngit annex find --lackingcopies 0           # still two files have only 1 copy\ngit annex find --in=here                   # but they are both here already ==&gt; makes sense\nLet’s go to central and do the same:\n\n\n\ncd ~/tmp/distributed\nfor file in $(git annex find --lackingcopies 0); do\n    datalad get $file\ndone\ngit annex find --lackingcopies 0   # none: now all files have at least two copies\ngit annex whereis                  # here where everything is\nThe file books/datalad.pdf is in two locations, although one of them is the web. You can correct that manually: go to t7 and run get there.\nTry dropping a local file:\ndatalad drop books/theLinuxCommandLine.pdf   # successful, since t7 is also mounted\ndatalad get books/theLinuxCommandLine.pdf    # get it back\nSet the minimum number of copies and try dropping again:\ngit annex numcopies 2\ndatalad drop books/theLinuxCommandLine.pdf   # can't: need minimum 2 copies!"
  },
  {
    "objectID": "datalad.html#publish-a-dataset-on-github-with-annexed-files-in-a-special-private-remote",
    "href": "datalad.html#publish-a-dataset-on-github-with-annexed-files-in-a-special-private-remote",
    "title": "Distributed datasets with DataLad",
    "section": "(3) publish a dataset on GitHub with annexed files in a special private remote",
    "text": "(3) publish a dataset on GitHub with annexed files in a special private remote\nAt some stage, you might want to publish a dataset on GitHub that contains some annexed data. The problem is that annexed data could be large, and you can quickly run into problems with GitHub’s storage/bandwidth limitations. Moreover, free accounts on GitHub do not support working with annexed data.\nWith DalaLad, however, you can host large/annexed files elsewhere and still have the dataset published on GitHub. This is done with so-called special remotes. The published dataset on GitHub stores the information about where to obtain the annexed file contents when you run datalad get.\nSpecial remotes can point to Amazon S3, Dropbox, Google Drive, WebDAV, sftp servers, etc.\nLet’s create a small dataset with an annexed file:\ncd ~/tmp\nchmod -R u+wX publish && /bin/rm -r publish\n\ndatalad create --description \"published dataset\" -c text2git publish\ncd publish\ndd if=/dev/urandom of=test1 bs=1024 count=$(( RANDOM + 1024 ))\ndatalad save -m \"added test1\"\nNext, we can set up a special remote on the Alliance’s Nextcloud service. DataLad talks to special remotes via rclone protocol, so we need to install it (along with git-annex-remote-rclone utility) and then configure an rclone remote of type WebDAV:\nbrew install rclone\nbrew install git-annex-remote-rclone\nrclone config\n  new remote\n  Name: nextcloud\n  Type of storage: 46 / WebDAV\n  URL: https://nextcloud.computecanada.ca/remote.php/webdav/\n  Vendor: 1 / Nextcloud\n  User name: razoumov\n  Password: type and confirm your password\n  no bearer_token\n  no advanced config\n  keep this remote\n  quit\nInside our dataset we set a nextcloud remote on which we’ll write into the directory annexedData:\ngit annex initremote nextcloud type=external externaltype=rclone encryption=none target=nextcloud prefix=annexedData\ngit remote -v\ndatalad siblings\ndatalad push --to nextcloud --data anything\nIf you want to share your annexedData folder with another CCDB user, log in to https://nextcloud.computecanada.ca with your CC credentials, click “share” on annexedData, then optionally type in the name/username of the user to share with.\nNext, we publish on dataset on GitHub. The following command creates an empty repository called testPublish on GitHub and sets a publication dependency: all new annexed content will automatically go to Nextcloud when we push to GitHub.\ndatalad create-sibling-github -d . testPublish --publish-depends nextcloud\ndatalad siblings   # +/- indicates the presence/absence of a remote data annex at this remote\ndatalad push --to github\ndd if=/dev/urandom of=test2 bs=1024 count=$(( RANDOM + 1024 ))\ndatalad save -m \"added test2\"\ndatalad push --to github   # automatically pushes test2 to nextcloud!\nImagine we are another user trying to download the dataset. In this demo I will use the same credentials, but in principle this could be another researcher (at least for reading only):\nuser001\nmodule load git-annex   # need this each time you use DalaLad\nalias datalad=/project/def-sponsor00/shared/datalad-env/bin/datalad   # best to add this line to your ~/.bashrc file\ndatalad clone https://github.com/razoumov/testPublish.git publish     # note that access to nextcloud is not enabled yet\ncd publish\ndu -s .                       # the annexed file is not here\ngit annex whereis --in=here   # no annexed file stored locally\ngit annex whereis test*       # two copies: \"published dataset\" and nextcloud\ndatalad update --how merge    # if you need to update the local copy (analogue of `git pull`)\n\nrclone config   # set up exactly the same configuration as before\ndatalad siblings -d . enable --name nextcloud   # enable access to this special remote\ndatalad siblings                                # should now see nextcloud\ndatalad get test1\ngit annex whereis --in=here                     # now we have a local copy\n\ndd if=/dev/urandom of=test3 bs=1024 count=$(( RANDOM + 1024 ))\ndatalad save -m \"added test3\"\ndatalad push --to origin              # push non-annexed files to GitHub\n\ndatalad push --to nextcloud           # push annexed files\ndatalad push --to origin              # update GitHub of this\nBack in the original “published dataset” on my laptop:\ndatalad update --how merge\nls                        # now can see test3\ndatalad get test3\ngit annex whereis test3   # it is here"
  },
  {
    "objectID": "datalad.html#publish-a-dataset-on-github-with-publicly-accessible-annexed-files-on-nextcloud",
    "href": "datalad.html#publish-a-dataset-on-github-with-publicly-accessible-annexed-files-on-nextcloud",
    "title": "Distributed datasets with DataLad",
    "section": "(4) publish a dataset on GitHub with publicly-accessible annexed files on Nextcloud",
    "text": "(4) publish a dataset on GitHub with publicly-accessible annexed files on Nextcloud\nStarting from scratch, let’s push some files to\ncd ~/tmp\nchmod -R u+wX publish && /bin/rm -r publish\n\ndd if=/dev/urandom of=test1 bs=1024 count=$(( RANDOM + 1024 ))\nrclone copy test1 nextcloud:    # works since we've already set up the `nextcloud` remote in rclone\nLog in to https://nextcloud.computecanada.ca with your CC credentials, on test1 click “share” followed by “share link” and “copy link”. Add /download to the copied link to form something like https://nextcloud.computecanada.ca/index.php/s/YeyNrjJfpQQ7WTq/download.\ndatalad create --description \"published dataset\" -c text2git publish\ncd publish\n\ncat &lt;&lt; EOF &gt; list.csv\nfile,link\ntest1,https://nextcloud.computecanada.ca/index.php/s/YeyNrjJfpQQ7WTq/download\nEOF\n\ndatalad addurls --fast list.csv '{link}' '{file}'   # --fast means do not download, just add URL\ngit annex whereis test1   # one copy (web)\nLater, when needed, we can download this file with datalad get test1.\ndatalad create-sibling-github -d . testPublish2   # create am empty repo on GitHub\ndatalad siblings   # +/- indicates the presence/absence of a remote data annex at this remote\ndatalad push --to github\n\nuser001\nmodule load git-annex   # need this each time you use DalaLad\nalias datalad=/project/def-sponsor00/shared/datalad-env/bin/datalad   # best to add this line to your ~/.bashrc file\nchmod -R u+wX publish && /bin/rm -r publish\ndatalad clone https://github.com/razoumov/testPublish2.git publish    # \"remote origin not usable by git-annex\"\ncd publish\ngit annex whereis test1   # one copy (web)\ndatalad get test1\ngit annex whereis test1   # now we have a local copy"
  },
  {
    "objectID": "datalad.html#if-we-have-time-manage-multiple-git-repos-under-one-dataset",
    "href": "datalad.html#if-we-have-time-manage-multiple-git-repos-under-one-dataset",
    "title": "Distributed datasets with DataLad",
    "section": "(5) if we have time: manage multiple Git repos under one dataset",
    "text": "(5) if we have time: manage multiple Git repos under one dataset\nCreate a new dataset and inside clone a couple of subdatasets:\ncd ~/tmp\ndatalad create -c text2git envelope\ncd envelope\n# let's clone few regular Git (not DataLad!) repos\ndatalad clone --dataset . https://github.com/razoumov/radiativeTransfer.git projects/radiativeTransfer\ndatalad clone --dataset . https://github.com/razoumov/sharedSnippets projects/sharedSnippets\ngit log   # can see those two new subdatasets\nGo into one of these subdatasets, modify a file, and commit it to GitHub:\n\ncd projects/sharedSnippets\n&gt;&gt;&gt; add an empty line to mpiContainer.md\ngit status\ngit add mpiContainer.md\ngit commit -m \"added another line to mpiContainer.md\"\ngit push\nThis directory is still a pure GitHub repository, i.e. there no DalaLad files.\nLet’s clone out entire dataset to another location:\ncd ~/tmp\ndatalad install --description \"copy of envelope\" -r -s envelope copy   # `clone` has no recursive option\ncd copy\ncd projects/sharedSnippets\ngit log   # cloned as of the moment of that dataset's creation; no recent update there yet\nRecursively update all child Git repositories:\ngit remote -v     # remote is origin = GitHub\ncd ../..          # to ~/tmp/copy\ngit remote -v     # remote is origin = ../envelope\n# pull recent changes from \"proper origin\" for each subdataset\ndatalad update -s origin --how=merge --recursive\ncd projects/sharedSnippets\ngit log"
  }
]