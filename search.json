[
  {
    "objectID": "chapel-compact.html",
    "href": "chapel-compact.html",
    "title": "Parallel programming in Chapel",
    "section": "",
    "text": "June 12th, 1:30pm-4:30pm Pacific Time\nChapel is a modern programming language designed for both shared and distributed memory systems, offering high-level, easy-to-use abstractions for task and data parallelism. Its intuitive syntax makes it an excellent choice for novice HPC users learning parallel programming. Chapel supports a wide range of parallel hardware – from multicore processors and multi-node clusters to GPUs – using consistent syntax and concepts across all levels of hardware parallelism.\nChapel dramatically reduces the complexity of parallel coding by combining the simplicity of Python-style programming with the performance of compiled languages like C and Fortran. Parallel operations that might require dozens of lines in MPI can often be written in just a few lines of Chapel. As an open-source language, it runs on most Unix-like operating systems and scales from laptops to large HPC systems.\nThis course begins with Chapel fundamentals, then focuses on data parallelism through two numerical examples: one embarrassingly parallel and one tightly coupled. We’ll also briefly explore task parallelism (a more complex topic, and not the primary focus in this course). Finally, we’ll introduce GPU programming with Chapel.\nInstructor: Alex Razoumov (SFU)\nPrerequisites: basic understanding of HPC at the introductory level (how to submit jobs with Slurm scheduler) and basic knowledge of the Linux command line.\nSoftware: For the hands-on, we will use Chapel on our training cluster. To access the training cluster, you will need a remote secure shell (SSH) client installed on your computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there). We will provide guest accounts to all participants. No need to install Chapel on your computer.\n\nSolutions\nYou can find the solutions here.\n\n\nLinks\n\nChapel homepage\nWhat is Chapel? (HPE Developer Portal)\nLinuxCon 2023 Introducing Chapel slides (PDF)\nGetting started guide for Python programmers\nLearn X=Chapel in Y minutes\nChapel on StackOverflow\nWatch Chapel: Productive, Multiresolution Parallel Programming talk by Brad Chamberlain\nWestGrid’s April 2019 webinar Working with distributed unstructured data in Chapel\nWestGrid’s March 2020 webinar Working with data files and external C libraries in Chapel discusses writing arrays to NetCDF and HDF5 files from Chapel",
    "crumbs": [
      "Front page"
    ]
  },
  {
    "objectID": "solutions-plotly.html",
    "href": "solutions-plotly.html",
    "title": "Various courses",
    "section": "",
    "text": "import plotly.graph_objs as go\nfrom numpy import linspace, sin\nx1 = linspace(0.01,1,100)\ny1 = sin(1/x1)\nline1 = go.Scatter(x=x1, y=y1, mode='lines+markers', name='sin(1/x)')\nx2 = linspace(0.4,1,500)\ny2 = 0.3*sin(0.5/(x2-0.39)) - 0.5\nline2 = go.Scatter(x=x2, y=y2, mode='lines+markers', name='0.3*sin(0.5/(x2-0.39))-0.5')\nfig = go.Figure([line1, line2])\nfig.show()\nNow we have two lines in the plot, each in its own colour, along with a legend in the corner!"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-1",
    "href": "solutions-plotly.html#solution-to-exercise-1",
    "title": "Various courses",
    "section": "",
    "text": "import plotly.graph_objs as go\nfrom numpy import linspace, sin\nx1 = linspace(0.01,1,100)\ny1 = sin(1/x1)\nline1 = go.Scatter(x=x1, y=y1, mode='lines+markers', name='sin(1/x)')\nx2 = linspace(0.4,1,500)\ny2 = 0.3*sin(0.5/(x2-0.39)) - 0.5\nline2 = go.Scatter(x=x2, y=y2, mode='lines+markers', name='0.3*sin(0.5/(x2-0.39))-0.5')\nfig = go.Figure([line1, line2])\nfig.show()\nNow we have two lines in the plot, each in its own colour, along with a legend in the corner!"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-2",
    "href": "solutions-plotly.html#solution-to-exercise-2",
    "title": "Various courses",
    "section": "Solution to Exercise 2",
    "text": "Solution to Exercise 2\nThe default mode is ‘lines+markers’ as you can see from the plot. You’ll need to update the list to [line1, line2, dots]. You can see that we don’t need numpy arrays for data: dots just has two lists of numbers."
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-3",
    "href": "solutions-plotly.html#solution-to-exercise-3",
    "title": "Various courses",
    "section": "Solution to Exercise 3",
    "text": "Solution to Exercise 3\ndots = go.Scatter(x=[.2,.4,.6,.8], y=[2,1.5,2,1.2], line=dict(color=('rgb(10,205,24)'),width=4))\nfig = go.Figure([line1, line2, dots])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-3b",
    "href": "solutions-plotly.html#solution-to-exercise-3b",
    "title": "Various courses",
    "section": "Solution to Exercise 3b",
    "text": "Solution to Exercise 3b\nimport plotly.graph_objs as go\nimport numpy as np\nn = 300\nopacity = np.random.rand(n)\nsc = go.Scatter(x=np.random.rand(n), y=np.random.rand(n), mode='markers',\n                marker=dict(color='rgb(0,0,255)', opacity=(1-opacity), size=80*opacity))\nfig = go.Figure(data=[sc])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-3c",
    "href": "solutions-plotly.html#solution-to-exercise-3c",
    "title": "Various courses",
    "section": "Solution to Exercise 3c",
    "text": "Solution to Exercise 3c\nheatmap = go.Heatmap(z=data, x=months, y=yticks, colorscale='Viridis')\nfig = go.Figure([heatmap])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-4",
    "href": "solutions-plotly.html#solution-to-exercise-4",
    "title": "Various courses",
    "section": "Solution to Exercise 4",
    "text": "Solution to Exercise 4\ndata = [recordHigh[:-1], averageHigh[:-1], dailyMean[:-1], averageLow[:-1], recordLow[:-1]]\nyticks = ['record high', 'aver.high', 'daily mean', 'aver.low', 'record low']\nheatmap = go.Contour(z=data, x=months[:-1], y=yticks)\nfig = go.Figure([heatmap])\nfig.show()"
  },
  {
    "objectID": "solutions-plotly.html#solution-to-exercise-5",
    "href": "solutions-plotly.html#solution-to-exercise-5",
    "title": "Various courses",
    "section": "Solution to Exercise 5",
    "text": "Solution to Exercise 5\nImmediately fater reading the dataframe from the file, ad the following:\ndf.sort_values(by='pop', ascending=False, inplace=True)\ndf = df.iloc[:10]"
  },
  {
    "objectID": "solutions-plotly.html#exercise-6",
    "href": "solutions-plotly.html#exercise-6",
    "title": "Various courses",
    "section": "Exercise 6",
    "text": "Exercise 6\nOne possible solution is with lambda-functions, with i=0..n-1,     j=0..n-1,     x=i/(n-1)=0..1,     y=j/(n-1)=0..1:\nimport plotly.graph_objs as go\nfrom numpy import fromfunction, sin, pi\nn = 100   # plot resolution\nF = fromfunction(lambda i, j: (1-j/(n-1))*sin(pi*i/(n-1)) + \\\n                 j/(n-1)*(sin(2*pi*i/(n-1)))**2, (n,n), dtype=float)\nsurface = go.Surface(z=F)\nlayout = go.Layout(width=1200, height=1200)\nfig = go.Figure([surface], layout=layout)\nfig.show()\nIf you don’t like lambda-functions, you can replace F = fromfunction(...) line with:\nfrom numpy import zeros, linspace\nF = zeros((n,n))\nfor i, x in enumerate(linspace(0,1,n)):\n    for j, y in enumerate(linspace(0,1,n)):\n        F[i,j] = (1-y)*sin(pi*x) + y*(sin(2*pi*x))**2\nAs a third option, you can use array operations to compute f:\nfrom numpy import linspace, meshgrid\nx = linspace(0,1,n)\ny = linspace(0,1,n)\nY, X = meshgrid(x, y)   # meshgrid() returns two 2D arrays storing x/y respectively at each point\nF = (1-Y)*sin(pi*X) + Y*(sin(2*pi*X))**2   # array operation\nIf we want to scale the z-range, we can add scene=go.layout.Scene(zaxis=go.ZAxis(range=[-1,2])) inside go.Layout()."
  },
  {
    "objectID": "plotly.html#links",
    "href": "plotly.html#links",
    "title": "Plot.ly for scientific visualization",
    "section": "Links",
    "text": "Links\n\nOnline gallery with code examples by category\nPlotly Express tutorial, also plotly.express\nPlotly Community Forum\nKeyword index\nGetting started guide\nPlotly vs matplotlib (with video)\nDisplaying Figures (including offscreen into a file)\nSaving static images (PNG, PDF, etc)\nPlotly and IPython / Jupyter notebook with additional plotting examples"
  },
  {
    "objectID": "plotly.html#installation",
    "href": "plotly.html#installation",
    "title": "Plot.ly for scientific visualization",
    "section": "Installation",
    "text": "Installation\n\nYou will need Python 3, along with some Python package manager\nUse your favourite installer:\n\n$ pip install plotly\n$ uv pip install plotly\n$ conda install -c conda-forge plotly\n\nOther recommended libraries to install for today’s session: jupyter, numpy, pandas, networkx, scikit-image, kaleido"
  },
  {
    "objectID": "plotly.html#displaying-plotly-figures",
    "href": "plotly.html#displaying-plotly-figures",
    "title": "Plot.ly for scientific visualization",
    "section": "Displaying Plotly figures",
    "text": "Displaying Plotly figures\n\n\n\nWith Plotly, you can: 1. work inside a Python shell, 2. save your script into a *.py file and then run it, or 3. run code inside a Jupyter Notebook (start a notebook with jupyter notebook or even better jupyter lab).\nPlotly supports a number of renderers, and it will attempt to choose an appropriate renderer automatically (in my experience, not very successfully). You can examine the selected default renderer with:\nimport plotly.io as pio\npio.renderers   # show default and available renderers\nYou can overwrite the default by setting it manually inside your session or inside your code, e.g.\npio.renderers.default = 'browser'    # open each plot in a new browser tab\npio.renderers.default = 'notebook'   # plot inside a Jupyter notebook\nIf you want to have this setting persistent across sessions (and not set it manually or in the code), you can create a file ~/.plotly_startup.py with the following:\ntry:\n    import plotly.io as pio\n    pio.renderers.default = \"browser\"\nexcept ImportError:\n    pass\nand set export PYTHONSTARTUP=~/.plotly_startup.py in your ~/.bashrc file.\n\n\n\n\n\nLet’s create a simple line plot:\nimport plotly.graph_objs as go\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\ny = sin(1/x)\nline = go.Scatter(x=x, y=y, mode='lines+markers', name='sin(1/x)')\nfig = go.Figure([line])\nfig.show()   # should open in your browser\n\nfig.write_image(\"/Users/razoumov/tmp/lines.png\", scale=2)   # static, supports svg, png, jpg/jpeg, webp, pdf\nfig.write_html(\"/Users/razoumov/tmp/lines.html\")            # interactive\nfig.write_json(\"/Users/razoumov/tmp/2007.json\")             # for further editing\nIn general, use fig.show() when working outside of a Jupyter Notebook, or when you want to save your plot to a file. If you want to display plots inline inside a Jupyter notebook, set pio.renderers.default = 'notebook' and use the command\ngo.Figure([line])\nthat should display plots right inside the notebook, without a need for fig.show().\nYou can find more details at https://plotly.com/python/renderers ."
  },
  {
    "objectID": "plotly.html#plotly-express-data-exploration-library",
    "href": "plotly.html#plotly-express-data-exploration-library",
    "title": "Plot.ly for scientific visualization",
    "section": "Plotly Express (data exploration) library",
    "text": "Plotly Express (data exploration) library\nNormally, in this workshop I would teach plotly.graph_objs (Graph Objects) which is the standard module in Plotly.py – you saw its example in the previous section.\nPlotly Express is a higher-level interface to Plotly.py that sits on top of Graph Objects and provides 30+ functions for creating different types of figures in a single function call. It works with NumPy arrays, Xarrays, Pandas dataframes, basic Python iterables, etc.\n\nHere is one way to create a line plot from above in Plotly Express, using just NumPy arrays:\nimport plotly.express as px\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\ny = sin(1/x)\nfig = px.line(x=x, y=y, markers=True)\nfig.show()\nYou can also use feed a dataframe into the plotting function:\nimport plotly.express as px\nfrom numpy import linspace, sin\nimport pandas as pd\nx = linspace(0.01,1,100)\ndf = pd.DataFrame({'col1': x, 'col2': sin(1/x)})\nfig = px.line(df, x='col1', y='col2', markers=True)\nfig.show()\nOr you can feed a dictionary:\nimport plotly.express as px\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\nd = {'key1': x, 'key2': sin(1/x)}\nfig = px.line(d, x='key1', y='key2', markers=True)\nfig.show()\nTo see Plotly Express really shine, we should play with a slightly larger dataset containing several variables. The module px.data comes with several datasets included. Let’s take a look at the Gapminder data that contains one row per country per year.\nimport plotly.express as px\ndf = px.data.gapminder().query(\"year==2007\")\n\npx.line(df, x=\"gdpPercap\", y=\"lifeExp\", markers=True)   # this should be familiar\n# 1. replace df with df.sort_values(by='gdpPercap')\n# 2. add log_x=True\n# 3. change line to scatter, remove markers=True\n# 4. don't actually need to sort now, with no markers\n# 5. add hover_name=\"country\"\n# 6. add size=\"pop\"\n# 7. add size_max=60\n# 8. add color=\"continent\" - can now turn continents off/on\n\npx.strip(df, x=\"lifeExp\")   # single-axis scatter plot\n# 1. add hover_name=\"country\"\n# 2. add color=\"continent\"\n# 3. change strip to histogram\n# 4. can turn continents off/on in the legend\n# 5. add marginal=\"rug\" to show countries in a rug plot\n# 6. add y=\"pop\" to switch from country count to population along the vertical axis\n# 7. add facet_col=\"continent\" to break continents into facet columns\n\npx.bar(df, color=\"lifeExp\", x=\"pop\", y=\"continent\", hover_name=\"country\")\n\npx.sunburst(df, color=\"lifeExp\", values=\"pop\", path=[\"continent\", \"country\"],\n            hover_name=\"country\", height=800)\n\npx.treemap(df, color=\"lifeExp\", values=\"pop\", path=[\"continent\", \"country\"],\n           hover_name=\"country\", height=500)\n\npx.choropleth(df, color=\"lifeExp\", locations=\"iso_alpha\", hover_name=\"country\", height=580)\n\n\n\n\n\nHere is an ternary plot example with Montreal elections data (58 electoral districts, 3 candidates):\ndf = px.data.election()\npx.scatter_ternary(df, a=\"Joly\", b=\"Coderre\", c=\"Bergeron\", color=\"winner\",\n                   size=\"total\", hover_name=\"district\", size_max=15,\n                   color_discrete_map={\"Joly\": \"blue\", \"Bergeron\": \"green\", \"Coderre\": \"red\"})"
  },
  {
    "objectID": "plotly.html#plotting-via-graph-objects",
    "href": "plotly.html#plotting-via-graph-objects",
    "title": "Plot.ly for scientific visualization",
    "section": "Plotting via Graph Objects",
    "text": "Plotting via Graph Objects\nWhile Plotly Express is excellent for quick data exploration, it has some limitations: it supports fewer plot types and does not allow combining different plot types directly in a single figure. Plotly Graph Objects, on the other hand, is a lower-level library that offers greater functionality and customization with layouts. In this section, we’ll explore Graph Objects, starting with 2D plots.\nGraph Objects supports many plot types:\nimport plotly.graph_objs as go\ndir(go)\n['AngularAxis', 'Annotation', 'Annotations', 'Bar', 'Barpolar', 'Box', 'Candlestick', 'Carpet', 'Choropleth', 'Choroplethmap', 'Choroplethmapbox', 'ColorBar', 'Cone', 'Contour', 'Contourcarpet', 'Contours', 'Data', 'Densitymap', 'Densitymapbox', 'ErrorX', 'ErrorY', 'ErrorZ', 'Figure', 'FigureWidget', 'Font', 'Frame', 'Frames', 'Funnel', 'Funnelarea', 'Heatmap', 'Histogram', 'Histogram2d', 'Histogram2dContour', 'Histogram2dcontour', 'Icicle', 'Image', 'Indicator', 'Isosurface', 'Layout', 'Legend', 'Line', 'Margin', 'Marker', 'Mesh3d', 'Ohlc', 'Parcats', 'Parcoords', 'Pie', 'RadialAxis', 'Sankey', 'Scatter', 'Scatter3d', 'Scattercarpet', 'Scattergeo', 'Scattergl', 'Scattermap', 'Scattermapbox', 'Scatterpolar', 'Scatterpolargl', 'Scattersmith', 'Scatterternary', 'Scene', 'Splom', 'Stream', 'Streamtube', 'Sunburst', 'Surface', 'Table', 'Trace', 'Treemap', 'Violin', 'Volume', 'Waterfall', 'XAxis', 'XBins', 'YAxis', 'YBins', 'ZAxis', 'bar', 'barpolar', 'box', 'candlestick', 'carpet', 'choropleth', 'choroplethmap', 'choroplethmapbox', 'cone', 'contour', 'contourcarpet', 'densitymap', 'densitymapbox', 'funnel', 'funnelarea', 'heatmap', 'histogram', 'histogram2d', 'histogram2dcontour', 'icicle', 'image', 'indicator', 'isosurface', 'layout', 'mesh3d', 'ohlc', 'parcats', 'parcoords', 'pie', 'sankey', 'scatter', 'scatter3d', 'scattercarpet', 'scattergeo', 'scattergl', 'scattermap', 'scattermapbox', 'scatterpolar', 'scatterpolargl', 'scattersmith', 'scatterternary', 'splom', 'streamtube', 'sunburst', 'surface', 'table', 'treemap', 'violin', 'volume', 'waterfall']\n\nScatter plots\nWe already saw an example of a Scatter plot with Graph Objects:\nimport plotly.graph_objs as go\nfrom numpy import linspace, sin\nx = linspace(0.01,1,100)\ny = sin(1/x)\nline = go.Scatter(x=x, y=y, mode='lines+markers', name='sin(1/x)')\ngo.Figure([line])\nLet’s print the dataset line:\ntype(line)\nprint(line)\nIt is a plotly object which is actually a Python dictionary, with all elements clearly identified (plot type, x numpy array, y numpy array, line type, legend line name). So, go.Scatter simply creates a dictionary with the corresponding type element. This variable/dataset line completely describes our plot!* Then we create a list of such objects and pass it to the plotting routine.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\nPass a list of two objects to the plotting routine with data = [line1,line2]. Let the second dataset line2 contain another mathematical function. The idea is to have multiple objects in the plot.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHovering over each data point will reveal their coordinates. Use the toolbar at the top. Double-clicking on the plot will reset it.\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nAdd a bunch of dots to the plot with dots = go.Scatter(x=[.2,.4,.6,.8], y=[2,1.5,2,1.2]). What is default scatter mode?\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\nChange line colour and width by adding the dictionary line=dict(color=('rgb(10,205,24)'),width=4) to dots.\n\n\n\n\n\n\n\n\n\nExercise 3b\n\n\n\n\n\nCreate a scatter plot of 300 random filled blue circles inside a unit square. Their random opacity must anti-correlate with their size (bigger circles should be more transparent) – see the plot below.\n\n\n\n\n\nBar plots\nLet’s try a Bar plot, constructing data directly in one line from the dictionary:\nimport plotly.graph_objs as go\nbar = go.Bar(x=['Vancouver', 'Calgary', 'Toronto', 'Montreal', 'Halifax'],\n               y=[2463431, 1392609, 5928040, 4098927, 403131])\nfig = go.Figure(data=[bar])\nfig.show()\nLet’s plot inner city population vs. greater metro area for each city:\nimport plotly.graph_objs as go\ncities = ['Vancouver', 'Calgary', 'Toronto', 'Montreal', 'Halifax']\nproper = [662_248, 1_306_784, 2_794_356, 1_762_949, 439_819]\nmetro = [3_108_926, 1_688_000, 6_491_000, 4_615_154, 530_167]\nbar1 = go.Bar(x=cities, y=proper, name='inner city')\nbar2 = go.Bar(x=cities, y=metro, name='greater area')\nfig = go.Figure(data=[bar1,bar2])\nfig.show()\nLet’s now do a stacked plot, with outer city population on top of inner city population:\noutside = [m-p for p,m in zip(proper,metro)]   # need to subtract\nbar1 = go.Bar(x=cities, y=proper, name='inner city')\nbar2 = go.Bar(x=cities, y=outside, name='outer city')\nfig = go.Figure(data=[bar1,bar2], layout=go.Layout(barmode='stack'))   # new element!\nfig.show()\nWhat else can we modify in the layout?\nhelp(go.Layout)\nThere are many attributes!\n\n\nHeatmaps\n\ngo.Area() for plotting wind rose charts\ngo.Box() for basic box plots\n\nLet’s plot a heatmap of monthly temperatures at the South Pole:\nimport plotly.graph_objs as go\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Year']\nrecordHigh = [-14.4,-20.6,-26.7,-27.8,-25.1,-28.8,-33.9,-32.8,-29.3,-25.1,-18.9,-12.3,-12.3]\naverageHigh = [-26.0,-37.9,-49.6,-53.0,-53.6,-54.5,-55.2,-54.9,-54.4,-48.4,-36.2,-26.3,-45.8]\ndailyMean = [-28.4,-40.9,-53.7,-57.8,-58.0,-58.9,-59.8,-59.7,-59.1,-51.6,-38.2,-28.0,-49.5]\naverageLow = [-29.6,-43.1,-56.8,-60.9,-61.5,-62.8,-63.4,-63.2,-61.7,-54.3,-40.1,-29.1,-52.2]\nrecordLow = [-41.1,-58.9,-71.1,-75.0,-78.3,-82.8,-80.6,-79.3,-79.4,-72.0,-55.0,-41.1,-82.8]\ndata = [recordHigh, averageHigh, dailyMean, averageLow, recordLow]\nyticks = ['record high', 'aver.high', 'daily mean', 'aver.low', 'record low']\nheatmap = go.Heatmap(z=data, x=months, y=yticks)\nfig = go.Figure([heatmap])\nfig.show()\n\n\n\n\n\n\nExercise 3c\n\n\n\n\n\nTry a few different colourmaps, e.g. ‘Viridis’, ‘Jet’, ‘Rainbow’. What colourmaps are available?\n\n\n\n\n\nContour maps\n\n\n\n\n\n\nExercise 4\n\n\n\n\n\nPretend that our heatmap is defined over a 2D domain and plot the same temperature data as a contour map. Remove the Year data (last column) and use go.Contour to plot the 2D contour map.\n\n\n\n\n\nDownload data\nOpen a terminal window inside Jupyter (New | Terminal) and run these commands:\nwget https://tinyurl.com/pvzip -O paraview.zip\nunzip paraview.zip\nmv data/*.{csv,nc} .\n\n\nGeographical scatterplot\nGo back to your Python Jupyter Notebook. Now let’s do a scatterplot on top of a geographical map:\nimport plotly.graph_objs as go\nimport pandas as pd\nfrom math import log10\ndf = pd.read_csv('cities.csv')   # lists name,pop,lat,lon for 254 Canadian cities and towns\ndf['text'] = df['name'] + '&lt;br&gt;Population ' + \\\n             (df['pop']/1e6).astype(str) +' million' # add new column for mouse-over\n\nlargest, smallest = df['pop'].max(), df['pop'].min()\ndef normalize(x):\n    return log10(x/smallest)/log10(largest/smallest)   # x scaled into [0,1]\n\ndf['logsize'] = round(df['pop'].apply(normalize)*255)   # new column\ncities = go.Scattergeo(\n    lon = df['lon'], lat = df['lat'], text = df['text'],\n    marker = dict(\n        size = df['pop']/5000,\n        color = df['logsize'],\n        colorscale = 'Viridis',\n        showscale = True,   # show the colourbar\n        line = dict(width=0.5, color='rgb(40,40,40)'),\n        sizemode = 'area'))\nlayout = go.Layout(title = 'City populations',\n                       showlegend = False,   # do not show legend for first plot\n                       geo = dict(\n                           scope = 'north america',\n                           resolution = 50,   # base layer resolution of km/mm\n                           lonaxis = dict(range=[-130,-55]), lataxis = dict(range=[44,70]), # plot range\n                           showland = True, landcolor = 'rgb(217,217,217)',\n                           showrivers = True, rivercolor = 'rgb(153,204,255)',\n                           showlakes = True, lakecolor = 'rgb(153,204,255)',\n                           subunitwidth = 1, subunitcolor = \"rgb(255,255,255)\",   # province border\n                           countrywidth = 2, countrycolor = \"rgb(255,255,255)\"))  # country border\nfig = go.Figure(data=[cities], layout=layout)\nfig.show()\n\n\n\n\n\n\nExercise 5\n\n\n\n\n\nModify the code to display only 10 largest cities.\n\n\n\nRecall how we combined several scatter plots in one figure before. You can combine several plots on top of a single map – let’s combine scattergeo + choropleth:\nimport plotly.graph_objs as go\nimport pandas as pd\ndf = pd.read_csv('cities.csv')\ndf['text'] = df['name'] + '&lt;br&gt;Population ' + \\\n             (df['pop']/1e6).astype(str)+' million' # add new column for mouse-over\ncities = go.Scattergeo(lon = df['lon'],\n                       lat = df['lat'],\n                       text = df['text'],\n                       marker = dict(\n                           size = df['pop']/5000,\n                           color = \"lightblue\",\n                           line = dict(width=0.5, color='rgb(40,40,40)'),\n                           sizemode = 'area'))\ngdp = pd.read_csv('gdp.csv')   # read name, gdp, code for 222 countries\nc1 = [0,\"rgb(5, 10, 172)\"]     # define colourbar from top (0) to bottom (1)\nc2, c3 = [0.35,\"rgb(40, 60, 190)\"], [0.5,\"rgb(70, 100, 245)\"]\nc4, c5 = [0.6,\"rgb(90, 120, 245)\"], [0.7,\"rgb(106, 137, 247)\"]\nc6 = [1,\"rgb(220, 220, 220)\"]\ncountries = go.Choropleth(locations = gdp['CODE'],\n                          z = gdp['GDP (BILLIONS)'],\n                          text = gdp['COUNTRY'],\n                          colorscale = [c1,c2,c3,c4,c5,c6],\n                          autocolorscale = False,\n                          reversescale = True,\n                          marker = dict(line = dict(color='rgb(180,180,180)',width = 0.5)),\n                          zmin = 0,\n                          colorbar = dict(tickprefix = '$',title = 'GDP&lt;br&gt;Billions US$'))\nlayout = go.Layout(hovermode = \"x\", showlegend = False)  # do not show legend for first plot\nfig = go.Figure(data=[cities,countries], layout=layout)\nfig.show()\n\n\n3D Topographic elevation\nLet’s plot some tabulated topographic elevation data:\nimport plotly.graph_objs as go\nimport pandas as pd\ntable = pd.read_csv('mt_bruno_elevation.csv')\nsurface = go.Surface(z=table.values)  # use 2D numpy array format\nlayout = go.Layout(title='Mt Bruno Elevation',\n                   width=1200, height=1200,    # image size\n                   margin=dict(l=65, r=10, b=65, t=90))   # margins around the plot\nfig = go.Figure([surface], layout=layout)\nfig.show()\n\n\n\n\n\n\nExercise 6\n\n\n\n\n\nPlot a 2D function f(x,y) = (1−y) sin(πx) + y sin^2(2πx), where x,y ∈ [0,1] on a 100^2 grid.\n\n\n\n\n\nElevated 2D functions\nLet’s define a different colourmap by adding colorscale='Viridis' inside go.Surface(). This is our current code:\nimport plotly.graph_objs as go\nfrom numpy import *\nn = 100   # plot resolution\nx = linspace(0,1,n)\ny = linspace(0,1,n)\nY, X = meshgrid(x, y)   # meshgrid() returns two 2D arrays storing x/y respectively at each mesh point\nF = (1-Y)*sin(pi*X) + Y*(sin(2*pi*X))**2   # array operation\ndata = go.Surface(z=F, colorscale='Viridis')\nlayout = go.Layout(width=1000, height=1000, scene=go.layout.Scene(zaxis=go.layout.scene.ZAxis(range=[-1,2])));\nfig = go.Figure(data=[data], layout=layout)\nfig.show()\n\n\nLighting control\nLet’s change the default light in the room by adding lighting=dict(ambient=0.1) inside go.Surface(). Now our plot is much darker!\n\nambient controls the light in the room (default = 0.8)\nroughness controls amount of light scattered (default = 0.5)\ndiffuse controls the reflection angle width (default = 0.8)\nfresnel controls light washout (default = 0.2)\nspecular induces bright spots (default = 0.05)\n\nLet’s try lighting=dict(ambient=0.1,specular=0.3) – now we have lots of specular light!\n\n\n3D parametric plots\nIn plotly documentation you can find quite a lot of different 3D plot types. Here is something visually very different, but it still uses go.Surface(x,y,z):\nimport plotly.graph_objs as go\nfrom numpy import pi, sin, cos, mgrid\ndphi, dtheta = pi/250, pi/250    # 0.72 degrees\n[phi, theta] = mgrid[0:pi+dphi*1.5:dphi, 0:2*pi+dtheta*1.5:dtheta]\n        # define two 2D grids: both phi and theta are (252,502) numpy arrays\nr = sin(4*phi)**3 + cos(2*phi)**3 + sin(6*theta)**2 + cos(6*theta)**4\nx = r*sin(phi)*cos(theta)   # x is also (252,502)\ny = r*cos(phi)              # y is also (252,502)\nz = r*sin(phi)*sin(theta)   # z is also (252,502)\nsurface = go.Surface(x=x, y=y, z=z, colorscale='Viridis')\nlayout = go.Layout(title='parametric plot')\nfig = go.Figure(data=[surface], layout=layout)\nfig.show()\n\n\n3D scatter plots\nLet’s take a look at a 3D scatter plot using the country index data from http://www.prosperity.com for 142 countries:\nimport plotly.graph_objs as go\nimport pandas as pd\ndf = pd.read_csv('legatum2015.csv')\nspheres = go.Scatter3d(x=df.economy,\n                       y=df.entrepreneurshipOpportunity,\n                       z=df.governance,\n                       text=df.country,\n                       mode='markers',\n                       marker=dict(\n                           sizemode = 'diameter',\n                           sizeref = 0.3,   # max(safetySecurity+5.5) / 32\n                           size = df.safetySecurity+5.5,\n                           color = df.education,\n                           colorscale = 'Viridis',\n                           colorbar = dict(title = 'Education'),\n                           line = dict(color='rgb(140, 140, 170)')))   # sphere edge\nlayout = go.Layout(height=900, width=900,\n                   title='Each sphere is a country sized by safetySecurity',\n                   scene = dict(xaxis=dict(title='economy'),\n                                yaxis=dict(title='entrepreneurshipOpportunity'),\n                                zaxis=dict(title='governance')))\nfig = go.Figure(data=[spheres], layout=layout)\nfig.show()\n\n\n3D graphs\nWe can plot 3D graphs. Consider a Dorogovtsev-Goltsev-Mendes graph: in each subsequent generation, every edge from the previous generation yields a new node, and the new graph can be made by connecting together three previous-generation graphs.\nimport plotly.graph_objs as go\nimport networkx as nx\nimport sys\ngeneration = 5\nH = nx.dorogovtsev_goltsev_mendes_graph(generation)\nprint(H.number_of_nodes(), 'nodes and', H.number_of_edges(), 'edges')\n# Force Atlas 2 graph layout from https://github.com/tpoisot/nxfa2.git\npos = nx.spectral_layout(H,scale=1,dim=3)\nXn = [pos[i][0] for i in pos]   # x-coordinates of all nodes\nYn = [pos[i][1] for i in pos]   # y-coordinates of all nodes\nZn = [pos[i][2] for i in pos]   # z-coordinates of all nodes\nXe, Ye, Ze = [], [], []\nfor edge in H.edges():\n    Xe += [pos[edge[0]][0], pos[edge[1]][0], None]   # x-coordinates of all edge ends\n    Ye += [pos[edge[0]][1], pos[edge[1]][1], None]   # y-coordinates of all edge ends\n    Ze += [pos[edge[0]][2], pos[edge[1]][2], None]   # z-coordinates of all edge ends\n\ndegree = [deg[1] for deg in H.degree()]   # list of degrees of all nodes\nlabels = [str(i) for i in range(H.number_of_nodes())]\nedges = go.Scatter3d(x=Xe, y=Ye, z=Ze,\n                     mode='lines',\n                     marker=dict(size=12,line=dict(color='rgba(217, 217, 217, 0.14)',width=0.5)),\n                     hoverinfo='none')\nnodes = go.Scatter3d(x=Xn, y=Yn, z=Zn,\n                     mode='markers',\n                     marker=dict(sizemode = 'area',\n                                 sizeref = 0.01, size=degree,\n                                 color=degree, colorscale='Viridis',\n                                 line=dict(color='rgb(50,50,50)', width=0.5)),\n                     text=labels, hoverinfo='text')\n\naxis = dict(showline=False, zeroline=False, showgrid=False, showticklabels=False, title='')\nlayout = go.Layout(\n    title = str(generation) + \"-generation Dorogovtsev-Goltsev-Mendes graph\",\n    width=1000, height=1000,\n    showlegend=False,\n    scene=dict(xaxis=go.layout.scene.XAxis(axis),\n               yaxis=go.layout.scene.YAxis(axis),\n               zaxis=go.layout.scene.ZAxis(axis)),\n    margin=go.layout.Margin(t=100))\nfig = go.Figure(data=[edges,nodes], layout=layout)\nfig.show()\n\n\n3D functions\nLet’s create an isosurface of a decoCube function at f=0.03. Isosurfaces are returned as a list of polygons, and for plotting polygons in plotly we need to use plotly.figure_factory.create_trisurf() which replaces plotly.graph_objs.Figure():\nfrom plotly import figure_factory as FF\nfrom numpy import mgrid\nfrom skimage import measure\nX,Y,Z = mgrid[-1.2:1.2:30j, -1.2:1.2:30j, -1.2:1.2:30j] # three 30^3 grids, each side [-1.2,1.2] in 30 steps\nF = ((X*X+Y*Y-0.64)**2 + (Z*Z-1)**2) * \\\n    ((Y*Y+Z*Z-0.64)**2 + (X*X-1)**2) * \\\n    ((Z*Z+X*X-0.64)**2 + (Y*Y-1)**2)\nvertices, triangles, normals, values = measure.marching_cubes(F, 0.03)  # create an isosurface\nx,y,z = zip(*vertices)   # zip(*...) is opposite of zip(...): unzips a list of tuples\nfig = FF.create_trisurf(x=x, y=y, z=z, plot_edges=False,\n                        simplices=triangles, title=\"Isosurface\", height=1200, width=1200)\nfig.show()\nTry switching plot_edges=False to plot_edges=True – you’ll see individual polygons!"
  },
  {
    "objectID": "plotly.html#dash-library-for-making-interactive-web-applications",
    "href": "plotly.html#dash-library-for-making-interactive-web-applications",
    "title": "Plot.ly for scientific visualization",
    "section": "Dash library for making interactive web applications",
    "text": "Dash library for making interactive web applications\nPlotly Dash library is a framework for making interactive data applications.\n\nDash Python User Guide https://dash.plotly.com\nhttps://dash.gallery/Portal has ~100 app examples\n\n\ncan create a dropdown to select data to plot\ncan enter a value into a box to select or interpolate data to plot\nselection in one plot shows in the other plot\nmix and match these into a single web app\ncan create different tabs inside the app, with the render switching between them\ncan make entire website with user guides, plots, code examples, etc."
  },
  {
    "objectID": "chapel2/chapel-07-timing.html",
    "href": "chapel2/chapel-07-timing.html",
    "title": "Measuring code performance",
    "section": "",
    "text": "The code generated after Exercise “Basic.4” is the full implementation of our calculation. We will be using it as a benchmark, to see how much we can improve the performance with Chapel’s parallel programming features in the following lessons.\nBut first, we need a quantitative way to measure the performance of our code. Perhaps the easiest way to do this is to use the Unix command time:\n$ time ./juliaSetSerial --n=500\nreal ...\nuser ...\nsys  ...\nThe real time is what interest us. Our code is taking … seconds from the moment it is called at the command line until it returns. Sometimes, however, it could be useful to take the execution time of specific parts of the code. This can be achieved by modifying the code to output the information that we need. This process is called instrumentation of the code.\nAn easy way to instrument our code with Chapel is by using the module Time. Modules in Chapel are libraries of useful functions and methods that can be used in our code once the module is loaded. To load a module we use the keyword use followed by the name of the module. Once the Time module is loaded we can create a variable of the type stopwatch, and use the methods start, stopand elapsed to instrument our code.\nuse Time;\nvar watch: stopwatch;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n$ chpl --fast juliaSetSerial.chpl\n$ ./juliaSetSerial --n=500\n\n\n\n\n\n\nQuestion Basic.5\n\n\n\n\n\nTry recompiling without --fast and see how it affects the execution time. If it becomes too slow, try reducing the problem size. What is the speedup factor with --fast?\n\n\n\nHere is our complete serial code juliaSetSerial.chpl:\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');",
    "crumbs": [
      "Measuring code performance"
    ]
  },
  {
    "objectID": "chapel2/chapel-07-timing.html#timing-the-execution-of-your-chapel-code",
    "href": "chapel2/chapel-07-timing.html#timing-the-execution-of-your-chapel-code",
    "title": "Measuring code performance",
    "section": "",
    "text": "The code generated after Exercise “Basic.4” is the full implementation of our calculation. We will be using it as a benchmark, to see how much we can improve the performance with Chapel’s parallel programming features in the following lessons.\nBut first, we need a quantitative way to measure the performance of our code. Perhaps the easiest way to do this is to use the Unix command time:\n$ time ./juliaSetSerial --n=500\nreal ...\nuser ...\nsys  ...\nThe real time is what interest us. Our code is taking … seconds from the moment it is called at the command line until it returns. Sometimes, however, it could be useful to take the execution time of specific parts of the code. This can be achieved by modifying the code to output the information that we need. This process is called instrumentation of the code.\nAn easy way to instrument our code with Chapel is by using the module Time. Modules in Chapel are libraries of useful functions and methods that can be used in our code once the module is loaded. To load a module we use the keyword use followed by the name of the module. Once the Time module is loaded we can create a variable of the type stopwatch, and use the methods start, stopand elapsed to instrument our code.\nuse Time;\nvar watch: stopwatch;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n$ chpl --fast juliaSetSerial.chpl\n$ ./juliaSetSerial --n=500\n\n\n\n\n\n\nQuestion Basic.5\n\n\n\n\n\nTry recompiling without --fast and see how it affects the execution time. If it becomes too slow, try reducing the problem size. What is the speedup factor with --fast?\n\n\n\nHere is our complete serial code juliaSetSerial.chpl:\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');",
    "crumbs": [
      "Measuring code performance"
    ]
  },
  {
    "objectID": "chapel2/chapel-03-ranges-and-arrays.html",
    "href": "chapel2/chapel-03-ranges-and-arrays.html",
    "title": "Ranges, arrays, and loops",
    "section": "",
    "text": "A series of integers (1,2,3,4,5, for example), is called a range in Chapel. Ranges are generated with the .. operator, and are useful, among other things, to declare arrays of variables. For example, the following variable\nis a 2D array (matrix) with n rows and n columns of integer numbers, all initialized as 0. The two ranges 1..n not only define the size and shape of the array, they stand for the indices with which we could access particular elements of the array using the [X,X] notation. For example, stability[1,1] is the integer variable located at the first row and first column of the array stability, while stability[3,7] sits at the 3rd row and 7th column; stability[2,3..15] access columns 3 to 15 of the 2nd row, and stability[1..4,4] corresponds to the first 4 rows on the 4th column of stability.\nWe are now ready to start coding our computation … here is what we are going to do:",
    "crumbs": [
      "Ranges and arrays"
    ]
  },
  {
    "objectID": "chapel2/chapel-03-ranges-and-arrays.html#structured-iterations-with-for-loops",
    "href": "chapel2/chapel-03-ranges-and-arrays.html#structured-iterations-with-for-loops",
    "title": "Ranges, arrays, and loops",
    "section": "Structured iterations with for-loops",
    "text": "Structured iterations with for-loops\nWe want to iterate over all points in our image. When it comes to iterating over a given number of elements, the for-loop is what we want to use. The for-loop has the following general syntax:\nfor index in iterand do\n  instruction;\n  \nfor index in iterand {\n  instruction1;\n  instruction2;\n  ...\n  }\nThe iterand is a statement that expresses an iteration, e.g. it could be a range 1..15. index is a variable that exists only in the context of the for-loop, and that will be taking the different values yielded by the iterand. The code flows as follows: index takes the first value yielded by the iterand, and keeps it until all the instructions inside the curly brackets are executed one by one; then, index takes the second value yielded by the iterand, and keeps it until all the instructions are executed again. This pattern is repeated until index takes all the different values exressed by the iterand.\nIn our case we iterate both over all rows and all columns in the image to compute every pixel. This can be done with nested for loops like this:\nfor i in 1..n { // process row i\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n { // process column j, row i\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nTo be able to compile the code, we also need a prototype pixel() function:\nproc pixel(z0) {\n  return z0; // to be replaced with an actual calculation\n}\nNow let’s compile and execute our code again:\n$ chpl juliaSetSerial.chpl\n$ sbatch serial.sh\n$ tail -f solution.out",
    "crumbs": [
      "Ranges and arrays"
    ]
  },
  {
    "objectID": "chapel2/chapel-04-control-flow.html",
    "href": "chapel2/chapel-04-control-flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Chapel, as most high-level programming languages, has different staments to control the flow of the program or code. The conditional statements are the if statement and the while statement.\nThe general syntax of a while statement is one of the two:\nwhile condition do \n  instruction;\n  \nwhile condition {\n  instruction1;\n  ...\n  instructionN;\n}\nWith multiple instructions inside the curly brackets, all of them are executed one by one if the condition evaluates to True. This block will be repeated over and over again until the condition does not hold anymore.\nIn our Julia set code we don’t use the while construct, however, we need to check if our iteration goes beyond the \\(|z|=4\\) circle – this is the type of control that an if statement gives us. The general syntax is:\nif condition then\n  instruction1;\nelse\n  instruction2;\nand you can group multiple instructions into a block by using the curly brackets {}.\nLet’s package our calculation into a function: for each complex number we want to iterate until either we reach the maximum number of iterations, or we cross the \\(|z|=4\\) circle:\nproc pixel(z0) {\n  const c: complex = 0.355 + 0.355i;   // Julia set constant\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s compile and run our code using the job script serial.sh:\n#!/bin/bash\n#SBATCH --time=00:05:00      # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\n./juliaSetSerial\n$ chpl juliaSetSerial.chpl\n$ sbatch serial.sh\n$ tail -f solution.out",
    "crumbs": [
      "Control flow"
    ]
  },
  {
    "objectID": "chapel2/chapel-21-synchronising-tasks.html",
    "href": "chapel2/chapel-21-synchronising-tasks.html",
    "title": "Synchronization of tasks",
    "section": "",
    "text": "sync block\nThe keyword sync provides all sorts of mechanisms to synchronize tasks in Chapel. We can simply use sync to force the parent task to stop and wait until its spawned child-task ends. Consider this sync1.chpl:\nvar x = 0;\nwriteln('This is the main task starting a synchronous task');\nsync {\n  begin {\n    var count = 0;\n    while count &lt; 10 {\n      count += 1;\n      writeln('task 1: ', x + count);\n    }\n  }\n}\nwriteln('The first task is done ...');\nwriteln('This is the main task starting an asynchronous task');\nbegin {\n  var count = 0;\n  while count &lt; 10 {\n    count += 1;\n    writeln('task 2: ', x + count);\n  }\n}\nwriteln('This is the main task, I am done ...');\n$ chpl sync1.chpl -o sync1\n$ sed -i -e 's|gmax|sync1|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task starting a synchronous task\ntask 1: 1\ntask 1: 2\ntask 1: 3\ntask 1: 4\ntask 1: 5\ntask 1: 6\ntask 1: 7\ntask 1: 8\ntask 1: 9\ntask 1: 10\nThe first task is done ...\nThis is the main task starting an asynchronous task\nThis is the main task, I am done ...\ntask 2: 1\ntask 2: 2\ntask 2: 3\ntask 2: 4\ntask 2: 5\ntask 2: 6\ntask 2: 7\ntask 2: 8\ntask 2: 9\ntask 2: 10\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\nWhat would happen if we swap sync and begin in the first task:\nbegin {\n  sync {\n    var c = 0;\n    while c &lt; 10 {\n      c += 1;\n      writeln('task 1: ', x + c);\n    }\n  }\n}\nwriteln('The first task is done ...');\nDiscuss your observations.\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion Task.3\n\n\n\n\n\nUse begin and sync statements to reproduce the functionality of cobegin in cobegin.chpl, i.e., the main task should not continue until both tasks 1 and 2 are completed.\n\n\n\n\n\nsync variables\nA more elaborated and powerful use of sync is as a type qualifier for variables. When a variable is declared as sync, a state that can be full or empty is associated with it.\nTo assign a new value to a sync variable, its state must be empty (after the assignment operation is completed, the state will be set as full). On the contrary, to read a value from a sync variable, its state must be full (after the read operation is completed, the state will be set as empty again).\nvar x: sync int;\nwriteln('this is the main task launching a new task');\nbegin {\n  for i in 1..10 do\n    writeln('this is the new task working: ', i);\n  x.writeEF(2);   // write the value, state changes from Empty to Full\n  writeln('New task finished');\n}\nwriteln('this is the main task after launching new task ... I will wait until x is full');\nx.readFE();         // read the value, state changes from Full to Empty\nwriteln('and now it is done');\n$ chpl sync2.chpl -o sync2\n$ sed -i -e 's|sync1|sync2|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nthis is main task launching a new task\nthis is main task after launching new task ... I will wait until x is full\nthis is new task working: 1\nthis is new task working: 2\nthis is new task working: 3\nthis is new task working: 4\nthis is new task working: 5\nthis is new task working: 6\nthis is new task working: 7\nthis is new task working: 8\nthis is new task working: 9\nthis is new task working: 10\nNew task finished\nand now it is done\nHere the main task does not continue until the variable is full and can be read.\n\nLet’s add another line x.readFE(); – now it is stuck since we cannot read x while it’s empty!\nLet’s add x.writeEF(5); right before the last x.readFE(); – now we set is to full again (and assigned 5), and it can be read again.\n\nThere are a number of methods defined for sync variables. Suppose x is a sync variable of a given type:\n// general methods\nx.reset() - set the state as empty and the value as the default of x's type\nx.isfull() - return true is the state of x is full, false if it is empty\n\n// blocking read and write methods\nx.writeEF(value) - block until the state of x is empty, then assign the value and\n                   set the state to full\nx.writeFF(value) - block until the state of x is full, then assign the value and\n                   leave the state as full\nx.readFE() - block until the state of x is full, then return x's value and set\n             the state to empty\nx.readFF() - block until the state of x is full, then return x's value and\n             leave the state as full\n\n// non-blocking read and write methods\nx.writeXF(value) - assign the value no matter the state of x, then set the state as full\nx.readXX() - return the value of x regardless its state; the state will remain unchanged\n\n\nAtomic variables\nChapel also implements atomic operations with variables declared as atomic, and this provides another option to synchronize tasks. Atomic operations run completely independently of any other task or process. This means that when several tasks try to write an atomic variable, only one will succeed at a given moment, providing implicit synchronization between them. There is a number of methods defined for atomic variables, among them sub(), add(), write(), read(), and waitfor() are very useful to establish explicit synchronization between tasks, as shown in the next code atomic.chpl:\nvar lock: atomic int;\nconst numtasks = 5;\n\nlock.write(0);               // the main task set lock to zero\n\ncoforall id in 1..numtasks {\n  writeln('greetings form task ', id, '... I am waiting for all tasks to say hello');\n  lock.add(1);               // task id says hello and atomically adds 1 to lock\n  lock.waitFor(numtasks);  // then it waits for lock to be equal numtasks (which will happen when all tasks say hello)\n  writeln('task ', id, ' is done ...');\n}\n$ chpl atomic.chpl -o atomic\n$ sed -i -e 's|sync2|atomic|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\ngreetings form task 4... I am waiting for all tasks to say hello\ngreetings form task 5... I am waiting for all tasks to say hello\ngreetings form task 2... I am waiting for all tasks to say hello\ngreetings form task 3... I am waiting for all tasks to say hello\ngreetings form task 1... I am waiting for all tasks to say hello\ntask 1 is done...\ntask 5 is done...\ntask 2 is done...\ntask 3 is done...\ntask 4 is done...\n\nComment out the line lock.waitfor(numtasks) in the code above to clearly observe the effect of task synchronization. {.note}\n\n\n\n\n\n\n\nQuestion Task.4\n\n\n\n\n\nSuppose we want to add another synchronization point right after the last writeln() command. What is wrong with adding the following at the end of the coforall loop?\n  lock.sub(1);      // task id says hello and atomically subtracts 1 from lock\n  lock.waitFor(0);  // then it waits for lock to be equal 0 (which will happen when all tasks say hello)\n  writeln('task ', id, ' is really done ...');\n\n\n\n\n\n\n\n\n\n\nQuestion Task.5\n\n\n\n\n\nOk, then what is the solution if we want two synchronization points?\n\n\n\nFinally, with everything learned so far, we should be ready to parallelize our code for the simulation of the heat transfer equation.",
    "crumbs": [
      "Synchronization of tasks"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html",
    "title": "Heat transfer solver on distributed domains",
    "section": "",
    "text": "For a tightly coupled parallel calculation, consider a heat diffusion problem:\n\nwe have a square metallic plate initially at 25 degrees (initial condition)\nwe want to simulate the evolution of the temperature across the plate; governed by the 2D heat (diffusion) equation: \ndiscretize the solution \\(T(x,y,t)\\approx T^{(n)}_{i,j}\\) with \\(i=1,...,{\\rm rows}\\) and \\(j=1,...,{\\rm cols}\\)\n\nthe upper left corner is (1,1) and the lower right corner is (rows,cols)\n\nthe plate’s border is in contact with a different temperature distribution (boundary condition):\n\nupper side \\(T^{(n)}_{0,1..{\\rm cols}}\\equiv 0\\)\nleft side \\(T^{(n)}_{1..{\\rm rows},0}\\equiv 0\\)\nbottom side \\(T^{(n)}_{{\\rm rows}+1,1..{\\rm cols}} = {80\\cdot j/\\rm cols}\\) (linearly increasing from 0 to 80 degrees)\nright side \\(T^{(n)}_{1..{\\rm rows},{\\rm cols}+1} = 80\\cdot i/{\\rm rows}\\) (linearly increasing from 0 to 80 degrees)\n\n\nWe discretize the equation with forward Euler time stepping:\n\nIf for simplicity we assume \\(\\Delta x=\\Delta y=1\\) and \\(\\Delta t=1/4\\), our finite difference equation becomes:\n\nAt each time iteration, at each point we’ll be computing the new temperature Tnew according to:\nTnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1])\n\nTnew = new temperature computed at the current iteration\nT = temperature calculated at the past iteration (or the initial conditions at the first iteration)\nthe indices (i,j) indicate the grid point located at the i-th row and the j-th column\n\n\n\n\n\n\n\n\n\n\nHere is a serial implementation of this solver baseSolver.chpl:\nuse Time;\nconfig const rows, cols = 100;   // number of rows and columns in our matrix\nconfig const niter = 500;        // max number of iterations\nconfig const tolerance = 1e-4: real;   // temperature difference tolerance\nvar count = 0: int;                    // the iteration counter\nvar delta: real;   // the greatest temperature difference between Tnew and T\nvar tmp: real;     // for temporary results\n\nvar T: [0..rows+1,0..cols+1] real;\nvar Tnew: [0..rows+1,0..cols+1] real;\nT[1..rows,1..cols] = 25;\n\ndelta = tolerance*10;    // some safe initial large value\nvar watch: stopwatch;\nwatch.start();\nwhile (count &lt; niter && delta &gt;= tolerance) {\n  for i in 1..rows do T[i,cols+1] = 80.0*i/rows;   // right side boundary condition\n  for j in 1..cols do T[rows+1,j] = 80.0*j/cols;   // bottom side boundary condition\n  count += 1;    // update the iteration counter\n  for i in 1..rows {\n    for j in 1..cols {\n      Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n    }\n  }\n  delta = 0;\n  for i in 1..rows {\n    for j in 1..cols {\n      tmp = abs(Tnew[i,j]-T[i,j]);\n      if tmp &gt; delta then delta = tmp;\n    }\n  }\n  if count%100 == 0 then writeln(\"delta = \", delta);\n  T = Tnew;\n}\nwatch.stop();\n\nwriteln('Largest temperature difference was ', delta);\nwriteln('Converged after ', count, ' iterations');\nwriteln('Simulation took ', watch.elapsed(), ' seconds');\nchpl --fast baseSolver.chpl\n./baseSolver --rows=650 --cols=650 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.00199985\nSimulation took 17.3374 seconds",
    "crumbs": [
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#case-study-2-solving-the-heat-transfer-problem",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#case-study-2-solving-the-heat-transfer-problem",
    "title": "Heat transfer solver on distributed domains",
    "section": "",
    "text": "For a tightly coupled parallel calculation, consider a heat diffusion problem:\n\nwe have a square metallic plate initially at 25 degrees (initial condition)\nwe want to simulate the evolution of the temperature across the plate; governed by the 2D heat (diffusion) equation: \ndiscretize the solution \\(T(x,y,t)\\approx T^{(n)}_{i,j}\\) with \\(i=1,...,{\\rm rows}\\) and \\(j=1,...,{\\rm cols}\\)\n\nthe upper left corner is (1,1) and the lower right corner is (rows,cols)\n\nthe plate’s border is in contact with a different temperature distribution (boundary condition):\n\nupper side \\(T^{(n)}_{0,1..{\\rm cols}}\\equiv 0\\)\nleft side \\(T^{(n)}_{1..{\\rm rows},0}\\equiv 0\\)\nbottom side \\(T^{(n)}_{{\\rm rows}+1,1..{\\rm cols}} = {80\\cdot j/\\rm cols}\\) (linearly increasing from 0 to 80 degrees)\nright side \\(T^{(n)}_{1..{\\rm rows},{\\rm cols}+1} = 80\\cdot i/{\\rm rows}\\) (linearly increasing from 0 to 80 degrees)\n\n\nWe discretize the equation with forward Euler time stepping:\n\nIf for simplicity we assume \\(\\Delta x=\\Delta y=1\\) and \\(\\Delta t=1/4\\), our finite difference equation becomes:\n\nAt each time iteration, at each point we’ll be computing the new temperature Tnew according to:\nTnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1])\n\nTnew = new temperature computed at the current iteration\nT = temperature calculated at the past iteration (or the initial conditions at the first iteration)\nthe indices (i,j) indicate the grid point located at the i-th row and the j-th column\n\n\n\n\n\n\n\n\n\n\nHere is a serial implementation of this solver baseSolver.chpl:\nuse Time;\nconfig const rows, cols = 100;   // number of rows and columns in our matrix\nconfig const niter = 500;        // max number of iterations\nconfig const tolerance = 1e-4: real;   // temperature difference tolerance\nvar count = 0: int;                    // the iteration counter\nvar delta: real;   // the greatest temperature difference between Tnew and T\nvar tmp: real;     // for temporary results\n\nvar T: [0..rows+1,0..cols+1] real;\nvar Tnew: [0..rows+1,0..cols+1] real;\nT[1..rows,1..cols] = 25;\n\ndelta = tolerance*10;    // some safe initial large value\nvar watch: stopwatch;\nwatch.start();\nwhile (count &lt; niter && delta &gt;= tolerance) {\n  for i in 1..rows do T[i,cols+1] = 80.0*i/rows;   // right side boundary condition\n  for j in 1..cols do T[rows+1,j] = 80.0*j/cols;   // bottom side boundary condition\n  count += 1;    // update the iteration counter\n  for i in 1..rows {\n    for j in 1..cols {\n      Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n    }\n  }\n  delta = 0;\n  for i in 1..rows {\n    for j in 1..cols {\n      tmp = abs(Tnew[i,j]-T[i,j]);\n      if tmp &gt; delta then delta = tmp;\n    }\n  }\n  if count%100 == 0 then writeln(\"delta = \", delta);\n  T = Tnew;\n}\nwatch.stop();\n\nwriteln('Largest temperature difference was ', delta);\nwriteln('Converged after ', count, ' iterations');\nwriteln('Simulation took ', watch.elapsed(), ' seconds');\nchpl --fast baseSolver.chpl\n./baseSolver --rows=650 --cols=650 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.00199985\nSimulation took 17.3374 seconds",
    "crumbs": [
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#distributed-version",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#distributed-version",
    "title": "Heat transfer solver on distributed domains",
    "section": "Distributed version",
    "text": "Distributed version\nNow let us use distributed domains to write a parallel version of our original heat transfer solver code. We’ll start by copying baseSolver.chpl into parallelSolver.chpl and making the following modifications to the latter:\n\nAdd\n\nuse BlockDist;\nconst mesh: domain(2) = {1..rows, 1..cols};   // local 2D domain\n\nAdd a larger \\((rows+2)\\times (cols+2)\\) block-distributed domain largerMesh with a layer of ghost points on perimeter locales, and define a temperature array T on top of it, by adding the following to our code:\n\nconst largerMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = {0..rows+1, 0..cols+1};\n\nChange the definitions of T and Tnew (delete those two lines) to\n\nvar T, Tnew: [largerMesh] real;   // block-distributed arrays of temperatures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMove the linearly increasing boundary conditions (right/bottom sides) before the while loop.\nReplace the loop for computing inner Tnew:\n\nfor i in 1..rows {  // do smth for row i\n  for j in 1..cols {   // do smth for row i and column j\n    Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n  }\n}\nwith a parallel forall loop (contains a mistake on purpose!):\nforall (i,j) in mesh do\n  Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n\n\n\n\n\n\nQuestion Data.4\n\n\n\n\n\nCan anyone spot a mistake in this loop?\n\n\n\n\nReplace\n\ndelta = 0;\nfor i in 1..rows {\n  for j in 1..cols {\n    tmp = abs(Tnew[i,j]-T[i,j]);\n    if tmp &gt; delta then delta = tmp;\n  }\n}\nwith\ndelta = max reduce abs(Tnew[1..rows,1..cols]-T[1..rows,1..cols]);\n\nReplace\n\nT = Tnew;\nwith the inner-only update\nT[1..rows,1..cols] = Tnew[1..rows,1..cols];   // uses parallel `forall` underneath",
    "crumbs": [
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#benchmarking",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#benchmarking",
    "title": "Heat transfer solver on distributed domains",
    "section": "Benchmarking",
    "text": "Benchmarking\nLet’s compile both serial and data-parallel versions using the same multi-locale compiler (and we will need -nl flag when running both):\n$ source /project/def-sponsor00/shared/syncHPC/startMultiLocale.sh\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ chpl --fast parallelSolver.chpl -o parallelSolver\nFirst, let’s try this on a smaller problem. Let’s write a job submission script distributed.sh:\n#!/bin/bash\n# this is distributed.sh\n#SBATCH --time=0:15:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\necho Running on $SLURM_NNODES nodes\n./baseSolver --rows=30 --cols=30 --niter=2000 -nl $SLURM_NNODES\n# ./parallelSolver --rows=30 --cols=30 --niter=2000 -nl $SLURM_NNODES\n\n\nLet’s run both codes, (un)commenting the relevant lines in distributed.sh:\n$ sbatch distributed.sh\nLargest temperature difference was 9.9534e-05\nConverged after 1148 iterations\nSimulation took ... seconds\nWait for the jobs to finish and then check the results:\n\n\n\n--nodes\n1\n4\n\n\n--cpus-per-task\n1\n2\n\n\nbaseSolver (sec)\n0.00725\n\n\n\nparallelSolver (sec)\n\n67.5\n\n\n\nAs you can see, on the training cluster the parallel code on 4 nodes (with 2 cores each) ran ~9,300 times slower than a serial code on a single node … What is going on here!? Shouldn’t the parallel code run ~8X faster, since we have 8X as many processors?\nThis is a fine-grained parallel code that needs a lot of communication between tasks, and relatively little computing. So, we are seeing the communication overhead. The training cluster has a very slow interconnect, so the problem is even worse there than on a production cluster!\nIf we increase our 2D problem size, there will be more computation (scaling as \\(O(n^2)\\)) in between communications (scaling as \\(O(n)\\)), and at some point the parallel code should catch up to the serial code and eventually run faster. Let’s try these problem sizes:\n--rows=650 --cols=650 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 7766 iterations\n\n--rows=2000 --cols=2000 --niter=9500 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 9158 iterations\n\n--rows=8000 --cols=8000 --niter=9800 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 9725 iterations\n\n--rows=16000 --cols=16000 --niter=9900 --tolerance=0.002\nLargest temperature difference was 0.0019989\nConverged after 9847 iterations\n\nOn the training cluster (slower interconnect)\nI switched both codes to single precision (change real to real(32) and use (80.0*i/rows):real(32) when assigning to real(32) variables), to be able to accommodate larger arrays. The table below shows the slowdown factor when going from serial to parallel:\n\n\n\n\n\n\n\n\n\n\n\n\n30^2\n650^2\n2,000^2\n8,000^2\n16,000^2\n\n\n\n\n--nodes=4 --cpus-per-task=8\n5104\n14.78\n2.29\n1/1.95\n1/3.31",
    "crumbs": [
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-16-distributed-heat-transfer.html#final-parallel-code",
    "href": "chapel2/chapel-16-distributed-heat-transfer.html#final-parallel-code",
    "title": "Heat transfer solver on distributed domains",
    "section": "Final parallel code",
    "text": "Final parallel code\nHere is the final single-precision parallel version of the code, minus the comments:\nuse Time, BlockDist;\nconfig const rows, cols = 100;\nconfig const niter = 500;\nconfig const tolerance = 1e-4: real(32);\nvar count = 0: int;\nvar delta: real(32);\nvar tmp: real(32);\n\nconst mesh: domain(2) = {1..rows, 1..cols};\nconst largerMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = {0..rows+1, 0..cols+1};\nvar T, Tnew: [largerMesh] real(32);\nT[1..rows,1..cols] = 25;\n\ndelta = tolerance*10;\nvar watch: stopwatch;\nwatch.start();\nfor i in 1..rows do T[i,cols+1] = (80.0*i/rows):real(32);   // right side boundary condition\nfor j in 1..cols do T[rows+1,j] = (80.0*j/cols):real(32);   // bottom side boundary condition\nwhile (count &lt; niter && delta &gt;= tolerance) {\n  count += 1;\n  forall (i,j) in largerMesh[1..rows,1..cols] do\n    Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n  delta = max reduce abs(Tnew[1..rows,1..cols]-T[1..rows,1..cols]);\n  if count%100 == 0 then writeln(\"delta = \", delta);\n  T[1..rows,1..cols] = Tnew[1..rows,1..cols];   // uses parallel `forall` underneath\n}\nwatch.stop();\n\nwriteln('Largest temperature difference was ', delta);\nwriteln('Converged after ', count, ' iterations');\nwriteln('Simulation took ', watch.elapsed(), ' seconds');\nThis is the entire multi-locale, data-parallel, hybrid shared-/distributed-memory solver!",
    "crumbs": [
      "Heat transfer solver on distributed domains + heat transfer description"
    ]
  },
  {
    "objectID": "chapel2/chapel-08-output.html",
    "href": "chapel2/chapel-08-output.html",
    "title": "Writing output",
    "section": "",
    "text": "From Chapel you can save data as binary (not recommended, for portability reasons), as HDF5 (serial or parallel), or NetCDF. Here is a modified serial code juliaSetSerial.chpl with NetCDF output:\nuse Time;\nuse NetCDF.C_NetCDF;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i:c_int;\n  }\n  return 255:c_int;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] c_int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nThe reason we are using C types (c_int) here – and not Chapel’s own int(32) or int(64) – is that we can save the resulting array stability into a compressed netCDF file. To the best of my knowledge, this can only be done using NetCDF.C_NetCDF library that relies on C types. You can add this to your code:\nwriteln(\"Writing NetCDF ...\");\nuse NetCDF.C_NetCDF;\nproc cdfError(e) {\n  if e != NC_NOERR {\n    writeln(\"Error: \", nc_strerror(e): string);\n    exit(2);\n  }\n}\nvar ncid, xDimID, yDimID, varID: c_int;\nvar dimIDs: [0..1] c_int;   // two elements\ncdfError(nc_create(\"test.nc\", NC_NETCDF4, ncid));       // const NC_NETCDF4 =&gt; file in netCDF-4 standard\ncdfError(nc_def_dim(ncid, \"x\", n, xDimID)); // define the dimensions\ncdfError(nc_def_dim(ncid, \"y\", n, yDimID));\ndimIDs = [xDimID, yDimID];                              // set up dimension IDs array\ncdfError(nc_def_var(ncid, \"stability\", NC_INT, 2, dimIDs[0], varID));   // define the 2D data variable\ncdfError(nc_def_var_deflate(ncid, varID, NC_SHUFFLE, deflate=1, deflate_level=9)); // compress 0=no 9=max\ncdfError(nc_enddef(ncid));                              // done defining metadata\ncdfError(nc_put_var_int(ncid, varID, stability[1,1]));  // write data to file\ncdfError(nc_close(ncid));\nTesting on my laptop, it took the code 0.471 seconds to compute a \\(2000^2\\) fractal.\nTry running it yourself! It will produce a file test.nc that you can download to your computer and render with ParaView or other visualization tool. Does the size of test.nc make sense?",
    "crumbs": [
      "Writing output"
    ]
  },
  {
    "objectID": "chapel2/chapel-08-output.html#write-to-a-netcdf-file",
    "href": "chapel2/chapel-08-output.html#write-to-a-netcdf-file",
    "title": "Writing output",
    "section": "",
    "text": "From Chapel you can save data as binary (not recommended, for portability reasons), as HDF5 (serial or parallel), or NetCDF. Here is a modified serial code juliaSetSerial.chpl with NetCDF output:\nuse Time;\nuse NetCDF.C_NetCDF;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i:c_int;\n  }\n  return 255:c_int;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] c_int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nThe reason we are using C types (c_int) here – and not Chapel’s own int(32) or int(64) – is that we can save the resulting array stability into a compressed netCDF file. To the best of my knowledge, this can only be done using NetCDF.C_NetCDF library that relies on C types. You can add this to your code:\nwriteln(\"Writing NetCDF ...\");\nuse NetCDF.C_NetCDF;\nproc cdfError(e) {\n  if e != NC_NOERR {\n    writeln(\"Error: \", nc_strerror(e): string);\n    exit(2);\n  }\n}\nvar ncid, xDimID, yDimID, varID: c_int;\nvar dimIDs: [0..1] c_int;   // two elements\ncdfError(nc_create(\"test.nc\", NC_NETCDF4, ncid));       // const NC_NETCDF4 =&gt; file in netCDF-4 standard\ncdfError(nc_def_dim(ncid, \"x\", n, xDimID)); // define the dimensions\ncdfError(nc_def_dim(ncid, \"y\", n, yDimID));\ndimIDs = [xDimID, yDimID];                              // set up dimension IDs array\ncdfError(nc_def_var(ncid, \"stability\", NC_INT, 2, dimIDs[0], varID));   // define the 2D data variable\ncdfError(nc_def_var_deflate(ncid, varID, NC_SHUFFLE, deflate=1, deflate_level=9)); // compress 0=no 9=max\ncdfError(nc_enddef(ncid));                              // done defining metadata\ncdfError(nc_put_var_int(ncid, varID, stability[1,1]));  // write data to file\ncdfError(nc_close(ncid));\nTesting on my laptop, it took the code 0.471 seconds to compute a \\(2000^2\\) fractal.\nTry running it yourself! It will produce a file test.nc that you can download to your computer and render with ParaView or other visualization tool. Does the size of test.nc make sense?",
    "crumbs": [
      "Writing output"
    ]
  },
  {
    "objectID": "chapel2/chapel-08-output.html#write-to-a-png-image",
    "href": "chapel2/chapel-08-output.html#write-to-a-png-image",
    "title": "Writing output",
    "section": "Write to a PNG image",
    "text": "Write to a PNG image\nChapel’s Image library lets you write arrays of pixels to a PNG file. The following code – when added to the non-GPU code juliaSetSerial.chpl – writes the array stability to a file 2000.png assuming that in the current directory you\n\nhave downloaded the colour map in CSV and\nhave added the file sciplot.chpl (pasted below)\n\nwriteln(\"Plotting ...\");\nuse Image, Math, sciplot;\nwatch.clear();\nwatch.start();\nconst smin = min reduce(stability);\nconst smax = max reduce(stability);\nvar colour: [1..n, 1..n] 3*int;\nvar cmap = readColourmap('nipy_spectral.csv');   // cmap.domain is {1..256, 1..3}\nfor i in 1..n {\n  for j in 1..n {\n    var idx = ((stability[i,j]:real-smin)/(smax-smin)*255):int + 1; //scale to 1..256\n    colour[i,j] = ((cmap[idx,1]*255):int, (cmap[idx,2]*255):int, (cmap[idx,3]*255):int);\n  }\n}\nvar pixels = colorToPixel(colour);               // array of pixels\nwriteImage(n:string+\".png\", imageType.png, pixels);\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n// save this as sciplot.chpl\nuse IO;\nuse List;\n\nproc readColourmap(filename: string) {\n  var reader = open(filename, ioMode.r).reader();\n  var line: string;\n  if (!reader.readLine(line)) then   // skip the header\n    halt(\"ERROR: file appears to be empty\");\n  var dataRows : list(string); // a list of lines from the file\n  while (reader.readLine(line)) do   // read all lines into the list\n    dataRows.pushBack(line);\n  var cmap: [1..dataRows.size, 1..3] real;\n  for (i, row) in zip(1..dataRows.size, dataRows) {\n    var c1 = row.find(','):int;    // position of the 1st comma in the line\n    var c2 = row.rfind(','):int;   // position of the 2nd comma in the line\n    cmap[i,1] = row[0..c1-1]:real;\n    cmap[i,2] = row[c1+1..c2-1]:real;\n    cmap[i,3] = row[c2+1..]:real;\n  }\n  reader.close();\n  return cmap;\n}\nHere are the typical timings on a CPU:\nComputing 2000x2000 Julia set ...\nIt took 0.382409 seconds\nPlotting ...\nIt took 0.192508 seconds",
    "crumbs": [
      "Writing output"
    ]
  },
  {
    "objectID": "chapel2/chapel-22-task-parallel-heat-transfer.html",
    "href": "chapel2/chapel-22-task-parallel-heat-transfer.html",
    "title": "Task-parallelizing the heat transfer solver",
    "section": "",
    "text": "Important: In a shorter Chapel course, we suggest skipping this section and focus mostly on data parallelism, coming back here only when you have time.\nHere is our plan to task-parallelize the heat transfer equation:\n\ndivide the entire grid of points into blocks and assign blocks to individual tasks,\neach task should compute the new temperature of its assigned points,\nperform a reduction over the whole grid, to update the greatest temperature difference between Tnew and T.\n\nFor the reduction of the grid we can simply use the max reduce statement, which is already parallelized. Now, let’s divide the grid into rowtasks * coltasks subgrids, and assign each subgrid to a task using the coforall loop (we will have rowtasks * coltasks tasks in total).\nRecall out code gmax.chpl in which we broke the 1D array with 1e8 elements into numtasks=2 blocks, and each task was processing elements start..finish. Now we’ll do exactly the same in 2D. First, let’s write a quick serial code test.chpl to test the indices:\nconfig const rows, cols = 100;               // number of rows and columns in our matrix\n\nconfig const rowtasks = 3, coltasks = 4; // number of blocks in x- and y-dimensions\n// each block processed by a separate task\n// let's pretend we have 12 cores\n\nconst nr = rows / rowtasks;   // number of rows per task\nconst rr = rows % rowtasks;   // remainder rows (did not fit into the last row of tasks)\nconst nc = cols / coltasks;   // number of columns per task\nconst rc = cols % coltasks;   // remainder columns (did not fit into the last column of tasks)\n\ncoforall taskid in 0..coltasks*rowtasks-1 {\n  var row1, row2, col1, col2: int;\n  row1 = taskid/coltasks*nr + 1;\n  row2 = taskid/coltasks*nr + nr;\n  if row2 == rowtasks*nr then row2 += rr; // add rr rows to the last row of tasks\n  col1 = taskid%coltasks*nc + 1;\n  col2 = taskid%coltasks*nc + nc;\n  if col2 == coltasks*nc then col2 += rc; // add rc columns to the last column of tasks\n  writeln('task ', taskid, ': rows ', row1, '-', row2, ' and columns ', col1, '-', col2);\n}\n$ chpl test.chpl -o test\n$ sed -i -e 's|atomic|test|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\ntask 0: rows 1-33 and columns 1-25\ntask 1: rows 1-33 and columns 26-50\ntask 2: rows 1-33 and columns 51-75\ntask 3: rows 1-33 and columns 76-100\ntask 4: rows 34-66 and columns 1-25\ntask 5: rows 34-66 and columns 26-50\ntask 6: rows 34-66 and columns 51-75\ntask 7: rows 34-66 and columns 76-100\ntask 8: rows 67-100 and columns 1-25\ntask 9: rows 67-100 and columns 26-50\ntask 10: rows 67-100 and columns 51-75\ntask 11: rows 67-100 and columns 76-100\nAs you can see, dividing Tnew computation between concurrent tasks could be cumbersome. Chapel provides high-level abstractions for data parallelism that take care of all the data distribution for us. We will study data parallelism in the following lessons, but for now let’s compare the benchmark solution (baseSolver.chpl) with our coforall parallelization to see how the performance improved.\nNow we’ll parallelize our heat transfer solver. Let’s copy baseSolver.chpl into parallel1.chpl and then start editing the latter. We’ll make the following changes in parallel1.chpl:\ndiff baseSolver.chpl parallel1.chpl\n18a19,24\n&gt; config const rowtasks = 3, coltasks = 4;   // let's pretend we have 12 cores\n&gt; const nr = rows / rowtasks;    // number of rows per task\n&gt; const rr = rows - nr*rowtasks; // remainder rows (did not fit into the last task)\n&gt; const nc = cols / coltasks;    // number of columns per task\n&gt; const rc = cols - nc*coltasks; // remainder columns (did not fit into the last task)\n&gt;\n31,32c37,46\n&lt;   for i in 1..rows {    // do smth for row i\n&lt;     for j in 1..cols {  // do smth for row i and column j\n&lt;       Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n&lt;     }\n&lt;   }\n---\n&gt;   coforall taskid in 0..coltasks*rowtasks-1 { // each iteration processed by a separate task\n&gt;     var row1, row2, col1, col2: int;\n&gt;     row1 = taskid/coltasks*nr + 1;\n&gt;     row2 = taskid/coltasks*nr + nr;\n&gt;     if row2 == rowtasks*nr then row2 += rr; // add rr rows to the last row of tasks\n&gt;     col1 = taskid%coltasks*nc + 1;\n&gt;     col2 = taskid%coltasks*nc + nc;\n&gt;     if col2 == coltasks*nc then col2 += rc; // add rc columns to the last column of tasks\n&gt;     for i in row1..row2 {\n&gt;       for j in col1..col2 {\n&gt;         Tnew[i,j] = 0.25 * (T[i-1,j] + T[i+1,j] + T[i,j-1] + T[i,j+1]);\n&gt;       }\n&gt;     }\n&gt;   }\n&gt;\n36,42d49\n&lt;   delta = 0;\n&lt;   for i in 1..rows {\n&lt;     for j in 1..cols {\n&lt;       tmp = abs(Tnew[i,j]-T[i,j]);\n&lt;       if tmp &gt; delta then delta = tmp;\n&lt;     }\n&lt;   }\n43a51,52\n&gt;   delta = max reduce abs(Tnew[1..rows,1..cols]-T[1..rows,1..cols]);\nLet’s compile and run both codes on the same large problem:\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ sed -i -e 's|test|baseSolver --rows=650 --iout=200 --niter=10_000 --tolerance=0.002 --nout=1000|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] after 7750 iterations is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 8.96548 seconds\n\n$ chpl --fast parallel1.chpl -o parallel1\n$ sed -i -e 's|baseSolver|parallel1|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] after 7750 iterations is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 25.106 seconds\nBoth ran to 7750 iterations, with the same numerical results, but the parallel code is nearly 3X slower – that’s terrible!\n\nDiscussion\nWhat happened!?…\n\nTo understand the reason, let’s analyze the code. When the program starts, the main task does all the declarations and initializations, and then, it enters the main loop of the simulation (the while loop). Inside this loop, the parallel tasks are launched for the first time. When these tasks finish their computations, the main task resumes its execution, it updates delta and T, and everything is repeated again. So, in essence, parallel tasks are launched and terminated 7750 times, which introduces a significant amount of overhead (the time the system needs to effectively start and destroy tasks in the specific hardware, at each iteration of the while loop).\nClearly, a better approach would be to launch the parallel tasks just once, and have them execute all the time steps, before resuming the main task to print the final results.\nLet’s copy parallel1.chpl into parallel2.chpl and then start editing the latter. We’ll make the following changes:\n\nMove the rows\n\n  coforall taskid in 0..coltasks*rowtasks-1 { // each iteration processed by a separate task\n    var row1, row2, col1, col2: int;\n    row1 = taskid/coltasks*nr + 1;\n    row2 = taskid/coltasks*nr + nr;\n    if row2 == rowtasks*nr then row2 += rr; // add rr rows to the last row of tasks\n    col1 = taskid%coltasks*nc + 1;\n    col2 = taskid%coltasks*nc + nc;\n    if col2 == coltasks*nc then col2 += rc; // add rc columns to the last column of tasks\nand the corresponding closing bracket } of this coforall loop outside the while loop, so that while is now nested inside coforall.\n\nSince now copying Tnew into T is a local operation for each task, i.e. we should replace T = Tnew; with\n\nT[row1..row2,col1..col2] = Tnew[row1..row2,col1..col2];\nBut this is not sufficient! We need to make sure we finish computing all elements of Tnew in all tasks before computing the greatest temperature difference delta. For that we need to synchronize all tasks, right after computing Tnew. We’ll also need to synchronize tasks after computing delta and T from Tnew, as none of the tasks should jump into the new iteration without having delta and T! So, we need two synchronization points inside the coforall loop.\n\n\n\n\nDefine two atomic variables that we’ll use for synchronization\n\nvar lock1, lock2: atomic int;\nand add after the (i,j)-loops to compute Tnew the following:\nlock1.add(1);   // each task atomically adds 1 to lock\nlock1.waitFor(coltasks*rowtasks*count);   // then it waits for lock to be equal coltasks*rowtasks\nand after T[row1..row2,col1..col2] = Tnew[row1..row2,col1..col2]; the following:\nlock2.add(1);   // each task atomically subtracts 1 from lock\nlock2.waitFor(coltasks*rowtasks*count);   // then it waits for lock to be equal 0\nNotice that we have a product coltasks*rowtasks*count, since lock1/lock2 will be incremented by all tasks at all iterations.\n\nMove var count = 0: int; into coforall so that it becomes a local variable for each task. Also, remove count instance (in writeln()) after coforall ends.\nMake delta atomic:\n\nvar delta: atomic real;    // the greatest temperature difference between Tnew and T\n...\ndelta.write(tolerance*10); // some safe initial large value\n...\nwhile (count &lt; niter && delta.read() &gt;= tolerance) {\n\nDefine an array of local delta’s for each task and use it to compute delta:\n\nvar arrayDelta: [0..coltasks*rowtasks-1] real;\n...\nvar tmp: real;  // inside coforall\n...\ntmp = 0;        // inside while\n...\ntmp = max(abs(Tnew[i,j]-T[i,j]),tmp);    // next line after Tnew[i,j] = ...\n...\narrayDelta[taskid] = tmp;   // right after (i,j)-loop to compute Tnew[i,j]\n...\nif taskid == 0 then {       // compute delta right after lock1.waitFor()\n    delta.write(max reduce arrayDelta);\n    if count%nout == 0 then writeln('Temperature at iteration ', count, ': ', Tnew[iout,jout]);\n}\n\nRemove the original T[iout,jout] output line.\nFinally, move the boundary conditions (right+bottom edges) before the while loop. Why can we do it now?\n\nNow let’s compare the performance of parallel2.chpl to the benchmark serial solution baseSolver.chpl:\n$ sed -i -e 's|parallel1|baseSolver|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] after 7750 iterations is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 9.40637 seconds\n\n$ chpl --fast parallel2.chpl -o parallel2\n$ sed -i -e 's|baseSolver|parallel2|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nWorking with a matrix 650x650 to 10000 iterations or dT below 0.002\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position [200,300] is: 24.9671\nThe largest temperature difference was 0.00199985\nThe simulation took 4.74536 seconds\nWe get a speedup of 2X on two cores, as we should.\nFinally, here is a parallel scaling test on Cedar inside a 32-core interactive job:\n$ ./parallel2 ... --rowtasks=1 --coltasks=1\nThe simulation took 32.2201 seconds\n\n$ ./parallel2 ... --rowtasks=2 --coltasks=2\nThe simulation took 10.197 seconds\n\n$ ./parallel2 ... --rowtasks=4 --coltasks=4\nThe simulation took 3.79577 seconds\n\n$ ./parallel2 ... --rowtasks=4 --coltasks=8\nThe simulation took 2.4874 seconds",
    "crumbs": [
      "Task-parallelizing the heat transfer solver"
    ]
  },
  {
    "objectID": "chapel2/chapel-13-multi-locale-chapel.html",
    "href": "chapel2/chapel-13-multi-locale-chapel.html",
    "title": "Multi-locale Chapel",
    "section": "",
    "text": "So far we have been working with single-locale Chapel codes that may run on one or many cores on a single compute node, making use of the shared memory space and accelerating computations by launching parallel threads on individual cores. Chapel codes can also run on multiple nodes on a compute cluster. In Chapel this is referred to as multi-locale execution.\n\n\nIf you work inside a Chapel Docker container, e.g., chapel/chapel, the container environment simulates a multi-locale cluster, so you would compile and launch multi-locale Chapel codes directly by specifying the number of locales with -nl flag:\n$ chpl --fast mycode.chpl -o mybinary\n$ ./mybinary -nl 3\nInside the Docker container on multiple locales your code will not run any faster than on a single locale, since you are emulating a virtual cluster, and all tasks run on the same physical node. To achieve actual speedup, you need to run your parallel multi-locale Chapel code on a real HPC cluster. {.note}\n\nOn an HPC cluster you would need to submit either an interactive or a batch job asking for several nodes and then run a multi-locale Chapel code inside that job. In practice, the exact commands to run multi-locale Chapel codes depend on how Chapel was configured on the cluster.\nWhen you compile a Chapel code with the multi-locale Chapel compiler, two binaries will be produced. One is called mybinary and is a launcher binary used to submit the real executable mybinary_real. If the Chapel environment is configured properly with the launcher for the cluster’s physical interconnect, then you would simply compile the code and use the launcher binary mybinary to run a multi-locale code.\nFor the rest of this class we assume that you have a working multi-locale Chapel environment, whether provided by a Docker container or by multi-locale Chapel on a physical HPC cluster. We will run all examples on 3 nodes with 2 cores per node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s write a job submission script distributed.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=1000   # in MB\n#SBATCH --nodes=3\n#SBATCH --cpus-per-task=2\n#SBATCH --output=solution.out\n./test -nl 3   # in this case the 'srun' launcher is already configured for our interconnect",
    "crumbs": [
      "Multi-locale Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-13-multi-locale-chapel.html#setup",
    "href": "chapel2/chapel-13-multi-locale-chapel.html#setup",
    "title": "Multi-locale Chapel",
    "section": "",
    "text": "So far we have been working with single-locale Chapel codes that may run on one or many cores on a single compute node, making use of the shared memory space and accelerating computations by launching parallel threads on individual cores. Chapel codes can also run on multiple nodes on a compute cluster. In Chapel this is referred to as multi-locale execution.\n\n\nIf you work inside a Chapel Docker container, e.g., chapel/chapel, the container environment simulates a multi-locale cluster, so you would compile and launch multi-locale Chapel codes directly by specifying the number of locales with -nl flag:\n$ chpl --fast mycode.chpl -o mybinary\n$ ./mybinary -nl 3\nInside the Docker container on multiple locales your code will not run any faster than on a single locale, since you are emulating a virtual cluster, and all tasks run on the same physical node. To achieve actual speedup, you need to run your parallel multi-locale Chapel code on a real HPC cluster. {.note}\n\nOn an HPC cluster you would need to submit either an interactive or a batch job asking for several nodes and then run a multi-locale Chapel code inside that job. In practice, the exact commands to run multi-locale Chapel codes depend on how Chapel was configured on the cluster.\nWhen you compile a Chapel code with the multi-locale Chapel compiler, two binaries will be produced. One is called mybinary and is a launcher binary used to submit the real executable mybinary_real. If the Chapel environment is configured properly with the launcher for the cluster’s physical interconnect, then you would simply compile the code and use the launcher binary mybinary to run a multi-locale code.\nFor the rest of this class we assume that you have a working multi-locale Chapel environment, whether provided by a Docker container or by multi-locale Chapel on a physical HPC cluster. We will run all examples on 3 nodes with 2 cores per node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s write a job submission script distributed.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=1000   # in MB\n#SBATCH --nodes=3\n#SBATCH --cpus-per-task=2\n#SBATCH --output=solution.out\n./test -nl 3   # in this case the 'srun' launcher is already configured for our interconnect",
    "crumbs": [
      "Multi-locale Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-13-multi-locale-chapel.html#simple-multi-locale-codes",
    "href": "chapel2/chapel-13-multi-locale-chapel.html#simple-multi-locale-codes",
    "title": "Multi-locale Chapel",
    "section": "Simple multi-locale codes",
    "text": "Simple multi-locale codes\nLet us test our multi-locale Chapel environment by launching the following code:\n\n\nwriteln(Locales);\n$ source /project/def-sponsor00/shared/syncHPC/startMultiLocale.sh\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\nThis code will print the built-in global array Locales. Running it on four locales will produce\nLOCALE0 LOCALE1 LOCALE2\nWe want to run some code on each locale (node). For that, we can cycle through locales:\nfor loc in Locales do   // this is still a serial program\n  on loc do             // run the next line on locale `loc`\n    writeln(\"this locale is named \", here.name[0..4]);   // `here` is the locale on which the code is running\nThis will produce\nthis locale is named node1\nthis locale is named node2\nthis locale is named node3\nHere the built-in variable class here refers to the locale on which the code is running, and here.name is its hostname. We started a serial for loop cycling through all locales, and on each locale we printed its name, i.e., the hostname of each node. This program ran in serial starting a task on each locale only after completing the same task on the previous locale. Note the order in which locales were listed.\nTo run this code in parallel, starting four simultaneous tasks, one per locale, we simply need to replace for with forall:\nforall loc in Locales do   // now this is a parallel loop\n  on loc do\n    writeln(\"this locale is named \", here.name[0..4]);\nThis starts four tasks in parallel, and the order in which the print statement is executed depends on the runtime conditions and can change from run to run:\nthis locale is named node1\nthis locale is named node3\nthis locale is named node2\nWe can print few other attributes of each locale. Here it is actually useful to revert to the serial loop for so that the print statements appear in order:\nuse MemDiagnostics;\nfor loc in Locales do\n  on loc {\n    writeln(\"locale #\", here.id, \"...\");\n    writeln(\"  ...is named: \", here.name);\n    writeln(\"  ...has \", here.numPUs(), \" processor cores\");\n    writeln(\"  ...has \", here.physicalMemory(unit=MemUnits.GB, retType=real), \" GB of memory\");\n    writeln(\"  ...has \", here.maxTaskPar, \" maximum parallelism to expect\");\n  }\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\nlocale #0...\n  ...is named: node1.int.school.vastcloud.org\n  ...has 2 processor cores\n  ...has 29.124 GB of memory\n  ...has 2 maximum parallelism to expect\nlocale #1...\n  ...is named: node2.int.school.vastcloud.org\n  ...has 2 processor cores\n  ...has 29.124 GB of memory\n  ...has 2 maximum parallelism to expect\nlocale #2...\n  ...is named: node3.int.school.vastcloud.org\n  ...has 2 processor cores\n  ...has 29.124 GB of memory\n  ...has 2 maximum parallelism to expect\nNote that while Chapel correctly determines the number of cores available inside our job on each node (maximum parallelism) but lists the total physical memory on each node available to all running jobs which is not the same as the total memory per node allocated to our job.",
    "crumbs": [
      "Multi-locale Chapel"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Various courses",
    "section": "",
    "text": "If you want to get informed about upcoming courses and webinars, please join our mailing list below. We will only email you about training events.\n\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "ray.html",
    "href": "ray.html",
    "title": "Part 2: distributed computing with Ray",
    "section": "",
    "text": "There is a number of high-level open-source parallel frameworks for Python that are quite popular in data science and beyond:\nHere we’ll focus on Ray, a unified framework for scaling AI and general Python workflows. Since this is not a machine learning workshop, we will not touch Ray’s AI capabilities, but will focus on its core distributed runtime and data libraries. We will learn several different approaches to parallelizing purely numerical (and therefore CPU-bound) workflows, both with and without reduction. We will also look at I/O-bound workflows."
  },
  {
    "objectID": "ray.html#initializing-ray",
    "href": "ray.html#initializing-ray",
    "title": "Part 2: distributed computing with Ray",
    "section": "Initializing Ray",
    "text": "Initializing Ray\nimport ray\nray.init()   # start a Ray cluster  and connect to it\n             # no longer necessary, will run by default when you first use it\nHowever, ray.init() is very useful for passing options at initialization. For example, Ray is quite verbose when you do things in it. To turn off this logging output to the terminal, you can do\nray.init(configure_logging=False)   # hide Ray's copious logging output\nYou can run ray.init() only once. If you want to re-run it, first you need to run ray.shutdown(). Alternatively, you can pass the argument ignore_reinit_error=True to the call.\nYou can specify the number of cores for Ray to use, and you can combine multiple options, e.g.\nray.init(num_cpus=4, configure_logging=False)\nBy default Ray will use all available CPU cores, e.g. on my laptop ray.init() will start 8 ray::IDLE processes (workers), and you can monitor these in a separate shell with htop --filter \"ray::IDLE\" command (you may want to hide threads – typically thrown in green – with Shift+H).\n\nDiscussion\nHow many “ray::IDLE” processes do you see, and why? Recall that you can use srun --jobid=&lt;jobID&gt; --pty bash to open an interactive shell process inside your currently running job, and run htop --filter \"ray::IDLE\" there.\n\n\n\n\n\n\n\nQuestion 11\n\n\n\n\n\nHow would you pass the actual number of processor cores to the Ray cluster? Consider three options:\n1. Using a Slurm environment variable. How would you pass it to ray.init()?\n2. Launching a single-node Ray cluster as described in our Ray documentation.\n3. Not passing anything at all, in which case Ray will try – unsuccessfully – to grab all cores."
  },
  {
    "objectID": "ray.html#ray-tasks",
    "href": "ray.html#ray-tasks",
    "title": "Part 2: distributed computing with Ray",
    "section": "Ray tasks",
    "text": "Ray tasks\nIn Ray you can execute any Python function asynchronously on separate workers. Such functions are called Ray remote functions, and their asynchronous invocations are called Ray tasks:\nimport ray\nray.init(configure_logging=False)   # optional\n\n@ray.remote             # declare that we want to run this function remotely\ndef square(x):\n    return x * x\n\nr = square.remote(10)   # launch/schedule a remote calculation (non-blocking call)\ntype(r)                 # ray._raylet.ObjectRef (object reference)\nray.get(r)              # retrieve the result (=100) (blocking call)\nThe calculation may happen any time between &lt;function&gt;.remote() and ray.get() calls, i.e. it does not necessarily start when you launch it. This is called lazy execution: the operation is often executed when you try to access the result.\na = square.remote(10)   # launch a remote calculation\nray.cancel(a)           # cancel it\nray.get(a)              # either error or 100, depending on whether the calculation\n                        # has finished before cancellation\nYou can launch several Ray tasks at once, to be executed in parallel in the background, and you can retrieve their results either individually or through the list:\nr = [square.remote(i) for i in range(4)]   # launch four parallel tasks (non-blocking call)\nprint([ray.get(r[i]) for i in range(4)])   # retrieve the results (multiple blocking calls)\nprint(ray.get(r))          # more compact way to do the same (single blocking call)\n\nTask output\nConsider a code in which each Ray task sleeps for 10 seconds, prints a message and returns its task ID:\nimport ray\nfrom time import sleep, time\nray.init(num_cpus=4, configure_logging=False)\n\n@ray.remote\ndef nap():\n    sleep(10)\n    print(\"almost done\")\n    return ray.get_runtime_context().get_task_id()\nLet’s run it with timing on:\nstart = time()\nr = [nap.remote() for i in range(4)]\nray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nI get 10.013 seconds since I have enough cores to run all of them in parallel. However, most likely, I see printout (“almost done”) from only one process and a message “repeated 3x across cluster”. To enable print messages from all tasks, you need set the bash shell environment variable export RAY_DEDUP_LOGS=0.\nNotice that Ray task IDs are not integers but 48-character hexadecimal numbers.\n\n\nDistributed progress bars\nfrom ray.experimental.tqdm_ray import tqdm\n\n@ray.remote\ndef busy(name):\n    if \"2\" in name: sleep(2)\n    for x in tqdm(range(100), desc=name):\n        sleep(0.1)\n\n[busy.remote(\"task 1\"), busy.remote(\"task 2\")]\nA side effect of tqdm() is that these tasks start running immediately.\n\n\n\n\n\n\nQuestion 11b\n\n\n\n\n\nImplement the same for 10 tasks using a for loop.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRay’s tqdm() is somewhat buggy, so you might want to restart Python and your Ray cluster, if you don’t want to see artifacts from previous bars sometimes popping up in your session.\n\n\n\n\n\n\n\n\n\n\n\nParallelizing the slow series with Ray tasks\nLet’s perform our slow series calculation as a Ray task. This is our original serial implementation, now with a Ray remote function running on one of the workers:\nfrom time import time\nimport ray\nray.init(num_cpus=4, configure_logging=False)\n\n@ray.remote\ndef slow(n: int):\n    total = 0\n    for i in range(1,n+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    return total\n\nstart = time()\nr = slow.remote(100_000_000)\ntotal = ray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nWe should get the same timing as before (~6-7 seconds). You can call ray.get() on the previously computed result again without having to redo the calculation:\nstart = time()\ntmp = ray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))   # 0.001 seconds\nprint(tmp)\nLet’s speed up this calculation with parallel Ray tasks! Instead of doing the full sum over range(1,n+1), let’s calculate a partial sum on each task:\nfrom time import time\nimport psutil, ray\nray.init(num_cpus=4, configure_logging=False)\n\n@ray.remote\ndef slow(interval):\n    total = 0\n    for i in range(interval[0],interval[1]+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    return total\nThis would be a serial calculation:\nn = 100_000_000\nstart = time()\nr = slow.remote((1, n))   # takes in one argument\ntotal = ray.get(r)\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nTo launch it in parallel, we need to subdivide the interval:\nncores = psutil.cpu_count(logical=False)   # good option on a standalone computer\nncores = 4                                 # on a cluster\n\nsize = n//ncores   # size of each batch\nintervals = [(i*size+1,(i+1)*size) for i in range(ncores)]\nif n &gt; intervals[-1][1]: intervals[-1] = (intervals[-1][0], n)   # add the remainder (if any)\n\nstart = time()\nr = [slow.remote(intervals[i]) for i in range(ncores)]\ntotal = sum(ray.get(r))   # compute total sum\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nOn 8 cores I get the average runtime of 1.282 seconds – not too bad, considering that some of the cores are low-efficiency (slower) cores.\n\n\n\n\n\n\nQuestion 12\n\n\n\n\n\nIncrease n to 1_000_000_000 and run htop --filter \"ray::IDLE in a separate shell to monitor CPU usage of individual processes.\n\n\n\n\n\nRunning Numba-compiled functions as Ray tasks\nWe ended Part 1 with a Numba-compiled version of the slow series code that works almost as well as a Julia/Chapel code. As you just saw, Ray itself can distribute the calculation, speeding up the code with parallel execution, but individual tasks still run native Python code that is slow.\nWouldn’t it be great if we could use Ray to distribute execution of Numba-compiled functions to workers? It turns out we can, but we have to be careful with syntax. We would need to define remote compiled functions, but neither Ray, nor Numba let you combine their decorators (@ray.remote and @numba.jit, respectively) for a single function. You can do this in two steps:\nimport ray\nfrom numba import jit\n\nray.init(num_cpus=4, configure_logging=False)\n\n@jit(nopython=True)\ndef square(x):\n    return x*x\n\n@ray.remote\ndef runCompiled():\n    return square(5)\n\nr = runCompiled.remote()\nray.get(r)\nHere we “jit” the function on the main process and send it to workers for execution. Alternatively, you can “jit” on workers:\nimport ray\nfrom numba import jit\n\nray.init(num_cpus=4, configure_logging=False)\n\ndef square(x):\n    return x*x\n\n@ray.remote\ndef runCompiled():\n    compiledSquare = jit(square)\n    return compiledSquare(5)\n\nr = runCompiled.remote()\nray.get(r)\nIn my tests with more CPU-intensive functions, both versions produce equivalent runtimes.\n\n\n\n\n\n\nQuestion Combining Numba and Ray remotes for the slow series (big one!)\n\n\n\n\n\nWrite a slow series solver with Numba-compiled functions executed as Ray tasks. 1. Numba-compiled combined(k) that returns either 1/x or 0.\n2. Numba-compiled slow(interval) for partial sums.\n3. Ray-enabled remote function runCompiled(interval) to launch partial sums on workers.\n4. Very important step: you must do small runCompiled() runs on workers to copy code over to them – no need to time these runs. Without this “pre-compilation” step you will not get fast execution on workers on the bigger problem.\n5. The rest of the code will look familiar:\nstart = time()\nr = [runCompiled.remote(intervals[i]) for i in range(ncores)]\ntotal = sum(ray.get(r))   # compute total sum\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAveraged (over three runs) times:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n0.439\n0.235\n0.130\n0.098\n\n\n\nUsing a combination of Numba and Ray tasks on 8 cores, we accelerated the calculation by ~68X.\n\n\nGetting partial results from Ray tasks\nConsider the following code:\nfrom time import sleep\nimport ray, random\n\n@ray.remote\ndef longSleep():\n    duration = random.randint(1,100) # random integer from [1,100]\n    sleep(duration)   # sleep for this number of seconds\n    return f\"I slept for {duration} seconds\"\n\nrefs = [longSleep.remote() for x in range(1,21)]   # start 20 tasks\nIf we now call ray.get(refs), that would block until all of these remote tasks finish. If we want to, let’s say, one of them to finish and then continue on the main task, we can do:\nready_refs, remaining_refs = ray.wait(refs, num_returns=1, timeout=None) # wait for one of them to finish\nprint(ready_refs, remaining_refs)   # print the IDs of the finished task, and the other 19 IDs\nray.get(ready_refs)                 # get finished results\nLet’s wait for the first 5 answers:\nready_refs, remaining_refs = ray.wait(refs, num_returns=5, timeout=None) # wait for 5 of them to finish\nray.get(ready_refs)                 # get finished results\nfor i in remaining_refs:\n    ray.cancel(i)   # cancel the unfinished ones\n\n\nMultiple returns from Ray tasks\nSimilar to normal Python functions, Ray tasks can return tuples:\nimport ray\n\n@ray.remote\ndef threeNumbers():\n    return 10, 20, 30\n\nr = threeNumbers.remote()\nprint(ray.get(r)[0])        # get the result (tuple) and show its first element\nAlternatively, you can pipe each number to a separate object ref (tell the decorator!):\n@ray.remote(num_returns=3)\ndef threeNumbers():\n    return 10, 20, 30\n\nr1, r2, r3 = threeNumbers.remote()\nray.get(r1)   # get the first result only\nYou can also create a remote generator that will return only one number at a time, to reduce memory usage:\n@ray.remote(num_returns=3)\ndef threeNumbers():\n    for i in range(3):   # should return 10,20,30\n        yield (i+1)*10\n\nr1, r2, r3 = threeNumbers.remote()\nray.get(r1)   # get the first number only\n\n\nLinking remote tasks\nIn addition to values, object refs can also be passed to remote functions. Define two functions:\nimport ray\n\n@ray.remote\ndef one():\n    return 1\n\n@ray.remote\ndef increment(value):\n    return value + 1\nIn by-now familiar syntax:\nr1 = one.remote()                    # create the first Ray task\nr2 = increment.remote(ray.get(r1))   # pass its result as an argument to another Ray task\nprint(ray.get(r2))\nYou can also shorten this syntax:\nr1 = one.remote()           # create the first Ray task\nr2 = increment.remote(r1)   # pass its object ref as an argument to another Ray task\nprint(ray.get(r2))\nAs the second task depends on the output of the first task, Ray will not execute the second task until the first task has finished.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPersistent storage on Ray workers\n\n\nYou might have noticed that Ray functions (remote tasks) are stateless, i.e. they can run on any processor that happens to be more idle at the time, and they do not store any data on that processor in between the function calls.\nIf you are trying to parallelize a tightly-coupled problem, you might want to store arrays on the workers permanently, and then repeatedly call quick functions to do some computations on these arrays, without copying the arrays back and forth at each step.\nTo do this in Ray, we can use Ray actors (https://docs.ray.io/en/latest/ray-core/actors.html). A Ray actor is essentially a stateful (bound to a processor) worker that is created via a Python class instance with its own persistent variables and methods, and it stays permanently on that worker until we destroy this instance.\n\n\n\n\n\nimport ray\nimport numpy as np\n\nray.init()\n\n@ray.remote\nclass ArrayStorage:         # define an actor (ArrayStorage class) with a persistent array\n    def __init__(self):\n        self.array = None   # persistent array variable\n    def store_array(self, arr):\n        self.array = arr    # store an array in the actor's state\n    def get_array(self):\n        return self.array   # retrieve the stored array\n\nstorage_actor = ArrayStorage.remote()   # create an instance of the actor\n\narr = np.array([1, 2, 3, 4, 5])\nray.get(storage_actor.store_array.remote(arr))   # store an array\n\nr = ray.get(storage_actor.get_array.remote())    # retrieve the stored array\nprint(r)  # Output: [1 2 3 4 5]\n\n\n\n\nTo scale this to multiple workers, we can do the same with an array of workers:\nworkers = [ArrayStorage.remote() for i in range(2)]   # create two instances of the actor\n\nr = [workers[i].store_array.remote(np.ones(5)*(i+1)) for i in range(2)]\nprint(ray.get(r))\n\nprint(ray.get(workers[0].get_array.remote()))   # [1. 1. 1. 1. 1.]\nprint(ray.get(workers[1].get_array.remote()))   # [2. 2. 2. 2. 2.]\n\nr = [workers[i].get_array.remote() for i in range(2)]\nprint(ray.get(r))   # both arrays in one go\nIf we want to make sure that these arrays stay on the same workers, we can retrieve and print their IDs and the node IDs by adding these two functions to the actor class:\n...\n    def get_actor_id(self):\n        return self.actor_id\n    def get_node_id(self):\n        return self.node_id   # the node ID where this actor is running\n...\nprint([ray.get(workers[i].get_actor_id.remote()) for i in range(2)])   # actor IDs\nprint([ray.get(workers[i].get_node_id.remote()) for i in range(2)])    # node IDs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can even use NumPy on workers. For example, if we were to implement a linear algebra solver on a worker and wanted to have the solution array stored there permanently, we could do it this way:\nimport numpy as np\nimport ray\n\nray.init(num_cpus=2, configure_logging=False)\n\nn = 500\nh = 1/(n+1)\nb = np.exp(-(100*(np.linspace(0,1,n)-0.45))**2)*h*h\n\n@ray.remote\nclass ArrayStorage:\n    def __init__(self, n):\n        self.b = None   # persistent variable\n        self.u = None   # persistent variable\n        flatIdentity = np.identity(n).reshape([n*n])\n        a = -2.0*flatIdentity\n        a[1:n*n-1] = a[1:n*n-1] + flatIdentity[:n*n-2] + flatIdentity[2:]\n        self.a = a.reshape([n,n])   # persistent variable\n    def store_b(self, arr):\n        self.b = arr                # store an array in the actor's state\n    def get_u(self):\n        return self.u\n    def localSolve(self):\n        self.u = np.linalg.solve(self.a,self.b)\n\nworker = ArrayStorage.remote(n)\nworker.store_b.remote(b)\nworker.localSolve.remote()\nu = ray.get(worker.get_u.remote())\nprint(\"The solution is\", u)\nFinally, you can delete a Ray actor with:\nray.kill(worker)\n\n\n\n\n\n\nNote\n\n\n\nIs it possible to use JIT-compiled functions inside a Ray actor? The answer is “probably yes”: in Numba there is an experimental feature to compile Python classes with @jitclass. Whether you can use it to compile Ray classes is an open question – I’d be very interested to hear your findings on this."
  },
  {
    "objectID": "ray.html#ray-data",
    "href": "ray.html#ray-data",
    "title": "Part 2: distributed computing with Ray",
    "section": "Ray Data",
    "text": "Ray Data\nRay Data is a parallel data processing library for ML workflows. As you will see in this section, Ray Data can be easily used for non-ML workflows. To process large datasets, Ray Data uses streaming/lazy execution, i.e. processing does not happen until you try to access (“consume” in Ray’s language) the result.\n\nThe core object in Ray Data is a dataset which is a distributed data collection. Ray datasets can store general multidimensional array data that are too large to fit into a single machine’s memory. In Ray, these large arrays will be: 1. distributed in memory across a number of Ray tasks and 2. saved to disk once they are no longer in use.\nRay’s dataset operates over a sequence of Ray object references to blocks. Each block contains a disjoint subset of rows, and Ray Data loads and transforms these blocks in parallel. Each row in Ray’s datasets is a dictionary.\n\n\n\n\n\n\nNote\n\n\n\nRecall: a Python dictionary is a collection of key-value pairs.\n\n\n\nCreating datasets\nimport ray\nray.init(num_cpus=4, configure_logging=False)\n\nds = ray.data.range(1000)   # create a dataset from a range\nds                # Dataset(num_rows=1000, schema={id: int64})\nds.count()        # explicitly count the number of rows; might be expensive (to load the dataset into memory)\nds.take(3)        # return first 3 rows as a list of dictionaries; default keys are often 'id' or 'item'\nlen(ds.take())    # default is 20 rows\nds.show(3)        # first 3 rows in a different format (one row per line)\nUntil recently, you could easily displays the number of blocks in a dataset, but now you have to materialize it first, and the number of blocks can change during execution:\nds.materialize().num_blocks()   # will show the number of blocks; might be expensive\nRows can be generated from arbitrary items:\nray.data.from_items([10,20,30]).take()   # [{'item': 10}, {'item': 20}, {'item': 30}]\nRows can have multiple key-value pairs:\n\n\n\n\n\n\nNote\n\n\n\nRecall: each row is a dictionary, and dictionaries can have multiple entries.\n\n\nlistOfDict = [{\"col1\": i, \"col2\": i ** 2} for i in range(1000)] # each dict contains 2 pairs\nds = ray.data.from_items(listOfDict)\nds.show(5)\nA dataset can also be loaded from a file:\n\n\n\n\n\n\nNote\n\n\n\nThis example works on my computer (pyenv activate hpc-env), but not on the training cluster where arrow/19.0.1 was compiled without S3 support.\n\n\ndd = ray.data.read_csv(\"s3://anonymous@air-example-data/iris.csv\")   # load a predefined dataset\ndd\ndd.show()   # might pause to read data, default is 20 rows\n\n\nTransforming datasets\n\n\n\n\n\n\nRay datasets become useful once you start processing them. Let’s initialize a simple dataset from a range:\nimport ray\nray.init(num_cpus=4, configure_logging=False)\n\nds = ray.data.range(1000)   # create a dataset\nds.show(5)\nWe will apply a function to each row in this dataset. This function must return a dictionary that will form each row in the new dataset:\nds.map(lambda row: row).show(3)                  # takes a row, returns the same row\n\nds.map(lambda row: {\"key\": row[\"id\"]}).show(3)   # takes a row, returns a similar row;\n                       # `row[\"id\"]` is needed to refer to the value in each\n                       # original row; `key` is a new, arbitrary key name\n\na = ds.map(lambda row: {\"long\": str(row[\"id\"])*3})  # takes a row, returns a dict with 'id' values\n                                                    # converted to strings and repeated 3X\na   # it is a new dataset\na.show(5)\nWith .map() you can also use familiar non-lambda (i.e. non-anonymous) functions:\ndef squares(row):\n    return {'squares': row['id']**2}\n\nds.map(squares).show(5)\nYour function can also add new key-value pairs to the existing rows, instead of returning brand new rows:\nds = ray.data.range(1000)\ndef addSquares(row):\n    row['square'] = row['id']**2   # add a new entry to each row\n    return row                       \n\nb = ds.map(addSquares)\nds.show(5)  # original dataset\nb.show(5)   # contains both the original entries and the squares\nYou might have already noticed by now that all processing in Ray Data is lazy, i.e. it happens when we request results:\nds = ray.data.range(10_000_000)   # define a bigger dataset\nds.map(addSquares)   # no visible calculation, just a request, not consuming results\nds.map(addSquares).show()   # print results =&gt; start calculation\nds.map(addSquares).show()   # previous results were not stored, so this will re-run the calculation\n\nb = ds.map(addSquares)   # this does not start calculation\nb.show()                 # this starts calculation\nEvery time you print or use b, it’ll re-do the calculation. For this reason, you can think of b not as a variable with a value but as a data stream.\nHow can you do the calculation once and store the results for repeated use? You can convert b into a more permanent (not a data stream) object, e.g. a list:\nb = ds.map(addSquares)   # this does not start calculation\nc = b.take()             # create a list, fast (only first 20 elements)\nc = b.take(10_000_000)   # takes a couple of minutes, runs in parallel\nThis is not very efficient … certainly, computing 10,000,000 squares should not take a couple of minutes in parallel! The problem is that we have too many rows, and – similar to Python lists – Ray datasets perform poorly with too many rows. Think about subdividing your large computation into a number of chunks where each chunk comes with its own computational and communication overhead – you want to keep their number small.\nLet’s rewrite this problem:\n\n\n\n\n\n\n\n\n\nn = 10_000_000\nn1 = n // 2\nn2 = n - n1\n\nfrom time import time\nimport numpy as np\nds = ray.data.from_items([np.arange(n1), n1 + np.arange(n2)])\nds.show()   # 2 rows\n\ndef squares(row):\n    return {'squares': row['item']**2}   # compute element-wise square of an array\n\nstart = time()\nb = ds.map(squares).take()\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(b)\nNow the runtime is 0.072 seconds.\n\n\n\n\n\n\nVectorizable dataset transformations\nThe function .map_batches() will process batches (blocks) of rows, vectorizing operations on NumPy arrays inside each batch. We won’t study it here, as in practical terms it does not solve the specific problems in this course any faster. With .map_batches() you are limited in terms of data types that you can use inside a function passed to .map_batches() – in general it expects vectorizable NumPy arrays. If you are not planning to vectorize via map_batches(), use map() instead, and you will still get parallelization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlow series with Ray Data\nLet’s start with a serial implementation:\nfrom time import time\nimport ray\nray.init(configure_logging=False)\n\nintervals = ray.data.from_items([{\"a\": 1, \"b\": 100_000_000}])   # one item\nintervals.show()\n\ndef slow(row):\n    total = 0\n    for i in range(row['a'], row['b']+1):\n        if not \"9\" in str(i):\n            total += 1.0 / i\n    row['sum'] = total             # add key-value pair to the row\n    return row\n\nstart = time()\npartial = intervals.map(slow)      # define the calculation\ntotal = partial.take()[0]['sum']   # request the result =&gt; start the calculation on 1 CPU core\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\nprint(total)\nI get the average runtime of 6.978 seconds. To parallelize this, you can redefine intervals:\nintervals = ray.data.from_items([\n    {\"a\": 1, \"b\": 50_000_000},\n    {\"a\": 50_000_001, \"b\": 100_000_000},\n])\n\nstart = time()\npartial = intervals.map(slow)     # define the calculation\ntotal = sum([p['sum'] for p in partial.take()])   # request the result =&gt; start the calculation on 2 CPU cores\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))   # 4.322, 4.857, 4.782 and 3.852 3.812 3.823\nprint(total)\nOn 2 cores I get the average time of 3.765 seconds.\n\n\n\n\n\n\nQuestion 14\n\n\n\n\n\nParallelize this for 4 CPU cores. Hint: programmatically form a list of dictionaries, each containing two key-value pairs (a and b) with the sub-intervals.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn my laptop I am getting:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n6.978\n3.765\n1.675\n1.574\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing Ray datasets with Numba-compiled functions\nSo far we processed Ray datasets with non-Numba functions, either lambda (anonymous) or plain Python functions, and we obtained very good parallel scaling with multiple Ray processes. However, these functions are slower than their Numba-compiled versions.\nIs it possible to process Ray datasets with Numba-compiled functions, similar to how earlier we executed Numba-compiled functions on remote workers? The answer is a firm yes, but I will leave it to you to write an implementation for the slow series problem. Even though I have the solution on my laptop, I am not providing it here, as I feel this would be an excellent take-home exercise.\nA couple of considerations: 1. Numba does not work with Python dictionaries, instead providing its own dictionary type which is not compatible with Ray datasets. You can easily sidestep this problem, but you will have to find the solution yourself. 2. Very important: you must do small runs on workers to copy your functions over to them – no need to time these runs. Without this “pre-compilation” step you will not get fast execution on workers on the bigger problem the first time you run it.\n\nWith this Numba-compiled processing, on my laptop I am getting:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n0.447\n0.234\n0.238\n0.137\n\n\n\nI am not quite sure why going 2 → 4 cores does not result in better runtimes, but there could be some inherent overhead in Ray tasks implementation on my laptop that shows up in this small problem – or more likely the efficiency cores enter the calculation at this time?\nWith the same code on the training cluster I get better scaling:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n1.054\n0.594\n0.306\n0.209"
  },
  {
    "objectID": "ray.html#running-ray-workflows-on-a-single-node",
    "href": "ray.html#running-ray-workflows-on-a-single-node",
    "title": "Part 2: distributed computing with Ray",
    "section": "Running Ray workflows on a single node",
    "text": "Running Ray workflows on a single node\n\n\n\n\n\n\n\nQuestion 15\n\n\n\n\n\nLet’s try running the slow series without Numba-compiled functions on the training cluster as a batch job.\n1. Save the entire Python code for the slow series problem into rayPartialMap.py\n2. Modify the code to take  as a command-line argument:\nimport sys\nncores = int(sys.argv[1])\nray.init(num_cpus=ncores, configure_logging=False)\nand test it from the command line inside your interactive job\npython rayPartialMap.py $SLURM_CPUS_PER_TASK\n\nQuit the interactive job.\n\nBack on the login node, write a Slurm job submission script, in which you launch rayPartialMap.py in the last line of the script.\n\nSubmit your job with sbatch to 1, 2, 4, 8 CPU cores, all on the same node.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting on the training cluster:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n15.345\n7.890\n4.264\n2.756\n\n\n\nTesting on Cedar (averaged over 2 runs):\n\n\n\n\n\n\n\n\n\n\n\n\nncores\n1\n2\n4\n8\n16\n32\n\n\nwallclock runtime (sec)\n18.263\n9.595\n5.228\n3.048\n2.069\n1.836\n\n\n\nIn our Ray documentation there are instructions for launching a single-node Ray cluster. Strictly speaking, this is not necessary, as on a single machine (node) a call to ray.init() will start a new Ray cluster and will automatically connect to it."
  },
  {
    "objectID": "ray.html#running-ray-workflows-on-multiple-nodes",
    "href": "ray.html#running-ray-workflows-on-multiple-nodes",
    "title": "Part 2: distributed computing with Ray",
    "section": "Running Ray workflows on multiple nodes",
    "text": "Running Ray workflows on multiple nodes\n\nTo run Ray workflows on multiple cluster nodes, you must create a virtual Ray cluster first. You can find details of Ray’s virtual clusters in the official Ray documentation https://docs.ray.io/en/latest/cluster/getting-started.html.\nHere we’ll take a look at the example which I copied and adapted from our documentation at https://docs.alliancecan.ca/wiki/Ray#Multiple_Nodes. I made several changes in this workflow:\n\nmade it interactive,\nnot creating virtual environments in $SLURM_TMPDIR inside the job, but using the already existing one in /project/def-sponsor00/shared/hpc-env,\nremoved GPUs.\n\nLet’s quit our current Slurm job (if any), back on the login node start the following interactive job, and then run the following commands:\nmodule load python/3.12.4 arrow/19.0.1 scipy-stack/2025a netcdf/4.9.2\nsource /project/def-sponsor00/shared/hpc-env/bin/activate\n\nsalloc --nodes 2 --ntasks-per-node=1 --cpus-per-task=2 --mem-per-cpu=3600 --time=0:60:0\n\nexport HEAD_NODE=$(hostname)   # head node's address -- different from the login node!\nexport RAY_PORT=34567          # a port to start Ray on the head node \nNext, we will start a Ray cluster on the head node as a background process:\nray start --head --node-ip-address=$HEAD_NODE --port=$RAY_PORT --num-cpus=$SLURM_CPUS_PER_TASK --block &\nsleep 10   # wait for the prompt; it'll ask to enable usage stats collection\n...\n# Eventually should say \"Ray runtime started.\"\nThen on each node inside our Slurm job, except the head node, we launch the worker nodes of the Ray cluster:\ncat &lt;&lt; EOF &gt; launchRay.sh\n#!/bin/bash\nmodule load python/3.12.4 arrow/19.0.1 scipy-stack/2025a netcdf/4.9.2\nsource /project/def-sponsor00/shared/hpc-env/bin/activate\nif [[ \"\\$SLURM_PROCID\" -eq \"0\" ]]; then   # if MPI rank is 0\n        echo \"Ray head node already started...\"\n        sleep 10\nelse\n        ray start --address \"${HEAD_NODE}:${RAY_PORT}\" --num-cpus=\"${SLURM_CPUS_PER_TASK}\" --block\n        sleep 5\n        echo \"Ray worker started!\"\nfi\nEOF\nchmod u+x launchRay.sh\nsrun launchRay.sh &\nray_cluster_pid=$!   # get its process ID\nNext, we launch a Python script that connects to the Ray cluster, checks the nodes and all available CPUs:\nimport ray\nimport os\nray.init(address=f\"{os.environ['HEAD_NODE']}:{os.environ['RAY_PORT']}\",_node_ip_address=os.environ['HEAD_NODE'])\n...\n# Eventually should say \"Connected to Ray cluster.\"\nprint(\"Nodes in the Ray cluster:\", ray.nodes())   # should see two nodes with 'Alive' status\nprint(ray.available_resources())                  # should see 4 CPUs and 2 nodes\nFinally, from bash, we shut down the Ray worker nodes:\nkill $ray_cluster_pid\nand terminate the job."
  },
  {
    "objectID": "ray.html#distributed-data-processing-on-ray-and-io-bound-workflows",
    "href": "ray.html#distributed-data-processing-on-ray-and-io-bound-workflows",
    "title": "Part 2: distributed computing with Ray",
    "section": "Distributed data processing on Ray and I/O-bound workflows",
    "text": "Distributed data processing on Ray and I/O-bound workflows\n\nSimple distributed dataset example\nWith multiple CPU cores available, run the following Python code line by line, while watching memory usage in a separate window with htop --filter \"ray::IDLE\":\nimport numpy as np\nimport ray\n\nray.init(configure_logging=False, _system_config={ 'automatic_object_spilling_enabled':False })\n# 1. hide Ray's copious logging output\n# 2. start as many ray::IDLE process as the physical number of cores -- we'll use only two of them below\n# 3. disable automatic object spilling to disk\n\nb = ray.data.from_items([(800,800,800), (800,800,800)])   # 800**3 will take noticeable 3.81GB memory\nb.show()\n\ndef initArray(row):\n    nx, ny, nz = row['item']\n    row['array'] = np.zeros((nx,ny,nz))\n    return row\n\nc = b.map(initArray)\nc.show()\nWe’ll see the two arrays being initialized in memory, on two processes, with a couple of GBs of memory consumed per process. Ray writes (“spills”) objects to storage once they are no longer in use, as it tries to minimize the total number of “materialized” (in-memory) blocks. On Linux and MacOS, the temporary spill folder is /tmp/ray, but you can customize its location as described here.\nWith the automatic object spilling to disk disabled (our second flag to ray.init() above), these arrays will stay in memory.\nWith that flag removed, and hence with the usual automatic object spilling to disk, these array blocks will be automatically written to disk, and the memory usage goes back to zero after a fraction of a second. If next you try to access these arrays, e.g.\nd = c.take()\ntype(d[0]['array'])   # numpy.ndarray\nd[0]['array'].shape   # (800, 800, 800)\nthey will be loaded temporarily into memory, which you can monitor with htop --filter \"ray::IDLE\".\n\n\n\n\n\n\n\nPandas on Ray (Modin)\nYou can run many types of I/O workflows on top of Ray. One famous example is “Modin” (previously called Pandas on Ray) which is a drop-in replacement for Pandas on top of Ray. We won’t study here, but it will run all your pandas workflows, and you don’t need to modify your code, except importing the library, e.g.\nimport pandas as pd\ndata = pd.read_csv(\"filename.csv\", &lt;other flags&gt;)\nwill become\nimport modin.pandas as pd\nimport ray\ndata = pd.read_csv(\"filename.csv\", &lt;other flags&gt;)\nModin will run your workflows using Ray tasks on multiple cores, potentially speeding up large workflows. You can find more information at https://github.com/modin-project/modin\nSimilarly, many other Ray Data read functions will read your data in parallel, distributing it to multiple processes if necessary for larger processing:\n&gt;&gt;&gt; import ray\n&gt;&gt;&gt; ray.data.read_\nray.data.read_api                 ray.data.read_databricks_tables(  ray.data.read_mongo(              ray.data.read_sql(\nray.data.read_bigquery(           ray.data.read_datasource(         ray.data.read_numpy(              ray.data.read_text(\nray.data.read_binary_files(       ray.data.read_images(             ray.data.read_parquet(            ray.data.read_tfrecords(\nray.data.read_csv(                ray.data.read_json(               ray.data.read_parquet_bulk(       ray.data.read_webdataset(\n\n\n\n\nProcessing images\n\n\n\n\n\n\nNote\n\n\n\nThis example won’t work on the training cluster, where arrow/14.0.1 was compiled without proper filesystem support. However, I can demo this on my computer.\n\n\nHere is a simple example of processing a directory with images with Ray Data. Suppose we have a \\(2874\\times\n2154\\) image tuscany.avif. Let’s crop 100 random \\(300\\times 300\\) images out of it:\ntmpdir=${RANDOM}${RANDOM}\nmkdir -p ~/$tmpdir && cd ~/$tmpdir\npyenv activate hpc-env\n\nwget https://wgpages.netlify.app/img/tuscany.jpg\nls -l tuscany.jpg\nmkdir -p images\nwidth=$(identify tuscany.jpg | awk '{print $3}' | awk -Fx '{print $1}')\nheight=$(identify tuscany.jpg | awk '{print $3}' | awk -Fx '{print $2}')\n\nfor num in $(seq -w 00 99); do   # crop it into hundred 300x300 random images\n    echo $num\n    x=$(echo \"scale=8; $RANDOM / 32767 * ($width-300)\" | bc)   # $RANDOM goes from 0 to 32767\n    x=$(echo $x | awk '{print int($1+0.5)}')\n    y=$(echo \"scale=8; $RANDOM / 32767 * ($height-300)\" | bc)\n    y=$(echo $y | awk '{print int($1+0.5)}')\n    convert tuscany.jpg -crop 300x300+$x+$y images/small$num.png\ndone\nNow we will load these images into Ray Data. First, let’s set export RAY_DEDUP_LOGS=0, and then do:\nimport ray\nds = ray.data.read_images(\"images/\")\nds   # 100 rows (one image per row) split into ... blocks\nds.take(1)[0]                  # first image\ntype(ds.take(1)[0][\"image\"])   # stored as a numpy array\nds.take(1)[0][\"image\"].shape   # 300x300 and three channels (RGB)\nds.take(1)[0][\"image\"].max()   # 255 =&gt; they are stored as 8-bit images (0..255)\nLet’s print min/max values for all images:\ndef minmax(row):\n    print(row[\"image\"].min(), row[\"image\"].max())\n    return row  # must return a dictionary, otherwise `map` will fail\n\nb = ds.map(minmax)   # on output; we scheduled the calculation, but have not started it yet\nb.materialize()      # force the calculation =&gt; now we see printouts from individual rows\nLet’s compute their negatives:\ndef negate(row):\n    return {'image': 255-row['image']}\n\nnegative = ds.map(negate)\nnegative.write_images(\"output\", column='image')\nIn output/ subdirectory you will find 100 negative images.\n\n\nCoding parallel I/O by hand\nIn Ray Data you can also write your own parallel I/O workflows by hand, defining functions to process rows that will load certain data/file into a specific row, e.g. with something like this:\na = ray.data.from_items([\n    {'file': '/path/to/file1'},\n    {'file': '/path/to/file2'},\n    {'file': '/path/to/file3'}\n])\ndef readFileInParallel(row):\n    &lt;load data from file row['file']&gt;\n    &lt;process these data&gt;\n    row['status'] = 'done'\n    return row\n\nb = a.map(readFileInParallel)\nb.show()\n\n\nA CPU-intensive problem without reduction\nSo far we’ve been working with problems where calculations from individual tasks add to form a single number (sum of a slow series) – this is called reduction. Let’s now look at a problem without reduction, i.e. where results stay distributed.\nLet’s compute a mathematical Julia set defined as a set of points on the complex plane that remain bound under an infinite recursive transformation \\(z_{i+1}=f(z_i)\\). For the recursive function, we will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\n\npick a point \\(z_0\\in\\mathbb{C}\\)\ncompute iterations \\(z_{i+1}=z_i^2+c\\) until \\(|z_i|&gt;4\\) (arbitrary fixed radius; here \\(c\\) is a complex constant)\nstore the iteration number \\(\\xi(z_0)\\) at which \\(z_i\\) reaches the circle \\(|z|=4\\)\nlimit max iterations at 255\n4.1 if \\(\\xi(z_0)\\lt 255\\), then \\(z_0\\) is a stable point\n4.2 the quicker a point diverges, the lower its \\(\\xi(z_0)\\) is\nplot \\(\\xi(z_0)\\) for all \\(z_0\\) in a rectangular region   \\(-1.2&lt;=\\mathfrak{Re}(z_0)&lt;=1.2\\)   and   \\(-1.2&lt;=\\mathfrak{Im}(z_0)&lt;=1.2\\)\n\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\n\n\n\n\n\n\n\nNote\n\n\n\nYou might want to try these values too:\n\\(c = 1.2e^{1.1πi}\\) \\(~\\Rightarrow~\\) original textbook example\n\\(c = -0.4-0.59i\\) and 1.5X zoom-out \\(~\\Rightarrow~\\) denser spirals\n\\(c = 1.34-0.45i\\) and 1.8X zoom-out \\(~\\Rightarrow~\\) beans\n\\(c = 0.34-0.05i\\) and 1.2X zoom-out \\(~\\Rightarrow~\\) connected spiral boots\n\n\nAs you must be accustomed by now, this calculation runs much faster when implemented in a compiled language. I tried Julia (0.676s) and Chapel (0.489s), in both cases running the code on my laptop in serial (one CPU core), both with exactly the same workflow and the same \\(2000^2\\) image size.\nBelow is the serial implementation in Python – let’s save it as juliaSetSerial.py:\nfrom time import time\nimport numpy as np\n\nnx = ny = 2000   # image size\n\ndef pixel(z):\n    c = 0.355 + 0.355j\n    for i in range(1, 256):\n        z = z**2 + c\n        if abs(z) &gt;= 4:\n            return i\n    return 255\n\nprint(\"Computing Julia set ...\")\nstability = np.zeros((nx,ny), dtype=np.int32)\n\nstart = time()\nfor i in range(nx):\n    for k in range(ny):\n        point = 1.2*((2*(i+0.5)/nx-1) + (2*(k+0.5)/ny-1)*1j) # rescale to -1.2:1.2 in the complex plane\n        stability[i,k] = pixel(point)\n\nend = time()\nprint(\"Time in seconds:\", round(end-start,3))\n\nimport netCDF4 as nc\nf = nc.Dataset('test.nc', 'w', format='NETCDF4')\nf.createDimension('x', nx)\nf.createDimension('y', ny)\noutput = f.createVariable('stability', 'i4', ('x', 'y'))   # 4-byte integer\noutput[:,:] = stability\nf.close()\nHere we are timing purely the computational part, not saving the image to a NetCDF file. Let’s run it and look at the result in ParaView! When running this on my laptop, it takes 9.220 seconds.\nHow would you parallelize this problem with ray.data? The main points to remember are:\n\nYou need to subdivide your problem into blocks – let’s do this in the ny (vertical) dimension.\nYou have to construct a dataset with each row containing inputs for a single task. These inputs will be the size of each image block and the offset (the starting row number of this block inside the image).\nYou need to write a function computeStability(row) that acts on each input row in the dataset. The result will be a NumPy array stored as a new entry stability in each row.\nTo write the final image to a NetCDF file, you need to merge these arrays into a single \\(2000\\times 2000\\) array, and then write this square array to disk.\n\nThe solution is in the file juliaSetParallel.py on instructor’s laptop. Here are the runtimes for \\(2000\\times 2000\\) (averaged over three runs):\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n9.220\n4.869\n2.846\n2.210"
  },
  {
    "objectID": "ray.html#quickly-on-parallel-python-in-ml-frameworks-marie",
    "href": "ray.html#quickly-on-parallel-python-in-ml-frameworks-marie",
    "title": "Part 2: distributed computing with Ray",
    "section": "Quickly on parallel Python in ML frameworks (Marie)",
    "text": "Quickly on parallel Python in ML frameworks (Marie)\n\n\n\nMachine learning (ML) frameworks in Python usually come with their own parallelization tools, so you do not need to use general parallel libraries that we discussed in this course.\n\nJAX\n\nSingle-Program Multi-Data\nSeveral parallel computing sections in the advanced guide\n\n\n\nPyTorch\n\nPyTorch Distributed (torch.distributed) Overview\nDistributed and Parallel Training Tutorials\nGetting Started with DistributedDataParallel (DDP) module\nUsing multiple GPUs with DataParallel\n\n\n\nTensorFlow\n\nDistributed training with TensorFlow\nMulti-GPU and distributed training"
  },
  {
    "objectID": "programmableFilter.html",
    "href": "programmableFilter.html",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "",
    "text": "November 14th, 10am-noon Pacific Time\nInstructor: Alex Razoumov (SFU)\nSoftware: To participate in the course exercises, you will need to install ParaView on your computer and download this ZIP file (27MB). Unpack it to find the following data files: cube.txt, tabulatedGrid.csv, compact150.nc, and sineEnvelope.nc."
  },
  {
    "objectID": "programmableFilter.html#paraview",
    "href": "programmableFilter.html#paraview",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "ParaView",
    "text": "ParaView\n\n\nhttp://www.paraview.org\nStarted in 2000 as a collaboration between Los Alamos National Lab and Kitware Inc., later joined by Sandia National Labs and other partners; first public release in 2002\nSource code available on GitHub, or can be downloaded as a pre-compiled binary for Linux/Mac/Windows\nTo visualize extremely large datasets on distributed memory machines\nBoth interactive and Python scripting\nClient-server for remote interactive visualization\nUses MPI for distributed-memory parallelism on HPC clusters\nParaView is based on VTK (Visualization Toolkit)\n\nnot the only VTK-based open-source scientific renderer, e.g. VisIt, MayaVi (Python + numpy + scipy + VTK), an of course a number of Kitware’s own tools besides ParaView are based on VTK\nVTK can be used as a standalone renderer from C++, Python, and now JavaScript\n\n\nWe teach both beginner’s (typically full-day) ParaView workshops and additional, more advanced topics:\n🔵 VTK programming (overlap with today’s topic)\n✅ Web-based 3D scientific visualization: ParaViewWeb, VTK.js, ParaView Glance, ParaView Lite, Visualizer\n✅ ParaView Cinema\n✅ Catalyst in-situ visualization library\n🔵 AMR (multi-resolution) & multi-block datasets\n✅ CPU-based ray tracing and photorealistic rendering with OSPRay\n✏️ Ray tracing on CUDA-supported cards with Nvidia OptiX ✏️ Scalable 3D volumetric visualization on GPU clusters with NVIDIA IndeX\n✅ Remote and parallel interactive visualization\n✅ Batch visualization\n✅ Advanced scripting\n✅ Programmable Filter & Source in ParaView\n🔵 Writing ParaView plugins\n✅ Advanced animation\n✅ Topological data analysis\n✏️ Using ParaView with special hardware: HMDs, stereo projectors, Looking Glass\nYou can watch some of these webinar recordings at https://bit.ly/vispages.\n\nPython scripting\nInside ParaView you can use Python scripting in several places:\n\nAny ParaView workflow can be saved as a Python script (Tools | Start/Stop Trace)\n\nrun via pvpython, or pvbatch (especially useful for batch visualization)\nrun via View | Python Shell\n\nAny ParaView state can be saved as a Python script (File | Save State)\n\nrun via File | Load State\n\nFilters | Python Calculator\nFilters | Programmable Filter / Source\n\nWhy would you want to use the Programmable Filter / Source? Fundamentally, these tools let you create custom VTK objects directly from ParaView. Let’s say we want to plot a projection of a cubic dataset along one of its principal axes, or do some other transformation for which there is no filter.\n\nCalculator / Python Calculator filter cannot modify the geometry …\n\n\nVTK data types\n\nIn today’s examples we will create datasets in the following formats:\n\nvtkImageData,\nvtkPolyData, and\nvtkStructuredGrid.\n\nFor simplicity, we will skip the examples with Output Data Set Type = vtkUnstructuredGrid – that would produce the most versatile object, but with any unstructured cells you will have to describe the connections between their points needed to form these cells. You can find a couple of examples of creating vtkUnstructuredGrid in a 2021 webinar.\n\n\nProgrammable Filter workflow\n\nApply Programmable Filter to a ParaView pipeline object\nSelect Output Data Set Type: either Same as Input, or one of the VTK data types from above\nIn the Script box, write Python code: input from a pipeline object → output\n\n\nuse inputs[0].Points[:,0:3] and/or inputs[0].PointData['array name'] and/or inputs[0].CellData['array name'] to compute your output: points, cells, and data arrays\nsome objects take/pass multiple inputs[:]\n\n\n\n\nDepending on output’s type, might need to describe it in the RequestInformation Script box\nHit Apply\n\n\n\nProgrammable Source workflow\nSame as Programmable Filter but without an input.\n\nbuild an object programmatically\nread and process data from a file"
  },
  {
    "objectID": "programmableFilter.html#simple-programmable-filter-example",
    "href": "programmableFilter.html#simple-programmable-filter-example",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Simple Programmable Filter example",
    "text": "Simple Programmable Filter example\nStart with a very simple example. Let’s create a 2D Gaussian\n\\[f(\\vec r)=e^{-\\frac{|\\vec r-\\vec r_0|^2}{2\\sigma^2}},\\quad\\vec r\\in\\mathbb{R}^2\\]\ncentered at (0,0).\n\nApply Sources | Plane at \\(100^2\\) resolution\nApply Programmable Filter\n\n\nset Output Data Set Type = Same as Input\nthis will create the same discretization (vtkPolyData) as its input, without the fields (PointData)\n\n\nLet’s print some info\n\nprint(type(output))   # paraview.vtk.numpy_interface.dataset_adapter.PolyData\nprint(dir(output))\nprint(output.Points.shape)   # actually a 3D dataset\nprint(output.Points)         # but all z-coordinates are 0s\n\nPaste into Script\n\nx = inputs[0].Points[:,0]\ny = inputs[0].Points[:,1]\noutput.Points[:,2] = 0.5*exp(-(x**2+y**2)/(2*0.02))\ndataArray = 0.3 + 2*output.Points[:,2]\noutput.PointData.append(dataArray, \"pnew\")   # add the new data array to our output\n\nDisplay in 3D, colour with pnew\n\n\n\n\n\n\n\n\nA scalar field is stored as a flat, 1D array over 3D points\n\nprint(output.PointData[\"pnew\"].shape)\n\nMultiple ways to access data, e.g. these two lines point to the same array\n\nprint(output.PointData[\"pnew\"])\nprint(output.GetPointData().GetArray(\"pnew\"))\n\nThe same for points\n\nprint(output.GetNumberOfPoints())\nprint(output.GetPoint(1005))\nprint(output.Points[1005,:])"
  },
  {
    "objectID": "programmableFilter.html#programmable-source",
    "href": "programmableFilter.html#programmable-source",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Programmable Source",
    "text": "Programmable Source\nLet’s switch to the Programmable Source. It has no input, so we will need to set its Output Data Set Type to one of the VTK data types.\n\nBuild a custom reader\nA common use of Programmable Source is to prototype readers. Let’s quickly create our own reader for a CSV file with 3D data on a grid.\nWe will start with a conventional approach to reading CSV files:\n\nFile | Open, navigate to tabulatedGrid.csv, select CSV Reader\nPass tabular data through Table To Structured Grid\n\n\nset data extent in each dimension\nspecify x/y/z columns\ncolour with scalar\n\nNow let’s replace this with reading the CSV file directly to the Cartesian mesh, without using the File | Open dialogue. Reset your ParaView and then:\n\nApply Programmable Source, set Output Data Set Type = vtkImageData\nInto Script paste this code: \n\nimport numpy as np\ndata = np.genfromtxt(\"/Users/razoumov/programmableFilter/tabulatedGrid.csv\",\n                     dtype=None, names=True, delimiter=',', autostrip=True)\n\nnx = len(set(data['x']))\nny = len(set(data['y']))\nnz = len(set(data['z']))\n\noutput.SetDimensions(nx,ny,nz)\noutput.SetExtent(0,nx-1,0,ny-1,0,nz-1)\noutput.SetOrigin(0,0,0)\noutput.SetSpacing(.1,.1,.1)\noutput.AllocateScalars(vtk.VTK_FLOAT,1)\n\nvtk_data_array = vtk.util.numpy_support.numpy_to_vtk(num_array=data['scalar'], array_type=vtk.VTK_FLOAT)\nvtk_data_array.SetNumberOfComponents(1)\nvtk_data_array.SetName(\"density\")\noutput.GetPointData().SetScalars(vtk_data_array)\n\nThe Image (Uniform Rectilinear Grid) array is properly created (check the Information tab!), but nothing shows up …\n\nWe need to tell the ParaView pipeline about the dimensionality of our vtkImageData!\n\nInto Request Information paste this code:\n\nfrom paraview import util\nnx, ny, nz = 10, 10, 10\nutil.SetOutputWholeExtent(self, [0,nx-1,0,ny-1,0,nz-1])\n\n\nExercise: reveal the data in the cube\nThe file cube.txt has 125,000 integers that were computed and printed with this Julia code: - 50 lines - each line contains 2500 integers\nfor i in 1:50\n    for j in 1:50\n        for k in 1:50\n            print(data[i,j,k], \" \")\n        end\n    end\n    println()\nend\nYou need to modify the previous Programmable Source to read this data.\n\n\n\nWrite a 3D function directly into Cartesian mesh: numpy → VTK\nSimilar to using a Source to read data from a file, you can use a Source to create a discretized version of a 3D function.\n\nApply Programmable Source, set Output Data Set Type = vtkImageData\nInto Script paste this code:\n\nfrom numpy import linspace, sin, sqrt\n\nn = 100   # the size of our grid\noutput.SetDimensions(n,n,n)\noutput.SetOrigin(0,0,0)\noutput.SetSpacing(.1,.1,.1)\noutput.SetExtent(0,n-1,0,n-1,0,n-1)\noutput.AllocateScalars(vtk.VTK_FLOAT,1)\n\nx = linspace(-7.5,7.5,n)   # three orthogonal discretization vectors\ny = x.reshape(n,1)\nz = x.reshape(n,1,1)\ndata = ((sin(sqrt(y*y+x*x)))**2-0.5)/(0.001*(y*y+x*x)+1.)**2 + \\\n    ((sin(sqrt(z*z+y*y)))**2-0.5)/(0.001*(z*z+y*y)+1.)**2 + 1.\n\nvtk_data_array = vtk.util.numpy_support.numpy_to_vtk(\n    num_array=data.ravel(), deep=False, array_type=vtk.VTK_FLOAT)\nvtk_data_array.SetNumberOfComponents(1)\nvtk_data_array.SetName(\"density\")\noutput.GetPointData().SetScalars(vtk_data_array)   # use it as a scalar field in the output\n\nInto Request Information paste this code:\n\nfrom paraview import util\nn = 100\nutil.SetOutputWholeExtent(self, [0,n-1,0,n-1,0,n-1])\n\n\n\nSingle helix source example from ParaView docs\nLet’s create from scratch a Polygonal Data object.\n\nApply Programmable Source, set Output Data Set Type = vtkPolyData\nPaste this code:   \n\nimport numpy as np\nimport vtk.numpy_interface.algorithms as alg\n\nnumPoints, length, rounds = 300, 8.0, 5\n\nindex = np.arange(0, numPoints, dtype=np.int32)   # 0, ..., numPoints-1\nphi = rounds * 2 * np.pi * index / numPoints\nx, y, z = index * length / numPoints, np.sin(phi), np.cos(phi)\ncoordinates = alg.make_vector(x, y, z)   # numpy array (numPoints,3)\n\noutput.Points = coordinates             # set point coordinates\noutput.PointData.append(phi, 'angle')   # append a scalar field on points\n\npointIds = vtk.vtkIdList()\npointIds.SetNumberOfIds(numPoints)\nfor i in range(numPoints):   # define a single polyline connecting all the points in order\n   pointIds.SetId(i, i)      # point i in the line is formed from point i in vtkPoints\n\noutput.Allocate(1)     # allocate space for one vtkPolyLine 'cell' to the vtkPolyData object\noutput.InsertNextCell(vtk.VTK_POLY_LINE, pointIds)   # add this 'cell' to the vtkPolyData object\n\n\n\n\nProgrammatically generated point cloud\nHere we use exactly the same technique (use Source to create a vtkPolyData object, add a bunch of points, create a single cell) with a very different looking result.\n\nApply Programmable Source, set Output Data Set Type = vtkPolyData\nPaste this code:    \n\nfrom numpy import abs, random, sin, cos, pi, arcsin\n\nnumPoints = 1_000_000\nr = abs(random.randn(numPoints))               # 1D array drawn from a normal (Gaussian) distribution\ntheta = arcsin(2.*random.rand(numPoints)-1.)   # use 1D array drawn from a uniform [0,1) distribution\nphi = 2.*pi*random.rand(numPoints)             # use 1D array drawn from a uniform [0,1) distribution\n\nx = r*cos(theta)*sin(phi)\ny = r*cos(theta)*cos(phi)\nz = r*sin(theta)\ncoordinates = vtk.numpy_interface.algorithms.make_vector(x, y, z)   # numpy array (numPoints,3)\n\noutput.Points = coordinates              # set point coordinates\noutput.PointData.append(r, \"r\")          # append a scalar field on points\noutput.PointData.append(phi, \"phi\")      # append another scalar field on points\n\nPoints are not visible in ParaView ⇨ either (a) switch to Point Gaussian representation, or (b) in Script’s output create a single cell without connections\n\npointIds = vtk.vtkIdList()\npointIds.SetNumberOfIds(numPoints)\nfor p in range(numPoints):\n    pointIds.SetId(p, p)\noutput.Allocate(1)     # allocate space for a single cell\noutput.InsertNextCell(vtk.VTK_POLY_VERTEX, pointIds)\n\n\nColour by radius\nApply Clip (need the cell for that)\nOptionally switch back to Point Gaussian representation"
  },
  {
    "objectID": "programmableFilter.html#projection-to-a-plane-numpy-vtk",
    "href": "programmableFilter.html#projection-to-a-plane-numpy-vtk",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Projection to a plane: numpy → VTK",
    "text": "Projection to a plane: numpy → VTK\n\nLoad sineEnvelope.nc\nApply Programmable Filter, set Output Data Set Type = vtkImageData\nInto Script paste this code: \n\nnumPoints = inputs[0].GetNumberOfPoints()\nside = round(numPoints**(1./3.))\nlayer = side*side\nrho = inputs[0].PointData['density']\n\noutput.SetOrigin(inputs[0].GetPoint(0)[0], inputs[0].GetPoint(0)[1], -20.)\noutput.SetSpacing(1.0, 1.0, 1.0)\noutput.SetDimensions(side, side, 1)\noutput.SetExtent(0,99,0,99,0,1)\noutput.AllocateScalars(vtk.VTK_FLOAT, 1)\n\nrho3D = rho.reshape(side, side, side)\nvtk_data_array = vtk.util.numpy_support.numpy_to_vtk(rho3D.sum(axis=2).ravel(),\n                       deep=False, array_type=vtk.VTK_FLOAT)\nvtk_data_array.SetNumberOfComponents(1)\nvtk_data_array.SetName(\"projection\")\noutput.GetPointData().SetScalars(vtk_data_array)\n\nInto Request Information paste this code:\n\nfrom paraview import util\nn = 100\nutil.SetOutputWholeExtent(self, [0,n-1,0,n-1,0,0])"
  },
  {
    "objectID": "programmableFilter.html#exercise-plotting-spherical-dataset-as-3d-mollweide-map",
    "href": "programmableFilter.html#exercise-plotting-spherical-dataset-as-3d-mollweide-map",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Exercise: plotting spherical dataset as 3D Mollweide map",
    "text": "Exercise: plotting spherical dataset as 3D Mollweide map\n\n\n\nSpherical dataset animation: the traditional view\nBest Cover Visualization submission with the 3D Mollweide projection\nOn presenter’s laptop watch the full video open $(fd vis21g-sub1007-i5.mp4 ~/Documents)\n\n\nPlaying with the dataset in standalone Python\nimport xarray as xr\ndata = xr.open_dataset('/Users/razoumov/programmableFilter/compact150.nc')\nprint(data)                      # show all variables inside this dataset\n\nprint(data.r)                    # radial discretization\nprint(data.temperature.values)   # this is a 180x201x360 numpy array\ndata[\"temperature anomaly\"].shape\n\nprint(data.lon.values)\nprint(data.lat.values)\nAlthough the Mollweide projection is available in packages like matplotlib, we need a standalone implementation that takes the geographical coordinates and returns the Cartesian coordinates in the projection. I used the formulae from the wiki page to write my own function:\nfrom math import sin, cos, pi, sqrt, radians\ndef mollweide(lon, lat):\n    \"\"\"\n    lon: input longitude in degrees\n    lat: input latitude in degrees\n    \"\"\"\n    lam, phi = radians(lon), radians(lat)\n    lam0 = radians(180)   # central meridian\n    tolerance = 1.0e-5\n    if abs(abs(phi)-pi/2.) &lt; tolerance:\n        theta = phi\n    else:\n        theta, dtheta = phi, 1e3\n        while abs(dtheta) &gt; tolerance:\n            twotheta = 2 * theta\n            dtheta = theta\n            theta = theta - (twotheta + sin(twotheta) - pi*sin(phi))/(2 + 2*cos(twotheta))\n            dtheta -= theta\n    s2 = sqrt(2)\n    x = 2 * s2/pi * (lam-lam0) * cos(theta)\n    y = s2 * sin(theta)\n    return (x,y)\n\nlam = data.lon.values[100]\nphi = data.lat.values[50]\nmollweide(lam,phi)\n\n\nBack to ParaView\nOur goal is to create something like this: \n\nload compact150.nc, uncheck Spherical Coordinates\napply Programmable Filter, set Output Data Set Type = vtkStructuredGrid\nlet’s start playing with the data in the Script dialogue:\n\nprint(inputs[0].GetNumberOfPoints())         # 13024800 = 360*201*180\npointIndex = 0   # any inteteger between 0 and 13024799\nprint(inputs[0].GetPoint(pointIndex)[0:3])   # (0.0, 3485.0, 90.0)\nprint(inputs[0].PointData['temperature'])\nprint(inputs[0].PointData[\"temperature\"].GetValue(pointIndex))\nWe can paste the entire mollweide(lon, lat) function definition into the Programmable Filter, or – to shorten the code inside the filter – we can load it from a file. Let’s save it in mollweide.py and then test it from a standalone Python shell:\nimport sys\nsys.path.insert(0, \"/Users/razoumov/programmableFilter\")\nfrom mollweide import mollweide\nmollweide(100,3)\nHere is what our filter will look like:\nfrom math import sin, cos, pi, sqrt, radians\nimport numpy as np\nimport sys\nsys.path.insert(0, \"/Users/razoumov/programmableFilter\")\nfrom mollweide import mollweide\n\ntemp_in = inputs[0].PointData[\"temperature\"]\n\nnlon, nr, nlat = 360, 201, 180\npoints = vtk.vtkPoints()\npoints.Allocate(nlon*nr*nlat)\ntemp = vtk.vtkDoubleArray() # create vtkPoints instance, to contain 100^2 points in the projection\ntemp.SetName(\"temperature\")\n\noutput.SetExtent([0,nlon-1,0,nr-1,0,nlat-1])   # should match SetOutputWholeExtent below\nx, y = np.zeros((nlon,nlat)), np.zeros((nlon,nlat))\nfor i in range(nlat):   # the order of loops should reverse-match SetExtent\n    for j in range(nr):\n        for k in range(nlon):\n            inx = k + i*nlon*nr + j*nlon\n            if j == 0:   # inner radius = base layer\n                lon, r, lat = inputs[0].GetPoint(inx)[0:3]\n                x[k,i], y[k,i] = mollweide(lon,lat)\n            points.InsertNextPoint(x[k,i],y[k,i],0.01*j)\n            temp.InsertNextValue(temp_in.GetValue(inx))\n\noutput.SetPoints(points)\noutput.GetPointData().SetScalars(temp)\nOptionally, you can paste the following into RequestInformation Script:\nfrom paraview import util\nnlon, nr, nlat = 360, 201, 180\nutil.SetOutputWholeExtent(self,[0,nlon-1,0,nr-1,0,nlat-1])   # the order is fixed and copied from input"
  },
  {
    "objectID": "programmableFilter.html#saving-workflow",
    "href": "programmableFilter.html#saving-workflow",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Saving workflow",
    "text": "Saving workflow\n\nSave as a ParaView state file ⇨ filter’s Python code with appear inside an XML element\nSave as a Python state file ⇨ filter’s Python code with appear inside programmableFilter1.Script variable"
  },
  {
    "objectID": "programmableFilter.html#summary",
    "href": "programmableFilter.html#summary",
    "title": "Hands-on with ParaView’s Programmable Filter & Source for scientific visualization",
    "section": "Summary",
    "text": "Summary\n\nWorkflow suggestions\n\nuse print from inside the Filter/Source\ngo slowly and save frequently, as ParaView will crash when you don’t allocate objects properly inside Programmable Filter/Source\n\nLinks\n\nProgrammable Filter in the offical ParaView documentation\nan article NumPy to VTK: Converting your NumPy arrays to VTK arrays and files\nfirst half of the talk by Jean M. Favre (Swiss National Supercomputing Centre)\n\nFuture topic: converting Programmable Filter’s code to a plugin\n\ncode its own custom GUI Properties using Python decorators\nafter loading your plugin, it should be available as a normal filter or source in the menu"
  },
  {
    "objectID": "chapel2/chapel-02-variables.html",
    "href": "chapel2/chapel-02-variables.html",
    "title": "Basic syntax and variables",
    "section": "",
    "text": "The basic concept of parallel computing is simple to understand: we divide our job into tasks that can be executed at the same time, so that we finish the job in a fraction of the time that it would have taken if the tasks are executed one by one.\n\nTask is a unit of computation that can run in parallel with other tasks. In this course, we’ll be using a more general term “task” that – depending on the context – could mean either a Unix process (MPI task or rank) or a Unix thread. Consequently, parallel execution in Chapel could mean either multiprocessing and multithreading, or both (hybrid parallelism). In Chapel in many cases this distinction is hidden from the programmer. {.note}\n\nImplementing parallel computations is not always easy. How easy it is to parallelize a code really depends on the underlying problem you are trying to solve. This can result in:\n\na fine-grained, or tightly coupled parallel code that needs a lot of communication / synchronization between tasks, or\na coarse-grained code that requires little communication between tasks.\n\nIn this sense grain size refers to the amount of independent computing in between communication events. An extreme case of a coarse-grained problem would be an embarrassing parallel problem where all tasks can be executed completely independent from each other (no communications required).\nIn the non-GPU part of this course we’ll be solving two numerical problems:\n\nJulia set is an embarrassingly parallel problem (no communication between tasks), and\nheat diffusion is a tightly coupled problem that requires communication between tasks at each step of the iteration.\n\nWe’ll start with a serial of the Julia set and will use it to learn the basics of Chapel.",
    "crumbs": [
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "chapel2/chapel-02-variables.html#types-of-parallel-problems",
    "href": "chapel2/chapel-02-variables.html#types-of-parallel-problems",
    "title": "Basic syntax and variables",
    "section": "",
    "text": "The basic concept of parallel computing is simple to understand: we divide our job into tasks that can be executed at the same time, so that we finish the job in a fraction of the time that it would have taken if the tasks are executed one by one.\n\nTask is a unit of computation that can run in parallel with other tasks. In this course, we’ll be using a more general term “task” that – depending on the context – could mean either a Unix process (MPI task or rank) or a Unix thread. Consequently, parallel execution in Chapel could mean either multiprocessing and multithreading, or both (hybrid parallelism). In Chapel in many cases this distinction is hidden from the programmer. {.note}\n\nImplementing parallel computations is not always easy. How easy it is to parallelize a code really depends on the underlying problem you are trying to solve. This can result in:\n\na fine-grained, or tightly coupled parallel code that needs a lot of communication / synchronization between tasks, or\na coarse-grained code that requires little communication between tasks.\n\nIn this sense grain size refers to the amount of independent computing in between communication events. An extreme case of a coarse-grained problem would be an embarrassing parallel problem where all tasks can be executed completely independent from each other (no communications required).\nIn the non-GPU part of this course we’ll be solving two numerical problems:\n\nJulia set is an embarrassingly parallel problem (no communication between tasks), and\nheat diffusion is a tightly coupled problem that requires communication between tasks at each step of the iteration.\n\nWe’ll start with a serial of the Julia set and will use it to learn the basics of Chapel.",
    "crumbs": [
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "chapel2/chapel-02-variables.html#case-study-1-computing-the-julia-set",
    "href": "chapel2/chapel-02-variables.html#case-study-1-computing-the-julia-set",
    "title": "Basic syntax and variables",
    "section": "Case study 1: computing the Julia set",
    "text": "Case study 1: computing the Julia set\nThis project is a mathematical problem to compute a Julia set, defined as a set of points on the complex plane that remain bound under infinite recursive transformation \\(f(z)\\). We will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\n\npick a point \\(z_0\\in\\mathbb{C}\\)\ncompute iterations \\(z_{i+1}=z_i^2+c\\) until \\(|z_i|&gt;4\\) (arbitrary fixed radius; here \\(c\\) is a complex constant)\nstore the iteration number \\(\\xi(z_0)\\) at which \\(z_i\\) reaches the circle \\(|z|=4\\)\nlimit max iterations at 255\n4.1 if \\(\\xi(z_0)=255\\), then \\(z_0\\) is a stable point\n4.2 the quicker a point diverges, the lower its \\(\\xi(z_0)\\) is\nplot \\(\\xi(z_0)\\) for all \\(z_0\\) in a rectangular region \\(-1&lt;=\\mathfrak{Re}(z_0)&lt;=1\\), \\(-1&lt;=\\mathfrak{Im}(z_0)&lt;=1\\)\n\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\n\n\n\n\n\n\n\nNote\n\n\n\nYou might want to try these values too:\n\\(c = 1.2e^{1.1πi}\\) \\(~\\Rightarrow~\\) original textbook example\n\\(c = -0.4-0.59i\\) and 1.5X zoom-out \\(~\\Rightarrow~\\) denser spirals\n\\(c = 1.34-0.45i\\) and 1.8X zoom-out \\(~\\Rightarrow~\\) beans\n\\(c = 0.34-0.05i\\) and 1.2X zoom-out \\(~\\Rightarrow~\\) connected spiral boots",
    "crumbs": [
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "chapel2/chapel-02-variables.html#variables",
    "href": "chapel2/chapel-02-variables.html#variables",
    "title": "Basic syntax and variables",
    "section": "Variables",
    "text": "Variables\nChapel is a statically typed language, i.e. the type of every variable must be known at compile time.\n\n\n\n\nVariables in Chapel are declared with the var or const keywords. When a variable declared as const is initialized, its value cannot be modified anymore during the execution of the program.\nTo declare a variable, we must either (1) specify its type or (2) initialize it in place with some value from which the compiler will infer its type. The common variable types in Chapel are:\n\ninteger int – defaults to int(64), or you can explicitly specify int(32),\nfloating point number real – defaults to real(64), or you can explicitly specify real(32),\nboolean bool,\nstring string\n\nIf a variable is declared without a type, Chapel will infer it from the given initial value, for example (let’s store this in juliaSetSerial.chpl):\n\nconst n = 2_000;   // vertical and horizontal size of our image\nAll these constant variables will be created as integers, and no other values can be assigned to these variables during the execution of the program.\nOn the other hand, if a variable is declared without an initial value, Chapel will initialize it with a default value depending on the declared type:\nvar y: real;        // vertical coordinate in our plot; real(64) variable set to 0.0\nvar point: complex; // current point in our image; complex(64) variable set to 0.0+0.0*1i\nOf course, we can use both, the initial value and the type, when declaring a varible as follows:\nconst c: complex = 0.355 + 0.355i;   // Julia set constant\n\nNote that these two notations are different, but produce the same result in the end:\nvar a: real = 10;   // we specify both the type and the value\nvar a = 10: real;   // we specify only the value (integer 10 converted to real)\n{.note}\n\nLet’s print our configuration after we set all parameters:\nwriteln('Computing ', n, 'x', n, ' Julia set ...');\nAlternatively, you can use formatted output:\nwritef(\"Computing %ix%i Julia set\\n\", n, n);\nFor other format specifiers, check the table at https://chapel-lang.org/docs/modules/standard/IO/FormattedIO.html\n\nPrinting variable’s type\nTo check a variable’s type, use .type query:\nvar x = 1e8:int;\ntype t = x.type;\nwriteln(t:string);\nor in a single line:\nwriteln((1e8:int).type:string);\nwriteln((0.355 + 0.355i).type: string);",
    "crumbs": [
      "Basic syntax and variables + Julia set description"
    ]
  },
  {
    "objectID": "chapel2/chapel-15-distributed-julia-set.html",
    "href": "chapel2/chapel-15-distributed-julia-set.html",
    "title": "Parallel Julia set",
    "section": "",
    "text": "Recall the serial code juliaSetSerial.chpl (without output):\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nNow let’s parallelize this code with forall in shared memory (single locale). Copy juliaSetSerial.chpl into juliaSetParallel.chpl and start modifying it:\n\nFor the outer loop, replace for with forall. This will produce an error about the scope of variables y and point:\n\nerror: cannot assign to const variable\nnote: The shadow variable 'y' is constant due to task intents in this loop\nerror: cannot assign to const variable\nnote: The shadow variable 'point' is constant due to task intents in this loop\n\n\nWhy do you think this message was produced? How do we solve this problem?\n\n\nWhat do we do next?\n\nCompile and run the code on several CPU cores on 1 node:\n#!/bin/bash\n# this is shared.sh\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./juliaSetParallel\nmodule load chapel-multicore/2.4.0\nchpl --fast juliaSetParallel.chpl\nsbatch shared.sh\nOnce you have the working shared-memory parallel code, study its performance.\nHere are my timings on the training cluster:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n1.181\n0.568\n0.307\n0.197\n\n\n\n\n\nWhy do you think the code’s speed does not scale linearly (~6X on 8 cores) with the number of cores?",
    "crumbs": [
      "Parallel Julia set"
    ]
  },
  {
    "objectID": "chapel2/chapel-15-distributed-julia-set.html#shared-memory-julia-set",
    "href": "chapel2/chapel-15-distributed-julia-set.html#shared-memory-julia-set",
    "title": "Parallel Julia set",
    "section": "",
    "text": "Recall the serial code juliaSetSerial.chpl (without output):\nuse Time;\n\nconfig const c = 0.355 + 0.355i;\n\nproc pixel(z0) {\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if abs(z) &gt;= 4 then\n      return i;\n  }\n  return 255;\n}\n\nconfig const n = 2_000;   // vertical and horizontal size of our image\nvar y: real;\nvar point: complex;\nvar watch: stopwatch;\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nvar stability: [1..n,1..n] int;\nwatch.start();\nfor i in 1..n {\n  y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nNow let’s parallelize this code with forall in shared memory (single locale). Copy juliaSetSerial.chpl into juliaSetParallel.chpl and start modifying it:\n\nFor the outer loop, replace for with forall. This will produce an error about the scope of variables y and point:\n\nerror: cannot assign to const variable\nnote: The shadow variable 'y' is constant due to task intents in this loop\nerror: cannot assign to const variable\nnote: The shadow variable 'point' is constant due to task intents in this loop\n\n\nWhy do you think this message was produced? How do we solve this problem?\n\n\nWhat do we do next?\n\nCompile and run the code on several CPU cores on 1 node:\n#!/bin/bash\n# this is shared.sh\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./juliaSetParallel\nmodule load chapel-multicore/2.4.0\nchpl --fast juliaSetParallel.chpl\nsbatch shared.sh\nOnce you have the working shared-memory parallel code, study its performance.\nHere are my timings on the training cluster:\n\n\n\nncores\n1\n2\n4\n8\n\n\nwallclock runtime (sec)\n1.181\n0.568\n0.307\n0.197\n\n\n\n\n\nWhy do you think the code’s speed does not scale linearly (~6X on 8 cores) with the number of cores?",
    "crumbs": [
      "Parallel Julia set"
    ]
  },
  {
    "objectID": "chapel2/chapel-15-distributed-julia-set.html#julia-set-on-distributed-domains",
    "href": "chapel2/chapel-15-distributed-julia-set.html#julia-set-on-distributed-domains",
    "title": "Parallel Julia set",
    "section": "Julia set on distributed domains",
    "text": "Julia set on distributed domains\nCopy juliaSetParallel.chpl into juliaSetDistributed.chpl and start modifying it:\n\nLoad BlockDist\nReplace\n\nvar stability: [1..n,1..n] int;\nwith\nconst mesh: domain(2) = {1..n, 1..n};\nconst distributedMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = mesh;\nvar stability: [distributedMesh] int;\n\nLook into the loop variables: currently we have\n\nforall i in 1..n {\n  var y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    var point = 2*(j-0.5)/n - 1 + y*1i;   // rescale to -1:1 in the complex plane\n    stability[i,j] = pixel(point);\n  }\n}\n– in the previous, shared-memory version of the code this fragment gave you a parallel loop running on multiple cores on the same node. If you run this loop now, it’ll run entirely on the first node!\nIn the distributed version of the code you want to loop in parallel over all elements of the distributed mesh distributedMesh (or, equivalently, over all elements of the distributed array stability) – this will send the computation to the locales holding these blocks:\nforall (i,j) in distributedMesh {\n  var y = 2*(i-0.5)/n - 1;\n  var point = 2*(j-0.5)/n - 1 + y*1i;\n  stability[i,j] = pixel(point);\n}\nor (equivalent):\nforall (i,j) in stability.domain {\n  var y = 2*(i-0.5)/n - 1;\n  var point = 2*(j-0.5)/n - 1 + y*1i;\n  stability[i,j] = pixel(point);\n}\nCompile and run a larger problem (\\(8000^2\\)) across several nodes:\n#!/bin/bash\n# this is distributed.sh\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --nodes=4\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\necho Running on $SLURM_NNODES nodes\n./juliaSetDistributed --n=8000 -nl $SLURM_NNODES\nsource /project/def-sponsor00/shared/syncHPC/startMultiLocale.sh\nchpl --fast juliaSetDistributed.chpl\nsbatch distributed.sh\nHere are my timings on the training cluster (even over a slow interconnect!):\n\n\n\n--nodes\n1\n2\n4\n4\n\n\n--cpus-per-task\n1\n1\n1\n8\n\n\nwallclock runtime (sec)\n36.56\n17.91\n9.51\n0.985\n\n\n\nThey don’t call it “embarrassing parallel” for nothing! There is some overhead at the start and at the end of computing each block, but this overhead is much smaller than the computing part itself, hence leading to almost perfect speedup.\n\nHere we have an example of a hybrid parallel code, utilizing multiples processes (one per locale) and multiple threads (on each locale) when available. {.note}",
    "crumbs": [
      "Parallel Julia set"
    ]
  },
  {
    "objectID": "chapel2/chapel-10-intro-parallel.html",
    "href": "chapel2/chapel-10-intro-parallel.html",
    "title": "Intro to parallel computing",
    "section": "",
    "text": "Chapel provides high-level abstractions for parallel programming no matter the grain size of your tasks, whether they run in a shared memory or a distributed memory environment, or whether they are executed “concurrently” (frequently switching between tasks) or truly in parallel. As a programmer you can focus on the algorithm: how to divide the problem into tasks that make sense in the context of the problem, and be sure that the high-level implementation will run on any hardware configuration. Then you could consider the specificities of the particular system you are going to use (whether is shared or distributed, the number of cores, etc.) and tune your code/algorithm to obtain a better performance.\nTo this effect, concurrency (the creation and execution of multiple tasks) and locality (on which set of resources these tasks are executed) are orthogonal (separate) concepts in Chapel. For example, we could have a set of several tasks that would be running as shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd again, Chapel could take care of all the stuff required to run our algorithm in most of the scenarios, but we can always add more specific detail to gain performance when targeting a particular scenario.\n\nTask parallelism is a style of parallel programming in which parallelism is driven by programmer-specified tasks. This is in contrast with data parallelism which is a style of parallel programming in which parallelism is driven by computations over collections of data elements or their indices. {.note}\n\nChapel provides functionality for both task- and data-parallel programming. Since task parallelism is lower level (you tell the computer how to divide your computation into tasks) than data parallelism, it is considerably more difficult. In this course we will focus mostly on data parallelism, but we will briefly cover task parallelism towards the end of the course.",
    "crumbs": [
      "Intro to parallel computing"
    ]
  },
  {
    "objectID": "chapel2/chapel-10-intro-parallel.html#parallel-programming-in-chapel",
    "href": "chapel2/chapel-10-intro-parallel.html#parallel-programming-in-chapel",
    "title": "Intro to parallel computing",
    "section": "",
    "text": "Chapel provides high-level abstractions for parallel programming no matter the grain size of your tasks, whether they run in a shared memory or a distributed memory environment, or whether they are executed “concurrently” (frequently switching between tasks) or truly in parallel. As a programmer you can focus on the algorithm: how to divide the problem into tasks that make sense in the context of the problem, and be sure that the high-level implementation will run on any hardware configuration. Then you could consider the specificities of the particular system you are going to use (whether is shared or distributed, the number of cores, etc.) and tune your code/algorithm to obtain a better performance.\nTo this effect, concurrency (the creation and execution of multiple tasks) and locality (on which set of resources these tasks are executed) are orthogonal (separate) concepts in Chapel. For example, we could have a set of several tasks that would be running as shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd again, Chapel could take care of all the stuff required to run our algorithm in most of the scenarios, but we can always add more specific detail to gain performance when targeting a particular scenario.\n\nTask parallelism is a style of parallel programming in which parallelism is driven by programmer-specified tasks. This is in contrast with data parallelism which is a style of parallel programming in which parallelism is driven by computations over collections of data elements or their indices. {.note}\n\nChapel provides functionality for both task- and data-parallel programming. Since task parallelism is lower level (you tell the computer how to divide your computation into tasks) than data parallelism, it is considerably more difficult. In this course we will focus mostly on data parallelism, but we will briefly cover task parallelism towards the end of the course.",
    "crumbs": [
      "Intro to parallel computing"
    ]
  },
  {
    "objectID": "chapel2/chapel-10-intro-parallel.html#running-single-local-parallel-chapel",
    "href": "chapel2/chapel-10-intro-parallel.html#running-single-local-parallel-chapel",
    "title": "Intro to parallel computing",
    "section": "Running single-local parallel Chapel",
    "text": "Running single-local parallel Chapel\nMake sure you have loaded the single-locale Chapel environment:\n$ module load chapel-multicore/2.4.0\n\n\n\nIn this lesson, we’ll be running on several cores on one node with a script shared.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./test",
    "crumbs": [
      "Intro to parallel computing"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html",
    "href": "chapel2/chapel-01-intro.html",
    "title": "Introduction to Chapel",
    "section": "",
    "text": "Modern, open-source parallel programming language developed at Cray Inc. (acquired by Hewlett Packard Enterprise in 2019).\nOffers simplicity and readability of scripting languages such as Python or Matlab: “Python for parallel programming”.\nCompiled language \\(\\Rightarrow\\) provides the speed and performance of Fortran and C.\nSupports high-level abstractions for data distribution and data parallel processing, and for task parallelism.\nBased on the PGAS (Partitioned Global Address space) programming model: can access variables in global address space from each node, a lot of behind-the-scenes work to reduce/buffer remote memory access.\nProvides data-driven placement of computations. \nDesigned around a multi-resolution philosophy: users can incrementally add more detail to their original code, to bring it as close to the machine as required, at the same time they can achieve anything you can normally do with MPI and OpenMP.\n\n\nThe Chapel community is fairly small: relatively few people know/use Chapel  ⇄  too few libraries. However, you can use functions/libraries written in other languages:",
    "crumbs": [
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#chapel-a-language-for-parallel-computing-on-large-scale-systems",
    "href": "chapel2/chapel-01-intro.html#chapel-a-language-for-parallel-computing-on-large-scale-systems",
    "title": "Introduction to Chapel",
    "section": "",
    "text": "Modern, open-source parallel programming language developed at Cray Inc. (acquired by Hewlett Packard Enterprise in 2019).\nOffers simplicity and readability of scripting languages such as Python or Matlab: “Python for parallel programming”.\nCompiled language \\(\\Rightarrow\\) provides the speed and performance of Fortran and C.\nSupports high-level abstractions for data distribution and data parallel processing, and for task parallelism.\nBased on the PGAS (Partitioned Global Address space) programming model: can access variables in global address space from each node, a lot of behind-the-scenes work to reduce/buffer remote memory access.\nProvides data-driven placement of computations. \nDesigned around a multi-resolution philosophy: users can incrementally add more detail to their original code, to bring it as close to the machine as required, at the same time they can achieve anything you can normally do with MPI and OpenMP.\n\n\nThe Chapel community is fairly small: relatively few people know/use Chapel  ⇄  too few libraries. However, you can use functions/libraries written in other languages:",
    "crumbs": [
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-chapel-codes-on-cedar-graham-béluga-narval",
    "href": "chapel2/chapel-01-intro.html#running-chapel-codes-on-cedar-graham-béluga-narval",
    "title": "Introduction to Chapel",
    "section": "Running Chapel codes on Cedar / Graham / Béluga / Narval",
    "text": "Running Chapel codes on Cedar / Graham / Béluga / Narval\nOn Alliance’s production clusters Cedar / Graham / Béluga / Narval we have three versions of Chapel:\n\nsingle-locale (single-node) chapel-multicore\nmulti-locale chapel-ucx for InfiniBand clusters (all newer clusters)\nolder multi-locale chapel-ofi for OmniPath clusters (Cedar)\n\nYou can find the documentation on running Chapel in our wiki.\nIf you want to start single-locale Chapel, you will need to load chapel-multicore module, e.g.\n$ module spider chapel-multicore   # list all versions\n$ module load chapel-multicore/2.4.0",
    "crumbs": [
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-chapel-codes-inside-a-docker-container",
    "href": "chapel2/chapel-01-intro.html#running-chapel-codes-inside-a-docker-container",
    "title": "Introduction to Chapel",
    "section": "Running Chapel codes inside a Docker container",
    "text": "Running Chapel codes inside a Docker container\nIf you are familiar with Docker and have it installed, you can run multi-locale Chapel inside a Docker container (e.g., on your laptop, or inside an Ubuntu VM on Arbutus):\n$ docker pull chapel/chapel  # will emulate a cluster with 4 cores/node\n$ mkdir -p ~/tmp\n$ docker run -v /home/ubuntu/tmp:/mnt -it -h chapel chapel/chapel  # map host's ~/tmp to container's /mnt\n$ cd /mnt\n$ apt-get update\n$ apt-get install nano  # install nano inside the Docker container\n$ nano test.chpl        # file is /mnt/test.chpl inside the container and ~ubuntu/tmp/test.chpl on the host VM\n$ chpl test.chpl\n$ ./test -nl 8\nYou can find more information at https://hub.docker.com/r/chapel/chapel",
    "crumbs": [
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-single-locale-chapel-in-macos",
    "href": "chapel2/chapel-01-intro.html#running-single-locale-chapel-in-macos",
    "title": "Introduction to Chapel",
    "section": "Running single-locale Chapel in MacOS",
    "text": "Running single-locale Chapel in MacOS\nYou can compile and run Chapel codes in MacOS. Multi-locale codes (e.g. containing distributed arrays) will compile but will run only as single-locale.\nbrew install chapel",
    "crumbs": [
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#running-chapel-codes-on-the-training-cluster",
    "href": "chapel2/chapel-01-intro.html#running-chapel-codes-on-the-training-cluster",
    "title": "Introduction to Chapel",
    "section": "Running Chapel codes on the training cluster",
    "text": "Running Chapel codes on the training cluster\n\nNow we will distribute the usernames and passwords. Once you have these, log in to the training cluster and do the following: 1. load single-locale Chapel and compile a simple code, 2. write a makefile for compiling Chapel codes, and 3. submit a serial job script to run Chapel on a compute node. {.note}\n\n\n\nOn the training cluster, you can start single-locale Chapel with:\n\n\n\n$ module load chapel-multicore/2.4.0\n\n\n\n\nLet’s write a simple Chapel code, compile and run it:\n$ cd ~/tmp\n$ nano test.chpl\n$     writeln('If you can see this, everything works!');\n$ chpl test.chpl\n$ ./test\nYou can optionally pass the flag --fast to the compiler to optimize the binary to run as fast as possible for the given architecture.\n\n\n\n\n\n\n\nDepending on the code, it might utilize one / several / all cores on the current node. The command above implies that you are allowed to utilize all cores. This might not be the case on an HPC cluster, where a login node is shared by many people at the same time, and where it might not be a good idea to occupy all cores on a login node with CPU-intensive tasks. Therefore, we’ll be running test Chapel codes inside submitted jobs on compute nodes.\nLet’s write the job script serial.sh:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --output=solution.out\n./test\nand then submit it:\n$ chpl test.chpl\n$ sbatch serial.sh\n$ sq                         # same as `squeue -u $USER`\n$ cat slurm-jobID.out\nAlternatively, we could work inside a serial interactive job:\n$ salloc --time=2:0:0 --mem-per-cpu=3600",
    "crumbs": [
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-01-intro.html#makefiles",
    "href": "chapel2/chapel-01-intro.html#makefiles",
    "title": "Introduction to Chapel",
    "section": "Makefiles",
    "text": "Makefiles\nIn the rest of this workshop, we’ll be compiling codes test.chpl, baseSolver.chpl, begin.chpl, cobegin.chpl and many others. To simplify compilation, we suggest writing a file called Makefile in your working directory:\n%: %.chpl\n    chpl $^ -o $@\nclean:\n    @find . -maxdepth 1 -type f -executable -exec rm {} +\nNote that the 2nd and the 4th lines start with TAB and not with multiple spaces – this is very important!\nWith this makefile, to compile any Chapel code, e.g. test.chpl, you would type:\n$ make test\nAdd --fast flag to the makefile to optimize your code. And you can type make clean to delete all executables in the current directory.",
    "crumbs": [
      "Introduction to Chapel"
    ]
  },
  {
    "objectID": "chapel2/chapel-06-command-line-arguments.html",
    "href": "chapel2/chapel-06-command-line-arguments.html",
    "title": "Using command-line arguments",
    "section": "",
    "text": "If we want to resize our image, we would need to edit the line const n = 2_000; in the code and then recompile it. Wouldn’t it be great if we could pass this number to the binary when it is called at the command line, without having to recompile it?\nThe Chapel mechanism for this is config variables. When a variable is declared with the config keyword, in addition to var or const, like this:\nconfig const n = 2_000;   // vertical and horizontal size of our image\nyou can its new value from the command line:\n\n$ chpl juliaSetSerial.chpl   # using the default value 2_000\n$ ./juliaSetSerial --n=500   # passing another value from the command line\n\n\n\n\n\n\nQuestion Basic.4\n\n\n\n\n\nMake c a config variable, and test passing different values.",
    "crumbs": [
      "Using command-line arguments"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html",
    "title": "Domains and data parallelism",
    "section": "",
    "text": "We start this section by recalling the definition of a range in Chapel. A range is a 1D set of integer indices that can be bounded or infinite:\nvar a: range = 1..10;    // 1, 2, 3, ..., 10\n\nvar x1 = 1234, x2 = 5678;\nvar b: range = x1..x2;   // using variables\n\nvar c: range(strides=strideKind.positive) = 2..10 by 2;\nwriteln(c);              // 2, 4, 6, 8, 10\n\nvar d = 2..10 by 2;      // the same but skipping type description\n\nvar e = 1.. ;            // unbounded range\nOn the other hand, domains are multi-dimensional (including 1D) sets of integer indices that are always bounded. To stress the difference between domain ranges and domains, domain definitions always enclose their indices in curly brackets. Ranges can be used to define a specific dimension of a domain:\nvar domain1to10: domain(1) = {1..10};                // 1D domain from 1 to 10 defined using the range 1..10\n\nvar twoDimensions: domain(2) = {-2..2, 0..2};        // 2D domain over a product of two ranges\n\nvar thirdDim: range = 1..16;                         // a range\nvar threeDims: domain(3) = {1..10, 5..10, thirdDim}; // 3D domain over a product of three ranges\n\nwrite('cycle through all points in 2D: ');\nfor idx in twoDimensions do\n  write(idx, ', ');\nwriteln();\n\nwrite('cycle using explicit tuples: ');\nfor (x,y) in twoDimensions {                         // can also cycle using explicit tuples (x,y)\n  write(x,\",\",y,\"  \");\n}\nwriteln();\nLet us define an \\(n^2\\) domain called mesh. It is defined by the single task in our code and is therefore defined in memory on the same node (locale 0) where this task is running. For each of \\(n^2\\) mesh points, let us print out:\n\nm.locale.id = the ID of the locale holding that mesh point (should be 0)\nhere.id = the ID of the locale on which the code is running (should be 0)\nhere.maxTaskPar = the number of available cores (max parallelism with 1 task/core) (should be 3)\n\n\nWe already saw some of these variables / functions: numLocales, Locales, here.id, here.name, here.numPUs(), here.physicalMemory(), here.maxTaskPar. {.note}\n\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n}; // a 2D domain defined in shared memory on a single locale\nforall m in mesh do                   // go in parallel through all n^2 mesh points\n  writeln(m, ' ', m.locale.id, ' ', here.id, ' ', here.maxTaskPar);\n((7, 1), 0, 0, 2)\n((1, 1), 0, 0, 2)\n((7, 2), 0, 0, 2)\n((1, 2), 0, 0, 2)\n...\n((6, 6), 0, 0, 2)\n((6, 7), 0, 0, 2)\n((6, 8), 0, 0, 2)\nNow we are going to learn two very important features of Chapel domains.",
    "crumbs": [
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html#local-domains",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html#local-domains",
    "title": "Domains and data parallelism",
    "section": "",
    "text": "We start this section by recalling the definition of a range in Chapel. A range is a 1D set of integer indices that can be bounded or infinite:\nvar a: range = 1..10;    // 1, 2, 3, ..., 10\n\nvar x1 = 1234, x2 = 5678;\nvar b: range = x1..x2;   // using variables\n\nvar c: range(strides=strideKind.positive) = 2..10 by 2;\nwriteln(c);              // 2, 4, 6, 8, 10\n\nvar d = 2..10 by 2;      // the same but skipping type description\n\nvar e = 1.. ;            // unbounded range\nOn the other hand, domains are multi-dimensional (including 1D) sets of integer indices that are always bounded. To stress the difference between domain ranges and domains, domain definitions always enclose their indices in curly brackets. Ranges can be used to define a specific dimension of a domain:\nvar domain1to10: domain(1) = {1..10};                // 1D domain from 1 to 10 defined using the range 1..10\n\nvar twoDimensions: domain(2) = {-2..2, 0..2};        // 2D domain over a product of two ranges\n\nvar thirdDim: range = 1..16;                         // a range\nvar threeDims: domain(3) = {1..10, 5..10, thirdDim}; // 3D domain over a product of three ranges\n\nwrite('cycle through all points in 2D: ');\nfor idx in twoDimensions do\n  write(idx, ', ');\nwriteln();\n\nwrite('cycle using explicit tuples: ');\nfor (x,y) in twoDimensions {                         // can also cycle using explicit tuples (x,y)\n  write(x,\",\",y,\"  \");\n}\nwriteln();\nLet us define an \\(n^2\\) domain called mesh. It is defined by the single task in our code and is therefore defined in memory on the same node (locale 0) where this task is running. For each of \\(n^2\\) mesh points, let us print out:\n\nm.locale.id = the ID of the locale holding that mesh point (should be 0)\nhere.id = the ID of the locale on which the code is running (should be 0)\nhere.maxTaskPar = the number of available cores (max parallelism with 1 task/core) (should be 3)\n\n\nWe already saw some of these variables / functions: numLocales, Locales, here.id, here.name, here.numPUs(), here.physicalMemory(), here.maxTaskPar. {.note}\n\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n}; // a 2D domain defined in shared memory on a single locale\nforall m in mesh do                   // go in parallel through all n^2 mesh points\n  writeln(m, ' ', m.locale.id, ' ', here.id, ' ', here.maxTaskPar);\n((7, 1), 0, 0, 2)\n((1, 1), 0, 0, 2)\n((7, 2), 0, 0, 2)\n((1, 2), 0, 0, 2)\n...\n((6, 6), 0, 0, 2)\n((6, 7), 0, 0, 2)\n((6, 8), 0, 0, 2)\nNow we are going to learn two very important features of Chapel domains.",
    "crumbs": [
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html#feature-1-arrays-on-top-of-domains",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html#feature-1-arrays-on-top-of-domains",
    "title": "Domains and data parallelism",
    "section": "Feature 1: arrays on top of domains",
    "text": "Feature 1: arrays on top of domains\nDomains can be used to define arrays of variables of any type on top of them. For example, let us define an \\(n^2\\) array of real numbers on top of mesh:\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n}; // a 2D domain defined in shared memory on a single locale\nvar T: [mesh] real;                   // a 2D array of reals defined in shared memory on a single locale (mapped onto this domain)\nforall t in T do                      // go in parallel through all n^2 elements of T\n  writeln(t, ' ', t.locale.id);\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\n0.0 0\n0.0 0\n0.0 0\n...\n0.0 0\n0.0 0\n0.0 0\nBy default, all \\(n^2\\) array elements are set to zero, and all of them are defined on the same locale as the underlying mesh. We can also cycle through all indices of T by accessing its domain:\nforall idx in T.domain do\n  writeln(idx, ' ', T[idx]);   // idx is a tuple (i,j); also print the corresponding array element\n(7, 1) 0.0\n(1, 1) 0.0\n(7, 2) 0.0\n(1, 2) 0.0\n...\n(6, 6) 0.0\n(6, 7) 0.0\n(6, 8) 0.0\nSince we use a paralell forall loop, the print statements appear in a random runtime order.\nWe can also define multiple arrays on the same domain:\nconst grid = {1..100};          // 1D domain\nconst alpha = 5;                // some number\nvar A, B, C: [grid] real;       // local real-type arrays on this 1D domain\nB = 2; C = 3;\nforall (a,b,c) in zip(A,B,C) do // parallel loop\n  a = b + alpha*c;              // simple example of data parallelism on a single locale\nwriteln(A);",
    "crumbs": [
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-14-domains-and-data-parallel.html#feature-2-distributed-domains",
    "href": "chapel2/chapel-14-domains-and-data-parallel.html#feature-2-distributed-domains",
    "title": "Domains and data parallelism",
    "section": "Feature 2: distributed domains",
    "text": "Feature 2: distributed domains\nThe second important property of Chapel domains is that they can span multiple locales (nodes).\nDomains are fundamental Chapel concept for distributed-memory data parallelism.\nLet us now define an \\(n^2\\) distributed (over several locales) domain distributedMesh mapped to locales in blocks. On top of this domain we define a 2D block-distributed array A of strings mapped to locales in exactly the same pattern as the underlying domain. Let us print out:\n\na.locale.id = the ID of the locale holding the element a of A\nhere.name = the name of the locale on which the code is running\nhere.maxTaskPar = the number of cores on the locale on which the code is running\n\nInstead of printing these values to the screen, we will store this output inside each element of A as a string: a = int + string + int is a shortcut for a = int:string + string + int:string\nuse BlockDist; // use standard block distribution module to partition the domain into blocks\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n};\nconst distributedMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = mesh;\nvar A: [distributedMesh] string; // block-distributed array mapped to locales\nforall a in A { // go in parallel through all n^2 elements in A\n  // assign each array element on the locale that stores that index/element\n  a = a.locale.id:string + '-' + here.name[0..4] + '-' + here.maxTaskPar:string + '  ';\n}\nwriteln(A);\nThe syntax boundingBox=mesh tells the compiler that the outer edge of our decomposition coincides exactly with the outer edge of our domain. Alternatively, the outer decomposition layer could include an additional perimeter of ghost points if we specify the following:\nconst mesh: domain(2) = {1..n, 1..n};\nconst largerMesh: domain(2) dmapped new blockDist(boundingBox=mesh) = {0..n+1,0..n+1};\n\nbut let us not worry about this for now.\nRunning our code on 3 locales, with 2 cores per locale, produces the following output:\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n\nIf we were to run it on 4 locales, with 2 cores per locale, we might see something like this:\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n2-node4-2   2-node4-2   2-node4-2   2-node4-2   3-node3-2   3-node3-2   3-node3-2   3-node3-2\n{.note}\n\nAs we see, the domain distributedMesh (along with the string array A on top of it) was decomposed into 3x1 blocks stored on the three nodes, respectively. Equally important, for each element a of the array, the line of code filling in that element ran on the same locale where that element was stored. In other words, this code ran via the parallel forall loop on 3 nodes, using up to 2 cores on each node (or whatever you specified) to fill in the corresponding array elements. Once the parallel loop is finished, the last command writeln(A) runs on locale 0 gathering remote elements from the other locales and printing them to standard output.\nNow we can print the range of indices for each sub-domain by adding the following to our code:\nfor loc in Locales do\n  on loc do\n    writeln(A.localSubdomain());\nOn 3 locales we should get:\n{1..3, 1..8}\n{4..6, 1..8}\n{7..8, 1..8}\nLet us count the number of threads by adding the following to our code:\nvar count = 0;\nforall a in A with (+ reduce count) { // go in parallel through all n^2 elements\n  count = 1;\n}\nwriteln(\"actual number of threads = \", count);\nIf our n=8 is sufficiently large, there are probably enough array elements per node (\\(8*8/3\\approx 21\\) in our case) to fully utilize the two available cores on each node, so our output might be:\n$ chpl test.chpl -o test\n$ sbatch distributed.sh\n$ cat solution.out\nactual number of threads = 6\n\n\n\n\n\n\nQuestion Data.2\n\n\n\n\n\nTry reducing the array size n to see if that changes the output (fewer threads per locale), e.g., setting n=3. Also try increasing the array size to n=20 and study the output. Does the output make sense?\n\n\n\nSo far we looked at the block distribution BlockDist. It will distribute a 2D domain among nodes either using 1D or 2D decomposition (in our example it was 2D decomposition 2x2), depending on the domain size and the number of nodes.\nLet us take a look at another standard module for domain partitioning onto locales, called CyclicDist. For each element of the array we will print out again\n\na.locale.id = the ID of the locale holding the element a of A\nhere.name = the name of the locale on which the code is running\nhere.maxTaskPar = the number of cores on the locale on which the code is running\n\nuse CyclicDist; // elements are sent to locales in a round-robin pattern\nconfig const n = 8;\nconst mesh: domain(2) = {1..n, 1..n};  // a 2D domain defined in shared memory on a single locale\nconst m2: domain(2) dmapped new cyclicDist(startIdx=mesh.low) = mesh; // mesh.low is the first index (1,1)\nvar A2: [m2] string;\nforall a in A2 {\n  a = a.locale.id:string + '-' + here.name[0..4]:string + '-' + here.maxTaskPar:string + '  ';\n}\nwriteln(A2);\n$ chpl -o test test.chpl\n$ sbatch distributed.sh\n$ cat solution.out\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2   2-node3-2\n0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2   0-node1-2\n1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2   1-node2-2\n\nWith 4 locales, we might see something like this:\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2   0-node1-2   1-node4-2__\n2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2   2-node2-2   3-node3-2__\n{.note}\n\nAs the name CyclicDist suggests, the domain was mapped to locales in a cyclic, round-robin pattern. We can also print the range of indices for each sub-domain by adding the following to our code:\nfor loc in Locales do\n  on loc do\n    writeln(A2.localSubdomain());\n{1..8 by 3, 1..8}\n{1..8 by 3 align 2, 1..8}\n{1..8 by 3 align 0, 1..8}\nIn addition to BlockDist and CyclicDist, Chapel has several other predefined distributions: BlockCycDist, ReplicatedDist, DimensionalDist2D, ReplicatedDim, BlockCycDim, and StencilDist – for details please see Chapel’s documentation on distributions. I would particularly advise to check out StencilDist which is an enhanced variant of the blockDist distribution: it reduces the amount of communication necessary for array accesses across locale boundaries on a grid.",
    "crumbs": [
      "Domains and data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-20-fire-and-forget-tasks.html",
    "href": "chapel2/chapel-20-fire-and-forget-tasks.html",
    "title": "Fire-and-forget tasks",
    "section": "",
    "text": "Let’s now talk about task parallelism. This is a lower-level style of programming, in which you explicitly tell Chapel how to subdivide your computation into tasks. As this is more human-labour intensive than data parallelism, here we will only outline the main concepts and pitfalls, without going into too much detail.\nA Chapel program always starts as a single main task (and here we use the term “task” loosely as it could be a thread). You can then start concurrent tasks with the begin statement. A task spawned by the begin statement will run in a different task while the main task continues its normal execution. Let’s start a new code begin.chpl with the following lines:\nvar x = 100;\nwriteln('This is the main task starting first task');\nbegin {\n  var count = 0;\n  while count &lt; 10 {\n    count += 1;\n    writeln('task 1: ', x + count);\n  }\n}\nwriteln('This is the main task starting second task');\nbegin {\n  var count = 0;\n  while count &lt; 10 {\n    count += 1;\n    writeln('task 2: ', x + count);\n  }\n}\nwriteln('This is the main task, I am done ...');\n$ chpl begin.chpl -o begin\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task starting first task\nThis is the main task starting second task\nThis is the main task, I am done ...\ntask 2: 101\ntask 1: 101\ntask 2: 102\ntask 1: 102\ntask 2: 103\ntask 1: 103\ntask 2: 104\n...\ntask 1: 109\ntask 2: 109\ntask 1: 110\ntask 2: 110\nAs you can see the order of the output is not what we would expected, and actually it is somewhat unpredictable. This is a well known effect of concurrent tasks accessing the same shared resource at the same time (in this case the screen); the system decides in which order the tasks could write to the screen.\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\n\nWhat would happen if in the last code we move the definition of count into the main task, but try to assign it from tasks 1 and 2?\n\nAnswer: we’ll get an error at compilation (“cannot assign to const variable”), since then count would belong to the main task (would be defined within the scope of the main task), and we could modify its value only in the main task.\n\nWhat would happen if we try to insert a second definition var x = 10; inside the first begin statement?\n\nAnswer: that will actually work, as we’ll simply create another, local instance of x with its own value.\n\n\n\n\n\n\n\n\n\nKey idea\n\n\n\nAll variables have a scope in which they can be used. Variables declared inside a concurrent task are accessible only by that task. Variables declared in the main task can be read everywhere, but Chapel won’t allow other concurrent tasks to modify them.\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\nAre the concurrent tasks, spawned by the last code, running truly in parallel?\nAnswer: it depends on the number of cores available to your job. If you have a single core, they’ll run concurrently, with the CPU switching between the tasks. If you have two cores, task1 and task2 will likely run in parallel using the two cores.\n\n\n\n\n\n\n\n\n\nKey idea\n\n\n\nTo maximize performance, start as many tasks (threads) as the number of available cores.\n\n\nA slightly more structured way to start concurrent tasks in Chapel is by using the cobegin statement. Here you can start a block of concurrent tasks, one for each statement inside the curly brackets. Another difference between the begin and cobegin statements is that with the cobegin, all the spawned tasks are synchronized at the end of the statement, i.e. the main task won’t continue its execution until all tasks are done. Let’s start cobegin.chpl:\nvar x = 0;\nwriteln('This is the main task, my value of x is ', x);\ncobegin {\n  {\n    var x = 5;\n    writeln('This is task 1, my value of x is ', x);\n  }\n  writeln('This is task 2, my value of x is ', x);\n}\nwriteln('This message will not appear until all tasks are done ...');\n$ chpl cobegin.chpl -o cobegin\n$ sed -i -e 's|begin|cobegin|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task, my value of x is 0\nThis is task 2, my value of x is 0\nThis is task 1, my value of x is 5\nThis message will not appear until all tasks are done...\nAs you may have concluded from the Discussion exercise above, the variables declared inside a task are accessible only by the task, while those variables declared in the main task are accessible to all tasks.\nAnother, and one of the most useful ways to start concurrent/parallel tasks in Chapel, is the coforall loop. This is a combination of the for-loop and the cobeginstatements. The general syntax is:\ncoforall index in iterand\n{instructions}\nThis will start a new task (thread) for each iteration. Each task will then perform all the instructions inside the curly brackets. Each task will have a copy of the loop variable index with the corresponding value yielded by the iterand. This index allows us to customize the set of instructions for each particular task. Let’s write coforall.chpl:\nvar x = 10;\nconfig var numtasks = 2;\nwriteln('This is the main task: x = ', x);\ncoforall taskid in 1..numtasks {\n  var count = taskid**2;\n  writeln('this is task ', taskid, ': my value of count is ', count, ' and x is ', x);\n}\nwriteln('This message will not appear until all tasks are done ...');\n$ chpl coforall.chpl -o coforall\n$ sed -i -e 's|cobegin|coforall --numtasks=5|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main task: x = 10\nthis is task 1: my value of c is 1 and x is 10\nthis is task 2: my value of c is 4 and x is 10\nthis is task 4: my value of c is 16 and x is 10\nthis is task 3: my value of c is 9 and x is 10\nthis is task 5: my value of c is 25 and x is 10\nThis message will not appear until all tasks are done ...\nNotice the random order of the print statements. And notice how, once again, the variables declared outside the coforall can be read by all tasks, while the variables declared inside, are available only to the particular task.\n\n\n\n\n\n\nQuestion Task.1\n\n\n\n\n\nWould it be possible to print all the messages in the right order? Modify the code in the last example as required and save it as consecutive.chpl. Hint: you can use an array of strings declared in the main task, into which all the concurrent tasks could write their messages in the right order. Then, at the end, have the main task print all elements of the array.\n\n\n\n\n\n\n\n\n\nQuestion Task.2\n\n\n\n\n\nConsider the following code gmax.chpl to find the maximum array element. Complete this code, and also time the coforall loop.\nuse Random, Time;\nconfig const nelem = 1e8: int;\nvar x: [1..nelem] int;\nfillRandom(x);                     // fill array with random numbers\nvar gmax = 0;\n\nconfig const numtasks = 2;       // let's pretend we have 2 cores\nconst n = nelem / numtasks;      // number of elements per task\nconst r = nelem - n*numtasks;    // these elements did not fit into the last task\nvar lmax: [1..numtasks] int;    // local maxima for each task\ncoforall taskid in 1..numtasks {\n  var start, finish: int;\n  start = ...\n  finish = ...\n  ... compute lmax for this task ...\n}\n\n// put largest lmax into gmax\nfor taskid in 1..numtasks do                        // a serial loop\n  if lmax[taskid] &gt; gmax then gmax = lmax[taskid];\n\nwritef('The maximum value in x is %14.12dr\\n', gmax);   // formatted output\nwriteln('It took ', watch.elapsed(), ' seconds');\nWrite a parallel code to find the maximum value in the array x. Be careful: the number of tasks should not be excessive. Best to use numtasks to organize parallel loops. For each task compute the start and finish indices of its array elements and cycle through them to find the local maximum. Then in the main task cycle through all local maxima to find the global maximum.\n\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\n\n\nRun the code of last Exercise using different number of tasks, and different sizes of the array x to see how the execution time changes. For example:\n$ ./gmax --nelem=100_000_000 --numtasks=1\nDiscuss your observations. Is there a limit on how fast the code could run?\n\n\n\n\n\n\n\n\n\n\n\nTry this…\n\n\n\n\n\nSubstitute your addition to the code to find gmax in the last exercise with:\ngmax = max reduce x;   // 'max' is one of the reduce operators (data parallelism example)\nTime the execution of the original code and this new one. How do they compare?\n\n\n\n\n\n\n\n\n\n\nKey idea\n\n\n\nIt is always a good idea to check whether there is a built-in function or method that can do what we want as efficiently (or better) than our house-made code. In this case, the reduce statement reduces the given array to a single number using the operation max, and it is parallelized. Here is the full list of reduce operations: +   *   &&   ||   &   |   ^   min   max.",
    "crumbs": [
      "Fire-and-forget tasks"
    ]
  },
  {
    "objectID": "chapel2/chapel-11-single-locale-data-parallel.html",
    "href": "chapel2/chapel-11-single-locale-data-parallel.html",
    "title": "Single-locale data parallelism",
    "section": "",
    "text": "As we mentioned in the previous section, Data Parallelism is a style of parallel programming in which parallelism is driven by computations over collections of data elements or their indices. The main tool for this in Chapel is a forall loop – it’ll create an appropriate number of threads to execute a loop, dividing the loop’s iterations between them.\nWhat is the appropriate number of tasks/threads?\nConsider a simple code test.chpl:\nIn this code we update all elements of the array A. The code will run on a single node, lauching as many threads as the number of available cores. It is thread-safe, meaning that no two threads are writing into the same variable at the same time.",
    "crumbs": [
      "Single-locale data parallelism"
    ]
  },
  {
    "objectID": "chapel2/chapel-11-single-locale-data-parallel.html#reduction",
    "href": "chapel2/chapel-11-single-locale-data-parallel.html#reduction",
    "title": "Single-locale data parallelism",
    "section": "Reduction",
    "text": "Reduction\nConsider a simple code forall.chpl that we’ll run inside a 4-core interactive job. We have a range of indices 1..1000, and they get broken into 4 groups that are processed by individual threads:\nvar count = 0;\nforall i in 1..1000 with (+ reduce count) {   // parallel loop\n  count += i;\n}\nwriteln('count = ', count);\nIf we have not done so, let’s write a script shared.sh for submitting single-locale, two-core Chapel jobs:\n#!/bin/bash\n#SBATCH --time=0:5:0         # walltime in d-hh:mm or hh:mm:ss format\n#SBATCH --mem-per-cpu=3600   # in MB\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --output=solution.out\n./forall\n\n\n$ chpl forall.chpl -o forall\n$ sbatch shared.sh\n$ cat solution.out\ncount = 500500\n\nNumber of cores at runtime\nWe computed the sum of integers from 1 to 1000 in parallel. How many cores did the code run on? Looking at the code or its output, we don’t know. Most likely, on all 4 cores available to us inside the job. But we can actually check that! Do this:\n\nreplace count += i; with count = 1;\nchange the last line to writeln('actual number of threads = ', count);\n\n$ chpl forall.chpl -o forall\n$ sbatch shared.sh\n$ cat solution.out\nactual number of threads = 4\nIf you see one thread, try running this code as a batch multi-core job.\n\n\nAlternative syntax\nWe can also do parallel reduction over a loop in this way:\nvar count = (+ reduce forall i in 1..1000 do i**2);\nwriteln('count = ', count);\nWe can also initialize in array and do parallel reduction over all array elements:\nvar A = (for i in 1..1000 do i);\nvar count = (+ reduce A);   // multiple threads\nwriteln('count = ', count);\nOr we could do it this way if we want to do some processing on individual elements:\nvar A = (for i in 1..1000 do i);\nvar count = (+ reduce forall a in A do a**2);\nwriteln('count = ', count);\n \n\n\n\n\n\n\nQuestion Parallel \\(\\pi\\)\n\n\n\n\n\nUsing the first version of forall.chpl (where we computed the sum of integers 1..1000) as a template, write a Chapel code to compute \\(\\pi\\) by calculating the integral numerically via summation using forall parallelism. Implement the number of intervals as config variable.\nTo get you started, here is a serial version of this code pi.chpl:\nconfig const n = 1000;\nvar h, total: real;\nh = 1.0 / n;                          // interval width\nfor i in 1..n {\n  var x = h * ( i - 0.5 );\n  total += 4.0 / ( 1.0 + x**2);\n}\nwritef('pi is %3.10r\\n', total*h);    // C-style formatted write, r stands for real",
    "crumbs": [
      "Single-locale data parallelism"
    ]
  },
  {
    "objectID": "solutions-chapel.html",
    "href": "solutions-chapel.html",
    "title": "Solutions for Chapel course",
    "section": "",
    "text": "To see the evolution of the temperature at the top right corner of the plate, we just need to modify iout and jout. This corner correspond to the first row (iout=1) and the last column (jout=cols) of the plate.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 1.48171\nTemperature at iteration 40: 0.767179\n...\nTemperature at iteration 460: 0.068973\nTemperature at iteration 480: 0.0661081\nTemperature at iteration 500: 0.0634717\n\n\n\nTo get the linear distribution, the 80 degrees must be divided by the number of rows or columns in our plate. So, the following couple of for loops at the start of time iteration will give us what we want:\n// boundary conditions\nfor i in 1..rows do\n  T[i,cols+1] = i*80.0/rows;   // right side\nfor j in 1..cols do\n  T[rows+1,j] = j*80.0/cols;   // bottom side\nNote that 80 degrees is written as a real number 80.0. The division of integers in Chapel returns an integer, then, as rows and cols are integers, we must have 80 as real so that the result is not truncated.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nThe idea is simple: after each iteration of the while loop, we must compare all elements of Tnew and T, find the greatest difference, and update delta with that value. The following nested for loops should do the job:\n// update delta, the greatest difference between Tnew and T\ndelta = 0;\nfor i in 1..rows do {\n  for j in 1..cols do {\n    tmp = abs(Tnew[i,j] - T[i,j]);\n    if tmp &gt; delta then delta = tmp;\n  }\n}\nClearly there is no need to keep the difference at every single position in the array, we just need to update delta if we find a greater one.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nFor example, lets use a 650 x 650 grid and observe the evolution of the temperature at the position (200,300) for 10000 iterations or until the difference of temperature between iterations is less than 0.002; also, let’s print the temperature every 1000 iterations.\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ ./baseSolver --rows=650 --cols=650 --iout=200 --jout=300 --niter=10000 --tolerance=0.002 --nout=1000\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position after 7750 iterations is: 24.9671\nThe greatest difference in temperatures between the last two iterations was: 0.00199985\n\n\n\nWithout --fast the calculation will become slower by ~95X."
  },
  {
    "objectID": "solutions-chapel.html#part-1-basic-language-features",
    "href": "solutions-chapel.html#part-1-basic-language-features",
    "title": "Solutions for Chapel course",
    "section": "",
    "text": "To see the evolution of the temperature at the top right corner of the plate, we just need to modify iout and jout. This corner correspond to the first row (iout=1) and the last column (jout=cols) of the plate.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 1.48171\nTemperature at iteration 40: 0.767179\n...\nTemperature at iteration 460: 0.068973\nTemperature at iteration 480: 0.0661081\nTemperature at iteration 500: 0.0634717\n\n\n\nTo get the linear distribution, the 80 degrees must be divided by the number of rows or columns in our plate. So, the following couple of for loops at the start of time iteration will give us what we want:\n// boundary conditions\nfor i in 1..rows do\n  T[i,cols+1] = i*80.0/rows;   // right side\nfor j in 1..cols do\n  T[rows+1,j] = j*80.0/cols;   // bottom side\nNote that 80 degrees is written as a real number 80.0. The division of integers in Chapel returns an integer, then, as rows and cols are integers, we must have 80 as real so that the result is not truncated.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nThe idea is simple: after each iteration of the while loop, we must compare all elements of Tnew and T, find the greatest difference, and update delta with that value. The following nested for loops should do the job:\n// update delta, the greatest difference between Tnew and T\ndelta = 0;\nfor i in 1..rows do {\n  for j in 1..cols do {\n    tmp = abs(Tnew[i,j] - T[i,j]);\n    if tmp &gt; delta then delta = tmp;\n  }\n}\nClearly there is no need to keep the difference at every single position in the array, we just need to update delta if we find a greater one.\n$ chpl baseSolver.chpl -o baseSolver\n$ sbatch serial.sh\n$ tail -f solution.out\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 2.0859\nTemperature at iteration 40: 1.42663\n...\nTemperature at iteration 460: 0.826941\nTemperature at iteration 480: 0.824959\nTemperature at iteration 500: 0.823152\n\n\n\nFor example, lets use a 650 x 650 grid and observe the evolution of the temperature at the position (200,300) for 10000 iterations or until the difference of temperature between iterations is less than 0.002; also, let’s print the temperature every 1000 iterations.\n$ chpl --fast baseSolver.chpl -o baseSolver\n$ ./baseSolver --rows=650 --cols=650 --iout=200 --jout=300 --niter=10000 --tolerance=0.002 --nout=1000\nTemperature at iteration 0: 25.0\nTemperature at iteration 1000: 25.0\nTemperature at iteration 2000: 25.0\nTemperature at iteration 3000: 25.0\nTemperature at iteration 4000: 24.9998\nTemperature at iteration 5000: 24.9984\nTemperature at iteration 6000: 24.9935\nTemperature at iteration 7000: 24.9819\nFinal temperature at the desired position after 7750 iterations is: 24.9671\nThe greatest difference in temperatures between the last two iterations was: 0.00199985\n\n\n\nWithout --fast the calculation will become slower by ~95X."
  },
  {
    "objectID": "solutions-chapel.html#part-2-task-parallelism",
    "href": "solutions-chapel.html#part-2-task-parallelism",
    "title": "Solutions for Chapel course",
    "section": "Part 2: task parallelism",
    "text": "Part 2: task parallelism\n\nSolution to “Task.1”\nThe following code is a possible solution:\nvar x = 1;\nconfig var numthreads = 2;\nvar messages: [1..numthreads] string;\nwriteln('This is the main thread: x = ', x);\ncoforall threadid in 1..numthreads do {\n  var c = threadid**2;\n  messages[threadid] = 'this is thread ' + threadid:string + ': my value of c is ' + c:string + ' and x is ' + x:string;  // add to a string\n}\nwriteln('This message will not appear until all threads are done ...');\nfor i in 1..numthreads do  // serial loop, will be printed in sequential order\n  writeln(messages[i]);\n$ chpl consecutive.chpl -o consecutive\n$ sed -i -e 's|coforall --numthreads=5|consecutive --numthreads=5|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nThis is the main thread: x = 10\nThis message will not appear until all threads are done ...\nthis is thread 1: my value of c is 1 and x is 10\nthis is thread 2: my value of c is 4 and x is 10\nthis is thread 3: my value of c is 9 and x is 10\nthis is thread 4: my value of c is 16 and x is 10\nthis is thread 5: my value of c is 25 and x is 10\n\n\nSolution to “Task.2”\nconfig const numthreads = 12;     // let's pretend we have 12 cores\nconst n = nelem / numthreads;     // number of elements per thread\nconst r = nelem - n*numthreads;   // these did not fit into the last thread\nvar lmax: [1..numthreads] real;   // local maximum for each thread\n\ncoforall threadid in 1..numthreads do {   // each iteration processed by a separate thread\n  var start, finish: int;\n  start  = (threadid-1)*n + 1;\n  finish = (threadid-1)*n + n;\n  if threadid == numthreads then finish += r;    // add r elements to the last thread\n  for i in start..finish do\n    if x[i] &gt; lmax[threadid] then lmax[threadid] = x[i];\n }\n\nfor threadid in 1..numthreads do     // no need for a parallel loop here\n  if lmax[threadid] &gt; gmax then gmax = lmax[threadid];\n\n$ chpl --fast gmax.chpl -o gmax\n$ sed -i -e 's|coforall --numthreads=5|gmax|' shared.sh\n$ sbatch shared.sh\n$ cat solution.out\nthe maximum value in x is: 1.0\nWe use coforall to spawn threads that work concurrently in a fraction of the array. The trick here is to determine, based on the threadid, the initial and final indices that the thread will use. Each thread obtains the maximum in its fraction of the array, and finally, after the coforall is done, the main thread obtains the maximum of the array from the maximums of all threads.\n\n\nSolution to “Task.3”\nvar x = 0;\nwriteln('This is the main thread, my value of x is ', x);\n\nsync {\n  begin {\n     var x = 5;\n     writeln('this is thread 1, my value of x is ', x);\n  }\n  begin writeln('this is thread 2, my value of x is ', x);\n}\n\nwriteln('this message will not appear until all threads are done...');\n\n\nSolution to “Task.4”\nThe code most likely will lock (although sometimes it might not), as we’ll be hitting a race condition. Refer to the diagram for explanation.\n\n\nSolution to “Task.5”\nYou need two separate locks, and for simplicity increment them both:\nvar lock1, lock2: atomic int;\nconst numthreads = 5;\nlock1.write(0);   // the main thread set lock to zero\nlock2.write(0);   // the main thread set lock to zero\ncoforall id in 1..numthreads {\n  writeln('greetings form thread ', id, '... I am waiting for all threads to say hello');\n  lock1.add(1);              // thread id says hello and atomically adds 1 to lock\n  lock1.waitFor(numthreads);   // then it waits for lock=numthreads (which will happen when all threads say hello)\n  writeln('thread ', id, ' is done ...');\n  lock2.add(1);\n  lock2.waitFor(numthreads);\n  writeln('thread ', id, ' is really done ...');\n}"
  },
  {
    "objectID": "solutions-chapel.html#part-3-data-parallelism",
    "href": "solutions-chapel.html#part-3-data-parallelism",
    "title": "Solutions for Chapel course",
    "section": "Part 3: data parallelism",
    "text": "Part 3: data parallelism\n\nSolution to “Data.1”\nChange the line\nfor i in 1..n {\nto\nforall i in 1..n with (+ reduce total) {\n\n\nSolution to “Data.2”\nRun the code with\n$ ./test -nl 4 --n=3\n$ ./test -nl 4 --n=20\nFor n=3 we get fewer threads (7 in my case), for n=20 we still get 12 threads (the maximum available number of cores inside our job).\n\n\nSolution to “Data.3”\nSomething along the lines of m = here.id:string + '-' + m.locale.id:string; should work.\nIn most cases m.locale.id should be the same as here.id (computation follows data distribution).\n\n\nSolution to “Data.4”\nIt should be forall (i,j) in largerMesh[1..rows,1..cols] do (run on multiple locales in parallel) instead of forall (i,j) in mesh do (run in parallel on locale 0 only).\nAnother possible solution is forall (i,j) in Tnew.domain[1..rows,1..cols] do (run on multiple locales in parallel).\nAlso, we cannot have forall (i,j) in largerMesh do (run in parallel on multiple locales) as this would overwrite the boundaries.\n\n\nSolution to “Data.5”\nJust before temperature output (if count%nout == 0), insert the following:\n  var total = 0.0;\n  forall (i,j) in largerMesh[1..rows,1..cols] with (+ reduce total) do\n    total += T[i,j];\nand add total to the temperature output. It is decreasing as energy is leaving the system:\n$ chpl --fast parallel3.chpl -o parallel3\n$ ./parallel3 -nl 1 --rows=30 --cols=30 --niter=2000   # run this from inside distributed.sh\nTemperature at iteration 0: 25.0\nTemperature at iteration 20: 3.49566   21496.5\nTemperature at iteration 40: 2.96535   21052.6\n...\nTemperature at iteration 1100: 2.5809   18609.5\nTemperature at iteration 1120: 2.58087   18608.6\nTemperature at iteration 1140: 2.58085   18607.7\nFinal temperature at the desired position [1,30] after 1148 iterations is: 2.58084\nThe largest temperature difference was 9.9534e-05\nThe simulation took 0.114942 seconds\n\n\nSolution to “Data.6”\nHere is one possible solution examining the locality of the finite-difference stencil:\nvar message: [largerMesh] string = 'empty';\nand in the next line after computing Tnew[i,j] put\n    message[i,j] = \"%i\".format(here.id) + message[i,j].locale.id + message[i-1,j].locale.id +\n      message[i+1,j].locale.id + message[i,j-1].locale.id + message[i,j+1].locale.id + '  ';\nand before the end of the while loop\n  writeln(message);\n  assert(1&gt;2);\nThen run it\n$ chpl --fast parallel3.chpl -o parallel3\n$ ./parallel3 -nl 4 --rows=8 --cols=8   # run this from inside distributed.sh"
  },
  {
    "objectID": "make.html",
    "href": "make.html",
    "title": "Introduction to makefiles",
    "section": "",
    "text": "Originally, make command (first released in 1975) was created for automating compilations. Consider a large software project with hundreds of dependencies. When you compile it, each source file is converted into an object file, and then all of them are linked together to the libraries to form a final executable(s) or a final library.\nDay-to-day, you typically work on a small section of the program, e.g. debug a single function, with much of the rest of the program unchanged. When recompiling, it would be a waste of time to recompile all hundreds of source files every time you want to compile/run the code. You need to recompile just a single source file and then update the final executable.\nA makefile is a build manager to automate this process, i.e. to figure out what is up-to-date and what is not, and only run the commands that are necessary to rebuild the final target. A makefile is essentially a tree of dependencies stored in a text file along with the commands to create these dependencies. It ensures that if some of the source files have been updated, we only run the steps that are necessary to create the target with those new source files.\nMakefiles can be used for any project (not just compilation) with multiple steps producing intermediate results, when some of these steps are compute-heavy. Let’s look at an example! We will store the following text in the file text.md:\n## Design\n\nMakefiles can be used for automating any multiple-step workflow\nwhen there is a need to update only some targets, as opposed to\nrunning the entire workflow from start to finish.\n\n\\newpage\n\n## Make's builtin variables\n\n- `$@` is the \"target of this rule\"\n- `$ˆ` is \"all prerequisites of this rule\"\n- `$&lt;` is \"the first prerequisite of this rule\"\n- `$?` is \"all out-of-date prerequisites of this rule\"\nThis is our workflow to automate:\npandoc text.md -t beamer -o text.pdf\n\nwget https://wgpages.netlify.app/img/dolphin.png\nmagick dolphin.png dolphin.pdf\n\nwget https://wgpages.netlify.app/img/penguin.png\nmagick penguin.png penguin.pdf\n\ngs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\n/bin/rm -f dolphin.* penguin.* text.pdf\ncurl -F \"file=@slides.pdf\" https://temp.sh/upload && echo\nFirst version of Makefile automates creating of slides.pdf:\nslides.pdf: text.pdf dolphin.pdf penguin.pdf\n    gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\ntext.pdf: text.md\n    pandoc text.md -t beamer -o text.pdf\ndolphin.pdf: dolphin.png\n    magick dolphin.png dolphin.pdf\npenguin.pdf: penguin.png\n    magick penguin.png penguin.pdf\ndolphin.png:\n    wget https://wgpages.netlify.app/img/dolphin.png\npenguin.png:\n    wget https://wgpages.netlify.app/img/penguin.png\nRunning make will create the target slides.pdf – how many command will it run? That depends on how many intermediate files you have, and their timestamps.\n\n\n\n\n\n\nTest 1\n\n\n\nLet’s modify text.md, e.g. add a line there. The makefile will figure out what needs to be done to update slides.pdf. How many command will it run?\n\n\n\n\n\n\n\n\nTest 2\n\n\n\nLet’s remove dolphin.png. How many commands will make run?\n\n\n\n\n\n\n\n\nTest 3\n\n\n\nLet’s remove both PNG files. How many commands will make run?\n\n\nNow, add three special targets at the end:\nclean:\n    /bin/rm -f dolphin.* penguin.* text.pdf\ncleanall:\n    make clean\n    /bin/rm -f slides.pdf\nupload: slides.pdf\n    curl -F \"file=@slides.pdf\" https://temp.sh/upload && echo\nNext, we can make use of make’s builtin variables:\n\n$@ is the “target of this rule”\n$ˆ is “all prerequisites of this rule”\n$&lt; is “the first prerequisite of this rule”\n$? is “all out-of-date prerequisites of this rule”\n\n&lt; gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=slides.pdf {text,dolphin,penguin}.pdf\n---\n&gt; gs -dBATCH -dNOPAUSE -q -sDEVICE=pdfwrite -sOutputFile=$@ $^\n&lt; pandoc text.md -t beamer -o text.pdf\n---\n&gt; pandoc $^ -t beamer -o $@\nThe next simplification makes use of make wildcards to specify patterns:\n&lt; dolphin.pdf: dolphin.png\n&lt;   magick dolphin.png dolphin.pdf\n&lt; penguin.pdf: penguin.png\n&lt;   magick penguin.png penguin.pdf\n---\n&gt; %.pdf: %.png\n&gt;   magick $^ $@"
  },
  {
    "objectID": "chapel-gpu.html",
    "href": "chapel-gpu.html",
    "title": "GPU computing with Chapel",
    "section": "",
    "text": "June 12th, 1:30pm-4:30pm Pacific Time\nChapel was designed as a parallel-first programming language targeting any hardware that supports parallel execution: multicore processors, multiple nodes in an HPC cluster, and now also GPUs on HPC systems. This is very different from the traditional approach to parallel programming in which you would use a dedicated tool for each level of hardware parallelism, e.g. MPI, or OpenMP, or CUDA, etc.\nLater in this workshop I will show a compact Chapel code that uses multiple nodes on a cluster, multiple GPUs on each node, and automatically utilizes all available (in our Slum job) CPU cores on each node, while copying the data as needed between all these hardware layers. But let’s start with the basics!"
  },
  {
    "objectID": "chapel-gpu.html#running-gpu-chapel-on-the-alliance-systems",
    "href": "chapel-gpu.html#running-gpu-chapel-on-the-alliance-systems",
    "title": "GPU computing with Chapel",
    "section": "Running GPU Chapel on the Alliance systems",
    "text": "Running GPU Chapel on the Alliance systems\nIn general, you need to configure and compile Chapel for your specific GPU type and your specific cluster interconnect. For NVIDIA and AMD GPUs (the only ones currently supported), you must use LLVM as the backend compiler. With NVIDIA GPUs, you must build LLVM to use LLVM’s NVPTX backend to support GPU programming, so usually you cannot use the system-provided LLVM module – instead you should set CHPL_LLVM=bundled during Chapel compilation.\nAs of this writing, on the Alliance systems, you can use GPUs from Chapel on the following systems:\n\nnative multi-locale on Cedar, Graham, Béluga, Narval; the binaries might not be in place everywhere, so please get in touch if you want to run it on a specific system\non an Arbutus VM in a project with access to vGPUs (our plan for today)\nvia a single-locale GPU Chapel container on any Alliance system (clusters, cloud) with NVIDIA GPUs; let me know if you would like to access this container\n\nEfforts are underway to compile native Chapel 2.2 as a series of modules on all Alliance systems, but that might take a while."
  },
  {
    "objectID": "chapel-gpu.html#running-gpu-chapel-on-your-computer",
    "href": "chapel-gpu.html#running-gpu-chapel-on-your-computer",
    "title": "GPU computing with Chapel",
    "section": "Running GPU Chapel on your computer",
    "text": "Running GPU Chapel on your computer\nIf you have an NVIDIA GPU on your computer and run Linux, and have all the right GPU drivers and CUDA installed, it should be fairly straightforward to compile Chapel with GPU support. Here is what worked for me in AlmaLinux 9.4. Please let me know if these steps do not work for you.\nIn Windows, Chapel with GPU support works under the Windows Subsystem for Linux (WSL) as explained in this post. You could also run Chapel inside a Docker container, although you need to find a GPU-enabled Docker image."
  },
  {
    "objectID": "chapel-gpu.html#todays-setup",
    "href": "chapel-gpu.html#todays-setup",
    "title": "GPU computing with Chapel",
    "section": "Today’s setup",
    "text": "Today’s setup\nToday’s we’ll run Chapel on the virtual training cluster. We’ll now distribute the usernames and passwords – let’s try to log in.\nThere are two Chapel configurations we will use today.\n\nChapel with GPU support compiled for NVIDIA cards. We have 1 virtual GPU on the training cluster to share among all participants and the instructor, so we won’t be able to use it all at the same time. The idea is to try your final production code on this GPU allocating it only for a couple of minutes at a time per user:\n\n#!/bin/bash\n#SBATCH --time=00:02:00\n#SBATCH --mem=3600\n#SBATCH --gpus-per-node=1\nnvidia-smi\nchpl --fast test.chpl\n./test\nsource /project/def-sponsor00/shared/syncHPC/startSingleLocaleGPU.sh\nsbatch submit.sh\n\nWe will be doing most debugging on CPUs using the so-called ‘CPU-as-device’ mode to run a GPU code on a CPU. This is very handy for debugging a Chapel GPU code on a computer without a dedicated GPU and/or vendor SDK installed. You can find more details on this mode here. To enable this mode, I recompiled Chapel with export CHPL_GPU=cpu, but you need to load this version of Chapel separately, and you can use it via an interactive job:\n\nsource /project/def-sponsor00/shared/syncHPC/startSingleLocaleCPUasDevice.sh\nsalloc --time=2:0:0 --mem=3600\nchpl --fast test.chpl\n./test\nIn this mode there are some restrictions, e.g. data movement between the device and the host will not be captured (as there are no data moved!), some parallel reductions might not be available in this mode (to be confirmed), and the GPU kernel breakdown might be different. Still, all GPU kernels will be launched on the CPU, and you can even use some of the Chapel’s diagnostic features in this mode, e.g. @assertOnGpu and @gpu.assertEligible attributes will fail at compile time for ineligible loops.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease do not start nested Slurm jobs. When switching from one Chapel configuration to the other, please do this on the login node."
  },
  {
    "objectID": "chapel-gpu.html#useful-built-in-variables",
    "href": "chapel-gpu.html#useful-built-in-variables",
    "title": "GPU computing with Chapel",
    "section": "Useful built-in variables",
    "text": "Useful built-in variables\nFrom inside your Chapel code, you can access the following predefined variables:\n\nLocales is the list of locales (nodes) that your code can run on (invoked in code execution)\nnumLocales is the number of these locales\nhere is the current locale (node), and by extension current CPU\nhere.name is its name\nhere.maxTaskPar is the number of CPU cores on this locale\nhere.gpus is the list of available GPUs on this locale (sometimes called “sublocales”)\nhere.gpus.size is the number of available GPUs on this locale\nhere.gpus[0] is the first GPU on this locale\n\nLet’s try some of these out; store this code as test.chpl:\nwriteln(\"Locales: \", Locales);\nwriteln(\"on \", here.name, \" I see \", here.gpus.size, \" GPUs\");\nwriteln(\"and their names are: \", here.gpus);\nchpl --fast test.chpl\n./test\n\n\n\n\nLocales: LOCALE0\non cdr2514.int.cedar.computecanada.ca I see 1 GPUs\nand their names are: LOCALE0-GPU0\nThere is one GPU (even in ‘CPU-as-device’ mode), and it is available to us as the first (and only) element of the array here.gpus."
  },
  {
    "objectID": "chapel-gpu.html#our-first-gpu-code",
    "href": "chapel-gpu.html#our-first-gpu-code",
    "title": "GPU computing with Chapel",
    "section": "Our first GPU code",
    "text": "Our first GPU code\nTo benefit from GPU acceleration, you want to run a computation that can be broken into many independent identical pieces. An obvious example is a for loop in which each loop iteration does not depend on other iterations. Let’s run such an example on the GPU:\nconfig const n = 10;\non here.gpus[0] {\n  var A: [1..n] int;     // kernel launch to initialize an array\n  foreach i in 1..n do   // thread parallelism on a CPU or a GPU =&gt; kernel launch\n    A[i] = i**2;\n  writeln(\"A = \", A);    // copy A to host\n}\nA = 1 4 9 16 25 36 49 64 81 100\n\nthe array A is stored on the GPU\norder-independent loops will be executed in parallel on the GPU\nif instead of parallel foreach we use serial for, the loop will run on the CPU\nin our case the array A is both stored and computed on the GPU in parallel\ncurrently, to be computed on a GPU, an array must be stored on that GPU\nin general, when you run a code block on the device, parallel lines inside will launch kernels"
  },
  {
    "objectID": "chapel-gpu.html#alternative-syntax",
    "href": "chapel-gpu.html#alternative-syntax",
    "title": "GPU computing with Chapel",
    "section": "Alternative syntax",
    "text": "Alternative syntax\nWe can modify this code so that it runs on a GPU if present; otherwise, it will run on the CPU:\nvar operateOn =\n  if here.gpus.size &gt; 0 then here.gpus[0]   // use the first GPU\n  else here;                                // use the CPU\nwriteln(\"operateOn: \", operateOn);\nconfig const n = 10;\non operateOn {\n  var A: [1..n] int;\n  foreach i in 1..n do\n    A[i] = i**2;\n  writeln(\"A = \", A);\n}\nOf course, in ‘CPU-as-device’ mode this code will always run on the CPU. \n\n\nYou can also force a GPU check by hand:\nif here.gpus.size == 0 {\n  writeln(\"need a GPU ...\");\n  exit(1);\n}\noperateOn = here.gpus[0];"
  },
  {
    "objectID": "chapel-gpu.html#gpu-diagnostics",
    "href": "chapel-gpu.html#gpu-diagnostics",
    "title": "GPU computing with Chapel",
    "section": "GPU diagnostics",
    "text": "GPU diagnostics\nWrap our code into the following lines:\nuse GpuDiagnostics;\nstartGpuDiagnostics();\n...\nstopGpuDiagnostics();\nwriteln(getGpuDiagnostics());\noperateOn: LOCALE0-GPU0\nA = 1 4 9 16 25 36 49 64 81 100\n(kernel_launch = 2, host_to_device = 0, device_to_host = 10, device_to_device = 0)\nin 'CPU-as-device' mode: (kernel_launch = 2, host_to_device = 0, device_to_host = 0, device_to_device = 0)\nLet’s break down the events:\n\nwe have two kernel launches\n\n  var A: [1..n] int;     // kernel launch to initialize an array\n  foreach i in 1..n do   // kernel launch to run a loop in parallel\n    A[i] = i**2;\n\nwe copy 10 array elements device-to-host to print them (not shown in ‘CPU-as-device’ mode)\n\n  writeln(\"A = \", A);\n\nno other data transfer\n\nLet’s take a look at the example from https://chapel-lang.org/blog/posts/intro-to-gpus. They define a function:\nuse GpuDiagnostics;\nproc numKernelLaunches() {\n  stopGpuDiagnostics();   // assuming you were running one before\n  var result = getGpuDiagnostics().kernel_launch;\n  resetGpuDiagnostics();\n  startGpuDiagnostics();  // restart diagnostics\n  return result;\n}\nwhich can then be applied to these 3 examples (all in one on here.gpus[0] block):\nstartGpuDiagnostics();\non here.gpus[0] {\n  var E = 2 * [1,2,3,4,5]; // one kernel launch to initialize the array\n  writeln(E);\n  assert(numKernelLaunches() == 1);\n\n  use Math;\n  const n = 10;\n  var A = [i in 0..#n] sin(2 * pi * i / n); // one kernel launch\n  writeln(A);\n  assert(numKernelLaunches() == 1);\n\n  var rows, cols = 1..5;\n  var Square: [rows, cols] int;         // one kernel launch\n  foreach (r, c) in Square.indices do   // one kernel launch\n    Square[r, c] = r * 10 + c;\n  writeln(Square);\n  assert(numKernelLaunches() == 2); // 2 on GPU and 7 on CPU-as-device\n}\n\n\n\n\n\n\nNote\n\n\n\nOn CPU-as-device we have 7 kernel launches in the last block. My initial interpretation: one launch to initialize Square + one launch to access Square.indices + one launch per loop iteration, but it’s actually not entire correct …\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\n\n\nLet’s play with this in the CPU-as-device mode! Make the upper limit of rows, cols a config variable, recompile, and play with the array size."
  },
  {
    "objectID": "chapel-gpu.html#verifying-if-a-loop-can-run-on-a-gpu",
    "href": "chapel-gpu.html#verifying-if-a-loop-can-run-on-a-gpu",
    "title": "GPU computing with Chapel",
    "section": "Verifying if a loop can run on a GPU",
    "text": "Verifying if a loop can run on a GPU\nThe loop attribute @assertOnGpu (applied to a loop) does two things:\n\nat compilation, will fail to compile a code that cannot run on a GPU and will tell you why\nat runtime, will halt execution if called from outside a GPU\n\nConsider the following serial code:\nconfig const n = 10;\non here.gpus[0] {\n  var A: [1..n] int;\n  for i in 1..n do\n    A[i] = i**2;\n  writeln(\"A = \", A);\n}\nA = 1 4 9 16 25 36 49 64 81 100\nThis code compiles fine (chpl --fast test.chpl), and it appears to run fine, printing the array. But it does not run on the GPU! Let’s mark the for loop with @assertOnGpu and try to compile it again. Now we get:\nerror: loop marked with @assertOnGpu, but 'for' loops don't support GPU execution\nSerial for loops cannot run on a GPU! Without @assertOnGpu the code compiled for and ran on the CPU. To port this code to the GPU, replace for with either foreach or forall (both are parallel loops), and it should compile with @assertOnGpu.\n\n\n\n\n\n\nNote\n\n\n\nWhen running in ‘CPU-as-device’ mode, @assertOnGpu attribute will produce a warning “ignored with CHPL_GPU=cpu”.\n\n\nAlternatively, you can count kernel launches – it’ll be zero for the for loop.\nMore on @assertOnGpu and other attributes at https://chapel-lang.org/docs/main/modules/standard/GPU.html.\n\n\n\n\n\n\n\nNote\n\n\n\nStarting with Chapel 2.2, there is an additional attribute @gpu.assertEligible that asserts that a statement is suitable for GPU execution (same as @assertOnGpu), without requiring it to be executed on a GPU. This is perfect in the ‘CPU-as-device’ mode: fails to compile a for loop, but no warnings at runtime."
  },
  {
    "objectID": "chapel-gpu.html#timing-on-the-cpu",
    "href": "chapel-gpu.html#timing-on-the-cpu",
    "title": "GPU computing with Chapel",
    "section": "Timing on the CPU",
    "text": "Timing on the CPU\nLet’s pack our computation into a function, so that we can call it from both a CPU and a GPU. For timing, we can use a stopwatch from the Time module:\nuse Time;\n\nconfig const n = 10;\nvar watch: stopwatch;\n\nproc squares(device) {\n  on device {\n    var A: [1..n] int;\n    foreach i in 1..n do\n      A[i] = i**2;\n    writeln(\"A = \", A[n-2..n]); // last 3 elements\n  }\n}\n\nwriteln(\"--- on CPU:\"); watch.start();\nsquares(here);\nwatch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\n\nwatch.clear();\n\nwriteln(\"--- on GPU:\"); watch.start();\nsquares(here.gpus[0]);\nwatch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\n$ chpl --fast test.chpl\n$ ./test --n=100_000_000\n--- on CPU:\nA = 9999999600000004 9999999800000001 10000000000000000\nIt took 7.94598 seconds\n--- on GPU:\nA = 9999999600000004 9999999800000001 10000000000000000\nIt took 0.003673 seconds\nYou can also call start and stop functions from inside the on device block – they will still run on the CPU. We will see an example of this later in this workshop."
  },
  {
    "objectID": "chapel-gpu.html#timing-on-the-gpu",
    "href": "chapel-gpu.html#timing-on-the-gpu",
    "title": "GPU computing with Chapel",
    "section": "Timing on the GPU",
    "text": "Timing on the GPU\nObtaining timing from within a running CUDA kernel is tricky as you are running potentially thousands of simultaneous threads, so you definitely cannot measure the wallclock time. However, you can measure GPU clock cycles spent on a partucular part of the kernel function. The GPU module provides a function gpuClock() that returns the clock cycle counter (per multiprocessor), and it needs to be called to time code blocks within a GPU-enabled loop.\n\n\n\nHere is an example (modelled after measureGpuCycles.chpl) to demonstrate its use. This is not the most efficient code, as on the GPU we are parallelizing the loop with n=10 iterations, and then inside each iteration we run a serial loop to keep the (few non-idle) GPU cores busy, but it gives you an idea.\n\n\n\n\n\n\nNote\n\n\n\nDo not run this code in the ‘CPU-as-device’ mode, as its output will not be particularly meaningful: you need a physical GPU to see actual counts.\n\n\nuse GPU;\n\nconfig const n = 10;\n\non here.gpus[0] {\n  var A: [1..n] int;\n  var clockDiff: [1..n] uint;\n  @assertOnGpu foreach i in 1..n {\n    var start, stop: uint;\n    A[i] = i**2;\n    start = gpuClock();\n    for j in 0..&lt;1000 do\n      A[i] += i*j;\n    stop = gpuClock();\n    clockDiff[i] = stop - start;\n  }\n  writeln(\"Cycle count = \", clockDiff);\n  writeln(\"Time = \", (clockDiff[1]: real) / (gpuClocksPerSec(0): real), \" seconds\");\n  writeln(\"A = \", A);\n}\nCycle count = 227132 227132 227132 227132 227132 227132 227132 227132 227132 227132\nTime = 0.148452 seconds\nA = 49501 49604 49709 49816 49925 50036 50149 50264 50381 50500"
  },
  {
    "objectID": "chapel-gpu.html#prime-factorization-of-each-element-of-a-large-array",
    "href": "chapel-gpu.html#prime-factorization-of-each-element-of-a-large-array",
    "title": "GPU computing with Chapel",
    "section": "Prime factorization of each element of a large array",
    "text": "Prime factorization of each element of a large array\nNow let’s compute a more interesting problem where we do some significant processing of each element, but independently of other elements – this will port nicely to a GPU.\nPrime factorization of an integer number is finding all its prime factors. For example, the prime factors of 60 are 2, 2, 3, and 5. Let’s write a function that takes an integer number and returns the sum of all its prime factors. For example, for 60 it will return 2+2+3+5 = 12, for 91 it will return 7+13 = 20, for 101 it will return 101, and so on.\nproc primeFactorizationSum(n: int) {\n  var num = n, output = 0, count = 0;\n  while num % 2 == 0 {\n    num /= 2;\n    count += 1;\n  }\n  for j in 1..count do output += 2;\n  for i in 3..(sqrt(n:real)):int by 2 {\n    count = 0;\n    while num % i == 0 {\n      num /= i;\n      count += 1;\n    }\n    for j in 1..count do output += i;\n  }\n  if num &gt; 2 then output += num;\n  return output;\n}\nWe can test it quickly:\nwriteln(primeFactorizationSum(60));\nwriteln(primeFactorizationSum(91));\nwriteln(primeFactorizationSum(101));\nwriteln(primeFactorizationSum(100_000_000));\nSince 1 has no prime factors, we will start computing from 2, and then will apply this function to all integers in the range 2..n, where n is a larger number. We will do all computations separately from scratch for each number, i.e. we will not cache our results (caching can significantly speed up our calculations but the point here is to focus on brute-force computing).\nWith the procedure primeFactorizationSum defined, here is the CPU version primesSerial.chpl:\nconfig const n = 10;\nvar A: [2..n] int;\nfor i in 2..n do\n  A[i] = primeFactorizationSum(i);\n\nvar lastFewDigits =\n  if n &gt; 5 then n-4..n  // last 5 digits\n  else 2..n;            // or fewer\n\nwriteln(\"A = \", A[lastFewDigits]);\nHere is the GPU version primesGPU.chpl:\nconfig const n = 10;\non here.gpus[0] {\n  var A: [2..n] int;\n  @gpu.assertEligible foreach i in 2..n do\n    A[i] = primeFactorizationSum(i);\n  var lastFewDigits =\n    if n &gt; 5 then n-4..n  // last 5 digits\n    else 2..n;            // or fewer\n  writeln(\"A = \", A[lastFewDigits]);\n}\nchpl --fast primesSerial.chpl\n./primesSerial --n=10_000_000\nchpl --fast primesGPU.chpl\n./primesGPU --n=10_000_000\nIn both cases we should see the same output:\nA = 4561 1428578 5000001 4894 49\n\nLet’s add timing to both codes:\nuse Time;\nvar watch: stopwatch;\n...\nwatch.start();\n...\nwatch.stop(); writeln('It took ', watch.elapsed(), ' seconds');\nNote that this problem does not scale linearly with n, as with larger numbers you will get more primes. Here are my timings on Cedar’s V100 GPU:\n\n\n\nn\nCPU time in sec\nGPU time in sec\nspeedup factor\n\n\n\n\n1_000_000\n3.04051\n0.001649\n1844\n\n\n10_000_000\n92.8213\n0.042215\n2199\n\n\n100_000_000\n2857.04\n1.13168\n2525"
  },
  {
    "objectID": "chapel-gpu.html#finer-control",
    "href": "chapel-gpu.html#finer-control",
    "title": "GPU computing with Chapel",
    "section": "Finer control",
    "text": "Finer control\n\nThere are various settings that you can fine-tune via attributes for maximum performance, e.g. you can change the number of threads per block (default 512, should be a multiple of 32) when launching kernels:\n @gpu.blockSize(64) foreach i in 1..128 { ...}\nYou can also change the default when compiling Chapel via CHPL_GPU_BLOCK_SIZE variable, or when compiling Chapel codes by passing the flag --gpu-block-size=&lt;block_size&gt; to Chapel compiler\nAnother setting to play with is the number of iterations per thread:\n@gpu.itersPerThread(4) foreach i in 1..128 { ... }\nThis setting is probably specific to your computational problem. For these and other per-kernel attributes, please see this page."
  },
  {
    "objectID": "chapel-gpu.html#multiple-locales-and-multiple-gpus",
    "href": "chapel-gpu.html#multiple-locales-and-multiple-gpus",
    "title": "GPU computing with Chapel",
    "section": "Multiple locales and multiple GPUs",
    "text": "Multiple locales and multiple GPUs\nIf we have access to multiple locales and then multiple GPUs on each of those locales, we would utilize all this processing power through two nested loops, first cycling through all locales and then through all available GPUs on each locale:\ncoforall loc in Locales do\n  on loc {\n    writeln(\"on \", loc.name, \" I see \", loc.gpus.size, \" GPUs\");\n    coforall gpu in loc.gpus {\n      on gpu {\n        ... do some work in parallel ...\n      }\n    }\n  }\nHere we assume that we are running inside a multi-node job on the cluster, e.g.\nsalloc --time=1:0:0 --nodes=3 --mem-per-cpu=3600 --gpus-per-node=2 --account=...\nchpl --fast test.chpl\n./test -nl 3\nHow would we use this approach in practice? Let’s consider our primes factorization problem. Suppose we want to collect the results on one node (LOCALE0), maybe for printing or for some additional processing. We need to break our array A into pieces, each computed on a separate GPU from the total pool of 6 GPUs available to us inside this job. Here is our approach, following the ideas outlined in https://chapel-lang.org/blog/posts/gpu-data-movement – let’s store this file as primesGPU-distributed.chpl:\nimport RangeChunk.chunks;\n\nproc primeFactorizationSum(n: int) {\n  ...\n}\n\nconfig const n = 1000;\nvar A_on_host: [2..n] int;   // suppose we want to collect the results on one node (LOCALE0)\n\n// let's assume that numLocales = 3\n\ncoforall (loc, locChunk) in zip(Locales, chunks(2..n, numLocales)) {\n  /* loc=LOCALE0, locChunk=2..334 */\n  /* loc=LOCALE1, locChunk=335..667 */\n  /* loc=LOCALE2, locChunk=668..1000 */\n  on loc {\n    writeln(\"loc = \", loc, \"   chunk = \", locChunk);\n    const numGpus = here.gpus.size;\n    coforall (gpu, gpuChunk) in zip(here.gpus, chunks(locChunk, numGpus)) {\n      /* on LOCALE0 will see gpu=LOCALE0-GPU0, gpuChunk=2..168 */\n      /*                     gpu=LOCALE0-GPU1, gpuChunk=169..334 */\n      on gpu {\n        writeln(\"loc = \", loc, \"   gpu = \", gpu, \"   chunk = \", gpuChunk);\n        var A_on_device: [gpuChunk] int;\n        foreach i in gpuChunk do\n          A_on_device[i] = primeFactorizationSum(i);\n        A_on_host[gpuChunk] = A_on_device; // copy the chunk from the GPU via the host to LOCALE0\n      }\n    }\n  }\n}\n\nvar lastFewDigits = if n &gt; 5 then n-4..n else 2..n;   // last 5 or fewer digits\nwriteln(\"last few A elements: \", A_on_host[lastFewDigits]);\n\nRunning multi-GPU code on Cedar\nWhen we run this code on 2 Cedar nodes with 2 GPUs per node:\nsource /home/razoumov/startMultiLocaleGPU.sh\ncd ~/scratch\nsalloc --time=0:15:0 --nodes=2 --cpus-per-task=1 --mem-per-cpu=3600 --gpus-per-node=v100l:2 \\\n       --account=cc-debug\nchpl --fast primesGPU-distributed.chpl\n./primesGPU-distributed -nl 2\n\nwe get the following output:\nloc = LOCALE0   chunk = 2..501\nloc = LOCALE0   gpu = LOCALE0-GPU0   chunk = 2..251\nloc = LOCALE0   gpu = LOCALE0-GPU1   chunk = 252..501\nloc = LOCALE1   chunk = 502..1000\nloc = LOCALE1   gpu = LOCALE1-GPU0   chunk = 502..751\nloc = LOCALE1   gpu = LOCALE1-GPU1   chunk = 752..1000\nlast few A elements: 90 997 501 46 21\n\n\nDistributed A_on_host array\nIn the code above, the array A_on_host resides entirely in host’s memory on one node. With a sufficiently large problem, you can distribute A_on_host across multiple nodes using block distribution:\nuse BlockDist; // use standard block distribution module to partition the domain into blocks\nconfig const n = 1000;\nconst distributedMesh: domain(1) dmapped new blockDist(boundingBox={2..n}) = {2..n};\nvar A_on_host: [distributedMesh] int;\nThis way, when copying from device to host, you will copy only to the locally stored part of A_on_host."
  },
  {
    "objectID": "chapel-gpu.html#julia-set-problem",
    "href": "chapel-gpu.html#julia-set-problem",
    "title": "GPU computing with Chapel",
    "section": "Julia set problem",
    "text": "Julia set problem\nIn the Julia set problem we need to compute a set of points on the complex plane that remain bound under infinite recursive transformation \\(f(z)\\). We will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\n\npick a point \\(z_0\\in\\mathbb{C}\\)\ncompute iterations \\(z_{i+1}=z_i^2+c\\) until \\(|z_i|&gt;4\\) (arbitrary fixed radius; here \\(c\\) is a complex constant)\nstore the iteration number \\(\\xi(z_0)\\) at which \\(z_i\\) reaches the circle \\(|z|=4\\)\nlimit max iterations at 255\n4.1 if \\(\\xi(z_0)=255\\), then \\(z_0\\) is a stable point\n4.2 the quicker a point diverges, the lower its \\(\\xi(z_0)\\) is\nplot \\(\\xi(z_0)\\) for all \\(z_0\\) in a rectangular region \\(-1&lt;=\\mathfrak{Re}(z_0)&lt;=1\\), \\(-1&lt;=\\mathfrak{Im}(z_0)&lt;=1\\)\n\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\n\n\n\n\n\n\n\nNote\n\n\n\nYou might want to try these values too:\n\\(c = 1.2e^{1.1πi}\\) \\(~\\Rightarrow~\\) original textbook example\n\\(c = -0.4-0.59i\\) and 1.5X zoom-out \\(~\\Rightarrow~\\) denser spirals\n\\(c = 1.34-0.45i\\) and 1.8X zoom-out \\(~\\Rightarrow~\\) beans\n\\(c = 0.34-0.05i\\) and 1.2X zoom-out \\(~\\Rightarrow~\\) connected spiral boots\n\n\nBelow is the serial code juliaSetSerial.chpl:\nuse Time;\n\nconfig const n = 2_000;   // 2000^2 image\nvar watch: stopwatch;\nconfig const save = false;\n\nproc pixel(z0) {\n  const c = 0.355 + 0.355i;\n  var z = z0*1.2;   // zoom out\n  for i in 1..255 {\n    z = z*z + c;\n    if z.re**2+z.im**2 &gt;= 16 then // abs(z)&gt;=4 does not work with LLVM\n      return i;\n  }\n  return 255;\n}\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nwatch.start();\nvar stability: [1..n,1..n] int;\nfor i in 1..n {\n  var y = 2*(i-0.5)/n - 1;\n  for j in 1..n {\n    var point = 2*(j-0.5)/n - 1 + y*1i;\n    stability[i,j] = pixel(point);\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nchpl  --fast juliaSetSerial.chpl\n./juliaSetSerial\nIt took me 2.34679 seconds to compute a \\(2000^2\\) fractal.\n\nPorting the Julia set problem to a GPU\nLet’s port this problem to a GPU! Copy juliaSetSerial.chpl to juliaSetGPU.chpl and make the following changes.\nStep 1 (optional, will work only on a physical GPU):\n&gt; if here.gpus.size == 0 {\n&gt;   writeln(\"need a GPU ...\");\n&gt;   exit(1);\n&gt; }\nAs of this writing, Chapel 2.2 does not support complex arithmetic on a GPU. Independently of the precision, you will get errors at compilation (if marked with @gpu.assertEligible):\nerror: Loop is marked with @gpu.assertEligible but is not eligible for execution on a GPU\n... function calls out to extern function (_chpl_complex128), which is not marked as GPU eligible\n... function calls out to extern function (_chpl_complex64), which is not marked as GPU eligible\nIf not marked with @gpu.assertEligible, the code compiles with complex arithmetic on a GPU, but it seems to take forever to finish.\nFortunately, we can implement complex arithmetic manually:\nStep 2:\n&lt; proc pixel(z0) {\n---\n&gt; proc pixel(x0,y0) {\nStep 3:\n&lt;   var z = z0*1.2;   // zoom out\n---\n&gt;   var x = x0*1.2;   // zoom out\n&gt;   var y = y0*1.2;   // zoom out\nStep 4:\n&lt;     z = z*z + c;\n---\n&gt;     var xnew = x**2 - y**2 + c.re;\n&gt;     var ynew = 2*x*y + c.im;\n&gt;     x = xnew;\n&gt;     y = ynew;\nStep 5:\n&lt;     if z.re**2+z.im**2 &gt;= 16 then // abs(z)&gt;=4 does not work with LLVM\n---\n&gt;     if x**2+y**2 &gt;= 16 then\nStep 6:\n&lt; var stability: [1..n,1..n] int;\n---\n&gt; on here.gpus[0] {\n&gt;   var stability: [1..n,1..n] int;\n...\n&gt; }\nStep 7:\n&lt; for i in 1..n {\n---\n&gt;   @gpu.assertEligible foreach i in 1..n {\nStep 8:\n&lt;     var point = 2*(j-0.5)/n - 1 + y*1i;\n&lt;     stability[i,j] = pixel(point);\n---\n&gt;       var x = 2*(j-0.5)/n - 1;\n&gt;       stability[i,j] = pixel(x,y);\nHere is the full GPU version of the code juliaSetGPU.chpl:\nuse Time;\n\nconfig const n = 2_000;   // 2000^2 image\nvar watch: stopwatch;\nconfig const save = false;\n\nif here.gpus.size == 0 {\n  writeln(\"need a GPU ...\");\n  exit(1);\n}\n\nproc pixel(x0,y0) {\n  const c = 0.355 + 0.355i;\n  var x = x0*1.2; // zoom out\n  var y = y0*1.2; // zoom out\n  for i in 1..255 {\n    var xnew = x**2 - y**2 + c.re;\n    var ynew = 2*x*y + c.im;\n    x = xnew;\n    y = ynew;\n    if x**2+y**2 &gt;= 16 then\n      return i;\n  }\n  return 255;\n}\n\nwriteln(\"Computing \", n, \"x\", n, \" Julia set ...\");\nwatch.start();\non here.gpus[0] {\n  var stability: [1..n,1..n] int;\n  @gpu.assertEligible foreach i in 1..n {\n    var y = 2*(i-0.5)/n - 1;\n    for j in 1..n {\n      var x = 2*(j-0.5)/n - 1;\n      stability[i,j] = pixel(x,y);\n    }\n  }\n}\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\nIt took 0.017364 seconds on the GPU.\n\n\n\nProblem size\nCPU time in sec\nGPU time in sec\nspeedup factor\n\n\n\n\n\\(2000\\times 2000\\)\n1.64477\n0.017372\n95\n\n\n\\(4000\\times 4000\\)\n6.5732\n0.035302\n186\n\n\n\\(8000\\times 8000\\)\n26.1678\n0.067307\n389\n\n\n\\(16000\\times 16000\\)\n104.212\n0.131301\n794\n\n\n\n\n\nAdding plotting to run on a GPU\nChapel’s Image library lets you write arrays of pixels to a PNG file. The following code – when added to the non-GPU code juliaSetSerial.chpl – writes the array stability to a file 2000.png assuming you\n\ndownload the colour map in CSV and\nadd the file sciplot.chpl (pasted below)\n\nwriteln(\"Plotting ...\");\nuse Image, Math, sciplot;\nwatch.clear();\nwatch.start();\nconst smin = min reduce(stability);\nconst smax = max reduce(stability);\nvar colour: [1..n, 1..n] 3*int;\nvar cmap = readColourmap('nipy_spectral.csv');   // cmap.domain is {1..256, 1..3}\nfor i in 1..n {\n  for j in 1..n {\n    var idx = ((stability[i,j]:real-smin)/(smax-smin)*255):int + 1; //scale to 1..256\n    colour[i,j] = ((cmap[idx,1]*255):int, (cmap[idx,2]*255):int, (cmap[idx,3]*255):int);\n  }\n}\nvar pixels = colorToPixel(colour);               // array of pixels\nwriteImage(n:string+\".png\", imageType.png, pixels);\nwatch.stop();\nwriteln('It took ', watch.elapsed(), ' seconds');\n// save this as sciplot.chpl\nuse IO;\nuse List;\n\nproc readColourmap(filename: string) {\n  var reader = open(filename, ioMode.r).reader();\n  var line: string;\n  if (!reader.readLine(line)) then   // skip the header\n    halt(\"ERROR: file appears to be empty\");\n  var dataRows : list(string); // a list of lines from the file\n  while (reader.readLine(line)) do   // read all lines into the list\n    dataRows.pushBack(line);\n  var cmap: [1..dataRows.size, 1..3] real;\n  for (i, row) in zip(1..dataRows.size, dataRows) {\n    var c1 = row.find(','):int;    // position of the 1st comma in the line\n    var c2 = row.rfind(','):int;   // position of the 2nd comma in the line\n    cmap[i,1] = row[0..c1-1]:real;\n    cmap[i,2] = row[c1+1..c2-1]:real;\n    cmap[i,3] = row[c2+1..]:real;\n  }\n  reader.close();\n  return cmap;\n}\nHere are the typical timings on a CPU:\nComputing 2000x2000 Julia set ...\nIt took 0.382409 seconds\nPlotting ...\nIt took 0.192508 seconds\nThis plotting snippet won’t work with the GPU version, as in that version the array stability is defined on the GPU only. You can move the definition of stability to the host, and then the code with plotting on the CPU will work. However, plotting a large heatmap can really benefit from GPU acceleration.\n\n\n\n\n\n\nQuestion Port plotting to a GPU\n\n\n\n\n\nThis is a take-home exercise: try porting this plotting block to a GPU. On my side, I am running into an issue assigning to a tuple element on a GPU with\ncolour[i,j] = ((cmap[idx,1]*255):int, (cmap[idx,2]*255):int, (cmap[idx,3]*255):int);\ngetting a runtime “Error calling CUDA function: an illegal memory access was encountered (Code: 700)”. Image library is still marked unstable, so perhaps this is a bug. No issues running my code on ‘CPU-as-device’.\nIf you succeed in running this block on a real GPU, please send me your solution."
  },
  {
    "objectID": "chapel-gpu.html#reduction-operations",
    "href": "chapel-gpu.html#reduction-operations",
    "title": "GPU computing with Chapel",
    "section": "Reduction operations",
    "text": "Reduction operations\n\nBoth the prime factorization problem and the Julia set problem compute elements of a large array in parallel on a GPU, but they don’t do any reduction (combining multiple numbers into one). It turns out, you can do reduction operations on a GPU with the usual reduce intent in a parallel loop:\nconfig const n = 1e8: int;\nvar total = 0.0;\non here.gpus[0] {\n  forall i in 1..n with (+ reduce total) do\n    total += 1.0 / i**2;\n  writef(\"total = %{###.###############}\\n\", total);\n}\nAlternatively, you can use built-in reduction operations on an array (that must reside in GPU-accessible memory), e.g.\nuse GPU;\nconfig const n = 1e8: int;\non here.gpus[0] {\n  var a: [1..n] real;\n  forall i in 1..n do\n    a[i] = 1.0 / i**2;\n  writef(\"total = %{###.###############}\\n\", gpuSumReduce(a));\n}\nOther supported array reduction operations on GPUs are gpuMinReduce(), gpuMaxReduce(), gpuMinLocReduce() and gpuMaxLocReduce()."
  },
  {
    "objectID": "chapel-gpu.html#links",
    "href": "chapel-gpu.html#links",
    "title": "GPU computing with Chapel",
    "section": "Links",
    "text": "Links\n\nIntroduction to GPU Programming in Chapel\nChapel’s High-Level Support for CPU-GPU Data Transfers and Multi-GPU Programming\nGPU module functions and attributes"
  }
]